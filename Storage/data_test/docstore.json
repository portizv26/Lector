{"docstore/metadata": {"2e7028c2-5c20-430d-8e65-00af17767315": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}, "5d987162-5c27-47e4-b838-9f15aca03900": {"doc_hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "c54f92c1-e152-4fe1-9471-833d434bbabe": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}, "d617510b-22b8-4357-85a9-c70722c8ca69": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}, "db90e3a7-24ce-4a17-999f-759de8e8f5fc": {"doc_hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "c3fd3d46-79f2-4a4f-8d03-bbbd915cbb20": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}, "af863db7-a11d-4fbc-9a58-1335db1f46e9": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}, "70d875c7-ea8b-4309-93d9-f85a86112dd9": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}, "be06ce58-88b0-4334-b8d7-0afe5d022120": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}, "a15e0596-6968-4f4f-b1b9-8e136d2d4b7c": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}, "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac": {"doc_hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "08ba269e-837f-49f1-ad4b-ba868fab1bb5": {"doc_hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "b7e132c3-5106-4ff8-a7bd-4298ea265edc": {"doc_hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "63b54ee6-0451-4dc1-b208-b072390222be": {"doc_hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "2aad3f0c-29d6-48ec-965b-92fe5fa613be": {"doc_hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "c1e37f46-8bc9-459f-b4fa-260c3d46ae96": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}, "e728b3b4-dcaf-4b87-834b-27ba5c794b18": {"doc_hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "679f7644-80ce-48c5-b7ab-0bf96a7fdb82": {"doc_hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "d27aa04e-2383-4475-b5b1-5f317558d9e9": {"doc_hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "2f2be224-ddf2-48c4-890a-6911df0d2001": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}, "31446f05-8076-4744-ac19-6f31e41d4c17": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}, "efbe87bc-394b-4f97-bed3-7e3eaf5c6c92": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}, "3143df93-4ac1-412a-91f7-7b593a1f4ad2": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}, "8dd39538-0ef5-4ba6-8c40-1a3d7c53acfe": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}, "94efb204-4d56-4835-bdb4-976cb853cf33": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}, "8280cf22-1d25-4f46-9402-81cfd5ea7cf1": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}, "834a8280-7fd9-49de-812e-bde76e778847": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "ref_doc_id": "2e7028c2-5c20-430d-8e65-00af17767315"}, "1470f118-b9fb-4dc3-9cfc-f788dd90cbad": {"doc_hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "ref_doc_id": "5d987162-5c27-47e4-b838-9f15aca03900"}, "e2747987-58e1-4d35-aed9-407e68aae337": {"doc_hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58", "ref_doc_id": "5d987162-5c27-47e4-b838-9f15aca03900"}, "3c8d1750-344c-4bf0-9b20-b9be3c9469f0": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "ref_doc_id": "c54f92c1-e152-4fe1-9471-833d434bbabe"}, "7bbc47b3-73b6-48ea-837b-f59b73db4694": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "ref_doc_id": "d617510b-22b8-4357-85a9-c70722c8ca69"}, "a862cbed-d252-45b0-8e1d-8db050ac54e9": {"doc_hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "ref_doc_id": "db90e3a7-24ce-4a17-999f-759de8e8f5fc"}, "84791bc2-04ae-4892-9ae3-eb6f6639838b": {"doc_hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb", "ref_doc_id": "db90e3a7-24ce-4a17-999f-759de8e8f5fc"}, "9ff9f167-293d-45d4-84bd-975811415565": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "ref_doc_id": "c3fd3d46-79f2-4a4f-8d03-bbbd915cbb20"}, "9e8cdd9d-df11-4443-8a94-42718a3c741a": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "ref_doc_id": "af863db7-a11d-4fbc-9a58-1335db1f46e9"}, "7e3c9eca-552c-49e4-a3c0-d4738f8bb76c": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "ref_doc_id": "70d875c7-ea8b-4309-93d9-f85a86112dd9"}, "30fc19e0-72b1-4f3d-b1cd-70a4cf0896fd": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "ref_doc_id": "be06ce58-88b0-4334-b8d7-0afe5d022120"}, "0512f07f-0b51-4dbc-937b-9db19a68a0a3": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "ref_doc_id": "a15e0596-6968-4f4f-b1b9-8e136d2d4b7c"}, "3643e4ba-8ba5-457c-bf87-3d96110eef0f": {"doc_hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "ref_doc_id": "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac"}, "7a895568-21c9-45a3-8fbc-0383ae583394": {"doc_hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de", "ref_doc_id": "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac"}, "105db419-c479-40c5-8d6d-a9485b1154d1": {"doc_hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "ref_doc_id": "08ba269e-837f-49f1-ad4b-ba868fab1bb5"}, "36eb9d2b-8a2e-4cf0-b3f5-e09a2af5665b": {"doc_hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce", "ref_doc_id": "08ba269e-837f-49f1-ad4b-ba868fab1bb5"}, "d2ec32e0-45fe-491a-9f6e-71ea1944fc41": {"doc_hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "ref_doc_id": "b7e132c3-5106-4ff8-a7bd-4298ea265edc"}, "cfebf655-8779-4869-b303-23adf65127c3": {"doc_hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2", "ref_doc_id": "b7e132c3-5106-4ff8-a7bd-4298ea265edc"}, "5517cdc3-dbf0-4218-a664-f03f4af89bcd": {"doc_hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "ref_doc_id": "63b54ee6-0451-4dc1-b208-b072390222be"}, "dbc8eade-e391-48ab-a10e-794380484b0d": {"doc_hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1", "ref_doc_id": "63b54ee6-0451-4dc1-b208-b072390222be"}, "57b5953e-180f-421e-8ccd-f15ee1f1774b": {"doc_hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "ref_doc_id": "2aad3f0c-29d6-48ec-965b-92fe5fa613be"}, "15cd62be-5c48-47a9-9102-b19106a910fc": {"doc_hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be", "ref_doc_id": "2aad3f0c-29d6-48ec-965b-92fe5fa613be"}, "b2af32c3-8ee0-49b7-a7a6-b20c56071acd": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "ref_doc_id": "c1e37f46-8bc9-459f-b4fa-260c3d46ae96"}, "fefe083b-81f2-444e-a1d1-0d4d8cf12250": {"doc_hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "ref_doc_id": "e728b3b4-dcaf-4b87-834b-27ba5c794b18"}, "c3c9ccd2-b2c5-4bd3-808e-c500b733abeb": {"doc_hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039", "ref_doc_id": "e728b3b4-dcaf-4b87-834b-27ba5c794b18"}, "d8228d29-8ee9-4d31-8fd4-c6886919812a": {"doc_hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "ref_doc_id": "679f7644-80ce-48c5-b7ab-0bf96a7fdb82"}, "6145c93f-851e-4099-9416-4f538f8a45b6": {"doc_hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5", "ref_doc_id": "679f7644-80ce-48c5-b7ab-0bf96a7fdb82"}, "ab841f20-89ab-46b1-a91a-20688dc8aea5": {"doc_hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "ref_doc_id": "d27aa04e-2383-4475-b5b1-5f317558d9e9"}, "5a7e856c-4b6f-4b73-9045-a15467ac6b15": {"doc_hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b", "ref_doc_id": "d27aa04e-2383-4475-b5b1-5f317558d9e9"}, "ec277da6-33b4-4b23-a506-a7a1013d94de": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "ref_doc_id": "2f2be224-ddf2-48c4-890a-6911df0d2001"}, "d7e9dcaf-5587-4974-b8d0-85ab6f8372ff": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "ref_doc_id": "31446f05-8076-4744-ac19-6f31e41d4c17"}, "e3151964-d154-42fe-ae2b-b338bb0f2567": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "ref_doc_id": "efbe87bc-394b-4f97-bed3-7e3eaf5c6c92"}, "281cb4a9-aae5-4466-bb9b-7222a5d55513": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "ref_doc_id": "3143df93-4ac1-412a-91f7-7b593a1f4ad2"}, "3c1f30ce-073e-435a-b1a1-c39b2a511bb6": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "ref_doc_id": "8dd39538-0ef5-4ba6-8c40-1a3d7c53acfe"}, "777fde82-0494-41ce-b3fb-6d51ad6b5321": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "ref_doc_id": "94efb204-4d56-4835-bdb4-976cb853cf33"}, "13a39d60-a203-4461-9bf1-44e24fa3c188": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "ref_doc_id": "8280cf22-1d25-4f46-9402-81cfd5ea7cf1"}}, "docstore/data": {"834a8280-7fd9-49de-812e-bde76e778847": {"__data__": {"id_": "834a8280-7fd9-49de-812e-bde76e778847", "embedding": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e7028c2-5c20-430d-8e65-00af17767315", "node_type": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "text": "Journalof Machine LearningResearch 3(2003)1157-1182 Submitted 11/02;Published3/03\nAnIntroduction to Variableand Feature Selection\nIsabelleGuyon ISABELLE @CLOPINET .COM\nClopinet\n955 Creston Road\nBerkeley, CA 94708-1501, USA\nAndr\u00b4eElisseeff ANDRE@TUEBINGEN .MPG.DE\nEmpirical Inference for Machine Learning and Perception De partment\nMax Planck InstituteforBiological Cybernetics\nSpemannstrasse 38\n72076 T\u00a8ubingen, Germany\nEditor:Leslie Pack Kaelbling\nAbstract\nVariable and feature selection have become the focus of much research in areas of application for\nwhich datasets with tens or hundreds of thousands of variabl es are available. These areas include\ntextprocessingofinternetdocuments,geneexpressionarr ayanalysis,andcombinatorialchemistry.\nThe objective of variable selection is three-fold: improvi ng the prediction performance of the pre-\ndictors,providingfasterandmorecost-effectivepredict ors,andprovidingabetterunderstandingof\nthe underlying process that generated the data. The contrib utions of this special issue cover a wide\nrange of aspects of such problems: providing a better de\ufb01nit ion of the objective function, feature\nconstruction, feature ranking, multivariate feature sele ction, ef\ufb01cient search methods, and feature\nvalidity assessment methods.\nKeywords: Variable selection, feature selection, space dimensional ity reduction, pattern discov-\nery, \ufb01lters, wrappers, clustering, information theory, su pport vector machines, model selection,\nstatistical testing, bioinformatics, computational biol ogy, gene expression, microarray, genomics,\nproteomics, QSAR, text classi\ufb01cation, information retrie val.\n1 Introduction\nAs of 1997, when a special issue on relevance including several pape rs on variable and feature\nselection was published (Blum and Langley, 1997, Kohavi and John, 19 97), few domains explored\nused more than 40 features. The situation has changed considerably in the past few years and, in\nthis special issue, most papers explore domains with hundreds to tens of tho usands of variables or\nfeatures:1Newtechniquesareproposedtoaddressthesechallengingtasksinvolvin gmanyirrelevant\nandredundant variablesandoftencomparably fewtrainingexamples.\nTwoexamplesaretypicalofthenewapplicationdomainsandserveusasillustr ationthroughout\nthis introduction. One is gene selection from microarray data and the other is te xt categorization.\nIn the gene selection problem, the variables are gene expression coef\ufb01c ients corresponding to the\n1. We call \u201cvariable\u201d the \u201craw\u201d input variables and \u201cfeatures\u201d variable s constructed for the input variables. We use\nwithoutdistinctiontheterms\u201cvariable\u201dand\u201cfeature\u201dwhenthereisnoimpac tontheselectionalgorithms,e.g.,when\nfeatures resulting from a pre-processing of input variables are explic itly computed. The distinction is necessary in\nthe case ofkernel methods forwhich features are notexplicitly compute d (seesection 5.3).\nc/circlecopyrt2003Isabelle Guyonand Andr \u00b4eElisseeff.", "start_char_idx": 0, "end_char_idx": 2946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1470f118-b9fb-4dc3-9cfc-f788dd90cbad": {"__data__": {"id_": "1470f118-b9fb-4dc3-9cfc-f788dd90cbad", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d987162-5c27-47e4-b838-9f15aca03900", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "3": {"node_id": "e2747987-58e1-4d35-aed9-407e68aae337", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58"}}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "text": "GUYON AND ELISSEEFF\nabundance of mRNA in a sample (e.g. tissue biopsy), for a number of patients . A typical clas-\nsi\ufb01cation task is to separate healthy patients from cancer patients, based on their gene expression\n\u201cpro\ufb01le\u201d. Usually fewer than 100 examples (patients) are available altogeth er for training and test-\ning. But, the number of variables in the raw data ranges from 6000 to 60,000 . Some initial \ufb01ltering\nusually brings the number of variables to a few thousand. Because the abu ndance of mRNA varies\nbyseveralordersofmagnitudedependingonthegene,thevariablesar eusuallystandardized. Inthe\ntext classi\ufb01cation problem, the documents are represented by a \u201cbag-of- words\u201d, that is a vector of\ndimensionthesizeofthevocabularycontainingwordfrequencycounts(p ropernormalizationofthe\nvariables also apply). Vocabularies of hundreds of thousands of wor ds are common, but an initial\npruning of the most and least frequent words may reduce the effective number of words to 15,000.\nLargedocumentcollections of5000to800,000documents areavailablefor research. Typicaltasks\ninclude the automatic sorting of URLs into a web directory and the detection of un solicited email\n(spam). Foralistofpubliclyavailabledatasetsusedinthisissue,seeTable1 attheendofthepaper.\nTherearemanypotentialbene\ufb01tsofvariableandfeatureselection: facilita tingdatavisualization\nanddataunderstanding,reducingthemeasurementandstoragerequire ments,reducingtrainingand\nutilization times, defying the curse of dimensionality to improve prediction performa nce. Some\nmethods put more emphasis on one aspect than another, and this is another p oint of distinction\nbetweenthisspecialissueandpreviouswork. Thepapersinthisissuefo cusmainlyonconstructing\nand selecting subsets of features that areusefulto build a good predictor. This contrasts with the\nproblemof\ufb01ndingorrankingallpotentiallyrelevantvariables. Selectingth emostrelevantvariables\nisusuallysuboptimalforbuildingapredictor,particularlyifthevariablesare redundant. Conversely,\na subset of useful variables may exclude many redundant, but relevan t, variables. For a discussion\nofrelevance vs.usefulnessandde\ufb01nitionsofthevariousnotionsofrelevance,seethere viewarticles\nof KohaviandJohn(1997)andBlum andLangley(1997).\nThis introduction surveys the papers presented in this special issue. The depth of treatment of\nvarioussubjectsre\ufb02ectstheproportionofpaperscoveringthem: thepro blemofsupervisedlearning\nis treated more extensively than that of unsupervised learning; classi\ufb01ca tion problems serve more\noftenasillustrationthanregressionproblems,andonlyvectorialinputdata isconsidered. Complex-\nity is progressively introduced throughout the sections: The \ufb01rst sectio n starts by describing \ufb01lters\nthat select variables by ranking them with correlation coef\ufb01cients (Section 2). Limitations of such\napproaches are illustrated by a set of constructed examples (Section 3). Subset selection methods\narethenintroduced(Section4). Theseinclude wrappermethods thatassesssubsetsofvariablesac-\ncording to their usefulness to a given predictor. We show how some embedd ed methods implement\nthesameidea,butproceedmoreef\ufb01cientlybydirectlyoptimizingatwo-partob jectivefunctionwith\nagoodness-of-\ufb01ttermandapenaltyforalargenumberofvariables. W ethenturntotheproblemof\nfeature construction, whose goals include increasing the predictor perf ormance and building more\ncompact feature subsets (Section 5). All of the previous steps bene\ufb01t f rom reliably assessing", "start_char_idx": 0, "end_char_idx": 3465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e2747987-58e1-4d35-aed9-407e68aae337": {"__data__": {"id_": "e2747987-58e1-4d35-aed9-407e68aae337", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5d987162-5c27-47e4-b838-9f15aca03900", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "2": {"node_id": "1470f118-b9fb-4dc3-9cfc-f788dd90cbad", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425"}}, "hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58", "text": "5). All of the previous steps bene\ufb01t f rom reliably assessing the\nstatistical signi\ufb01cance of the relevance of features. We brie\ufb02y review mode l selection methods and\nstatisticaltestsusedtothateffect(Section6). Finally,weconcludethepap erwithadiscussionsec-\ntion in which we go over more advanced issues (Section 7). Because the or ganization of our paper\ndoes not follow the work \ufb02ow of building a machine learning application, we summa rize the steps\nthatmaybe takentosolveafeatureselectionprobleminacheck list2:\n2. We caution the reader that this check list is heuristic. The only recommen dation that is almost surely valid is to try\nthe simplestthings \ufb01rst.\n1158", "start_char_idx": 3404, "end_char_idx": 4070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3c8d1750-344c-4bf0-9b20-b9be3c9469f0": {"__data__": {"id_": "3c8d1750-344c-4bf0-9b20-b9be3c9469f0", "embedding": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c54f92c1-e152-4fe1-9471-833d434bbabe", "node_type": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n1.Do youhavedomain knowledge? Ifyes,constructabetter setof \u201cad hoc\u201dfeatures.\n2.Are yourfeaturescommensurate? If no,consider normalizingthem.\n3.Doyoususpectinterdependenceoffeatures? Ifyes,expandyourfeaturesetbyconstructing\nconjunctive features or products of features, as much as your compute r resources allow you\n(seeexampleof useinSection4.4).\n4.Do you need to prune the input variables (e.g. for cost, speed or data understanding rea-\nsons)? If no, construct disjunctive features or weighted sums of featu res (e.g. by clustering\nor matrixfactorization,seeSection5).\n5.Doyouneedtoassessfeaturesindividually (e.g. tounderstandtheirin\ufb02uenceonthesystem\nor because their number is so large that you need to do a \ufb01rst \ufb01ltering)? If y es, use a variable\nrankingmethod (Section2andSection7.2);else,doitanyway togetbaselin eresults.\n6.Do youneed apredictor? Ifno,stop.\n7.Do you suspect your data is \u201cdirty\u201d (has a few meaningless input patterns and/or noisy\noutputs or wrong class labels)? If yes, detect the outlier examples using the top ranking\nvariables obtainedinstep5as representation;check and/or discardthem.\n8.Doyouknowwhattotry\ufb01rst? Ifno,usealinearpredictor.3Useaforwardselectionmethod\n(Section 4.2) with the \u201cprobe\u201d method as a stopping criterion (Section 6) or us e the/lscript0-norm\nembedded method (Section 4.3). For comparison, following the ranking of ste p 5, construct\na sequence of predictors of same nature using increasing subsets of fe atures. Can you match\nor improve performance with a smaller subset? If yes, try a non-linear pred ictor with that\nsubset.\n9.Do you have new ideas, time, computational resources, and enoug h examples? If yes,\ncompare several feature selection methods, including your new idea, cor relation coef\ufb01cients,\nbackwardselectionandembeddedmethods(Section4). Uselinearandnon -linearpredictors.\nSelect thebestapproachwithmodel selection(Section6).\n10.Do you want a stable solution (to improve performance and/or understanding)? If yes, sub-\nsampleyour dataand redoyour analysisforseveral\u201cbootstraps\u201d(Se ction7.1).\n2 Variable Ranking\nMany variable selection algorithms include variable ranking as a principal or auxiliary selection\nmechanism because of its simplicity, scalability, and good empirical success. S everal papers in this\nissue use variable ranking as a baseline method (see, e.g., Bekkerman et a l., 2003, Caruana and\nde Sa, 2003, Forman, 2003, Weston et al., 2003). Variable ranking is no t necessarily used to build\npredictors. One of its common uses in the microarray analysis domain is to discov er a set of drug\nleads (see, e.g., et al., 1999): A ranking criterion is used to \ufb01nd genes tha t discriminate between\nhealthy and disease patients; such genes may code for \u201cdrugable\u201d prote ins, or proteins that may\n3. By \u201clinear predictor\u201d we mean linear in the parameters. Feature constr uction may render the predictor non-linear in\nthe inputvariables.\n1159", "start_char_idx": 0, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bbc47b3-73b6-48ea-837b-f59b73db4694": {"__data__": {"id_": "7bbc47b3-73b6-48ea-837b-f59b73db4694", "embedding": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d617510b-22b8-4357-85a9-c70722c8ca69", "node_type": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "text": "GUYON AND ELISSEEFF\nthemselves be used as drugs. Validating drug leads is a labor intensive pro blem in biology that is\noutside of the scope of machine learning, so we focus here on building pre dictors. We consider in\nthis section ranking criteria de\ufb01ned for individual variables, independe ntly of the context of others.\nCorrelationmethodsbelongtothatcategory. Wealsolimitourselvestosuperv isedlearningcriteria.\nWerefer thereader toSection7.2for adiscussionof othertechniques.\n2.1 Principleofthe Method andNotations\nConsider a set of mexamples {xk,yk}(k=1,...m) consisting of ninput variables xk,i(i=1,...n)\nand one output variable yk. Variable ranking makes use of a scoring function S(i)computed from\nthe values xk,iandyk,k=1,...m. By convention, we assume that a high score is indicative of a\nvaluable variable and that we sort variables in decreasing order of S(i). To use variable ranking to\nbuild predictors, nested subsets incorporating progressively more and more variables of decreasing\nrelevance are de\ufb01ned. We postpone until Section 6 the discussion of sele cting an optimum subset\nsize.\nFollowingthe classi\ufb01cationof Kohaviand John(1997),variableranking isa\ufb01ltermethod: itis\napreprocessingstep,independentofthechoiceofthepredictor. Still,u ndercertainindependenceor\northogonality assumptions, it may be optimal with respect to a given predictor. For instance, using\nFisher\u2019scriterion4torankvariablesinaclassi\ufb01cationproblemwherethecovariancematrixisdia g-\nonal is optimum for Fisher\u2019s linear discriminant classi\ufb01er (Duda et al., 2001 ). Even when variable\nranking is not optimal, it may be preferable to other variable subset selection methods because of\nitscomputationalandstatisticalscalability: Computationally,itisef\ufb01cientsinceitr equiresonlythe\ncomputation of nscores and sorting the scores; Statistically, it is robust against over\ufb01tting because\nitintroduces biasbutitmayhave considerablyless variance(Hastieetal., 2001).5\nWeintroducesomeadditionalnotation: Iftheinputvector xcanbeinterpretedastherealization\nof a random vector drawn from an underlying unknown distribution, we d enote by Xithe random\nvariablecorrespondingtothe ithcomponentof x. Similarly, Ywillbetherandomvariableofwhich\nthe outcome yis a realization. We further denote by xithemdimensional vector containing all\nthe realizations of the ithvariable for the training examples, and by ythemdimensional vector\ncontaining allthetargetvalues.\n2.2 Correlation Criteria\nLet us consider \ufb01rst the prediction of a continuous outcome y. The Pearson correlation coef\ufb01cient\nisde\ufb01ned as:\nR(i) =cov(Xi,Y)/radicalbig\nvar(Xi)var(Y), (1)\nwherecovdesignatesthecovariance and varthevariance. Theestimateof R(i)is givenby:\nR(i) =\u2211m\nk=1(xk,i\u2212\u00afxi)(yk\u2212\u00afy)/radicalbig\n\u2211m\nk=1(xk,i\u2212\u00afxi)2\u2211m\nk=1(yk\u2212\u00afy)2, (2)\n4. The ratioof thebetween class variance tothe within-class variance.\n5. The similarity of variable ranking to the ORDERED-FS algorithm (Ng, 1998 ) indicates that its sample complexity\nmay be logarithmic in the number of irrelevant features, compared to a po wer law for \u201cwrapper\u201d subset selection\nmethods. This would mean that variable ranking can tolerate a number of ir relevant variables exponential in the\nnumber of trainingexamples.\n1160", "start_char_idx": 0, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a862cbed-d252-45b0-8e1d-8db050ac54e9": {"__data__": {"id_": "a862cbed-d252-45b0-8e1d-8db050ac54e9", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db90e3a7-24ce-4a17-999f-759de8e8f5fc", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "3": {"node_id": "84791bc2-04ae-4892-9ae3-eb6f6639838b", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb"}}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nwhere the bar notation stands for an average over the index k. This coef\ufb01cient is also the cosine\nbetween vectors xiandy, after they have been centered (their mean subtracted). Although the R(i)\nis derived from R(i)it may be used without assuming that the input values are realizations of a\nrandomvariable.\nIn linear regression, the coef\ufb01cient of determination, which is the square ofR(i), represents the\nfractionofthetotalvariancearoundthemeanvalue \u00af ythatisexplainedbythelinearrelationbetween\nxiandy. Therefore, using R(i)2as a variable ranking criterion enforces a ranking according to\ngoodness oflinear \ufb01tof individualvariables.6\nThe use of R(i)2can be extended to the case of two-class classi\ufb01cation, for which each cla ss\nlabel is mapped to a given value of y, e.g., \u00b11.R(i)2can then be shown to be closely related to\nFisher\u2019s criterion (Furey et al., 2000), to the T-test criterion, and other similar criteria (see, e.g.,\net al., 1999, Tusher et al., 2001, Hastie et al., 2001). As further develo ped in Section 6, the link\nto the T-test shows that the score R(i)may be used as a test statistic to assess the signi\ufb01cance of a\nvariable.\nCorrelation criteria such as R(i)can only detect linear dependencies between variable and tar-\nget. A simple way of lifting this restriction is to make a non-linear \ufb01t of the target with single\nvariablesandrankaccordingtothegoodnessof\ufb01t. Becauseoftherisk ofover\ufb01tting,onecanalter-\nnatively consider using non-linear preprocessing (e.g., squaring, tak ing the square root, the log, the\ninverse, etc.) and then using a simple correlation coef\ufb01cient. Correlation cr iteria are often used for\nmicroarraydataanalysis,as illustratedin thisissuebyWestonet al.(2003).\n2.3 SingleVariable Classi\ufb01ers\nAsalreadymentioned,using R(i)2asarankingcriterionfor regression enforcesarankingaccording\ntogoodnessoflinear\ufb01tofindividualvariables. Onecanextendtothe classi\ufb01cation casetheideaof\nselectingvariablesaccordingtotheirindividualpredictivepower,using ascriteriontheperformance\nofaclassi\ufb01erbuiltwithasinglevariable. Forexample,thevalueofthevariab leitself(oritsnegative,\ntoaccountforclasspolarity)canbeusedasdiscriminantfunction. Aclas si\ufb01erisobtainedbysetting\nathreshold \u03b8onthevalueofthevariable(e.g.,atthemid-pointbetweenthecenterofgrav ityofthe\ntwoclasses).\nThe predictive power of the variable can be measured in terms of error rate . But, various other\ncriteria can be de\ufb01ned that involve false positive classi\ufb01cation rate fprand false negative classi\ufb01-\ncation rate fnr. The tradeoff between fprandfnris monitored in our simple example by varying\nthe threshold \u03b8. ROC curves that plot \u201chit\u201d rate (1-fpr)as a function of \u201cfalse alarm\u201d rate fnrare\ninstrumental in de\ufb01ning criteria such as: The \u201cBreak Even Point\u201d (the hit ra te for a threshold value\ncorrespondingto fpr=fnr)andthe \u201cAreaUnder Curve\u201d(thearea under theROCcurve).\nIn the case where there is a large number of variables that separate the da ta perfectly, ranking\ncriteria based on classi\ufb01cation success rate cannot distinguish between th e top ranking variables.\nOnewillthenprefertouseacorrelationcoef\ufb01cientoranotherstatisticliketh emargin(thedistance\nbetween", "start_char_idx": 0, "end_char_idx": 3198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84791bc2-04ae-4892-9ae3-eb6f6639838b": {"__data__": {"id_": "84791bc2-04ae-4892-9ae3-eb6f6639838b", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db90e3a7-24ce-4a17-999f-759de8e8f5fc", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "2": {"node_id": "a862cbed-d252-45b0-8e1d-8db050ac54e9", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086"}}, "hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb", "text": "emargin(thedistance\nbetween theexamples of oppositeclassesthatareclosesttooneanother f oragivenvariable).\n6. Avariantofthisideaistousethemean-squared-error,but,ifthevar iablesarenotoncomparablescales,acomparison\nbetween mean-squared-errors is meaningless. Another variant is to u seR(i)to rank variables, not R(i)2. Positively\ncorrelated variables are then top ranked and negatively correlated var iables bottom ranked. With this method, one\ncan choose asubsetof variables withagiven proportion of positivelya nd negatively correlated variables.\n1161", "start_char_idx": 3171, "end_char_idx": 3721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9ff9f167-293d-45d4-84bd-975811415565": {"__data__": {"id_": "9ff9f167-293d-45d4-84bd-975811415565", "embedding": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c3fd3d46-79f2-4a4f-8d03-bbbd915cbb20", "node_type": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "text": "GUYON AND ELISSEEFF\nThe criteria described in this section extend to the case of binary variables. Forman (2003)\npresentsinthisissueanextensivestudyofsuchcriteriaforbinaryvaria bleswithapplicationsintext\nclassi\ufb01cation.\n2.4 InformationTheoretic RankingCriteria\nSeveral approaches to the variable selection problem using information the oretic criteria have been\nproposed (as reviewed in this issue by Bekkerman et al., 2003, Dhillon et a l., 2003, Forman, 2003,\nTorkkola,2003). Manyrelyonempiricalestimatesofthemutualinformationbe tweeneachvariable\nandthe target:\nI(i) =Z\nxiZ\nyp(xi,y)logp(xi,y)\np(xi)p(y)dxdy, (3)\nwherep(xi)andp(y)are the probability densities of xiandy, andp(xi,y)is the joint density. The\ncriterion I(i)is a measure of dependency between the density of variable xiand the density of the\ntargety.\nThe dif\ufb01culty is that the densities p(xi),p(y)andp(xi,y)are all unknown and are hard to\nestimatefromdata. Thecaseofdiscreteornominalvariablesisprobablyea siestbecausetheintegral\nbecomes asum:\nI(i) =\u2211\nxi\u2211\nyP(X=xi,Y=y)logP(X=xi,Y=y)\nP(X=xi)P(Y=y). (4)\nThe probabilities are then estimated from frequency counts. For example, in a three-class\nproblem, if a variable takes 4 values, P(Y=y)represents the class prior probabilities (3 fre-\nquency counts), P(X=xi)represents the distribution of the input variable (4 frequency counts),\nandP(X=xi,Y=y)istheprobabilityofthejointobservations(12frequencycounts). Thees tima-\ntionobviouslybecomes harder withlarger numbersof classesandvariable values.\nThe case of continuous variables (and possibly continuous targets) is the hardest. One can\nconsider discretizing the variables or approximating their densities with a non- parametric method\nsuch as Parzen windows (see, e.g., Torkkola, 2003). Using the normal distribution to estimate\ndensities would bring us back to estimating the covariance between XiandY, thus giving us a\ncriterionsimilartoacorrelationcoef\ufb01cient.\n3 Small but RevealingExamples\nWe present a series of small examples that outline the usefulness and the limitatio ns of variable\nranking techniques and present several situations in which the variable d ependencies cannot be\nignored.\n3.1 Can PresumablyRedundantVariables Help Each Other?\nOnecommoncriticismofvariablerankingisthatitleadstotheselectionofaredu ndantsubset. The\nsame performance could possibly be achieved with a smaller subset of comple mentary variables.\nStill, one may wonder whether adding presumably redundant variables can result in a performance\ngain.\n1162", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9e8cdd9d-df11-4443-8a94-42718a3c741a": {"__data__": {"id_": "9e8cdd9d-df11-4443-8a94-42718a3c741a", "embedding": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af863db7-a11d-4fbc-9a58-1335db1f46e9", "node_type": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22126\u22124\u221220246 \u22125 0 5\u22126\u22124\u221220246\u2212505\n(a) (b)\nFigure1: Informationgainfrompresumablyredundantvariables. (a)Atwoclassproblemwith\nindependently and identically distributed (i.i.d.) variables. Each class has a Ga ussian distribution\nwith no covariance. (b) The same example after a 45 degree rotation showin g that a combination\nof the two variables yields a separation improvement by a factor\u221a\n2. I.i.d. variables are not truly\nredundant.\nConsider the classi\ufb01cation problem of Figure 1. For each class, we drew at random m=100\nexamples,eachofthetwovariablesbeingdrawnindependentlyaccording toanormaldistributionof\nstandarddeviation1. Theclasscentersareplacedatcoordinates(-1; -1)and(1;1). Figure1.ashows\nthescatterplotinthetwo-dimensionalspaceoftheinputvariables. Wealsos howonthesame\ufb01gure\nhistogramsoftheprojectionsoftheexamplesontheaxes. Tofacilitateitsreadin g,thescatterplotis\nshowntwicewithanaxisexchange. Figure1.bshowsthesamescatterplotsaf teraforty\ufb01vedegree\nrotation. Inthisrepresentation,thex-axisprojectionprovidesabettersep arationofthetwoclasses:\nthe standard deviation of both classes is the same, but the distance between c enters in projection is\nnow 2\u221a\n2 instead of 2. Equivalently, if we rescale the x-axis by dividing by\u221a\n2 to obtain a feature\nthat is the average of the two input variables, the distance between centers is still 2, but the within\nclass standard deviation is reduced by a factor\u221a\n2. This is not so surprising, since by averaging n\ni.i.d. random variables we will obtain a reduction of standard deviation by a fa ctor of\u221an.Noise\nreduction and consequently better class separation may be obtain ed by adding variables that\nare presumably redundant. Variables that are independently and identically distributed are not\ntrulyredundant.\n3.2 HowDoes Correlation ImpactVariable Redundancy?\nAnother notion of redundancy is correlation. In the previous example, in s pite of the fact that the\nexamples are i.i.d. with respect to the class conditional distributions, the variab les are correlated\nbecause of the separation of the class center positions. One may wonder h ow variable redundancy\nis affected by adding within-class variable correlation. In Figure 2, the cla ss centers are positioned\n1163", "start_char_idx": 0, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e3c9eca-552c-49e4-a3c0-d4738f8bb76c": {"__data__": {"id_": "7e3c9eca-552c-49e4-a3c0-d4738f8bb76c", "embedding": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70d875c7-ea8b-4309-93d9-f85a86112dd9", "node_type": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n(a) (b)\nFigure2: Intra-classcovariance. Inprojectionontheaxes,thedistributionsofthetwovariablesare\nthe same as in the previous example. (a) The class conditional distributions hav e a high covariance\ninthedirectionofthelineofthetwoclasscenters. Thereisnosigni\ufb01cantgain inseparationbyusing\ntwo variables instead of just one. (b) The class conditional distributions ha ve a high covariance in\nthe direction perpendicular to the line of the two class centers. An important se paration gain is\nobtained byusingtwovariables insteadofone.\nsimilarly as in the previous example at coordinates (-1; -1) and (1; 1) but w e have added some\nvariableco-variance. Weconsider twocases:\nIn Figure 2.a, in the direction of the class center line, the standard deviation o f the class condi-\ntional distributions is\u221a\n2, while in the perpendicular direction it is a small value ( \u03b5=1/10). With\nthis construction, as \u03b5goes to zero, the input variables have the same separation power as in the\ncase of the example of Figure 1, with a standard deviation of the class distribu tions of one and a\ndistance of the class centers of 2. But the feature constructed as the sum of the input variables has\nnobetterseparationpower: astandarddeviationof\u221a\n2andaclasscenterseparationof2\u221a\n2(asim-\nple scaling that does not change the separation power). Therefore, in the limit of perfect variable\ncorrelation (zero variance in the direction perpendicular to the class cente r line), single variables\nprovide the same separation as the sum of the two variables. Perfectly correlated variables are\ntrulyredundant in thesensethatnoadditionalinformationis gained byaddingthem.\nIn contrast, in the example of Figure 2.b, the \ufb01rst principal direction of the c ovariance matrices\nof the class conditional densities is perpendicular to the class center line. In th is case, more is\ngainedbyaddingthetwovariablesthanintheexampleofFigure1. Onenotices thatinspiteoftheir\ngreatcomplementarity(inthesensethataperfectseparationcanbeachie vedinthetwo-dimensional\nspace spanned by the two variables), the two variables are (anti-)corre lated. More anti-correlation\nis obtained by making the class centers closer and increasing the ratio of the v ariances of the class\nconditional distributions. Very high variable correlation (or anti-correlation) does not mean\nabsenceof variablecomplementarity.\n1164", "start_char_idx": 0, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30fc19e0-72b1-4f3d-b1cd-70a4cf0896fd": {"__data__": {"id_": "30fc19e0-72b1-4f3d-b1cd-70a4cf0896fd", "embedding": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be06ce58-88b0-4334-b8d7-0afe5d022120", "node_type": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nThe examples of Figure 1 and 2 all have variables with the same distribution of e xamples (in\nprojection on the axis). Therefore, methods that score variables individ ually and independently of\neach other areatlosstodeterminewhichcombination of variableswouldgive bestperformance.\n3.3 Can aVariable thatis Useless byItself beUsefulwith Oth ers?\nOneconcernaboutmultivariatemethodsisthattheyarepronetoover\ufb01tting. Theproblemisaggra-\nvated when the number of variables to select from is large compared to the nu mber of examples.\nIt is tempting to use a variable ranking method to \ufb01lter out the least promising varia bles before us-\ning a multivariate method. Still one may wonder whether one could potentially lose s ome valuable\nvariablesthroughthat \ufb01lteringprocess.\nWe constructed an example in Figure 3.a. In this example, the two class condition al distribu-\ntions have identical covariance matrices, and the principal directions are o riented diagonally. The\nclass centers are separated on one axis, but not on the other. By itself o ne variable is \u201cuseless\u201d.\nStill, the two dimensional separation is better than the separation using the \u201cusef ul\u201d variable alone.\nTherefore, a variable that is completely useless by itself can provide a signi\ufb01cant p erformance\nimprovementwhen takenwith others.\nThe next question is whether two variables that are useless by themselves c an provide a good\nseparation when taken together. We constructed an example of such a cas e, inspired by the famous\nXOR problem.7In Figure 3.b, we drew examples for two classes using four Gaussians pla ced on\nthe corners of a square at coordinates (0; 0), (0; 1), (1; 0), and ( 1; 1). The class labels of these four\n\u201cclumps\u201dareattributedaccordingtothetruthtableofthelogicalXORfunction: f(0;0)=0,f(0;1)=1,\nf(1; 0)=1; f(1; 1)=0. We notice that the projections on the axes provide no class separation. Yet,\nin the two dimensional space the classes can easily be separated (albeit not with a linear decision\nfunction).8Twovariables thatare uselessbythemselvescan beusefultogeth er.\n7. The XOR problem is sometimes referred to as the two-bit parity problem a nd is generalizable to more than two\ndimensions (n-bit parity problem). A related problem is the chessboard p roblem in which the two classes pave\nthe space with squares of uniformly distributed examples with alternating clas s labels. The latter problem is also\ngeneralizable to the multi-dimensional case. Similar examples are used in se veral papers in this issue (Perkins et al.,\n2003, Stoppiglia etal., 2003).\n8. Incidentally, thetwo variables are alsouncorrelated with oneanother.\n1165", "start_char_idx": 0, "end_char_idx": 2666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0512f07f-0b51-4dbc-937b-9db19a68a0a3": {"__data__": {"id_": "0512f07f-0b51-4dbc-937b-9db19a68a0a3", "embedding": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a15e0596-6968-4f4f-b1b9-8e136d2d4b7c", "node_type": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22122\u22121 012\u2212505\u22122\u22121012\n\u22120.5 00.511.5 \u22120.5 00.511.5\u22120.500.511.5\u22120.500.511.5\n(a) (b)\nFigure 3: A variable useless by itself can be useful together with others. (a) One variable has\ncompletely overlapping class conditional densities. Still, using it jointly with the othe r variable\nimprovesclassseparabilitycomparedtousingtheothervariablealone. (b)X OR-likeorchessboard-\nlike problems. The classes consist of disjoint clumps such that in projection o n the axes the class\nconditional densities overlap perfectly. Therefore, individual variab les have no separation power.\nStill,takentogether,thevariablesprovidegood classseparability.\n4 Variable Subset Selection\nIn the previous section, we presented examples that illustrate the usefulnes s of selecting subsets\nof variables that together have good predictive power, as opposed to r anking variables according\nto their individual predictive power. We now turn to this problem and outline th e main directions\nthat have been taken to tackle it. They essentially divide into wrappers, \ufb01lter s, and embedded\nmethods. Wrappers utilize the learning machine of interest as a black box to score subsets of\nvariable according to their predictive power. Filtersselect subsets of variables as a pre-processing\nstep, independently of the chosen predictor. Embedded methods perform variable selection in the\nprocessof trainingandareusuallyspeci\ufb01ctogivenlearningmachines.\n4.1 WrappersandEmbeddedMethods\nThe wrapper methodology, recently popularized by Kohavi and John (1 997), offers a simple and\npowerful way to address the problem of variable selection, regardless of the chosen learning ma-\nchine. In fact, the learning machine is considered a perfect black box and the method lends itself\nto the use of off-the-shelf machine learning software packages. In its mos t general formulation, the\nwrapper methodology consists in using the prediction performance of a giv en learning machine to\nassess the relative usefulness of subsets of variables. In practice, o ne needs to de\ufb01ne: (i) how to\nsearch the space of all possible variable subsets; (ii) how to assess the p rediction performance of\na learning machine to guide the search and halt it; and (iii) which predictor to us e. An exhaustive\nsearch can conceivably be performed, if the number of variables is not too large. But, the problem\n1166", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3643e4ba-8ba5-457c-bf87-3d96110eef0f": {"__data__": {"id_": "3643e4ba-8ba5-457c-bf87-3d96110eef0f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "3": {"node_id": "7a895568-21c9-45a3-8fbc-0383ae583394", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de"}}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nisknowntobeNP-hard(AmaldiandKann,1998)andthesearchbecomes quicklycomputationally\nintractable. A wide range of search strategies can be used, including bes t-\ufb01rst, branch-and-bound,\nsimulated annealing, genetic algorithms (see Kohavi and John, 1997, for a review). Performance\nassessments are usually done using a validation set or by cross-validation (see Section 6). As il-\nlustrated in this special issue, popular predictors include decision trees, n a\u00a8\u0131ve Bayes, least-square\nlinear predictors,and supportvector machines.\nWrappersareoftencriticizedbecausetheyseemtobea\u201cbruteforce\u201dme thodrequiringmassive\namountsofcomputation,butitisnotnecessarilyso. Ef\ufb01cientsearchstra tegiesmaybedevised. Us-\ningsuchstrategiesdoesnotnecessarilymeansacri\ufb01cingpredictionperf ormance. Infact,itappears\nto be the converse in some cases: coarse search strategies may alleviate the problem of over\ufb01tting,\nas illustrated for instance in this issue by the work of Reunanen (2003). Gr eedy search strategies\nseem to be particularly computationally advantageous and robust against o ver\ufb01tting. They come in\ntwo \ufb02avors: forward selection andbackward elimination . In forward selection, variables are pro-\ngressively incorporated into larger and larger subsets, whereas in ba ckward elimination one starts\nwith the set of all variables and progressively eliminates the least promising o nes.9Both methods\nyieldnestedsubsets of variables.\nBy using the learning machine as a black box, wrappers are remarkably un iversal and simple.\nBut embedded methods that incorporate variable selection as part of the tra ining process may be\nmore ef\ufb01cient in several respects: they make better use of the available da ta by not needing to split\nthetrainingdataintoatrainingandvalidationset;theyreachasolutionfasterby avoidingretraining\na predictor from scratch for every variable subset investigated. Embed ded methods are not new:\ndecision trees such as CART, for instance, have a built-in mechanism to per form variable selection\n(Breiman et al., 1984). The next two sections are devoted to two families of emb edded methods\nillustratedby algorithmspublishedinthisissue.\n4.2 NestedSubsetMethods\nSome embedded methods guide their search by estimating changes in the objectiv e function value\nincurredbymakingmovesinvariablesubsetspace. Combinedwithgreedys earchstrategies(back-\nwardeliminationor forwardselection) theyyieldnestedsubsetsof variables .10\nLet us call sthe number of variables selected at a given algorithm step and J(s)the value of\nthe objective function of the trained learning machine using such a variable s ubset. Predicting the\nchange intheobjectivefunctionis obtainedby:\n1.Finite difference calculation: The difference between J(s)andJ(s+1)orJ(s\u22121)is com-\nputed forthevariables thatarecandidates foraddition or removal.\n2.Quadratic approximation of the cost function: This method was originally proposed to\nprune weights in neural networks (LeCun et al., 1990). It can be used for backward elimi-\nnation of variables, via the pruning of the input variable weights wi. A second order Taylor\nexpansion of Jis made. At the optimum of J, the \ufb01rst-order term can be neglected, yield-\n9. The name greedy comes from the fact that one never revisits forme r decisions to include (or exclude) variables in\nlight of newdecisions.\n10. Thealgorithmspresentedinthissectionandinthefollowinggenerallyben", "start_char_idx": 0, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a895568-21c9-45a3-8fbc-0383ae583394": {"__data__": {"id_": "7a895568-21c9-45a3-8fbc-0383ae583394", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "2": {"node_id": "3643e4ba-8ba5-457c-bf87-3d96110eef0f", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2"}}, "hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de", "text": "Thealgorithmspresentedinthissectionandinthefollowinggenerallyben e\ufb01tfromvariablenormalization,exceptif\nthey have an internalnormalization mechanism likethe Gram-Schmidtortho gonalization procedure .\n1167", "start_char_idx": 3354, "end_char_idx": 3557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "105db419-c479-40c5-8d6d-a9485b1154d1": {"__data__": {"id_": "105db419-c479-40c5-8d6d-a9485b1154d1", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08ba269e-837f-49f1-ad4b-ba868fab1bb5", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "3": {"node_id": "36eb9d2b-8a2e-4cf0-b3f5-e09a2af5665b", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce"}}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "text": "GUYON AND ELISSEEFF\ning for variable ito the variation DJi= (1/2)\u22022J\n\u2202w2\ni(Dwi)2. The change in weight Dwi=wi\ncorrespondstoremovingvariable i.\n3.Sensitivity of the objective function calculation: The absolute value or the square of the\nderivativeof Jwithrespectto xi(orwithrespectto wi) isused.\nSome training algorithms lend themselves to using \ufb01nite differences (method 1) be cause exact\ndifferencescanbecomputedef\ufb01ciently,withoutretrainingnewmodelsfor eachcandidatevariable.\nSuch is the case for the linear least-square model: The Gram-Schmidt orthog onolization procedure\npermits the performance of forward variable selection by adding at each s tep the variable that most\ndecreasesthemean-squared-error. Twopapersinthisissuearedev otedtothistechnique(Stoppiglia\netal.,2003,RivalsandPersonnaz,2003). Forotheralgorithmslikeker nelmethods,approximations\nof the difference can be computed ef\ufb01ciently. Kernel methods are learnin g machines of the form\nf(x) =\u2211m\nk=1\u03b1kK(x,xk), whereKis the kernel function, which measures the similarity between x\nandxk(Schoelkopf and Smola, 2002). The variation in J(s)is computed by keeping the \u03b1kvalues\nconstant. This procedure originally proposed for SVMs (Guyon et al., 2 002) is used in this issueas\nabaselinemethod (Rakotomamonjy,2003, Westonetal.,2003).\nThe \u201coptimum brain damage\u201d (OBD) procedure (method 2) is mentioned in this iss ue in the\npaper of Rivals and Personnaz (2003). The case of linear predictor sf(x) =w\u00b7x+bis particularly\nsimple. The authors of the OBD algorithm advocate using DJiinstead of the magnitude of the\nweights |wi|as pruning criterion. However, for linear predictors trained with an objec tive function\nJthatisquadraticin withesetwocriteriaareequivalent. Thisisthecase,forinstance,fortheline ar\nleast square model using J=\u2211m\nk=1(w\u00b7xk+b\u2212yk)2and for the linear SVM or optimum margin\nclassi\ufb01er, which minimizes J= (1/2)||w||2, under constraints (Vapnik, 1982). Interestingly, for\nlinear SVMs the \ufb01nite difference method (method 1) and the sensitivity method (me thod 3) also\nboildowntoselectingthevariablewithsmallest |wi|foreliminationateachstep(Rakotomamonjy,\n2003).\nThesensitivityoftheobjectivefunctiontochangesin wi(method3)isusedtodeviseaforward\nselection procedure in one paper presented in this issue (Perkins et al., 2 003). Applications of this\nproceduretoalinearmodelwithacross-entropyobjectivefunctionarep resented. Intheformulation\nproposed, the criterion is the absolute value of\u2202J\n\u2202wi=\u2211m\nk=1\u2202J\n\u2202\u03c1k\u2202\u03c1k\n\u2202wi, where\u03c1k=ykf(xk). In the case\nof the linear model f(x) =w\u00b7x+b, the criterion has a simple geometrical interpretation: it is the\nthedotproductbetweenthegradientoftheobjectivefunctionwithrespe cttothemarginvaluesand\nthevector [\u2202\u03c1k\n\u2202wi=xk,iyk]k=1...m. For thecross-entropylossfunction,wehave:\u2202J\n\u2202\u03c1k=1\n1+e\u03c1k.\nAn interesting variant of the sensitivity analysis method is obtained by replacin g the objective\nfunction by the leave-one-out cross-validation error. For some learning machines and some ob-\njective", "start_char_idx": 0, "end_char_idx": 2985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "36eb9d2b-8a2e-4cf0-b3f5-e09a2af5665b": {"__data__": {"id_": "36eb9d2b-8a2e-4cf0-b3f5-e09a2af5665b", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08ba269e-837f-49f1-ad4b-ba868fab1bb5", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "2": {"node_id": "105db419-c479-40c5-8d6d-a9485b1154d1", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668"}}, "hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce", "text": "cross-validation error. For some learning machines and some ob-\njective functions, approximate or exact analytical formulas of the leave-o ne-out error are known.\nIn this issue, the case of the linear least-square model (Rivals and Perso nnaz, 2003) and SVMs\n(Rakotomamonjy, 2003) are treated. Approximations for non-linear least-s quares have also been\ncomputed elsewhere (Monari and Dreyfus, 2000). The proposal of Rakotomamonjy (2003) is to\ntrain non-linear SVMs (Boser et al., 1992, Vapnik, 1998) with a regular tr aining procedure and\nselect features with backward elimination like in RFE (Guyon et al., 2002). Th e variable ranking\ncriterion however is not computed using the sensitivity of the objective func tionJ, but that of a\nleave-one-outbound.\n1168", "start_char_idx": 2914, "end_char_idx": 3675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d2ec32e0-45fe-491a-9f6e-71ea1944fc41": {"__data__": {"id_": "d2ec32e0-45fe-491a-9f6e-71ea1944fc41", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7e132c3-5106-4ff8-a7bd-4298ea265edc", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "3": {"node_id": "cfebf655-8779-4869-b303-23adf65127c3", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2"}}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n4.3 DirectObjective Optimization\nAlotofprogresshasbeenmadeinthisissuetoformalizetheobjectivefunction ofvariableselection\nand\ufb01ndalgorithmstooptimizeit. Generally,theobjectivefunctionconsistsoftwo termsthatcom-\npete with each other: (1) the goodness-of-\ufb01t (to be maximized), and (2) the number of variables\n(to be minimized). This approach bears similarity with two-part objective functio ns consisting of\na goodness-of-\ufb01t term and a regularization term, particularly when the ef fect of the regularization\nterm is to \u201cshrink\u201d parameter space. This correspondence is formally esta blished in the paper of\nWeston et al. (2003) for the particular case of classi\ufb01cation with linear pre dictorsf(x) =w\u00b7x+b,\nin the SVM framework (Boser et al., 1992, Vapnik, 1998). Shrinking reg ularizers of the type\n||w||p\np= (\u2211n\ni=1wp\ni)1/p(/lscriptp-norm) are used. In the limit as p\u21920, the /lscriptp-norm is just the number\nof weights, i.e., the number of variables. Weston et al. proceed with showing that the /lscript0-norm\nformulation of SVMs can be solved approximately with a simple modi\ufb01cation of the va nilla SVM\nalgorithm:\n1. Traina regularlinear SVM (using /lscript1-normor /lscript2-normregularization).\n2. Re-scaletheinputvariablesbymultiplyingthembytheabsolutevaluesofthe componentsof\nthe weightvector wobtained.\n3. Iteratethe\ufb01rst2steps untilconvergence.\nThemethodisreminiscentofbackwardeliminationproceduresbasedonthes mallest |wi|. Variable\nnormalizationis importantforsuchamethodtoworkproperly.\nWeston et al. note that, although their algorithm only approximately minimizes the /lscript0-norm, in\npracticeitmaygeneralizebetterthananalgorithmthatreallydidminimizethe /lscript0-norm,becausethe\nlatterwouldnotprovidesuf\ufb01cientregularization(alotofvarianceremain sbecausetheoptimization\nproblem has multiple solutions). The need for additional regularization is also stressed in the paper\nof Perkins et al. (2003). The authors use a three-part objective fun ction that includes goodness-\nof-\ufb01t, a regularization term ( /lscript1-norm or /lscript2-norm), and a penalty for large numbers of variables\n(/lscript0-norm). The authors propose a computationally ef\ufb01cient forward selectio n method to optimize\nsuchobjective.\nAnother paper in the issue, by Bi et al. (2003), uses /lscript1-norm SVMs, without iterative multi-\nplicative updates. The authors \ufb01nd that, for their application, the /lscript1-norm minimization suf\ufb01ces to\ndriveenoughweightstozero. Thisapproachwasalsotakeninthecontex tofleast-squareregression\nby other authors (Tibshirani, 1994). The number of variables can be fu rther reduced by backward\nelimination.\nTo our knowledge, no algorithm has been proposed to directly minimize the numb er of vari-\nablesfornon-linearpredictors. Instead,severalauthorshavesub stitutedfortheproblemofvariable\nselection that of variable scaling (Jebara and Jaakkola, 2000, Weston e t al., 2000, Grandvalet and\nCanu,2002). Thevariablescalingfactorsare\u201chyper-parameters\u201da djustedbymodelselection. The\nscaling factors obtained are used to assess variable relevance. A varia nt of the method consists\nof adjusting the scaling factors by", "start_char_idx": 0, "end_char_idx": 3166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cfebf655-8779-4869-b303-23adf65127c3": {"__data__": {"id_": "cfebf655-8779-4869-b303-23adf65127c3", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b7e132c3-5106-4ff8-a7bd-4298ea265edc", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "2": {"node_id": "d2ec32e0-45fe-491a-9f6e-71ea1944fc41", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246"}}, "hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2", "text": "A varia nt of the method consists\nof adjusting the scaling factors by gradient descent on a bound of the lea ve-one-out error (Weston\net al., 2000). This method is used as baseline method in the paper of Weston et al. (2003) in this\nissue.\n1169", "start_char_idx": 3097, "end_char_idx": 3341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5517cdc3-dbf0-4218-a664-f03f4af89bcd": {"__data__": {"id_": "5517cdc3-dbf0-4218-a664-f03f4af89bcd", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63b54ee6-0451-4dc1-b208-b072390222be", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "3": {"node_id": "dbc8eade-e391-48ab-a10e-794380484b0d", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1"}}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "text": "GUYON AND ELISSEEFF\n4.4 Filters for SubsetSelection\nSeveral justi\ufb01cations for the use of \ufb01lters for subset selection have bee n put forward in this special\nissue and elsewhere. It is argued that, compared to wrappers, \ufb01lters ar e faster. Still, recently pro-\nposed ef\ufb01cient embedded methods are competitive in that respect. Another a rgument is that some\n\ufb01lters (e.g. those based on mutual information criteria) provide a generic se lection of variables, not\ntunedfor/byagivenlearningmachine. Anothercompellingjusti\ufb01cationisthat\ufb01 lteringcanbeused\nas apreprocessingsteptoreducespacedimensionalityandovercomeove r\ufb01tting.\nIn that respect, it seems reasonable to use a wrapper (or embedded metho d) with a linearpre-\ndictor as a \ufb01lter and then train a more complex non-linear predictor on the resulting variables. An\nexampleofthisapproachisfoundinthepaperofBietal.(2003): alinear /lscript1-normSVMisusedfor\nvariable selection, but a non-linear /lscript1-norm SVM is used for prediction. The complexity of linear\n\ufb01lters can be ramped up by adding to the selection process products of inpu t variables (monomi-\nals of a polynomial) and retaining the variables that are part of any selected monomial. Another\npredictor, e.g., a neural network, is eventually substituted to the polynomial to perform predictions\nusing the selected variables (Rivals and Personnaz, 2003, Stoppiglia et al., 2003). In some cases\nhowever, one may on the contrary want to reduce the complexity of linear \ufb01lte rs to overcome over-\n\ufb01ttingproblems. Whenthenumberofexamplesissmallcomparedtothenumberofv ariables(inthe\ncase of microarray data for instance) one may need to resort to selecting v ariables with correlation\ncoef\ufb01cients (seeSection2.2).\nInformation theoretic \ufb01ltering methods such as Markov blanket11algorithms (Koller and Sa-\nhami,1996)constituteanotherbroadfamily. Thejusti\ufb01cationforclassi\ufb01cation problemsisthatthe\nmeasureofmutualinformationdoesnotrelyonanypredictionprocess,bu tprovidesaboundonthe\nerrorrateusinganyprediction schemeforthe givendistribution. Wedono t haveanyillustrationof\nsuchmethodsinthisissuefortheproblemofvariablesubsetselection. Wer efertheinterestedreader\ntoKollerandSahami(1996)andreferencestherein. However,theuse ofmutualinformationcriteria\nforindividualvariablerankingwascoveredinSection2andapplicationto featureconstructionand\nselectionareillustratedin Section5.\n5 FeatureConstruction and SpaceDimensionality Reduction\nIn some applications, reducing the dimensionality of the data by selecting a subs et of the original\nvariablesmaybeadvantageousforreasonsincludingtheexpenseofmak ing,storingandprocessing\nmeasurements. If these considerations are not of concern, other means of space dimensionality\nreductionshouldalsobeconsidered.\nThe art of machine learning starts with the design of appropriate data repre sentations. Better\nperformance is often achieved using features derived from the origina l input. Building a feature\nrepresentationisanopportunitytoincorporatedomainknowledgeintothedata andcanbeveryap-\nplicationspeci\ufb01c. Nonetheless,thereareanumberofgenericfeatureco nstructionmethods,includ-\ning: clustering;basiclineartransformsoftheinputvariables(PCA/SVD,L DA);moresophisticated\nlinear transforms like spectral transforms (Fourier, Hadamard), wavele t transforms or convolutions\nofkernels;andapplyingsimplefunctionstosubsetsofvariables,likeprod uctstocreatemonomials.\n11. The", "start_char_idx": 0, "end_char_idx": 3397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dbc8eade-e391-48ab-a10e-794380484b0d": {"__data__": {"id_": "dbc8eade-e391-48ab-a10e-794380484b0d", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "63b54ee6-0451-4dc1-b208-b072390222be", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "2": {"node_id": "5517cdc3-dbf0-4218-a664-f03f4af89bcd", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d"}}, "hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1", "text": "uctstocreatemonomials.\n11. The Markov blanket of a given variable xiis a set of variables not including xithat render xi\u201cunnecessary\u201d. Once\na Markov blanket is found, xican safely be eliminated. Furthermore, in a backward elimination procedu re, it will\nremain unnecessary atlater stages.\n1170", "start_char_idx": 3367, "end_char_idx": 3660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57b5953e-180f-421e-8ccd-f15ee1f1774b": {"__data__": {"id_": "57b5953e-180f-421e-8ccd-f15ee1f1774b", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2aad3f0c-29d6-48ec-965b-92fe5fa613be", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "3": {"node_id": "15cd62be-5c48-47a9-9102-b19106a910fc", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be"}}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nTwodistinctgoalsmaybepursuedforfeatureconstruction: achievingbe streconstructionofthe\ndata or being most ef\ufb01cient for making predictions. The \ufb01rst problem is an unsupervised learning\nproblem. Itiscloselyrelatedtothatofdatacompressionandalotofalgorithms areusedacrossboth\n\ufb01elds. The second problem is supervised. Are there reasons to select features in an unsupervised\nmanner when the problem is supervised? Yes, possibly several: Some pro blems, e.g., in text pro-\ncessingapplications,comewithmoreunlabelleddatathanlabelleddata. Also,un supervisedfeature\nselectionis lesspronetoover\ufb01tting.\nInthisissue,fourpapersaddresstheproblemoffeatureconstruction . Allofthemtakeaninfor-\nmation theoretic approach to the problem. Two of them illustrate the use of cluster ing to construct\nfeatures (Bekkerman et al., 2003, Dhillon et al., 2003), one provides a n ew matrix factorization al-\ngorithm (Globerson and Tishby, 2003), and one provides a supervise d means of learning features\nfrom a variety of models (Torkkola, 2003). In addition, two papers whos e main focus is directed\ntovariableselectionalsoaddresstheselectionofmonomialsofapolynomialmode landthehidden\nunits of a neural network (Rivals and Personnaz, 2003, Stoppiglia et a l., 2003), and one paper ad-\ndresses the implicit feature selection in non-linear kernel methods for polyn omial kernels (Weston\netal.,2003).\n5.1 Clustering\nClustering has long been used for feature construction. The idea is to rep lace a group of \u201csimilar\u201d\nvariables by a cluster centroid, which becomes a feature. The most popula r algorithms include\nK-means andhierarchicalclustering. Forareview,see,e.g.,thetextboo k ofDuda etal.(2001).\nClustering is usually associated with the idea of unsupervised learning. It c an be useful to\nintroduce some supervision in the clustering procedure to obtain more discrimin ant features. This\nistheideaofdistributionalclustering(Pereiraetal.,1993),whichisdevelo pedintwopapersofthis\nissue. Distributional clustering is rooted in the information bottleneck (IB) theo ry of Tishby et al.\n(1999). Ifwecall \u02dcXtherandomvariablerepresentingtheconstructedfeatures,theIBmethod seeks\nto minimize the mutual information I(X,\u02dcX), while preserving the mutual information I(\u02dcX,Y). A\nglobalobjectivefunctionisbuiltbyintroducingaLagrangemultiplier \u03b2:J=I(X,\u02dcX)\u2212\u03b2I(\u02dcX,Y). So,\nthemethodsearchesforthesolutionthatachievesthelargestpossiblecomp ression,whileretaining\ntheessentialinformationabout thetarget.\nText processing applications are usual targets for such techniques. P atterns are full documents\nand variables come from a bag-of-words representation: Each variab le is associated to a word and\nis proportional to the fraction of documents in which that word appears. In application to feature\nconstruction, clustering methods group words, not documents. In text ca tegorization tasks, the su-\npervisioncomesfromtheknowledgeofdocumentcategories. Itisintroduc edbyreplacingvariable\nvectorscontainingdocumentfrequencycountsbyshortervariablevec torscontainingdocumentcat-\negoryfrequencycounts,i.e.,thewordsarerepresentedas distribution sover documentcategories.\nThe simplest implementation of this idea is presented in the paper of Dhillon et al. (2 003) in\nthis issue. It uses K-means clustering on variables represented by a vec tor of document category\nfrequencycounts.", "start_char_idx": 0, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15cd62be-5c48-47a9-9102-b19106a910fc": {"__data__": {"id_": "15cd62be-5c48-47a9-9102-b19106a910fc", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2aad3f0c-29d6-48ec-965b-92fe5fa613be", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "2": {"node_id": "57b5953e-180f-421e-8ccd-f15ee1f1774b", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5"}}, "hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be", "text": "clustering on variables represented by a vec tor of document category\nfrequencycounts. The(non-symmetric)similaritymeasureusedistheKullbac k-Leiblerdivergence\nK(xj,\u02dcxi) =exp(\u2212\u03b2\u2211kxk,jln(xk,j/\u02dcxk,i)). In the sum, the index kruns over document categories. A\nmore elaborate approach is taken by Bekkerman et al. (2003) who use a \u201c soft\u201d version of K-means\n(allowing words to belong to several clusters) and who progressively d ivide clusters by varying the\nLagrangemultiplier \u03b2monitoringthetradeoffbetween I(X,\u02dcX)andI(\u02dcX,Y). Inthisway,documents\n1171", "start_char_idx": 3287, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b2af32c3-8ee0-49b7-a7a6-b20c56071acd": {"__data__": {"id_": "b2af32c3-8ee0-49b7-a7a6-b20c56071acd", "embedding": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1e37f46-8bc9-459f-b4fa-260c3d46ae96", "node_type": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "text": "GUYON AND ELISSEEFF\narerepresentedasadistributionoverwordcentroids. Bothmethodsperf ormwell. Bekkermanetal.\nmentionthatfewwordsendupbelongingtoseveralclusters,hintingthat\u201ch ard\u201dclusterassignment\nmaybe suf\ufb01cient.\n5.2 Matrix Factorization\nAnother widely used method of feature construction is singular value decomp osition (SVD). The\ngoal of SVD is to form a set of features that are linear combinations of the o riginal variables,\nwhich provide the best possible reconstruction of the original data in the lea st square sense (Duda\net al., 2001). It is an unsupervised method of feature construction. In th is issue, the paper of\nGloberson and Tishby (2003) presents an information theoretic unsuper vised feature construction\nmethod: suf\ufb01cient dimensionality reduction (SDR). The most informative fea tures are extracted by\nsolving an optimization problem that monitors the tradeoff between data recons truction and data\ncompression, similar to the information bottleneck of Tishby et al. (1999); the f eatures are found\nas Lagrange multipliers of the objective optimized. Non-negative matrices P of dimension (m, n)\nrepresentingthejointdistributionoftworandomvariables(forinstancethe co-occurrenceofwords\nin documents) are considered. The features are extracted by information theoretic I-projections,\nyielding a reconstructed matrix of special exponential form \u02dcP= (1/Z)exp(\u03a6\u03a8). For a set of d\nfeatures,\u03a6isa(m,d+2)matrixwhose (d+1)thcolumnisonesand \u03a8isa(d+2,n)matrixwhose\n(d+2)thcolumnisones,and Zisanormalizationcoef\ufb01cient. SimilarlytoSVD,thesolutionshows\nthesymmetryofthe problemwithrespecttopatterns andvariables.\n5.3 SupervisedFeatureSelection\nWe review three approaches for selecting features in cases where fea tures should be distinguished\nfromvariables becausebothappear simultaneouslyinthesamesystem:\nNested subset methods. A number of learning machines extract features as part of the learn-\ning process. These include neural networks whose internal nodes ar e feature extractors. Thus,\nnode pruning techniques such as OBD LeCun et al. (1990) are feature selection algorithms. Gram-\nSchmidt orthogonalization is presented in this issue as an alternative to OBD (S toppiglia et al.,\n2003).\nFilters.Torkkola (2003) proposes a \ufb01lter method for constructing features usin g a mutual in-\nformation criterion. The author maximizes I(\u03c6,y)formdimensional feature vectors \u03c6and target\nvectorsy.12Modelling the feature density function with Parzen windows allows him to compute\nderivatives \u2202I/\u2202\u03c6ithat are transform independent. Combining them with the transform-depend ent\nderivatives \u2202\u03c6i/\u2202w, he devises a gradient descent algorithm to optimize the parameters wof the\ntransform(thatneed notbelinear):\nwt+1=wt+\u03b7\u2202I\n\u2202w=wt+\u03b7\u2202I\n\u2202\u03c6i\u2202\u03c6i\n\u2202w. (5)\nDirect objective optimization. Kernel methods possess an implicit feature space revealed by\nthe kernel expansion: k(x,x/prime) =\u03c6(x).\u03c6(x/prime), where\u03c6(x)is a feature vector of possibly in\ufb01nite di-\nmension. Selecting these implicit features may improve generalization, but does not change the\n12. Infact, theauthor uses aquadratic measure of divergence instea d of theusualmutual information.\n1172", "start_char_idx": 0, "end_char_idx": 3136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fefe083b-81f2-444e-a1d1-0d4d8cf12250": {"__data__": {"id_": "fefe083b-81f2-444e-a1d1-0d4d8cf12250", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e728b3b4-dcaf-4b87-834b-27ba5c794b18", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "3": {"node_id": "c3c9ccd2-b2c5-4bd3-808e-c500b733abeb", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039"}}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nrunning time or help interpreting the prediction function. In this issue, Weston e t al. (2003) pro-\nposeamethodforselectingimplicitkernelfeaturesinthecaseofthepolynomial kernel,usingtheir\nframeworkof minimizationof the /lscript0-norm.\n6 Validation Methods\nWe group in this section all the issues related to out-of-sample performance pr ediction (generaliza-\ntion prediction) and model selection. These are involved in various aspects of variable and feature\nselection: to determine the number of variables that are \u201csigni\ufb01cant\u201d, to guide and halt the search\nfor good variable subsets, to choose hyperparameters, and to evaluate the \ufb01nal performance of the\nsystem.\nOne should \ufb01rst distinguish the problem of model selection from that of eva luating the \ufb01nal\nperformance of the predictor. For that last purpose, it is important to set aside an independent\ntest set. The remaining data is used both for training and performing model sele ction. Additional\nexperimentalsophisticationcanbeaddedbyrepeatingtheentireexperiment forseveraldrawingsof\nthetestset.13\nTo perform model selection (including variable/feature selection and hype rparameter optimiza-\ntion), the data not used for testing may be further split between \ufb01xed training and validation sets,\nor various methods of cross-validation can be used. The problem is then b rought back to that of\nestimating the signi\ufb01cance of differences in validation errors. For a \ufb01xed v alidation set, statistical\ntests can be used, but their validity is doubtful for cross-validation becau se independence assump-\ntions are violated. For a discussion of these issues, see for instance the w ork of Dietterich (1998)\nand Nadeau and Bengio (2001). If there are suf\ufb01ciently many examples, it may not be necessary to\nsplit the training data: Comparisons of training errors with statistical tests can b e used (see Rivals\nandPersonnaz,2003,inthisissue). Cross-validationcanbeextended totime-seriesdataand,while\ni.i.d.assumptionsdonotholdanymore,itisstillpossibletoestimategeneralizatione rrorcon\ufb01dence\nintervals(seeBengio andChapados,2003, inthisissue).\nChoosing what fraction of the data should be used for training and for va lidation is an open\nproblem. Manyauthorsresorttousingtheleave-one-outcross-valida tionprocedure,eventhoughit\nis known to be a high variance estimator of generalization error (Vapnik, 19 82) and to give overly\noptimistic results, particularly when data are not properly independently and identically sampled\nfrom the \u201dtrue\u201d distribution. The leave-one-out procedure consists of removing one example from\nthetrainingset,constructingthepredictoronthebasisonlyoftheremainingtra iningdata,thentest-\ningontheremovedexample. Inthisfashiononetestsallexamplesofthetraining dataandaverages\ntheresults. Aspreviouslymentioned,thereexistexactorapproximatefor mulasoftheleave-one-out\nerror for a number of learning machines (Monari and Dreyfus, 2000, Rivals and Personnaz, 2003,\nRakotomamonjy, 2003).\nLeave-one-out formulas can be viewed as corrected values of the train ing error. Many other\ntypes of penalization of the training error have been proposed in the literatu re (see, e.g., Vapnik,\n1998, Hastie et al., 2001). Recently, a new family of such methods called \u201cmetr ic-based methods\u201d\nhave been proposed (Schuurmans, 1997). The paper of Bengio and Chapados (2003) in this issue\n13. Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3c9ccd2-b2c5-4bd3-808e-c500b733abeb": {"__data__": {"id_": "c3c9ccd2-b2c5-4bd3-808e-c500b733abeb", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e728b3b4-dcaf-4b87-834b-27ba5c794b18", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "2": {"node_id": "fefe083b-81f2-444e-a1d1-0d4d8cf12250", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee"}}, "hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039", "text": "Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc anbecarriedoutasan\u201couterloop\u201d,outsidethe\nfeature/variableselectionprocess,toestimatethe\ufb01nalperformanceof thepredictor. Thiscomputationallyexpensive\nprocedure isusedincases where datais extremely scarce.\n1173", "start_char_idx": 3387, "end_char_idx": 3654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d8228d29-8ee9-4d31-8fd4-c6886919812a": {"__data__": {"id_": "d8228d29-8ee9-4d31-8fd4-c6886919812a", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "679f7644-80ce-48c5-b7ab-0bf96a7fdb82", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "3": {"node_id": "6145c93f-851e-4099-9416-4f538f8a45b6", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5"}}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "text": "GUYON AND ELISSEEFF\nillustrates their application to variable selection. The authors make use of unlab elled data, which\nare readily available in the application considered, time series prediction with a h orizon. Consider\ntwomodels fAandfBtrainedwithnestedsubsetsofvariables A\u2282B. Wecall d(fA,fB)thediscrep-\nancy of the two models. The criterion involves the ratio dU(fA,fB)/dT(fA,fB), wheredU(fA,fB)is\ncomputed with unlabelled data and dT(fA,fB)is computed with training data. A ratio signi\ufb01cantly\nlarger thanonesheds doubtontheusefulnessof thevariablesinsubse tBthat arenotin A.\nFor variable ranking or nested subset ranking methods (Sections 2 and 4 .2), another statisti-\ncal approach can be taken. The idea is to introduce a probe in the data that is a random variable.\nRoughly speaking, variables that have a relevance smaller or equal to tha t of the probe should be\ndiscarded. Bi et al. (2003) consider a very simple implementation of that idea : they introduce in\ntheirdatathreeadditional\u201cfakevariables\u201ddrawnrandomlyfromaGauss iandistributionandsubmit\nthem to their variable selection process with the other \u201ctrue variables\u201d. Subs equently, they discard\nall the variables that are less relevant than one of the three fake variable s (according to their weight\nmagnitude criterion). Stoppiglia et al. (2003) propose a more sophisticated me thod for the Gram-\nSchmidt forward selection method. For a Gaussian distributed probe, they p rovide an analytical\nformula to compute the rank of the probe associated with a given risk of acce pting an irrelevant\nvariable. A non-parametric variant of the probe method consists in creating \u201cfake variables\u201d by\nrandomly shuf\ufb02ing real variable vectors. In a forward selection proce ss, the introduction of fake\nvariables does not disturb the selection because fake variables can be d iscarded when they are en-\ncountered. Atagivenstepintheforwardselectionprocess,letuscall ftthefractionoftruevariables\nselected so far (among all true variables) and ffthe fraction of fake variables encountered (among\nall fake variables). As a halting criterion one can place a threshold on the r atioff/ft, which is an\nupper bound on the fraction of falsely relevant variables in the subset s elected so far. The latter\nmethodhasbeenusedforvariableranking(Tusheretal.,2001). Itspa rametricversionforGaussian\ndistributionsusingtheT statisticas rankingcriterionisnothingbuttheT-test.\n7 Advanced Topics and Open Problems\n7.1 Varianceof Variable SubsetSelection\nMany methods of variable subset selection are sensitive to small perturbatio ns of the experimental\nconditions. If the data has redundant variables, different subsets of variables with identical predic-\ntive power may be obtained according to initial conditions of the algorithm, remov al or addition of\na few variables or training examples, or addition of noise. For some applicatio ns, one might want\nto purposely generate alternative subsets that can be presented to a sub sequent stage of processing.\nStill one might \ufb01nd this variance undesirable because (i) variance is often th e symptom of a \u201cbad\u201d\nmodel that does not generalize well; (ii) results are not reproducible; an d (iii) one subset fails to\ncapturethe\u201cwhole picture\u201d.\nOnemethodto\u201cstabilize\u201dvariableselectionexploredinthisissueistousesever al\u201cbootstraps\u201d\n(Bi et al., 2003). The variable selection process is repeated with sub-sa mples of the training data.\nThe union of the subsets of variables selected", "start_char_idx": 0, "end_char_idx": 3456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6145c93f-851e-4099-9416-4f538f8a45b6": {"__data__": {"id_": "6145c93f-851e-4099-9416-4f538f8a45b6", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "679f7644-80ce-48c5-b7ab-0bf96a7fdb82", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "2": {"node_id": "d8228d29-8ee9-4d31-8fd4-c6886919812a", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8"}}, "hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5", "text": "mples of the training data.\nThe union of the subsets of variables selected in the various bootstraps is ta ken as the \ufb01nal \u201cstable\u201d\nsubset. This joint subset may be at least as predictive as the best bootstr ap subset. Analyzing the\nbehavior of the variables across the various bootstraps also provides f urther insight, as described\nin the paper. In particular, an index of relevance of individual variable s can be created considering\nhowfrequentlytheyappear inthebootstraps.\n1174", "start_char_idx": 3382, "end_char_idx": 3864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab841f20-89ab-46b1-a91a-20688dc8aea5": {"__data__": {"id_": "ab841f20-89ab-46b1-a91a-20688dc8aea5", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d27aa04e-2383-4475-b5b1-5f317558d9e9", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "3": {"node_id": "5a7e856c-4b6f-4b73-9045-a15467ac6b15", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b"}}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nRelated ideas have been described elsewhere in the context of Bayesian variable selection (Je-\nbara and Jaakkola, 2000, Ng and Jordan, 2001, Vehtari and Lampine n, 2002). A distribution over\na population of models using various variable subsets is estimated. Variables a re then ranked ac-\ncording to the marginal distribution, re\ufb02ecting how often they appear in importa nt subsets (i.e.,\nassociatedwiththemostprobablemodels).\n7.2 Variable Rankingin the Contextof Others\nIn Section 2, we limited ourselves to presenting variable ranking methods using a criterion com-\nputed from single variables, ignoring the context of others. In Section 4.2 , we introduced nested\nsubsetmethodsthatprovideausefulrankingofsubsets,notof indiv idualvariables: somevariables\nmayhavealowrankbecausetheyareredundantandyetbehighlyrelev ant. BootstrapandBayesian\nmethods presented in Section 7.1, may be instrumental in producing a good var iable ranking incor-\nporatingthecontext ofothers.\nThe relief algorithm uses another approach based on the nearest-neighb or algorithm (Kira and\nRendell,1992). Foreachexample,theclosestexampleofthesameclass(n earesthit)andtheclosest\nexampleofadifferentclass(nearestmiss)areselected. Thescore S(i)oftheithvariableiscomputed\nas the average over all examples of magnitude of the difference between th e distance to the nearest\nhitandthe distancetothenearestmiss,inprojectiononthe ithvariable.\n7.3 UnsupervisedVariable Selection\nSometimes, no target yis provided, but one still would want to select a set of most signi\ufb01cant\nvariables with respect to a de\ufb01ned criterion. Obviously, there are as many criteria as problems\ncan be stated. Still, a number of variable ranking criteria are useful acros s applications, including\nsaliency,entropy,smoothness ,densityandreliability . A variable is salient if it has a high variance\nor a large range, compared to others. A variable has a high entropy if the d istribution of examples\nis uniform. In a time series, a variable is smooth if on average its local curvatur e is moderate. A\nvariableisinahigh-densityregionifitishighlycorrelatedwithmanyothervaria bles. Avariableis\nreliable if the measurement error bars computed by repeating measurements a re small compared to\nthevariabilityof thevariablevalues(as quanti\ufb01ed,e.g., byanANOVA statistic) .\nSeveral authors have also attempted to perform variable or feature selec tion for clustering ap-\nplications (see,e.g., XingandKarp,2001, Ben-Hur andGuyon, 2003, andreferences therein).\n7.4 Forward vs.BackwardSelection\nItisoftenarguedthatforwardselectioniscomputationallymoreef\ufb01cienttha nbackwardelimination\nto generate nested subsets of variables. However, the defenders of b ackward elimination argue that\nweaker subsets are found by forward selection because the importance of variables is not assessed\nin the context of other variables not included yet. We illustrate this latter argume nt by the example\nofFigure4. Inthatexample,onevariableseparatesthetwoclassesbetter byitselfthaneitherofthe\ntwootheronestakenaloneandwillthereforebeselected\ufb01rstbyforwar dselection. Atthenextstep,\nwhen it is complemented by either of the two other variables, the resulting class s eparation in two\ndimensions will not be as good as the one obtained jointly by the two variables tha t were discarded\nat the \ufb01rst step. A backward selection method may outsmart forward selectio n by eliminating at\nthe \ufb01rst step the variable that by itself provides the best", "start_char_idx": 0, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a7e856c-4b6f-4b73-9045-a15467ac6b15": {"__data__": {"id_": "5a7e856c-4b6f-4b73-9045-a15467ac6b15", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d27aa04e-2383-4475-b5b1-5f317558d9e9", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "2": {"node_id": "ab841f20-89ab-46b1-a91a-20688dc8aea5", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52"}}, "hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b", "text": "at\nthe \ufb01rst step the variable that by itself provides the best separation to retain the two variables that\n1175", "start_char_idx": 3432, "end_char_idx": 3543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec277da6-33b4-4b23-a506-a7a1013d94de": {"__data__": {"id_": "ec277da6-33b4-4b23-a506-a7a1013d94de", "embedding": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f2be224-ddf2-48c4-890a-6911df0d2001", "node_type": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5 \u22125 0 5\u2212505\u2212505\u2212505\nFigure 4: Forward or backward selection? Of the three variables of this example, the third one\nseparatesthetwoclassesbestbyitself(bottomrighthistogram). Itisthere forethebestcandidatein\na forward selection process. Still, the two other variables are better taken to gether than any subset\nof twoincludingit. Abackwardselectionmethod mayperformbetter inthis case.\ntogether perform best. Still, if for some reason we need to get down to a sing le variable, backward\neliminationwillhavegotten ridof thevariablethatworks bestonitsown.\n7.5 TheMulti-class Problem\nSome variable selection methods treat the multi-class case directly rather than de composing it into\nseveral two-class problems: All the methods based on mutual information crite ria extend naturally\nto the multi-class case (see in this issue Bekkerman et al., 2003, Dhillon et al., 20 03, Torkkola,\n2003). Multi-classvariablerankingcriteriaincludeFisher\u2019scriterion(the ratioofthebetweenclass\nvariancetothewithin-classvariance). ItiscloselyrelatedtotheFstatisticuse dintheANOVAtest,\nwhichisonewayofimplementingtheprobemethod(Section6)forthemulti-classca se. Wrappers\nor embedded methods depend upon the capability of the classi\ufb01er used to han dle the multi-class\ncase. Examplesofsuchclassi\ufb01ersincludelineardiscriminantanalysis(LD A),amulti-classversion\nof Fisher\u2019s linear discriminant (Duda et al., 2001), and multi-class SVMs (s ee, e.g., Weston et al.,\n2003).\nOne may wonder whether it is advantageous to use multi-class methods for var iable selection.\nOn one hand, contrary to what is generally admitted for classi\ufb01cation, the multi- class setting is\nin some sense easier for variable selection than the two-class case. This is b ecause the larger the\n1176", "start_char_idx": 0, "end_char_idx": 1771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d7e9dcaf-5587-4974-b8d0-85ab6f8372ff": {"__data__": {"id_": "d7e9dcaf-5587-4974-b8d0-85ab6f8372ff", "embedding": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "31446f05-8076-4744-ac19-6f31e41d4c17", "node_type": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nnumberofclasses,thelesslikelya\u201crandom\u201dsetoffeaturesprovideago odseparation. Toillustrate\nthis point, consider a simple example where all features are drawn independ ently from the same\ndistribution Pand the \ufb01rst of them is the target y. Assume that all these features correspond to\nrollingadiewith Qfacesntimes(nisthenumberofsamples). Theprobabilitythatone\ufb01xedfeature\n(except the \ufb01rst one) is exactly yis then (1/Q)n. Therefore, \ufb01nding the feature that corresponds to\nthe target ywhen it is embedded in a sea of noisy features is easier when Qis large. On the other\nhand, Forman (2003) points out in this issue that in the case of uneven distr ibutions across classes,\nmulti-classmethodsmayover-representabundantoreasilyseparablecla sses. Apossiblealternative\nis to mix ranked lists of several two-class problems. Weston et al. (2003) pr opose one such mixing\nstrategy.\n7.6 Selectionof Examples\nThedualproblemsoffeatureselection/constructionarethoseofpatterns election/construction. The\nsymmetry of the two problems is made explicit in the paper of Globerson and Tishb y (2003) in\nthis issue. Likewise, both Stoppiglia et al. (2003) and Weston et al. (2003) point out that their\nalgorithm also applies to the selection of examples in kernel methods. Others ha ve already pointed\noutthesimilarityandcomplementarityofthetwoproblems(BlumandLangley,1997 ). Inparticular,\nmislabeledexamplesmayinducethechoiceofwrongvariables. Conversely, ifthelabelingishighly\nreliable,selectingwrongvariablesassociatedwithaconfoundingfactorma ybeavoidedbyfocusing\noninformativepatterns thatareclosetothedecisionboundary(Guyonet al.,2002).\n7.7 InverseProblems\nMostofthespecialissueconcentratesontheproblemof\ufb01ndinga(small)s ubsetofvariablesuseful\ntobuildagoodpredictor. Insomeapplications,particularlyinbioinformatics,th isisnotnecessarily\nthe only goal of variable selection. In diagnosis problems, for instance, it is important to identify\nthe factors that triggered a particular disease or unravel the chain of ev ents from the causes to\nthe symptoms. But reverse engineering the system that produced the data is a more challenging\ntask than building a predictor. The readers interested in these issues can c onsult the literature on\ngene networks in the conference proceedings of the paci\ufb01c symposium o n biocomputing (PSB) or\nintelligent systems for molecular biology conference (ISMB) and the causa lity inference literature\n(see, e.g., Pearl, 2000). At the heart of this problem is the distinction betwe en correlation and\ncausality. Observational data such as the data available to machine learning r esearchers allow us\nonly to observe correlations. For example, observations can be made abou t correlations between\nexpression pro\ufb01les of given genes or between pro\ufb01les and symptoms, b ut a leap of faith is made\nwhendeciding whichgene activatedwhichother one andinturntriggeredth esymptom.\nIn this issue, the paper of Caruana and de Sa (2003) presents interestin g ideas about using\nvariables discarded by variable selection as additional outputs of a neura l network. They show im-\nprovedperformanceonsyntheticandrealdata. Theiranalysissuppor tstheideathatsomevariables\naremoreef\ufb01cientlyusedasoutputsthanasinputs. Thiscouldbeasteptowar ddistinguishingcauses\nfromconsequences.\n1177", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e3151964-d154-42fe-ae2b-b338bb0f2567": {"__data__": {"id_": "e3151964-d154-42fe-ae2b-b338bb0f2567", "embedding": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "efbe87bc-394b-4f97-bed3-7e3eaf5c6c92", "node_type": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "text": "GUYON AND ELISSEEFF\nDataset Description patterns variables classes References\nLineara,bArti\ufb01ciallinear 10-1200 100-240 reg-2 SWBe\nMulti-clustercArti\ufb01cialnon-linear 1000-1300 100-500 2 PS\nQSARdChemistry 30-300 500-700 reg Bt\nUCIeMLrepository 8-60 500-16000 2-30ReBnToPC\nLVQ-PAKfPhoneme data 1900 20 20 T\nRaetch bench.gUCI/Delve/Statlog 200-7000 8-20 2 Ra\nMicroarrayaCancer classif. 6-100 2000-4000 2 WRa\nMicroarrayaGene classi\ufb01cation 200 80 5 W\nAstonUnivhPipeline transport 1000 12 3 T\nNIPS 2000iUnlabeled data 200-400 5-800 reg Ri\n20 Newsgroupj,oNews postings 20000 300-15000 2-20 GBkD\nText\ufb01lteringkTREC/OSHUMED 200-2500 3000-30000 6-17 F\nIRdatasetslMED/CRAN/CISI 1000 5000 30-225 G\nReuters-21578m,onewswiredocs. 21578 300-15000 114 BkF\nOpenDir. Proj.nWebdirectory 5000 14500 50 D\nTable 1:Publicly available data sets used in the special issue. Approximate numbers or ranges\nof patterns, variables, and classes effectively used are provided. T he \u201cclasses\u201d column indicates\n\u201creg\u201d for regression problems, or the number of queries for Informatio n Retrieval (IR) problems.\nFor arti\ufb01cial data sets, the fraction of variables that are relevant range s from 2 to 10. The initial of\nthe \ufb01rst author are provided as reference: Bk=Bekkerman, Bn=Ben gio, Bt=Bennett, C=Caruana,\nD=Dhillon, F=Forman, G=Globerson, P=Perkins, Re=Reunanen, Ra=R akotomamonjy, Ri=Rivals,\nS=Stoppiglia, T=Torkkola, W=Weston. Please also check the JMLR web site for later additions\nandpreprocesseddata.\na.http://www.kyb.tuebingen.mpg.de/bs/people/weston/l0 ( /lscript0not10)\nb.http://www.clopinet.com/isabelle/Projects/NIPS2001/Arti\ufb01cial.zip\nc.http://nis-www.lanl.gov/ \u223csimes/data/jmlr02/\nd.http://www.rpi.edu/ \u223cbij2/featsele.html\ne.http://www.ics.uci.edu/ \u223cmlearn/MLRepository.html\nf.http://www.cis.hut.\ufb01/research/software.shtml\ng.http://ida.\ufb01rst.gmd.de/ \u223craetsch/data/benchmarks.htm\nh.http://www.nerg.aston.ac.uk/GTM/3PhaseData.html\ni.http://q.cis.uoguelph.ca/skremer/NIPS2000/\nj.http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\nk.http://trec.nist.gov/data.html(FilteringTrackCollection)\nl.http://www.cs.utk.edu/ \u223clsi/\nm.http://www.daviddlewis.com/resources/testcollections/reuters21578/\nn.http://dmoz.org/andhttp://www.cs.utexas.edu/users/manyam/dmoz.txt\no.http://www.cs.technion.ac.il/ \u223cronb/thesis.html\n1178", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "281cb4a9-aae5-4466-bb9b-7222a5d55513": {"__data__": {"id_": "281cb4a9-aae5-4466-bb9b-7222a5d55513", "embedding": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3143df93-4ac1-412a-91f7-7b593a1f4ad2", "node_type": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n8 Conclusion\nThe recent developments in variable and feature selection have address ed the problem from the\npragmatic point of view of improving the performance of predictors. They h ave met the challenge\nof operating on input spaces of several thousand variables. Sophistic ated wrapper or embedded\nmethods improve predictor performance compared to simpler variable ranking methods like corre-\nlation methods, but the improvements are not always signi\ufb01cant: domains with lar ge numbers of\ninput variables suffer from the curse of dimensionality and multivariate metho ds may over\ufb01t the\ndata. For some domains, applying \ufb01rst a method of automatic feature construc tion yields improved\nperformance and a more compact set of features. The methods propose d in this special issue have\nbeentestedonawidevarietyofdatasets(seeTable1),whichlimitsthepossibility ofmakingcom-\nparisonsacrosspapers. Furtherworkincludestheorganizationofab enchmark. Theapproachesare\nvery diverse and motivated by various theoretical arguments, but a unif ying theoretical framework\nislacking. Becauseoftheseshortcomings,itisimportantwhenstartingwithan ewproblemtohave\na few baseline performance values. To that end, we recommend using a line ar predictor of your\nchoice (e.g. a linear SVM) and select variables in two alternate ways: (1) w ith a variable ranking\nmethod using a correlation coef\ufb01cient or mutual information; (2) with a nested subset selection\nmethod performing forward or backward selection or with multiplicative update s. Further down\nthe road, connections need to be made between the problems of variable and feature selection and\nthoseofexperimentaldesignandactivelearning,inanefforttomoveawa yfromobservationaldata\ntowardexperimentaldata,and toaddressproblemsof causalityinference .\nReferences\nE. Amaldi and V. Kann. On the approximation of minimizing non zero variables or unsatis\ufb01ed\nrelations inlinear systems. TheoreticalComputer Science , 209:237\u2013260, 1998.\nR. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. Distributional wor d clusters vs. words for\ntextcategorization. JMLR,3:1183\u20131208 (thisissue),2003.\nA. Ben-Hur and I. Guyon. Detecting stable clusters using principal compo nent analysis. In M.J.\nBrownsteinandA.Kohodursky,editors, MethodsInMolecularBiology ,pages159\u2013182.Humana\nPress,2003.\nY.BengioandN.Chapados. Extensionstometric-basedmodelselection. JMLR,3:1209\u20131227(this\nissue),2003.\nJ. Bi, K. Bennett, M. Embrechts, C. Breneman, and M. Song. Dimensionality r eduction via sparse\nsupportvector machines. JMLR,3:1229\u20131243 (thisissue),2003.\nA.BlumandP.Langley. Selectionofrelevantfeaturesandexamplesinmac hinelearning. Arti\ufb01cial\nIntelligence ,97(1-2):245\u2013271,December 1997.\nB. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin c lassi\ufb01ers. In Fifth\nAnnualWorkshoponComputational LearningTheory ,pages 144\u2013152, Pittsburgh,1992.ACM.\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classi\ufb01cation and Regression Trees .\nWadsworthandBrooks,1984.\n1179", "start_char_idx": 0, "end_char_idx": 3035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3c1f30ce-073e-435a-b1a1-c39b2a511bb6": {"__data__": {"id_": "3c1f30ce-073e-435a-b1a1-c39b2a511bb6", "embedding": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8dd39538-0ef5-4ba6-8c40-1a3d7c53acfe", "node_type": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "text": "GUYON AND ELISSEEFF\nR. Caruana and V. de Sa. Bene\ufb01tting from the variables that variable sele ction discards. JMLR, 3:\n1245\u20131264 (thisissue),2003.\nI. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic featur e clustering algorithm\nfortextclassi\ufb01cation. JMLR,3:1265\u20131287(this issue),2003.\nT. G. Dietterich. Approximate statistical test for comparing supervised class i\ufb01cation learning algo-\nrithms.NeuralComputation , 10(7):1895\u20131924,1998.\nR.O.Duda,P.E.Hart,andD.G.Stork. PatternClassi\ufb01cation . JohnWiley&amp;Sons,USA,2nd\nedition,2001.\nT. R. Golub et al. Molecular classi\ufb01cation of cancer: Class discovery an d class prediction by gene\nexpressionmonitoring. Science, 286:531\u2013537, 1999.\nG.Forman. Anextensiveempiricalstudyoffeatureselectionmetricsfor tex tclassi\ufb01cation. JMLR,\n3:1289\u20131306 (thisissue),2003.\nT. Furey, N. Cristianini, Duffy, Bednarski N., Schummer D., M., and D. Ha ussler. Support vector\nmachine classi\ufb01cation and validation of cancer tissue samples using microarra y expression data.\nBioinformatics ,16:906\u2013914, 2000.\nA.GlobersonandN.Tishby. Suf\ufb01cientdimensionalityreduction. JMLR,3:1307\u20131331(thisissue),\n2003.\nY.GrandvaletandS.Canu. Adaptive scalingfor featureselectioninSV Ms. InNIPS15,2002.\nI. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for can cer classi\ufb01cation using\nsupportvector machines. MachineLearning ,46(1-3):389\u2013422,2002.\nT. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning . Springer series in\nstatistics.Springer,NewYork,2001.\nT. Jebara and T. Jaakkola. Feature selection and dualities in maximum entrop y discrimination. In\n16thAnnualConference onUncertainty inArti\ufb01cialIntelligence ,2000.\nK. Kira and L. Rendell. A practical approach to feature selection. In D. S leeman and P. Edwards,\neditors,International Conference on Machine Learning , pages 368\u2013377, Aberdeen, July 1992.\nMorganKaufmann.\nR. Kohavi and G. John. Wrappers for feature selection. Arti\ufb01cial Intelligence , 97(1-2):273\u2013324,\nDecember 1997.\nD. Koller and M. Sahami. Toward optimal feature selection. In 13th International Conference on\nMachineLearning ,pages 284\u2013292,July1996.\nY. LeCun, J. Denker, S. Solla, R. E. Howard, and L. D. Jackel. Optimal brain damage. In D. S.\nTouretzky, editor, Advances in Neural Information Processing Systems II , San Mateo, CA, 1990.\nMorganKaufmann.\n1180", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "777fde82-0494-41ce-b3fb-6d51ad6b5321": {"__data__": {"id_": "777fde82-0494-41ce-b3fb-6d51ad6b5321", "embedding": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "94efb204-4d56-4835-bdb4-976cb853cf33", "node_type": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nG. Monari and G. Dreyfus. Withdrawing an example from the training set: a n analytic estimation\nof itseffectonanonlinear parameterized model. Neurocomputing Letters ,35:195\u2013201,2000.\nC. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning (to appear) ,\n2001.\nA. Y. Ng. On feature selection: learning with exponentially many irrelevant f eatures as train-\ning examples. In 15th International Conference on Machine Learning , pages 404\u2013412. Morgan\nKaufmann, SanFrancisco,CA, 1998.\nA. Y. Ng and M. Jordan. Convergence rates of the voting Gibbs classi\ufb01e r, with application to\nBayesian featureselection. In 18thInternationalConference on MachineLearning ,2001.\nJ.Pearl.Causality . Cambridge UniversityPress,2000.\nF. Pereira, N. Tishby, and L. Lee. Distributional clustering of English wo rds. InProc. Meeting of\ntheAssociationfor Computational Linguistics ,pages 183\u2013190,1993.\nS. Perkins, K. Lacker, and J. Theiler. Grafting: Fast incremental fea ture selection by gradient\ndescentinfunctionspace. JMLR,3:1333\u20131356 (thisissue),2003.\nA.Rakotomamonjy. VariableselectionusingSVM-basedcriteria. JMLR,3:1357\u20131370(thisissue),\n2003.\nJ. Reunanen. Over\ufb01tting in making comparisons between variable selection me thods.JMLR, 3:\n1371\u20131382 (thisissue),2003.\nI. Rivals and L. Personnaz. MLPs (mono-layer polynomials and multi-layer perceptrons) for non-\nlinear modeling. JMLR,3:1383\u20131398 (thisissue),2003.\nB. Schoelkopf andA.Smola. Learningwith Kernels . MITPress,Cambridge MA,2002.\nD. Schuurmans. A new metric-based approach to model selection. In 9th Innovative Applications\nofArti\ufb01cialIntelligence Conference , pages 552\u2013558,1997.\nH. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. Ranking a rando m feature for variable and\nfeatureselection. JMLR,3:1399\u20131414(this issue),2003.\nR. Tibshirani. Regression selection and shrinkage via the lasso. Technic al report, Stanford Univer-\nsity,PaloAlto,CA, June1994.\nN. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. InProc. of the 37th\nAnnualAllertonConference onCommunication, ControlandComputing , pages 368\u2013377, 1999.\nK. Torkkola. Feature extraction by non-parametric mutual information maximiza tion.JMLR, 3:\n1415\u20131438 (thisissue),2003.\nV.G.Tusher,R.Tibshirani,andG.Chu. Signi\ufb01canceanalysisofmicroa rraysappliedtotheionizing\nradiationresponse. PNAS,98:5116\u20135121,April2001.\nV. Vapnik. Estimation of dependencies based on empirical data . Springer series in statistics.\nSpringer,1982.\n1181", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13a39d60-a203-4461-9bf1-44e24fa3c188": {"__data__": {"id_": "13a39d60-a203-4461-9bf1-44e24fa3c188", "embedding": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8280cf22-1d25-4f46-9402-81cfd5ea7cf1", "node_type": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "text": "GUYON AND ELISSEEFF\nV.Vapnik. StatisticalLearningTheory . JohnWiley&amp; Sons,N.Y.,1998.\nA. Vehtari and J. Lampinen. Bayesian input variable selection using poste rior probabilities and\nexpected utilities. ReportB31, 2002.\nJ.Weston,A.Elisseff,B.Schoelkopf,andM.Tipping. Useofthezero normwithlinearmodelsand\nkernelmethods. JMLR,3:1439\u20131461(thisissue),2003.\nJ. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapn ik. Feature selection for\nSVMs. In NIPS13,2000.\nE.P. Xing and R.M. Karp. Cliff: Clustering of high-dimensional microarray d ata via iterative fea-\nture \ufb01ltering using normalized cuts. In 9th International Conference on Intelligence Systems for\nMolecular Biology ,2001.\n1182", "start_char_idx": 0, "end_char_idx": 701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"2e7028c2-5c20-430d-8e65-00af17767315": {"node_ids": ["834a8280-7fd9-49de-812e-bde76e778847"], "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}}, "5d987162-5c27-47e4-b838-9f15aca03900": {"node_ids": ["1470f118-b9fb-4dc3-9cfc-f788dd90cbad", "e2747987-58e1-4d35-aed9-407e68aae337"], "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}}, "c54f92c1-e152-4fe1-9471-833d434bbabe": {"node_ids": ["3c8d1750-344c-4bf0-9b20-b9be3c9469f0"], "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}}, "d617510b-22b8-4357-85a9-c70722c8ca69": {"node_ids": ["7bbc47b3-73b6-48ea-837b-f59b73db4694"], "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}}, "db90e3a7-24ce-4a17-999f-759de8e8f5fc": {"node_ids": ["a862cbed-d252-45b0-8e1d-8db050ac54e9", "84791bc2-04ae-4892-9ae3-eb6f6639838b"], "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}}, "c3fd3d46-79f2-4a4f-8d03-bbbd915cbb20": {"node_ids": ["9ff9f167-293d-45d4-84bd-975811415565"], "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}}, "af863db7-a11d-4fbc-9a58-1335db1f46e9": {"node_ids": ["9e8cdd9d-df11-4443-8a94-42718a3c741a"], "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}}, "70d875c7-ea8b-4309-93d9-f85a86112dd9": {"node_ids": ["7e3c9eca-552c-49e4-a3c0-d4738f8bb76c"], "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}}, "be06ce58-88b0-4334-b8d7-0afe5d022120": {"node_ids": ["30fc19e0-72b1-4f3d-b1cd-70a4cf0896fd"], "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}}, "a15e0596-6968-4f4f-b1b9-8e136d2d4b7c": {"node_ids": ["0512f07f-0b51-4dbc-937b-9db19a68a0a3"], "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}}, "3bc0631b-1b5b-41c5-9ebe-6c37fe7bbbac": {"node_ids": ["3643e4ba-8ba5-457c-bf87-3d96110eef0f", "7a895568-21c9-45a3-8fbc-0383ae583394"], "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}}, "08ba269e-837f-49f1-ad4b-ba868fab1bb5": {"node_ids": ["105db419-c479-40c5-8d6d-a9485b1154d1", "36eb9d2b-8a2e-4cf0-b3f5-e09a2af5665b"], "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}}, "b7e132c3-5106-4ff8-a7bd-4298ea265edc": {"node_ids": ["d2ec32e0-45fe-491a-9f6e-71ea1944fc41", "cfebf655-8779-4869-b303-23adf65127c3"], "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}}, "63b54ee6-0451-4dc1-b208-b072390222be": {"node_ids": ["5517cdc3-dbf0-4218-a664-f03f4af89bcd", "dbc8eade-e391-48ab-a10e-794380484b0d"], "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}}, "2aad3f0c-29d6-48ec-965b-92fe5fa613be": {"node_ids": ["57b5953e-180f-421e-8ccd-f15ee1f1774b", "15cd62be-5c48-47a9-9102-b19106a910fc"], "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}}, "c1e37f46-8bc9-459f-b4fa-260c3d46ae96": {"node_ids": ["b2af32c3-8ee0-49b7-a7a6-b20c56071acd"], "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}}, "e728b3b4-dcaf-4b87-834b-27ba5c794b18": {"node_ids": ["fefe083b-81f2-444e-a1d1-0d4d8cf12250", "c3c9ccd2-b2c5-4bd3-808e-c500b733abeb"], "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}}, "679f7644-80ce-48c5-b7ab-0bf96a7fdb82": {"node_ids": ["d8228d29-8ee9-4d31-8fd4-c6886919812a", "6145c93f-851e-4099-9416-4f538f8a45b6"], "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}}, "d27aa04e-2383-4475-b5b1-5f317558d9e9": {"node_ids": ["ab841f20-89ab-46b1-a91a-20688dc8aea5", "5a7e856c-4b6f-4b73-9045-a15467ac6b15"], "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}}, "2f2be224-ddf2-48c4-890a-6911df0d2001": {"node_ids": ["ec277da6-33b4-4b23-a506-a7a1013d94de"], "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}}, "31446f05-8076-4744-ac19-6f31e41d4c17": {"node_ids": ["d7e9dcaf-5587-4974-b8d0-85ab6f8372ff"], "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}}, "efbe87bc-394b-4f97-bed3-7e3eaf5c6c92": {"node_ids": ["e3151964-d154-42fe-ae2b-b338bb0f2567"], "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}}, "3143df93-4ac1-412a-91f7-7b593a1f4ad2": {"node_ids": ["281cb4a9-aae5-4466-bb9b-7222a5d55513"], "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}}, "8dd39538-0ef5-4ba6-8c40-1a3d7c53acfe": {"node_ids": ["3c1f30ce-073e-435a-b1a1-c39b2a511bb6"], "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}}, "94efb204-4d56-4835-bdb4-976cb853cf33": {"node_ids": ["777fde82-0494-41ce-b3fb-6d51ad6b5321"], "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}}, "8280cf22-1d25-4f46-9402-81cfd5ea7cf1": {"node_ids": ["13a39d60-a203-4461-9bf1-44e24fa3c188"], "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}}}}