{"docstore/metadata": {"f5b723fc-2a2f-4948-a3b3-1f653cff943c": {"doc_hash": "650d5f0d38950cbe23cf8999d922b5fe11eb8c7a6a128382d0db0ea0e6200310"}, "08c34df3-3bc1-45fe-9bbb-9f588c0d1807": {"doc_hash": "00f8e2f3ae00b5d8b9505292ba2f3c170c6407421f55402abff071eae6ef6104"}, "0c0b507a-870b-4dbe-8711-79f546a86280": {"doc_hash": "4d044a62bbe0ab9cb63fa3f3da4c249157e1417115a99a8c017a61a8fba2359f"}, "5ac57d1d-b8f7-49b6-a56d-e65f186921e0": {"doc_hash": "16607738c46feb2729281ad64320d54d7616c90418c7ec57d4f072461a97b1b9"}, "eae96a5e-8f42-4a52-bfae-1a9e0905f458": {"doc_hash": "a7635fe3b2b42806180a00b9e5b8443847ae6c40a0768a0623454f3937659ff3"}, "8379f36a-f221-43f2-b1fd-ceb2784fdca2": {"doc_hash": "1f90fef31d93f56b1cae303d08eed6e3ec52be026fd567bc051075b23dedc032"}, "9cf803f2-ac0e-4462-8207-dff6f8aec554": {"doc_hash": "592ce880082b8232655d7189c080b73d1ebb6abd1067e2ee96b38a079f39bbc5"}, "904052d6-e23e-4a7a-895d-934074a819dc": {"doc_hash": "d46bc8d7ddcd98800e3829a37273e8fa34a3f7efdd930db3e09f008764a4ba6f"}, "cfc341b8-7c7c-42bd-a397-6656a6719120": {"doc_hash": "72c15fd4f6ca4f253958e308dfc2107e14e17de1a4d175249f9012996582f370"}, "05100dcf-aae1-4774-a8fa-1153b914e00b": {"doc_hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "0285f646-8be1-4699-a3f0-38b1829c02bc": {"doc_hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "18393476-d62c-410e-b17f-f795cc1ec376": {"doc_hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "c8f4ef6c-2bd8-408e-8924-a330d3e0c356": {"doc_hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d"}, "9b071042-368b-4574-916c-56171c9bcede": {"doc_hash": "eb4549aa3fe8970830ceb7ede336adc8a4e61c273f25cdb0e04a9a9db070c96a"}, "919ef11e-1fa6-4961-8a5e-3f2ba143981a": {"doc_hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "77519e60-e254-4414-a4d8-dd26e47c0923": {"doc_hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "701e5445-9e31-4562-aa98-1f374562f74f": {"doc_hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "89236ff8-14bb-4e0d-b565-9320faa60783": {"doc_hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "6a776b20-f761-45dc-8e59-47d70437a52a": {"doc_hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "4fce819b-421d-49b9-b995-40ee1ba0d387": {"doc_hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "6b0ac33d-3b6a-4845-8f1c-8475b84ee395": {"doc_hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "2c4ed6bb-179c-49bf-95fe-8d948b9527d2": {"doc_hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "c35f800b-c539-4a91-ac9d-1e568091bcfc": {"doc_hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "faf94140-9051-486b-bd76-9b19bbc6de76": {"doc_hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "a08f6232-65ec-47b4-8564-c6274517669d": {"doc_hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "1223bf80-dba8-4575-939c-432249e037ac": {"doc_hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500": {"doc_hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "7906c566-aa7c-4424-90f1-0c02cb5faa53": {"doc_hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "551278d9-b81e-4bb4-ad7e-9a814ccd95a8": {"doc_hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d": {"doc_hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "86deeb8e-300b-4588-b9c4-3a595f07a249": {"doc_hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "b3c4f649-1a73-4279-ae68-a261bb02aa64": {"doc_hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f"}, "088ffcb3-8104-413c-9cd0-af1d1c3a7445": {"doc_hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "bab26705-5314-421d-8a00-aa0ceb0d94ea": {"doc_hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "55016afa-ef68-433b-ab36-85af350cde22": {"doc_hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d"}, "702049fa-a8c4-4f11-98b8-f3cdabe21869": {"doc_hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "f3249eb0-b356-4531-9bf2-ade92df3a8d6": {"doc_hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "1a512369-3428-41ba-881b-94bacaafa392": {"doc_hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "24a3dd37-d46d-40cf-a715-1dca69ce6102": {"doc_hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "d4432947-93c5-44a6-9506-440f898f3273": {"doc_hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "41166736-536f-4f15-919b-2bde324e2364": {"doc_hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "f20d19db-543f-4d8e-b038-4715499fb0a0": {"doc_hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "b2dccad2-a69d-41e5-8d00-adc812359e09": {"doc_hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "f96d6ac3-6f73-4e3e-ace1-a77a1d5e771e": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}, "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc": {"doc_hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "9d6bcaba-3b0b-4c08-b13a-7a0f0608a212": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}, "a75edbeb-8b17-4b85-aa05-bf0d6cc84390": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}, "9ce75a46-ef3b-4b59-9f97-770e1270b528": {"doc_hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "de8c06fd-f6b5-4d1c-82fe-63ff05835138": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}, "f8d615b7-a3ff-4a66-9db1-90b424c1f073": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}, "98401388-468a-4828-a610-37ab956db0bb": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}, "469995a2-27b0-416f-9e96-02aa527d4810": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}, "56c156b3-8227-4439-8ae8-9562c6d3cad9": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}, "a4974486-f0b2-4bd0-bc37-2bf4d107babd": {"doc_hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "750fd658-a6f1-4028-8d02-ad5d9ba341b1": {"doc_hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "9c5779f8-061a-4543-8c4d-379530914867": {"doc_hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "09bccbc7-34e7-49f5-8649-6ff13b981a15": {"doc_hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "dead74a5-9ffd-4904-a649-10d6e6408d9e": {"doc_hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "23c228c6-04d0-4fc9-8a8f-f6c3099ee4fd": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}, "a5d2c01a-e43d-4c1f-91de-b2e5430014d5": {"doc_hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "b76235e3-4910-4db8-8057-38c074ddc3dd": {"doc_hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "64cff834-ce6e-4029-8a5d-23c75728c8db": {"doc_hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "ba05e856-739f-476b-a70a-573a120a6cda": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}, "c59942b3-8ef2-4a35-a5b8-7a062d18f18f": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}, "339eb14b-a403-458d-a90b-4f4f00dcf161": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}, "be193238-d2a2-40ca-90f3-8143f602ad96": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}, "8aa0de19-8c9e-41aa-8fbe-b39f4f750174": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}, "f11af456-2b3a-46f6-9412-7df72ef636b4": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}, "a7b71192-b6fa-4f0b-a3bb-3cc7ed73be1d": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}, "8ce4202f-87ab-44fd-adb1-f12bbf48f224": {"doc_hash": "104ac59705109b5284550003d9992372c212dd58b298783696d6849c0953858d"}, "64fa3a3d-c466-495a-b6bc-dd9053dcda10": {"doc_hash": "93f3689c88e1ff4f0020eea9ccc5681671393154fd5359543d173661bbdfecf4"}, "525b571f-a260-4cb8-8ed5-9062dd74803c": {"doc_hash": "33bd3d348d17b5aebac514bb5f2707d537e65c0b043bc2736dd1c2d5adb79eb6"}, "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0": {"doc_hash": "3f897ba4d5dfb7ad5077816d7ff2baaab8c66a160845a5f66e70ea871458ff8d"}, "201136ce-f795-4b86-b23a-4feba505c627": {"doc_hash": "32e89ba79cd3d7f65c08ea7e9b4299ca39b6fed0e3104cad5394d697504473a0"}, "29566d85-e498-4767-8362-36e69e2deea3": {"doc_hash": "f61f3e3c9778509e3846c3a2f90ac4c892461204a1a884cf9de334ce241f6d1d"}, "7be1bbf4-4356-4e23-b140-55c33f115ccc": {"doc_hash": "49cd8355d9fcf6cad605895d221011124b5a0cdc64e3fea4c255779374cdfb19"}, "0a09293c-15d6-4168-a143-4ca043b3e3cc": {"doc_hash": "0e1202c4a357dfd27102ae190e9f3fe898e1efb93807c8e9bcf893997c474aba"}, "f23a4b70-f536-4585-8e92-286c459c5e2d": {"doc_hash": "887d5edcbb82d61768118464ac6fa82b9250b46f5afd5ef3eecfca24fa38708c"}, "c126f3d3-56ed-416e-8c77-7a89015a94a6": {"doc_hash": "3bc7bb504c3058fd6c196a6827450d71b85104bf11c10e74d7e1d86b84940b3d"}, "336a0ff8-9216-4f1c-85b3-20a8b1c529cf": {"doc_hash": "b92aaffce55f627c27c4a8113c82f6525c2b14a72bd43d1561221de33cbc949c"}, "67c6fcd3-9922-45dd-823f-9deac67ac662": {"doc_hash": "8f422dff8972d4662b23bcb9ae92836b69b198606e1c8d82077f51300e539538"}, "b549672b-d944-422c-850a-74fc852af027": {"doc_hash": "12fb114b0340ce9dfe734c30a7165da6be9bae3be920d17df8e84afd8899e9ec"}, "c30060bd-1f1a-4b7e-945e-632aa226a2a4": {"doc_hash": "c7264c918ba517da0707c240271da2a4121956b01590c40699252c5080862af3"}, "68d0bcc1-7d30-4c35-95a9-9fd96e4f52b0": {"doc_hash": "5ef095a76abc85902bec026f149d7b8a45e11091478fd3b963aaf0ed9a77b1c3"}, "c9763d30-6fcc-428f-83c9-3001953fb17b": {"doc_hash": "b977706d20a9f43d1acae6cb85d356fafc2ff8da9c7f6be00d7e702072998e08"}, "211ec4ae-796d-4880-a198-411bd743e0ea": {"doc_hash": "7ef55b24a10dea5000b4b463730db1ab18b95cf2cc81d15a5ea5a5f4e4454ce6"}, "fb9dddcc-38f5-4d24-9501-559b09bbf1f5": {"doc_hash": "942a6d1a99816add767a89c59268a8ef3808fc25f35374d3b0c406d6c03742d5"}, "e31273be-cbcb-42fe-9eb1-eeccd5389112": {"doc_hash": "dbcc8405b1cb0f8ce4aea946e687df4e2ec60951c2ba5d6efed6dd1d155632f3"}, "78e33315-f640-4781-948c-2d3ad5f2baf9": {"doc_hash": "f1c19eb41faa14bb22b940a30d7042674839fc09b3df4c23aa629d42238c8089"}, "b1532b3a-bea4-4655-a624-677381c7e0c3": {"doc_hash": "73933f3d9f02b7e422658df15dc8e35767ad100614497f8afde458dc77afea0a"}, "7c3939ad-4272-44d6-b398-73f8530a3f91": {"doc_hash": "900b56fa5e036826a21f0cbc68a709c59ac3f9ae92204dca5a83bca324b464ae"}, "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926": {"doc_hash": "b5d019d1625b2cdcb7026ba3372aac3acac6bc60da0b5626d22c2150a6fec624"}, "83ed9163-6ee0-44c4-a92d-5978408267f9": {"doc_hash": "cd614389f4f2c79f69253d4e27ad6935474c6df6b88bb7ba40899b707a167684"}, "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d": {"doc_hash": "5f6ae6ecea237871ff51ded158b24da188204472a83ce7ff62e4d47900e4bcb5"}, "4223da55-06cf-450c-9d00-4c56f099f5cf": {"doc_hash": "7103a20d7613ab8679031fc7f77f0f99fca493d9d672091214be4666e9f0d0a9"}, "fd963a5c-8382-4ea1-8afa-b0eb842a4568": {"doc_hash": "8a7d525a286224bcd3711f18bc090578fe7517303ab9ee39c857ff92f67c245a"}, "2d68f85a-81a3-402f-8583-ca18dc3c0682": {"doc_hash": "57d68a2398c3436191050aba730f5b91c9ab6ece58446a8ab1d530da98a9a165"}, "043f98c2-687a-451a-a86b-e0bd2441a38b": {"doc_hash": "f4c1215515138da8413f5723a7fe87a6ad171663266006ab5cfe32a88051d1b2"}, "cf87c3a2-9cf9-4a78-9273-db6fabe1644a": {"doc_hash": "af5ef4556ef45b9de64ad7e7de69ff76ff7f7e251f4294f491d3705401525c3b"}, "db728600-99fc-4a82-ad52-11fd6b0d956d": {"doc_hash": "05c5a701153f616e16bb93fc2b4a1177f4f43c0d4d91fc6e86d85feea420f6d8"}, "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb": {"doc_hash": "09af099d41af035c05ff3c05a210c4d1dca69b2e7eedd9f8728eb1d35645de1c"}, "e1f246f7-342b-4855-be98-1780f90f25b3": {"doc_hash": "ba4a1b2974f64e993c14713306179fd5d5aa04d4cb9210c0c5d1c9682fb12e8e"}, "61be5a83-3321-486a-97d3-5d1c8c693a2d": {"doc_hash": "cf8a133a01e114dee45d0f8735cc091fd55bccc08a51bc92552a95459c264f11"}, "24232b86-b4df-4a0c-ae88-6db8fd096838": {"doc_hash": "6f21f504ff4ee3af67ebcfa3667de0f08fc5cc48b8c5df101bbe9d9c8f34634e"}, "ba8740db-1722-4584-b585-a469555c0ef5": {"doc_hash": "e176fc43d0bf0b89db5ceca9890d74bff97f74cb8375467804ec408c2ff3c1df"}, "3a84ad94-6d87-4774-884a-487f13436df8": {"doc_hash": "96cdebdd0396bfeb5853dd15f9e519723df8fa157fd8c816a5adef617aa9da9a"}, "70f07b21-b7de-41dc-86f4-3cb847800e0f": {"doc_hash": "ee02b9977d6f5a6692d837094d1666b5cdee89a9447e6a660602bab8f42936f6"}, "f298ea32-3817-4de2-958f-5f09c5674ab0": {"doc_hash": "84332997d8ef8036d417ce3e1ff731aff4ffa7966dc3c154d10f2727153dd84d"}, "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc": {"doc_hash": "8bc14240e686c45ddb404313f741c00f6e75e05613474d3dddee9ce90e7f0f51"}, "e22a1468-6f1a-4e87-a5dc-3cbb33449b07": {"doc_hash": "e228c73aefbcdbfb967a28d4cd98e27418c1cbaddf4ae861e178704e70c1aedf"}, "ec1565e1-aa8f-41e8-a78c-1be832aaff8f": {"doc_hash": "d470ee82c8586d5c2e4b273c7b82633773bf045edb92de128d8a944a9ed40234"}, "2dc415f5-1f27-4d16-a0c1-c03f15ad0889": {"doc_hash": "22a1cd4278dc272aea5f97540b84d41d20d05ea398e6e1c1a3bf0f6de8504218"}, "7f77fd31-795f-4704-9908-667c24140881": {"doc_hash": "4f3e015b18ad2102faee39ac423a108bdc964f94965be1b9e912c76ec148ae4e"}, "5a303b42-cd78-408e-b164-0c66108f5b16": {"doc_hash": "82d7672721ca4198f24409c0e3a83c1ed73f2305d05e2c82849bb6e678d65aca"}, "1fcb2e57-dd0b-432a-8d0b-4a85370a6d34": {"doc_hash": "ff2de1fb874161ded9f959eaaa2f7ff05a6c46daa2842c744a0c526602306261", "ref_doc_id": "f5b723fc-2a2f-4948-a3b3-1f653cff943c"}, "58f3250a-3c15-49d8-a6b1-4ce7d7ec8d7d": {"doc_hash": "bad04b7679bbf196172d920dc0acbde562408ae2ec1568ee9d556ce6ede21e48", "ref_doc_id": "f5b723fc-2a2f-4948-a3b3-1f653cff943c"}, "26469a1c-1267-440e-9c92-4d5c000808ce": {"doc_hash": "7176f64cbfe86663754a4d12cd9bb9bc878829a53e77de3da5a9274a1666c5af", "ref_doc_id": "08c34df3-3bc1-45fe-9bbb-9f588c0d1807"}, "15d0316f-b6c7-4c79-808f-68aff8e2b7f8": {"doc_hash": "2840358ee88e0f73cdbff404975da068f9def2a15eeb2dc3fb2a33c649748c10", "ref_doc_id": "08c34df3-3bc1-45fe-9bbb-9f588c0d1807"}, "463e26d5-5bf8-4df1-8310-da982fbafee3": {"doc_hash": "f9c26df367762f3b5955e7bf350fa6f6de200b4b9bbb8528e0f454b72fd23c78", "ref_doc_id": "0c0b507a-870b-4dbe-8711-79f546a86280"}, "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd": {"doc_hash": "e6d7347c2dd72f1f2cedd8a685b96bb9201021538bb8848a9d2bbffaf8b4a99c", "ref_doc_id": "0c0b507a-870b-4dbe-8711-79f546a86280"}, "6930ab06-3b63-4130-a205-a8f9663830b7": {"doc_hash": "3aba50098488617777a5bbb3cfb2597cf458cc862ee429ff1087c0e742ff4731", "ref_doc_id": "0c0b507a-870b-4dbe-8711-79f546a86280"}, "727b5520-6845-40b1-b734-0a38b196bf67": {"doc_hash": "af9de27c0a155201aebcfa5d1efa482055037b503d5f06b59f4fac241068f820", "ref_doc_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0"}, "0c4d7c05-41d8-415d-9814-2916239e0a89": {"doc_hash": "6efef63ce1c601cfc462a4b9172f8b90fe0ae1ebdcdc7239c409a038f50a1cbb", "ref_doc_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0"}, "8bddc558-f028-4026-814c-7f84ef496937": {"doc_hash": "644ca6ff5f22d5f692253bff6e150a91fdc67205473bf2beae8c0ae0eee6e43f", "ref_doc_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0"}, "63c546c7-41a0-49fc-9c62-23f2a98a1ec6": {"doc_hash": "f1ec381cb46ffbc86303abc2ff64b0373ae3a3e2e37fde0694b264a3a0926ce8", "ref_doc_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458"}, "829c1fcf-275d-4e8f-9680-c71989401abe": {"doc_hash": "b3332f9c350242900582e14d7f504061a0285906499c4e3983ffc04fa6f0d6ff", "ref_doc_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458"}, "91ec94eb-0a66-42fc-bb91-773947f3a7b1": {"doc_hash": "a761ca40d599e32958c9d36c43c49caf3adcf2a3a5ad7b9019678234364fef6b", "ref_doc_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458"}, "25c627e2-0801-4294-a18b-157a0edf0b48": {"doc_hash": "6def2e744f0851283d6eb8dde1688e229ea4dda69b1da757ffe19f67e81a759d", "ref_doc_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2"}, "2946aa2b-e791-47ea-b86e-9cb9ce6eea43": {"doc_hash": "3c5ad694f290f4385da1a577705dc1d79ce4f01c9a94cc27a98b00b12d2663b6", "ref_doc_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2"}, "c4b7764a-daea-409f-ae9d-701d43e8cbf9": {"doc_hash": "b0da5e89929a0f009998f79cf92e8e44873b41cdf029ec550411b30ce4f720b0", "ref_doc_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2"}, "0fec8e8d-56cf-4489-8de0-b9b7a6028d34": {"doc_hash": "bf019081944e9b0eff0198394b851875d53de0000d88cde1c579a8de7c50687e", "ref_doc_id": "9cf803f2-ac0e-4462-8207-dff6f8aec554"}, "424857bc-8bdc-4854-983b-0d9c8c3b7da6": {"doc_hash": "09e68ad0705ff06826f3ef9eb73a4795ae606703dc25694bd81535dbe634c30a", "ref_doc_id": "9cf803f2-ac0e-4462-8207-dff6f8aec554"}, "2b6644d8-c129-402d-a4f4-f3e127145b3d": {"doc_hash": "c0af2dc69ebfeb46f21f4a6309e6f5a8382c38c9528278465d05cf7b42cc2284", "ref_doc_id": "904052d6-e23e-4a7a-895d-934074a819dc"}, "a9429596-81c8-4c43-8b73-a125392a8be6": {"doc_hash": "d6a5bb57608b50ffd73a88497608986431dfe406042203ef8251fbb662b1eaa0", "ref_doc_id": "904052d6-e23e-4a7a-895d-934074a819dc"}, "5abf8005-f0f5-4f14-aa1f-f1e79bd64672": {"doc_hash": "434c5b635ce63c64e77f775c23e7fb587bb2f699f4985e631ae8fc7ae26f63a4", "ref_doc_id": "cfc341b8-7c7c-42bd-a397-6656a6719120"}, "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d": {"doc_hash": "61cb705ddc2895cea945c7a4391b65c609ce7d933ea68c540e04e37d530499c8", "ref_doc_id": "cfc341b8-7c7c-42bd-a397-6656a6719120"}, "cbca2dff-8b71-46ca-8612-3a76543a55f9": {"doc_hash": "5a74d703ba40118eb7ccf6dfd5bcf64dd8fbbe1dfdf86a823f1ee875b081adf3", "ref_doc_id": "cfc341b8-7c7c-42bd-a397-6656a6719120"}, "c8c42a8c-dd74-4138-af03-344135c7165a": {"doc_hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236", "ref_doc_id": "05100dcf-aae1-4774-a8fa-1153b914e00b"}, "fd983614-a094-4a87-abbd-4dfe91740d3d": {"doc_hash": "0c928a8294d4358629697539a5977c257cbe0ffba4ecda86a2fe37f39803eca8", "ref_doc_id": "05100dcf-aae1-4774-a8fa-1153b914e00b"}, "962f9d3f-47d2-4b75-80d2-c6e383155097": {"doc_hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f", "ref_doc_id": "0285f646-8be1-4699-a3f0-38b1829c02bc"}, "cb9c835c-b4cb-46f9-9190-781b786f60c6": {"doc_hash": "6d4aa1864edb5c87fdc00c976c5fc7f7e86f40d01ace69e99a9fe207f6a1fd1e", "ref_doc_id": "0285f646-8be1-4699-a3f0-38b1829c02bc"}, "85703d6a-0930-4931-8f74-2ee7d93f5bd5": {"doc_hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5", "ref_doc_id": "18393476-d62c-410e-b17f-f795cc1ec376"}, "0dee2263-8357-4a25-b57d-59371e5a7f68": {"doc_hash": "1217385778dbc19d0fbff61e0eeb33a019196a9da911aa415b0fd0a92c797588", "ref_doc_id": "18393476-d62c-410e-b17f-f795cc1ec376"}, "5555f92c-7851-49e7-8f37-6e1e27ed8519": {"doc_hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d", "ref_doc_id": "c8f4ef6c-2bd8-408e-8924-a330d3e0c356"}, "353d37da-bb3c-447d-b3d5-5274abeb6317": {"doc_hash": "493f7df431511224349c07bd3d8f2485415412f03d4482c9997130b8f9b2065a", "ref_doc_id": "9b071042-368b-4574-916c-56171c9bcede"}, "b2203988-5ab2-4c03-8306-0deea9280a12": {"doc_hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a", "ref_doc_id": "919ef11e-1fa6-4961-8a5e-3f2ba143981a"}, "cfd6fe0d-bc37-4e17-be7b-c55d3fff9cde": {"doc_hash": "3fa99ab3db4dcf95cc276b945ad3d13dc0cac7ac894a33a6d0866eb5d9d60e96", "ref_doc_id": "919ef11e-1fa6-4961-8a5e-3f2ba143981a"}, "946381a4-0c98-48f8-95af-2a5a5d4bcafe": {"doc_hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4", "ref_doc_id": "77519e60-e254-4414-a4d8-dd26e47c0923"}, "6aa5575b-33d5-4c9f-a765-f8a9516cbed0": {"doc_hash": "3f5c7db8c90488be866bb0e0fa9c83c03343d6e4570e9a11c0458dff517910d2", "ref_doc_id": "77519e60-e254-4414-a4d8-dd26e47c0923"}, "25996ad9-fc2c-4de5-88b5-e76be56d489e": {"doc_hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef", "ref_doc_id": "701e5445-9e31-4562-aa98-1f374562f74f"}, "00ae5058-315c-4129-89c9-019e4da2e62f": {"doc_hash": "f4e77c46edf379b161f3cdc5fa533264e9a9b420440624a1768a2d6d39ff1751", "ref_doc_id": "701e5445-9e31-4562-aa98-1f374562f74f"}, "6e0bed58-196c-48cf-9fd7-2cea388f8377": {"doc_hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2", "ref_doc_id": "89236ff8-14bb-4e0d-b565-9320faa60783"}, "63936964-2624-4e35-9380-a5b697e69a19": {"doc_hash": "b906b56c9733c2eb9ccd4d7cff132330e7d1c92e12db643ea94015f5538a9661", "ref_doc_id": "89236ff8-14bb-4e0d-b565-9320faa60783"}, "afcc21c7-378b-4a88-9173-0ba1e0f53b3c": {"doc_hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d", "ref_doc_id": "6a776b20-f761-45dc-8e59-47d70437a52a"}, "83c51dca-eaa2-46c6-91dd-40b7f5e000b8": {"doc_hash": "737d61d54f7037e93d50b154c805df3d32db1c3d895e19d75825637620285cf0", "ref_doc_id": "6a776b20-f761-45dc-8e59-47d70437a52a"}, "506d1584-d7bb-41fd-9985-e2bc26359c10": {"doc_hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6", "ref_doc_id": "4fce819b-421d-49b9-b995-40ee1ba0d387"}, "ef0c1f7c-31c4-478d-8778-1ea866949bf8": {"doc_hash": "b90fba686d681124163d2b53f31c1bdaeed47296cdb5e1cfcef57df6b3c7fb7f", "ref_doc_id": "4fce819b-421d-49b9-b995-40ee1ba0d387"}, "8a3dc5b0-d106-49a9-b6e4-1b195fde88a5": {"doc_hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c", "ref_doc_id": "6b0ac33d-3b6a-4845-8f1c-8475b84ee395"}, "3f2f4ede-3176-49ec-b114-961912cc699c": {"doc_hash": "4ff09ae5ca032bf8fc345244862cc9d54eb92331aa373eb0d4ef2f341bf77ec0", "ref_doc_id": "6b0ac33d-3b6a-4845-8f1c-8475b84ee395"}, "b7d98762-2f56-4370-b6be-ce980d1ec451": {"doc_hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b", "ref_doc_id": "2c4ed6bb-179c-49bf-95fe-8d948b9527d2"}, "35f0cb4a-44da-4a09-a945-5903a16263e6": {"doc_hash": "97b648859bc2a226adf9017753a4e605f4ebf8adff469f8c650ac28c874667b0", "ref_doc_id": "2c4ed6bb-179c-49bf-95fe-8d948b9527d2"}, "060d849e-db0f-46ab-af16-6cbf6f335f10": {"doc_hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38", "ref_doc_id": "c35f800b-c539-4a91-ac9d-1e568091bcfc"}, "d0103b5f-a473-4495-a086-1256179309af": {"doc_hash": "2769b2877636983ba4a919a012e6ff35a0de00417b639595b63e544232e239ed", "ref_doc_id": "c35f800b-c539-4a91-ac9d-1e568091bcfc"}, "e45e714b-bb58-428c-8583-971665ec4fc5": {"doc_hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46", "ref_doc_id": "faf94140-9051-486b-bd76-9b19bbc6de76"}, "d0a1f3ca-4ae2-471f-a970-47b832c5f4f2": {"doc_hash": "8b8702fdaf808caaa4a3954d598d6bf6de9c4edc5ea0e5cb467d5feece2e0d0e", "ref_doc_id": "faf94140-9051-486b-bd76-9b19bbc6de76"}, "c38c6520-a97c-4a99-bdb9-5548c7fc6665": {"doc_hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b", "ref_doc_id": "a08f6232-65ec-47b4-8564-c6274517669d"}, "b4ebd8f0-dbbc-4820-9b83-a51aff565db9": {"doc_hash": "e71746930c2e328fb173ed955569eacad948b4761ea723327bf0f6ca538223b4", "ref_doc_id": "a08f6232-65ec-47b4-8564-c6274517669d"}, "15525363-3d90-43cd-a223-601934df3bcf": {"doc_hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0", "ref_doc_id": "1223bf80-dba8-4575-939c-432249e037ac"}, "f96e73ad-9c95-49fa-8f50-e3e7b617fa03": {"doc_hash": "94fddf28ce654e61aa9b5e59bf760a85075bff4bb9e2023501af9089e49e3567", "ref_doc_id": "1223bf80-dba8-4575-939c-432249e037ac"}, "532cb39b-74f8-401f-ad08-4af5fe2da04e": {"doc_hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead", "ref_doc_id": "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500"}, "39ea1c51-14b2-4f40-8115-117c27a1f724": {"doc_hash": "7efa399dff6278a0cc6a664985a863da9d8ac889c280b567406da66b348811ca", "ref_doc_id": "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500"}, "01813f96-3e63-47ed-8779-f0ca8a50b3db": {"doc_hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c", "ref_doc_id": "7906c566-aa7c-4424-90f1-0c02cb5faa53"}, "99e41b42-5242-4e72-8c10-73986a9a27f1": {"doc_hash": "191ed8bc2708d7eaf6cd911e79488cd5dd96e34229f0e76856c065df0d4f0738", "ref_doc_id": "7906c566-aa7c-4424-90f1-0c02cb5faa53"}, "a71a76c7-1e7b-4e8c-b48c-c39635de9755": {"doc_hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b", "ref_doc_id": "551278d9-b81e-4bb4-ad7e-9a814ccd95a8"}, "dd17a6f4-7ea6-4a76-a6c6-7fa3e8a6629f": {"doc_hash": "b6b29f8cb0a6a1d57cfb05647a5fb8bea427f30613b6fd3e4adda298d8892b33", "ref_doc_id": "551278d9-b81e-4bb4-ad7e-9a814ccd95a8"}, "c7c1381a-a188-4635-ac50-fc52a6c4541b": {"doc_hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487", "ref_doc_id": "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d"}, "58dfe800-c747-4d60-97bd-beddd57d59f1": {"doc_hash": "3fd6f5f7a68261dfb98c13d000ce8f6ae5e5f7733e0855b8381bb67b7294208f", "ref_doc_id": "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d"}, "dd3a61aa-aa78-4316-9ef8-3bf28352765b": {"doc_hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9", "ref_doc_id": "86deeb8e-300b-4588-b9c4-3a595f07a249"}, "18a240bd-af7f-43c0-a008-f23c6eae1915": {"doc_hash": "2d315cb4c8018dfb1779a7fe4b3d7fabf24dde95b43dd77e9423a39cd742925b", "ref_doc_id": "86deeb8e-300b-4588-b9c4-3a595f07a249"}, "335b5d31-c8fa-4764-a2fc-2c7235adf685": {"doc_hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f", "ref_doc_id": "b3c4f649-1a73-4279-ae68-a261bb02aa64"}, "fb4ec6a4-a3ae-418b-84b5-4e6f9d38d8f3": {"doc_hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b", "ref_doc_id": "088ffcb3-8104-413c-9cd0-af1d1c3a7445"}, "ae0974cb-1360-421d-aa06-4fa9fd539828": {"doc_hash": "eab61a3759eb69baf37d87d0da47fa8f71f5e24845eba382b9e927e3655e3e5f", "ref_doc_id": "088ffcb3-8104-413c-9cd0-af1d1c3a7445"}, "2743dcc2-71bb-4d6d-af3f-0f9e878efdb3": {"doc_hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba", "ref_doc_id": "bab26705-5314-421d-8a00-aa0ceb0d94ea"}, "40085889-33eb-4c53-9be0-99b3346eb316": {"doc_hash": "62353d53f7e685c0db92ab6a0a879d06caa8554d7086b9d297210a11e77d20c3", "ref_doc_id": "bab26705-5314-421d-8a00-aa0ceb0d94ea"}, "15e4f284-42a6-43ad-9dab-4c7e823b1ad9": {"doc_hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d", "ref_doc_id": "55016afa-ef68-433b-ab36-85af350cde22"}, "c1af0d32-7d3c-4e55-a822-953be8f57a9e": {"doc_hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394", "ref_doc_id": "702049fa-a8c4-4f11-98b8-f3cdabe21869"}, "b13b3430-9d01-4bab-bbe1-075bee6e9263": {"doc_hash": "fc5bef94211ae1dafe9dfc6cd28b0c001b047ed9145ae0f6249ccd8c58b5cbf4", "ref_doc_id": "702049fa-a8c4-4f11-98b8-f3cdabe21869"}, "67defbe9-d008-41d5-ba59-33112621edca": {"doc_hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334", "ref_doc_id": "f3249eb0-b356-4531-9bf2-ade92df3a8d6"}, "f54b8e69-215b-4141-a228-78566b779997": {"doc_hash": "7509af604895c8619373d8549c1d9aa022ada9622c1938abebae728637d9699e", "ref_doc_id": "f3249eb0-b356-4531-9bf2-ade92df3a8d6"}, "cc770eee-2b46-49c3-b4f9-cfd34c9e0bb5": {"doc_hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f", "ref_doc_id": "1a512369-3428-41ba-881b-94bacaafa392"}, "4517a38b-d3fb-47cd-a1e9-42bf07b04f51": {"doc_hash": "1c1d5e0391af521e3bbad2265e841974f49a0747185829f6e4cd8c359c102f25", "ref_doc_id": "1a512369-3428-41ba-881b-94bacaafa392"}, "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4": {"doc_hash": "b431683b355c46a836cc849285d0f6ea68603b7b9562c3d6fe9485f24eb2d1d0", "ref_doc_id": "1a512369-3428-41ba-881b-94bacaafa392"}, "810113cd-21c4-4998-991c-be7e184bb99d": {"doc_hash": "1cd7251ada1dee452caebd4791b0ee767e2ca3bf17f1d25fad94deff946d00d6", "ref_doc_id": "1a512369-3428-41ba-881b-94bacaafa392"}, "ae71050f-527b-4443-a07a-b5f231c8e5d7": {"doc_hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737", "ref_doc_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102"}, "8446f964-cbd4-4078-8677-8cf38f63f83e": {"doc_hash": "24991b881fe6f7bc872d17b93f2e1ce5bbe684c10780b04b2f7ab273d6567537", "ref_doc_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102"}, "97664fa0-9070-4279-9f21-065320de12c7": {"doc_hash": "96aa01133ac86a98be7ec1a9af3f0a87bc64c63ddb2cc0933fb2c4b33419ce20", "ref_doc_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102"}, "d630ecdd-40eb-4969-b37e-c977e1cd9d17": {"doc_hash": "268b1d9f30d597a60011fb647fa326a7624baa6ff21ea50b021a7acd29920ac0", "ref_doc_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102"}, "47bd0dde-315d-489d-ab4d-fac58b5b7527": {"doc_hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491", "ref_doc_id": "d4432947-93c5-44a6-9506-440f898f3273"}, "41b4e924-ed5e-42db-a942-13397fba5eef": {"doc_hash": "024de7916ef0c21911a1e13244c30f772c231b50272374313bf78e97a5de0815", "ref_doc_id": "d4432947-93c5-44a6-9506-440f898f3273"}, "2066e1f2-0af8-47e7-8dd9-0fe23c989ace": {"doc_hash": "cdb815738c0be0bfff719b14516ba743bf55a850cc7f4a79a188d0f551f76b4a", "ref_doc_id": "d4432947-93c5-44a6-9506-440f898f3273"}, "014cd91d-0ed6-4168-aab3-68a1d818a97f": {"doc_hash": "8c81f81d1782c54b1db102338c9ba9f5ab57865b3b0db2f726418f6268e47cb4", "ref_doc_id": "d4432947-93c5-44a6-9506-440f898f3273"}, "33599ae9-74bc-484d-8322-7cc5901e47fd": {"doc_hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7", "ref_doc_id": "41166736-536f-4f15-919b-2bde324e2364"}, "040d2a55-9557-4afb-b016-49b0c056b485": {"doc_hash": "9fbb62f2620c75edc2dcf0b8d1be4e66b79c72d606a4858bd5326e4be9e6a65d", "ref_doc_id": "41166736-536f-4f15-919b-2bde324e2364"}, "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8": {"doc_hash": "a3c10ef2dd766733a67071d28b495ffe6420400f50a85fdb1e4b1357dfaac4a5", "ref_doc_id": "41166736-536f-4f15-919b-2bde324e2364"}, "398aac6d-f3ec-4a24-955b-3efea64e7cb9": {"doc_hash": "c7c19ed2aabd602e26d6a8ce44bfefd678d9adaf541cabf55fe55bf5d310d6f2", "ref_doc_id": "41166736-536f-4f15-919b-2bde324e2364"}, "968838ad-f5fc-4a41-99f3-062c6abed929": {"doc_hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7", "ref_doc_id": "f20d19db-543f-4d8e-b038-4715499fb0a0"}, "87f6e9dc-a389-4d72-857e-ba0221e36cdb": {"doc_hash": "3483267210b60f2f0b11d4b78863afe4c8c7c7721a5d3dcdd51b052da437a0fb", "ref_doc_id": "f20d19db-543f-4d8e-b038-4715499fb0a0"}, "5dcc2ded-551c-4808-8bba-58debc0e37c8": {"doc_hash": "b577a0ce16d6b1d2c533bccd72250c444b030020c41878d5a3905a02da17acb9", "ref_doc_id": "f20d19db-543f-4d8e-b038-4715499fb0a0"}, "7a7c5737-1d34-46b6-87dc-29d12d88bcb4": {"doc_hash": "f7e62202ae4994ad9982e3d1bf85ab302af545267a33f57e8119c681adb4d0be", "ref_doc_id": "f20d19db-543f-4d8e-b038-4715499fb0a0"}, "fa30bdbf-604b-4ce6-ae49-75bfa61db9c4": {"doc_hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac", "ref_doc_id": "b2dccad2-a69d-41e5-8d00-adc812359e09"}, "11b43a7e-b2cb-4edb-854f-234a6e38fb90": {"doc_hash": "b143a149bccb0c99f5aa8380310d5dc29001d829a439bdcf4fc642b1f2e377a3", "ref_doc_id": "b2dccad2-a69d-41e5-8d00-adc812359e09"}, "0f6bafeb-afa6-488b-b63f-fd4971a1f0bb": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "ref_doc_id": "f96d6ac3-6f73-4e3e-ace1-a77a1d5e771e"}, "8a96cdbe-16ea-487c-aa14-7d502b1f171e": {"doc_hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "ref_doc_id": "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc"}, "8ac67088-1023-4df3-9a7d-baf022cf4b90": {"doc_hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58", "ref_doc_id": "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc"}, "35ab5ff0-9e87-4d1b-9502-358109c33b21": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "ref_doc_id": "9d6bcaba-3b0b-4c08-b13a-7a0f0608a212"}, "95340ff0-7c82-453e-bcf4-dea30b5beb85": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "ref_doc_id": "a75edbeb-8b17-4b85-aa05-bf0d6cc84390"}, "715b60c0-0a1f-4154-ae38-7ed6b53d77fb": {"doc_hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "ref_doc_id": "9ce75a46-ef3b-4b59-9f97-770e1270b528"}, "5d061b8c-a9a9-4c63-9046-778e3d59773d": {"doc_hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb", "ref_doc_id": "9ce75a46-ef3b-4b59-9f97-770e1270b528"}, "6062226d-a498-404a-ad40-71d80ab1339c": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "ref_doc_id": "de8c06fd-f6b5-4d1c-82fe-63ff05835138"}, "a700928c-a33b-44d5-a855-4bf3702f94f1": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "ref_doc_id": "f8d615b7-a3ff-4a66-9db1-90b424c1f073"}, "1501de03-1484-4e57-b4af-bbf848b1012c": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "ref_doc_id": "98401388-468a-4828-a610-37ab956db0bb"}, "6c5d0b69-48fb-4116-a3a7-180340fc4f6a": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "ref_doc_id": "469995a2-27b0-416f-9e96-02aa527d4810"}, "deb80a10-df07-48b3-905c-313c4bb4a014": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "ref_doc_id": "56c156b3-8227-4439-8ae8-9562c6d3cad9"}, "e47ab53c-3b04-43e5-af6b-0ba1a3fce25f": {"doc_hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "ref_doc_id": "a4974486-f0b2-4bd0-bc37-2bf4d107babd"}, "4d33b75c-78ed-4dbd-abb7-88a2110a240a": {"doc_hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de", "ref_doc_id": "a4974486-f0b2-4bd0-bc37-2bf4d107babd"}, "8259bb22-fab5-404b-80f1-36527104ad33": {"doc_hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "ref_doc_id": "750fd658-a6f1-4028-8d02-ad5d9ba341b1"}, "7e4bddcf-7e51-49e1-bf41-df22b57639b0": {"doc_hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce", "ref_doc_id": "750fd658-a6f1-4028-8d02-ad5d9ba341b1"}, "9b28bfdc-5df9-474a-96cd-f349ffc9c5fd": {"doc_hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "ref_doc_id": "9c5779f8-061a-4543-8c4d-379530914867"}, "1b02b20c-795f-4a43-8df6-cad472d2f30a": {"doc_hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2", "ref_doc_id": "9c5779f8-061a-4543-8c4d-379530914867"}, "1cb99a2b-7b48-48b8-8439-eb2fb79c0d2f": {"doc_hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "ref_doc_id": "09bccbc7-34e7-49f5-8649-6ff13b981a15"}, "746912eb-1d84-407a-9e3c-ad81a75bd7c6": {"doc_hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1", "ref_doc_id": "09bccbc7-34e7-49f5-8649-6ff13b981a15"}, "48688efd-4bc2-4ece-b55b-1f4481669419": {"doc_hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "ref_doc_id": "dead74a5-9ffd-4904-a649-10d6e6408d9e"}, "c42d5026-9e0a-406c-847e-d1a8a1355344": {"doc_hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be", "ref_doc_id": "dead74a5-9ffd-4904-a649-10d6e6408d9e"}, "abb82a42-5205-4f94-bc08-c4cbcc1b2749": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "ref_doc_id": "23c228c6-04d0-4fc9-8a8f-f6c3099ee4fd"}, "229f52d5-1f75-4cda-8162-598284efcdce": {"doc_hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "ref_doc_id": "a5d2c01a-e43d-4c1f-91de-b2e5430014d5"}, "c6f518ae-f8e9-49ea-b4bc-d1d9d8e66726": {"doc_hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039", "ref_doc_id": "a5d2c01a-e43d-4c1f-91de-b2e5430014d5"}, "0bcc64a5-3ea5-4b95-b1ae-6815aa0b3b13": {"doc_hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "ref_doc_id": "b76235e3-4910-4db8-8057-38c074ddc3dd"}, "7fab722e-f56f-452f-a4b5-58120c90b568": {"doc_hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5", "ref_doc_id": "b76235e3-4910-4db8-8057-38c074ddc3dd"}, "480b6e04-3058-4973-869a-cca9762bab88": {"doc_hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "ref_doc_id": "64cff834-ce6e-4029-8a5d-23c75728c8db"}, "ecb700cc-7c50-4360-915d-7f8a4e083b97": {"doc_hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b", "ref_doc_id": "64cff834-ce6e-4029-8a5d-23c75728c8db"}, "53ae76ee-6129-46da-a9cc-b4fabcd382ec": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "ref_doc_id": "ba05e856-739f-476b-a70a-573a120a6cda"}, "dbbfe5ca-b997-4453-b8ee-d3d0e0ff0b00": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "ref_doc_id": "c59942b3-8ef2-4a35-a5b8-7a062d18f18f"}, "92d4df26-a2ec-4d9e-9b45-2a02ece8cfc1": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "ref_doc_id": "339eb14b-a403-458d-a90b-4f4f00dcf161"}, "83c1b6d7-9928-453c-8262-dda07c18dc9c": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "ref_doc_id": "be193238-d2a2-40ca-90f3-8143f602ad96"}, "052f6a97-a7a9-4e53-aa2a-e0483aeaeef6": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "ref_doc_id": "8aa0de19-8c9e-41aa-8fbe-b39f4f750174"}, "a84e2c8d-9a22-4795-b8a5-f7b792c977a8": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "ref_doc_id": "f11af456-2b3a-46f6-9412-7df72ef636b4"}, "6c494711-d91a-4721-b6fb-f659de0de062": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "ref_doc_id": "a7b71192-b6fa-4f0b-a3bb-3cc7ed73be1d"}, "2fc61fe5-9dfc-41ca-a1ac-ba23017d1e12": {"doc_hash": "eb60c61f994952055de76f552edc2f9989b77bd49fecc79cf15a96d78d9d8243", "ref_doc_id": "8ce4202f-87ab-44fd-adb1-f12bbf48f224"}, "de0c5b40-7b1d-42ab-9788-debfe1e85b78": {"doc_hash": "4a18f93534a475a895f626d968d2298b2e1b3aa7a296ac93f718cef7f8320b48", "ref_doc_id": "8ce4202f-87ab-44fd-adb1-f12bbf48f224"}, "d0222da8-239c-4dad-89e0-9e8aeb695470": {"doc_hash": "f4db57d9ddc8b459af52b0397cf6e6bdf2e216e56de9fb8897cb181beebe888a", "ref_doc_id": "64fa3a3d-c466-495a-b6bc-dd9053dcda10"}, "5e19d9ae-f851-42d2-80a0-575113f15109": {"doc_hash": "918bd859ea8f64e487260d64e86d8c17f6a1d47fd510b1115c891271e7c795be", "ref_doc_id": "64fa3a3d-c466-495a-b6bc-dd9053dcda10"}, "ffa40d35-addb-4215-ade5-40d5e99a0794": {"doc_hash": "61ad54c4a9603417b7f1e08551a136780c972d1946ee16efb9957af14b9853a7", "ref_doc_id": "525b571f-a260-4cb8-8ed5-9062dd74803c"}, "cea5f0f6-713c-4a98-bf4a-c10a71482825": {"doc_hash": "5614f142aba9f125fd98bdfe760bc99123202c50edeb74ac5ba6ca31dd4d34bf", "ref_doc_id": "525b571f-a260-4cb8-8ed5-9062dd74803c"}, "8e830824-b646-4a31-b24f-24d469085926": {"doc_hash": "4515d5a11a7e7677bbe1b76e1caca290c14d3ebbf45e33118ccd29a055ffe7b7", "ref_doc_id": "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0"}, "1c5d8e2d-84d7-4ee1-8b44-b0c67a863f77": {"doc_hash": "f8ee67091c90178b09696bd9d2808082479942379b294bf28ed28c072d287bf6", "ref_doc_id": "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0"}, "779ac90b-690b-4dea-86d1-8cc1e795440a": {"doc_hash": "4190f466f7189d4d4ddccdffaf47d8d97e02c71304e2e31c611ef0e4260b1a22", "ref_doc_id": "201136ce-f795-4b86-b23a-4feba505c627"}, "31eae50a-01e2-472e-b1a7-ac5c75cf5ab3": {"doc_hash": "c9faa28545a0494dbcbe9b84539dee41140168a475c4c1fb976731ac4199cf4a", "ref_doc_id": "201136ce-f795-4b86-b23a-4feba505c627"}, "5846ae96-8da0-4508-9076-4ea967c70230": {"doc_hash": "f7bb5f7701f2bcd1364941cf6e9921f61e90068cf4a5846ca6a897ad5894817f", "ref_doc_id": "29566d85-e498-4767-8362-36e69e2deea3"}, "e58ca118-5200-446c-887a-ef2a97b870f0": {"doc_hash": "26628e28b99239bc04f7230fb0f496c2de78633d0641a2b66c630cc3b7318d2a", "ref_doc_id": "29566d85-e498-4767-8362-36e69e2deea3"}, "6f25503e-9d81-4dcc-8386-09da9b2b1eda": {"doc_hash": "91bf55c5931d26573b645630dfca0651ce5476ba531ca8ed24d51467422d2d0d", "ref_doc_id": "7be1bbf4-4356-4e23-b140-55c33f115ccc"}, "05771a4e-97d9-47cc-8f67-ce38a1288bb5": {"doc_hash": "78ca13cf0c22874a93220907b68a243566f29253a73ce81626f1880fe4dc37e3", "ref_doc_id": "7be1bbf4-4356-4e23-b140-55c33f115ccc"}, "565a65e4-4f3c-45d5-9edb-40c45f9f8dd6": {"doc_hash": "8ad4941f434543d0cd1bf39c3a3842bc28376f71de3ab057f7452b62bdfc4bcf", "ref_doc_id": "0a09293c-15d6-4168-a143-4ca043b3e3cc"}, "c3258fa9-5cd8-40ec-88c5-6397a1295197": {"doc_hash": "4d7a7dceab210ab5b79ab969d8a45f7e0fc3a43f75ca0ee8fe7ff7255237276b", "ref_doc_id": "0a09293c-15d6-4168-a143-4ca043b3e3cc"}, "3c789972-a947-411a-b5da-c0d09680da8a": {"doc_hash": "3723a20064ca76a3e869fbea2e861d8399e9c3e68dcd98d611eb282b14d0899e", "ref_doc_id": "f23a4b70-f536-4585-8e92-286c459c5e2d"}, "8c22712d-644d-4209-bc65-6b52fa516820": {"doc_hash": "6fc62fcb533523c5b7b1a46e94dfb38676c6114caabebe47e46a8ea7b15a2bd7", "ref_doc_id": "f23a4b70-f536-4585-8e92-286c459c5e2d"}, "2d1a2adc-5071-43eb-bf4a-410bd174bed1": {"doc_hash": "6aaaab140ee93dfdfadc24c9b19be17f1a88f3f122faf5eda4fec2b52a87fb12", "ref_doc_id": "c126f3d3-56ed-416e-8c77-7a89015a94a6"}, "d4de6415-2189-485d-b0d2-f6a67a303dac": {"doc_hash": "0852ca76e6a8cd6836ec76925592ebd53fcb5c3f5d732e186e9c0074e55c1dcc", "ref_doc_id": "c126f3d3-56ed-416e-8c77-7a89015a94a6"}, "7dac650b-20b9-44d0-a421-b4163e55cdc5": {"doc_hash": "4b546abd43627433ebca10d8e88d23b41d0d679726cbf4f57e77a9e9176c52be", "ref_doc_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf"}, "6a7e5d01-af71-4c39-9478-d85c81c048d9": {"doc_hash": "644a94a186a6435ae4e087272288546423ce3e76cb3d38fe109e83d85f377d83", "ref_doc_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf"}, "c8943b4b-2f25-4092-8631-2b31b2e8be9f": {"doc_hash": "cc1394e56ffe3a12753d910d28559e7dab38cf8b5dae736379583f655b5bb96c", "ref_doc_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf"}, "ddedcbde-18dc-40db-a01d-4bd767f7bb23": {"doc_hash": "ec4927860db5901b82b2d4f911b9719282775477102caeca890067dd4e47e089", "ref_doc_id": "67c6fcd3-9922-45dd-823f-9deac67ac662"}, "06da36df-7d8e-46c0-9ca0-d9715428a94d": {"doc_hash": "1e9b3a428beffae5ebf27545c979331d245570ef19594149f6058268208682ba", "ref_doc_id": "67c6fcd3-9922-45dd-823f-9deac67ac662"}, "a4f8d700-2d0c-493b-a8bc-22bd4e713274": {"doc_hash": "ee8c27ed0d3a2997cad2d2f66ca080e3578a552df8d6528a812a85a6119f2015", "ref_doc_id": "67c6fcd3-9922-45dd-823f-9deac67ac662"}, "08e42bea-5768-4b78-b076-2bb042521d63": {"doc_hash": "9fe95398da98bab5dfddbb57287eddab94b82bff3597413ef60bb9e6c73bd9b0", "ref_doc_id": "67c6fcd3-9922-45dd-823f-9deac67ac662"}, "45565f4d-a61c-4371-a344-12fa4d0e7568": {"doc_hash": "4e9b448e4cf73eebf4ddfda587bfd7399d9fbf7231f56b565ffa40aa674314d4", "ref_doc_id": "b549672b-d944-422c-850a-74fc852af027"}, "57d8b109-4bf7-44f9-836c-53eced112f4b": {"doc_hash": "5c4c39c593f630fa7fdc71fc3e353aa2c78c6a7eadd125b08a6ff8209c466762", "ref_doc_id": "b549672b-d944-422c-850a-74fc852af027"}, "8997ec64-5b20-4ee6-83d8-ef8837e5f5da": {"doc_hash": "a9d70090c32b438db8298d39a0d5b805646e6db371e5b7e1483206322fce0290", "ref_doc_id": "b549672b-d944-422c-850a-74fc852af027"}, "573c2a95-bf1c-4af3-8361-f37c326fab84": {"doc_hash": "7c755efd47218dc11f6649a7d0a4f4750f5cad7a45ffbcbf755e89e9f44508c8", "ref_doc_id": "b549672b-d944-422c-850a-74fc852af027"}, "af396777-2c81-4bb9-a0bb-ad71404d76ce": {"doc_hash": "c7264c918ba517da0707c240271da2a4121956b01590c40699252c5080862af3", "ref_doc_id": "c30060bd-1f1a-4b7e-945e-632aa226a2a4"}, "b7c68259-cd9f-47da-bdae-eff58aff0592": {"doc_hash": "5ef095a76abc85902bec026f149d7b8a45e11091478fd3b963aaf0ed9a77b1c3", "ref_doc_id": "68d0bcc1-7d30-4c35-95a9-9fd96e4f52b0"}, "95c99584-a3c9-4719-90db-1d50ff789de4": {"doc_hash": "08d409f42b1209910ad3bcef5d6709cdc0f07769b49dbdb47fd6e0bce8c0db8b", "ref_doc_id": "c9763d30-6fcc-428f-83c9-3001953fb17b"}, "3adbe498-a854-4baf-8ecd-7807b4efab46": {"doc_hash": "ac1f94e1770dbba7ad8ce036164a0a098645de368d4ba807ffde10d0dc20adf2", "ref_doc_id": "c9763d30-6fcc-428f-83c9-3001953fb17b"}, "ceb758fc-cb7c-48b4-8d0e-fd8ab86ba137": {"doc_hash": "c080a880b443b22bcd339c4975fc1a6ae31df5b0a4de8fa11c285ab86111d3c3", "ref_doc_id": "211ec4ae-796d-4880-a198-411bd743e0ea"}, "cecff25a-4f94-4ffd-baa9-028b9f38ed43": {"doc_hash": "8741f382de9716dbc4c93bb7d0790cb9b5ea90b5980dcca4de763780defc0141", "ref_doc_id": "211ec4ae-796d-4880-a198-411bd743e0ea"}, "87788c3b-80f9-42f4-9731-ee272e8871c7": {"doc_hash": "09c8cb8e2af36b5ba29eb2b9b9a0eca0976010d7a961e13a55ad0c632aa15370", "ref_doc_id": "fb9dddcc-38f5-4d24-9501-559b09bbf1f5"}, "3f162376-dd96-4779-85bf-334cbaa22f61": {"doc_hash": "d2df550573be6a21536b3f67c28eb664bf9bbe513af067168d8eceed26163829", "ref_doc_id": "fb9dddcc-38f5-4d24-9501-559b09bbf1f5"}, "a985d85b-a468-4956-8b55-f9cdb91c2421": {"doc_hash": "dbcc8405b1cb0f8ce4aea946e687df4e2ec60951c2ba5d6efed6dd1d155632f3", "ref_doc_id": "e31273be-cbcb-42fe-9eb1-eeccd5389112"}, "bc0bdd5d-4b83-466e-8069-d848f1b0e8ba": {"doc_hash": "c867ec4767af50193f98bc29c04545922ebc1cc5c257600226a0642328c7bbf4", "ref_doc_id": "78e33315-f640-4781-948c-2d3ad5f2baf9"}, "5a510654-e2cf-4a24-a874-20e89774268e": {"doc_hash": "429f2ccb9ed48f0f8ad1c5b433e57992a485357d57d142b063c160cb93ed8b88", "ref_doc_id": "78e33315-f640-4781-948c-2d3ad5f2baf9"}, "9a1d4b0f-fde6-48fe-9011-2023f53f1bbd": {"doc_hash": "424a61d9e23be4c091212fde729a2fd6a6502756e60b796fd5fd0222089e45f8", "ref_doc_id": "b1532b3a-bea4-4655-a624-677381c7e0c3"}, "04813eca-6f5e-432c-b7b5-ebc95bfadd76": {"doc_hash": "e2c2b0401692f6899254d9472fcebfafd2bed67812bc7bad94c4ded62c9b8674", "ref_doc_id": "b1532b3a-bea4-4655-a624-677381c7e0c3"}, "00ad34ef-4b64-4f07-b1c4-b642f5749761": {"doc_hash": "ad7d9cc4cb87cb620207237fa15732c8603274aeebcff7764922123cbd326c90", "ref_doc_id": "7c3939ad-4272-44d6-b398-73f8530a3f91"}, "8a0237f9-81f4-4cef-9500-e3615211a218": {"doc_hash": "9d2675886923b047e29de82d0ea29b097d19a4f3802a4e2ac88c999cec6bd6c4", "ref_doc_id": "7c3939ad-4272-44d6-b398-73f8530a3f91"}, "ccab159a-ce4f-49fd-91b1-aaa9b8991dac": {"doc_hash": "5b7632af600bcd64a7dc811d2f8bb287c001e923bd778c0c7e594449d86c1129", "ref_doc_id": "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926"}, "0e36bc01-5c79-4d36-8c38-b04430963953": {"doc_hash": "4e20e05ad6259ebbb64109edcb968ebb29e0a36fe82e5539af0e2a0d28ce0694", "ref_doc_id": "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926"}, "7d7781fd-9233-459d-93e4-e8570b097fcf": {"doc_hash": "6e12d78f4ade9d7bde45c3b6ebbba4e464a036e7dc69caafd8e66a8ee3066021", "ref_doc_id": "83ed9163-6ee0-44c4-a92d-5978408267f9"}, "139f4968-5b88-4fdc-9ff4-11b476b435a6": {"doc_hash": "0ecde3394f5eb1886d94ac9971ee0862cbccb7431e6f8a39a1da75f01dd1cff0", "ref_doc_id": "83ed9163-6ee0-44c4-a92d-5978408267f9"}, "690ecd27-48fe-4c63-bf45-6651769faf28": {"doc_hash": "48ca70b8d48d6293e940f8714d4662fc2f78ceef67e2e7d9cc9521f9c1106e6d", "ref_doc_id": "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d"}, "750202ef-9145-482f-8c72-567e9380d5c7": {"doc_hash": "88da2f48cfec338586a62819013616d82a44d06380bd7b8aa2a15503963cfce5", "ref_doc_id": "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d"}, "b200fcca-f4f3-4644-a420-d53fb30f7cc4": {"doc_hash": "081c88494cd0bc9580510446fc87e85f882438036c0f4319d61f96608d136032", "ref_doc_id": "4223da55-06cf-450c-9d00-4c56f099f5cf"}, "c96c8444-4bcd-4b2c-8fff-740c2edf2f5e": {"doc_hash": "1f0601cbed8d56328ee3f5b9fd519ac70b8e89c69413468bc5ddacf6646faf88", "ref_doc_id": "4223da55-06cf-450c-9d00-4c56f099f5cf"}, "ba598f3f-7c82-4bb3-bc5c-eb48f816ad2a": {"doc_hash": "c8b49bc5400fce5b5d25b4dbc045cf7e255fb62ee7778f491164fb3485ec8f69", "ref_doc_id": "fd963a5c-8382-4ea1-8afa-b0eb842a4568"}, "e4eaa70b-3adf-4556-9770-cc57fc59472a": {"doc_hash": "e0c12056879bbf0792f6e7ad57aeea7562fff3a1773dff62585b93bf48ad2cf7", "ref_doc_id": "fd963a5c-8382-4ea1-8afa-b0eb842a4568"}, "143cf4f0-c7eb-49be-a710-be91f81a1e1d": {"doc_hash": "642e85ad3eb2952f4da56f5318f8729e50388a95ffedeba9f9f822cc54bb4e8c", "ref_doc_id": "2d68f85a-81a3-402f-8583-ca18dc3c0682"}, "39e93058-b7f5-4e53-a1eb-3e6597d660e7": {"doc_hash": "74ee2b7485a66f521cf45e847ebaad58ead970dd3154f4e7ea9ce922c7d2331c", "ref_doc_id": "2d68f85a-81a3-402f-8583-ca18dc3c0682"}, "4c7eab4f-0ec0-4119-bb74-5b89e864f29a": {"doc_hash": "1287f4cdffecedadd77cc7439976439c965aaa01b0687405419e220f11dab40a", "ref_doc_id": "043f98c2-687a-451a-a86b-e0bd2441a38b"}, "887581e0-a6dd-40b2-83d2-925e6a6dbec5": {"doc_hash": "fa31adaf6a9a77331f4c7f73a71aeda9c5fb1e90bc8655cc768528633a4340fe", "ref_doc_id": "043f98c2-687a-451a-a86b-e0bd2441a38b"}, "6574937f-9c44-4d1f-b74e-4e3868986dcc": {"doc_hash": "a5955c4f537a8969a6f4ad2b7528af707f75b30896512581e2e5694e07d487a0", "ref_doc_id": "cf87c3a2-9cf9-4a78-9273-db6fabe1644a"}, "23744b20-264f-497a-a3ba-8e5a742c7575": {"doc_hash": "9f6cc9b59df00af51fc261eff22067bcd326deaa8c2ea4a048abce82162af7aa", "ref_doc_id": "cf87c3a2-9cf9-4a78-9273-db6fabe1644a"}, "a4b69d50-a8b9-4b68-8d7d-86c3cf36c00f": {"doc_hash": "73457974f44f278e53e3d5c01cd2975d95dfcd956751e30c4a701152f4e810c3", "ref_doc_id": "db728600-99fc-4a82-ad52-11fd6b0d956d"}, "31fdc1af-033d-4afd-9780-4c99bfc13301": {"doc_hash": "f4aa29309edf656bed2daebb461b864533ef18525b31104b7b5bc842681c4de4", "ref_doc_id": "db728600-99fc-4a82-ad52-11fd6b0d956d"}, "453e0062-d979-4ce4-a620-4e5941f9cacc": {"doc_hash": "dd438440fbcae60476d2ef1b1fbb128ad62af23851873048ef92c4a7781a0dcf", "ref_doc_id": "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb"}, "33d67853-e6c9-4f14-a6ba-c4b59ffcf35c": {"doc_hash": "e3baaf949a7b6668573cb8b00a0756f4a9e4b84c175b732844059cd278b30716", "ref_doc_id": "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb"}, "458cc8fe-f9fd-4eda-84ae-0f82e544263c": {"doc_hash": "fd7ab3e17f841fc9806ca5a7e5aac0ee9896cb13f417cba82685dbf1dd7df4dd", "ref_doc_id": "e1f246f7-342b-4855-be98-1780f90f25b3"}, "4db2653f-f003-4856-a2a6-2d5da7d7841b": {"doc_hash": "dc2d50943d8d39377bd14e97010e07cfa8cd2a806ae8ff90b1539eb2adce542f", "ref_doc_id": "e1f246f7-342b-4855-be98-1780f90f25b3"}, "ff8bb785-dd23-40b2-8148-ff05dc6b5d21": {"doc_hash": "9973e4633901639b826bbdf6d1f449ff30f214400330d4b1967df7a48c20659f", "ref_doc_id": "61be5a83-3321-486a-97d3-5d1c8c693a2d"}, "6576f2fb-0b95-4d45-8b56-ead8019e2a92": {"doc_hash": "5fa4b3eb4b5bf247cfde6886212c068b5a7fbd020a53ab6bc9644a6eef7020fc", "ref_doc_id": "61be5a83-3321-486a-97d3-5d1c8c693a2d"}, "454c1031-4ca1-4019-8e83-195c85ad9edf": {"doc_hash": "572f747816d299798676e4ce745f6bf339b70117146b7800ea12cfa8cccd948c", "ref_doc_id": "24232b86-b4df-4a0c-ae88-6db8fd096838"}, "ebf5b508-8483-455d-a697-c2f7a81eed04": {"doc_hash": "d60a0485458b25b5e38464392c1972443f7a45d9949c80ecdec144e23204b086", "ref_doc_id": "24232b86-b4df-4a0c-ae88-6db8fd096838"}, "822511ba-a3a0-4575-a7f3-6921fda4967d": {"doc_hash": "4f5827b5c3e1112d0034350b188c3850cbb4eb871b1a6f25574b2a645bc5ade0", "ref_doc_id": "ba8740db-1722-4584-b585-a469555c0ef5"}, "79f29ca5-b46f-4175-b8aa-a7f513b5dbe5": {"doc_hash": "6f54dfcc15116475dcc5afbcc0cf712a4e3b92b433c18cfa17c6382c00772980", "ref_doc_id": "ba8740db-1722-4584-b585-a469555c0ef5"}, "ab0311a8-f633-4052-8abc-a4877db7fb17": {"doc_hash": "dada1891bd2b6b3d9c84ab7520656eb95b0040f0ad2c884e55c0fc742fc355ba", "ref_doc_id": "3a84ad94-6d87-4774-884a-487f13436df8"}, "01b73d52-a1f6-46db-b1e0-dfb4c40e4e34": {"doc_hash": "880bc71b94500de38c6774761dd52beb5f588febcccb94e0b76d068a1de785af", "ref_doc_id": "3a84ad94-6d87-4774-884a-487f13436df8"}, "83721b6a-913f-4d5b-b4d7-37836f1de808": {"doc_hash": "3b448506e907e9c7b970f8f3cc6909c04b6b27ebc82c60088721bc36c233481a", "ref_doc_id": "70f07b21-b7de-41dc-86f4-3cb847800e0f"}, "605623b7-9e41-468f-a110-3c815fcd67b6": {"doc_hash": "e2a357c85d83975ceb3adc8fbdf308db80948caa5dfe8111d016febf94261ba2", "ref_doc_id": "70f07b21-b7de-41dc-86f4-3cb847800e0f"}, "97ca86dd-1809-41b5-9595-39d097001f1e": {"doc_hash": "8f02a2742b9bcebd6f871f6e137324c89de15b9370a9e44f3e5765144aaf2c2c", "ref_doc_id": "f298ea32-3817-4de2-958f-5f09c5674ab0"}, "b153b385-a273-49f0-9fd0-8ea1df4829bd": {"doc_hash": "c63ef34ad5a07bd2e08c61bcce3e51d299b10fc9bd02aee6434fc7dcc095f2d5", "ref_doc_id": "f298ea32-3817-4de2-958f-5f09c5674ab0"}, "94cd3887-69f9-4cd6-b672-162d18cf085c": {"doc_hash": "a5e2120d3f95602a652e258f7c32c7b3fc79e3155aef61885960f2ee9635a8fc", "ref_doc_id": "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc"}, "d53c3265-651b-4a0f-969e-d555e7950fc2": {"doc_hash": "b3736bc1d83e6f7161b6a283297b253ee6d1eb6f133099bf874aa35cc3ef95db", "ref_doc_id": "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc"}, "4a86b7b9-c47a-4308-b480-866e59b21c7b": {"doc_hash": "38259b5841cc37a4197e1c81877985872fd2877b324f0622ee10a7f887fce4d3", "ref_doc_id": "e22a1468-6f1a-4e87-a5dc-3cbb33449b07"}, "7cfd25f8-69ac-4f6b-bdde-3f6e77142b44": {"doc_hash": "3bb54a23cb1e10212dba0a6f422820a60882ce6fb47f213e4d79a7a7898f3763", "ref_doc_id": "e22a1468-6f1a-4e87-a5dc-3cbb33449b07"}, "8332f834-b7d6-40fd-be14-452612491e6d": {"doc_hash": "1b846e2e70456c92a8b7a3ebdbe9ae905d76450d369ee747512627a6c716a3a5", "ref_doc_id": "ec1565e1-aa8f-41e8-a78c-1be832aaff8f"}, "4ac8c7c3-bfd8-4754-978c-d37287648cc3": {"doc_hash": "60c69d80a5ce567b3169813a49b4a3fb416ec1335ab61f52453bfcce53d1cf9f", "ref_doc_id": "ec1565e1-aa8f-41e8-a78c-1be832aaff8f"}, "bd7e9bd3-9ede-4022-9cff-14dcd16bca56": {"doc_hash": "ef4eff346915ced17488f43f4794885fcc8d49a481bbf4d1dc453b9e85306075", "ref_doc_id": "2dc415f5-1f27-4d16-a0c1-c03f15ad0889"}, "db02def1-dc5f-4a97-82f5-e20f25c36ca3": {"doc_hash": "0195b4c60a343186ac87053a9e3da9e1117afeab66183c3466c323bb241a5e9a", "ref_doc_id": "2dc415f5-1f27-4d16-a0c1-c03f15ad0889"}, "cae98f70-b3b3-4d9c-9dfb-67e5ca000312": {"doc_hash": "c6ae125d4a9f6525cc733182339dfe2cb53d99429330cde03b452f98862a2a5a", "ref_doc_id": "7f77fd31-795f-4704-9908-667c24140881"}, "a17b4fab-1952-4e52-a7ec-9f34198adfcb": {"doc_hash": "1156ffe8218e868ae618dfe78f46efb3f4d45b884e465357d1c116a09eb01bab", "ref_doc_id": "7f77fd31-795f-4704-9908-667c24140881"}, "d008b1ee-788d-467f-88bf-d9da6886bc63": {"doc_hash": "7ec983aeba2adcf26826f3164684f8bfc84fbe883c5ec39dd3be5344090a4a56", "ref_doc_id": "5a303b42-cd78-408e-b164-0c66108f5b16"}, "9b1f12ca-379e-4036-b639-1bacbba77bd2": {"doc_hash": "8e122227738f3032b2f3027d2a3116f11c8cd5f8c3c1cd648a96f11f0810cc51", "ref_doc_id": "5a303b42-cd78-408e-b164-0c66108f5b16"}}, "docstore/data": {"1fcb2e57-dd0b-432a-8d0b-4a85370a6d34": {"__data__": {"id_": "1fcb2e57-dd0b-432a-8d0b-4a85370a6d34", "embedding": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5b723fc-2a2f-4948-a3b3-1f653cff943c", "node_type": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "hash": "650d5f0d38950cbe23cf8999d922b5fe11eb8c7a6a128382d0db0ea0e6200310"}, "3": {"node_id": "58f3250a-3c15-49d8-a6b1-4ce7d7ec8d7d", "node_type": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "hash": "bad04b7679bbf196172d920dc0acbde562408ae2ec1568ee9d556ce6ede21e48"}}, "hash": "ff2de1fb874161ded9f959eaaa2f7ff05a6c46daa2842c744a0c526602306261", "text": "Innovative Applications of O.R.\nGranting and managing loans for micro-entrepreneurs: New developments and\npractical experiences\nCristi\u00e1n Bravoa,b,\u21d1, Sebasti\u00e1n Maldonadoc, Richard Webera\naDepartment of Industrial Engineering, Universidad de Chile, Santiago, Chile\nbFinance Center, Department of Industrial Engineering, Universidad de Chile, Santiago, Chile\ncUniversidad de Los Andes, San Carlos de Apoquindo 2200, Las Condes, Santiago, Chile\narticle info\nArticle history:\nReceived 22 February 2012Accepted 26 October 2012\nAvailable online 10 November 2012\nKeywords:\nOR in developing countries\nData mining\nCredit scoringMicro-entrepreneursCut-off pointabstract\nWe present a methodology to grant and follow-up credits for micro-entrepreneurs. This segment of\ngrantees is very relevant for many economies, especially in developing countries, but shows a behavior\ndifferent to that of classical consumers where established credit scoring systems exist. Parts of our meth-\nodology follow a proven procedure we have applied successfully in several credit scoring projects. Otherparts, such as cut-off point construction and model follow-up, however, had to be developed and consti-\ntute original contributions of the present paper. The results from two credit scoring projects we devel-\noped in Chile, one for a private bank and one for a governmental credit granting institution, provideinteresting insights into micro-entrepreneurs\u2019 repayment behavior which could also be interesting forthe respective segment in countries with similar characteristics.\n/C2112012 Elsevier B.V. All rights reserved.\n1. Introduction\nCredit scoring corresponds to the use of statistical models to\ntransform relevant data into numerical measures that guide credit\ndecisions ( Anderson, 2007 ), and its main objective is to estimate\nthe probability of default, i.e. the event of a customer not paying\nback the loan in a given time period. Recent developments in credit\nscoring are oriented, for example, in analyzing imbalanced credit\ndatasets ( Brown and Mues, 2012 ), in survival analysis ( Bellotti\nand Crook, 2008; Tong et al., 2012 ), in correct ways to validate\ncredit scoring models and make them comprehensible ( Caster-\nmans et al., 2010 ), and in adapting new models for credit scoring\nuse ( Setiono et al., 2009 ).\nThis work focuses on micro-entrepreneurs, a segment different\nfrom independent persons or large companies in terms of size, in-\ncome, and organizational structure. Although several efforts havebeen undertaken to gain knowledge about the default risk of small\nand medium-sized enterprises \u2013 see, e.g., Kim and Sohn (2010) ,o r\nGool et al. (2011) \u2013, only few studies are available for micro-entre-\npreneurs ( Schreiner, 2000 ).\nThe aim of this paper is twofold: First, it provides experiences\nand insights we gained from several credit scoring projects \u2013 a pri-\nvate bank and a state-owned organization \u2013 where we had to adaptexisting methodologies for credit granting. Secondly, these projects\nrequired the development of new techniques for credit scoring,\nnamely cut-off point construction and model follow-up, which will\nbe described subsequently.\nThis paper is organized as follows. The next section character-\nizes Chilean micro-entrepreneurs. Subsequently, we present the\nmethodology used to construct credit scoring models putting spe-\ncial emphasis on stages where problems arose or special knowl-\nedge was revealed. The following section contains the results we\nobtained applying the proposed methodology to the state-owned\norganization. Finally, conclusions are drawn from our work in\nSection 6.\n2. Chilean Micro-entrepreneurs\nIn most countries, micro-entrepreneurs are an important ele-\nment of the economy, accounting for the creation of new business\nopportunities and employment. Chilean micro-entrepreneurs are\nde\ufb01ned as very small", "start_char_idx": 0, "end_char_idx": 3842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58f3250a-3c15-49d8-a6b1-4ce7d7ec8d7d": {"__data__": {"id_": "58f3250a-3c15-49d8-a6b1-4ce7d7ec8d7d", "embedding": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f5b723fc-2a2f-4948-a3b3-1f653cff943c", "node_type": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "hash": "650d5f0d38950cbe23cf8999d922b5fe11eb8c7a6a128382d0db0ea0e6200310"}, "2": {"node_id": "1fcb2e57-dd0b-432a-8d0b-4a85370a6d34", "node_type": null, "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}, "hash": "ff2de1fb874161ded9f959eaaa2f7ff05a6c46daa2842c744a0c526602306261"}}, "hash": "bad04b7679bbf196172d920dc0acbde562408ae2ec1568ee9d556ce6ede21e48", "text": "employment. Chilean micro-entrepreneurs are\nde\ufb01ned as very small \ufb01rms, with up to ten employees and an an-\nnual income of no more than EUR 97,000. They offer 21% of the jobs\nin the country ( Divisi\u00f3n Empresas de Menor Tama\u00f1o, 2009 ), and\nrepresent a large portion of Chilean companies, but they generate\nonly a small portion of annual sales, with micro-entrepreneurs rep-\nresenting 13.9% of total annual sales of all companies in the coun-\ntry ( Instituto Nacional de Estad\u00edsticas, 2002 ).\nMicro-entrepreneurs, usually receiving support only from gov-\nernments and not-for-pro\ufb01t initiatives, have become attractive\ncustomers for banks and loan-granting institutions in recent years,\n0377-2217/$ - see front matter /C2112012 Elsevier B.V. All rights reserved.\nhttp://dx.doi.org/10.1016/j.ejor.2012.10.040\u21d1Corresponding author at: Department of Industrial Engineering, Universidad de\nChile, Santiago, Chile. Tel.: +56 2 2978 4525; fax: +56 2 2978 4011.\nE-mail addresses: cbravo@di.uchile.cl (C. Bravo), smaldonado@uandes.cl (S.\nMaldonado), rweber@dii.uchile.cl (R. Weber).European Journal of Operational Research 227 (2013) 358\u2013366\nContents lists available at SciVerse ScienceDirect\nEuropean Journal of Operational Research\njournal homepage: www.else vier.com/locate/ejor", "start_char_idx": 3778, "end_char_idx": 5047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26469a1c-1267-440e-9c92-4d5c000808ce": {"__data__": {"id_": "26469a1c-1267-440e-9c92-4d5c000808ce", "embedding": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08c34df3-3bc1-45fe-9bbb-9f588c0d1807", "node_type": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "hash": "00f8e2f3ae00b5d8b9505292ba2f3c170c6407421f55402abff071eae6ef6104"}, "3": {"node_id": "15d0316f-b6c7-4c79-808f-68aff8e2b7f8", "node_type": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "hash": "2840358ee88e0f73cdbff404975da068f9def2a15eeb2dc3fb2a33c649748c10"}}, "hash": "7176f64cbfe86663754a4d12cd9bb9bc878829a53e77de3da5a9274a1666c5af", "text": "but the risk measures that accompany them have to follow suit. In\nparticular, micro-entrepreneurs have certain characteristics that\nmust be taken into account when developing a credit scoring sys-\ntem. For example, they are usually on a tight budget, regardless of\ntheir revenues. So it is not a question of how much money they\nmake, which takes away a natural candidate for a discriminatory\nvariable in credit scoring models. Considering the special charac-\nteristics that micro-entrepreneurs possess, there is still little\nknowledge on the variables that may determine their risk as loan\nborrowers. Additionally, most micro-entrepreneurs have not had\nany access to \ufb01nancial instruments previously, so the common\nscorecards that are available from major distributors are concen-\ntrated on higher risk segments (due to lack of information), and\nmakes the segment be considered as \u2018\u2018high risk\u2019\u2019 without any dee-\nper consideration ( Ministerio de Econom\u0131 \u00b4a, 2012 ).\nThe question may arise of whether funding is even needed for\nthis segment. A recent government study ( Ministerio de Econom\u0131\u00b4a,\n2012 ) found that 81 percent of micro-entrepreneurs fund their\nstart-ups using their personal savings or family loans. Only four\npercent of the start-ups were funded using banks or established\n\ufb01nancial institutions, reporting that the access to credit is given\nonly by supermarkets and some retailers (in the form of personal\ncredit cards). The main conclusion we draw from the information\npresented is that loans are necessary for this segment, but the per-\nceived risk of the micro-entrepreneur is too high for traditional\n\ufb01nancial institutions to provide coverage. We believe that existing\ncredit scoring methodologies must be adapted in order to correctly\nre\ufb02ect the reality of micro-entrepreneurs and, in that way, create\nthe conditions for an equal-opportunity and pro\ufb01table environ-\nment both for micro-entrepreneurs and for the institutions that\ngrant them loans.\n3. Developing a credit scoring system\nWe applied the KDD process ( Fayyad et al., 1996 ) to develop\nboth models; one for the private bank and the other for the\nstate-owned organization. In the following subsection we describe\nthe process of data acquisition and consolidation, followed by the\nprocess of data cleansing. Finally we present the process for esti-\nmating the probability of default for a solicited loan.\n3.1. Data set consolidation\nThe \ufb01rst steps were to identify the relevant databases in which\nthe information was scattered, extract the respective variables, and\nload them into a repository especially created for the construction\nof our models.\nIn order to develop an effective credit scoring system, a homo-\ngeneous and representative sample of the population for both clas-\nses (defaulters and non-defaulter) is needed. Based on the relevant\nliterature, e.g. Thomas et al. (2002) , and our experience from other\nsimilar credit scoring projects, we \ufb01rst segmented customers and\nthe requested loans by differentiating between new customers in\none group and renewing or current customers in the other.\n3.2. Data cleansing and variable selection\nThe next step consisted of data preparation and variable (fea-\nture) selection. The procedure applied had two goals: to select\nfeatures in a cascade-like approach thus minimizing the risk of\neliminating potentially useful ones, and to maximize the knowl-\nedge extracted from the respective dataset. The description of\nthe procedure follows:1.Concentration of feature values and analysis of missing values :I n\norder to quickly discard useless variables, those concentrated\nin a single value in more than 99% of the cases, and those with\nmore than 30% of missing values were eliminated. The rationale\nof the second criterion is to reduce the number of discarded\ncases or imputations realized", "start_char_idx": 0, "end_char_idx": 3815, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15d0316f-b6c7-4c79-808f-68aff8e2b7f8": {"__data__": {"id_": "15d0316f-b6c7-4c79-808f-68aff8e2b7f8", "embedding": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08c34df3-3bc1-45fe-9bbb-9f588c0d1807", "node_type": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "hash": "00f8e2f3ae00b5d8b9505292ba2f3c170c6407421f55402abff071eae6ef6104"}, "2": {"node_id": "26469a1c-1267-440e-9c92-4d5c000808ce", "node_type": null, "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}, "hash": "7176f64cbfe86663754a4d12cd9bb9bc878829a53e77de3da5a9274a1666c5af"}}, "hash": "2840358ee88e0f73cdbff404975da068f9def2a15eeb2dc3fb2a33c649748c10", "text": "second criterion is to reduce the number of discarded\ncases or imputations realized in order to construct the \ufb01nal\nmodel.\n2.Univariate analysis : The remaining variables were tested to \ufb01nd\ndistribution equality across groups, using the objective vari-\nables (defaulter/ non-defaulter) as the splitting criterion. In par-\nticular Kolmogorov\u2013Smirnov (K\u2013S) and v2-tests were applied to\ncontinuous and discrete variables, respectively.\n3.Final data preparation : The selected variables accounted for 20%\nof the original ones. The resulting dataset had very few null\ncases (less than 1%), which were eliminated, and resulted in arobust subset of the variables which were then used as input\nfor a logistic regression model.\n3.3. Estimating the probability of default\nThe \ufb01nal decision regarding the acceptance of the loan applica-\ntion can be made by comparing the calculated probability of de-\nfault, estimated using a suitable model and the variables\nobtained from previous steps, with a suitable pre-de\ufb01ned thresh-\nold ( Hand and Henley, 1997 ). Several studies in credit scoring have\nfocused on comparing the classi\ufb01cation performance of different\ntechniques (see, for example, Baesens et al. (2003), Finlay\n(2011) ). Their main conclusion is that the traditional credit scoring\nmethod logistic regression reaches performance levels comparable\nwith more sophisticated data mining approaches for modeling\ncredit risk, being therefore the main technique used for credit\nscorecard construction ( Thomas et al., 2002 ).\nThe objective of logistic regression is to construct a function\nwhich determines the probability of default for a given client. It\nconsiders Vdifferent regressors in a vector x\ni2RV;i\u00bc1;...;N,\nand an observed binary variable yi= 1 if borrower idefaults, and\nyi= 0 else. Considering the dependent variable as latent, the prob-\nability of default is:\np\u00f0yi\u00bc1jxi\u00de\u00bc1\n1\u00fee/C0b0\u00fePV\nj\u00bc1bjxij/C16/C17 ; \u00f01\u00de\nwhere b0is the intercept, and bjis the regression coef\ufb01cient associ-\nated to variable j. Since these parameters are unknown, they have to\nbe estimated using, e.g., a maximum likelihood algorithm, which re-\nsults in unbiased, asymptotically normally distributed estimators ^bj\n(Hosmer and Lemeshow, 2000 ). The expression in the exponent is a\nmeasure of the total contribution of the independent variables used\nin the model, and is known as the logit ( Greene, 1993 ).\nThe process of constructing the model uses a wrapper for vari-\nable selection, consisting of a greedy search of variables that ac-\ncounts for maximum discriminatory capacity. There are two\ncommon approaches ( Hosmer and Lemeshow, 2000 ):forward\nselection and a backward elimination . Both methods are based on\nmeasuring how relevant each variable is, as measured by a v2-test.\n4. New developments\nThe \ufb01nal step when constructing a credit scoring system is\ndeciding the cut-off to be used in order to transform the probabil-\nity obtained from the logistic regression into a binary decision.\nHowever, when pro\ufb01ts are not well-de\ufb01ned, this is not a trivial task\nas we found, e.g., in our project with the state-owned, non-pro\ufb01t\ninstitution. We developed a generic methodology to determine\nsuch cut-off points for the case of non-pro\ufb01t organizations. Addi-\ntionally, once the credit scoring model is being used, differentC. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366 359", "start_char_idx": 3732, "end_char_idx": 7109, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "463e26d5-5bf8-4df1-8310-da982fbafee3": {"__data__": {"id_": "463e26d5-5bf8-4df1-8310-da982fbafee3", "embedding": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c0b507a-870b-4dbe-8711-79f546a86280", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "4d044a62bbe0ab9cb63fa3f3da4c249157e1417115a99a8c017a61a8fba2359f"}, "3": {"node_id": "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "e6d7347c2dd72f1f2cedd8a685b96bb9201021538bb8848a9d2bbffaf8b4a99c"}}, "hash": "f9c26df367762f3b5955e7bf350fa6f6de200b4b9bbb8528e0f454b72fd23c78", "text": "kinds of changes might occur diminishing predictive capability. We\ndeveloped procedures to follow such shifts in the population and\nupdate the respective model parameters accordingly. These ap-\nproaches are presented in the subsequent subsection.\n4.1. Methodology for cut-off point construction\nThe methodology to determine a cut-off point starts by deter-\nmining the cost of accepting a bad applicant using the expected\nloss for a given loan. The second part is estimating the cost of\nrejecting a good applicant, point especially interesting for any gov-\nernmental institution, usually more interested in public wealth\nthan in monetary pro\ufb01ts. Finally, both numbers are considered\nand combined to calculate an optimal cut-off point to be applied\nto the result of the logistic regression model.\n4.1.1. Cost of accepting a bad applicant\nOur cut-off point methodology starts by segmenting the range\nof the possible values the estimated default probability can take.\nConsidering the database of loans that were used to validate the\nmodel, it is necessary to obtain the probabilities of default for each\ncustomer. Then these values are ordered and segmented in ranges\ndepending on the size of the database. In our experience, intervals\nof 0.05 turned out to provide the best results, generating 20 seg-\nments of the interval [0.1].\nThe direct cost for the organization when a grantee does not re-\npay his or her loan has to be calculated by taking into consideration\nall the resources owed. Additionally, the value of the collateral\nmust be discounted, that is, the real estate or different properties\nthat the grantee declares as security for the loan. In general, the\nloss per loan (given that default occurred) is the product of the fol-\nlowing quantities ( Ozdemir and Miu, 2009 ):\n/C15EAD: Exposure at default. Is the amount that the grantor is owed\nwhen default occurs, including all standing instalments and any\nowed interest. In the case of loans with guarantors, the value of\nthe loss and the exposure is different ( Superintendencia de Ban-\ncos e Instituciones Financieras, 2008 ), but for this particular\ncase (no guarantors considered) they are assumed to be the\nsame.\n/C15LGD: Loss Given Default. Proportion of the exposed capital\n(EAD) that is actually lost given the event of default. This value\nconsiders the expected proportion of the loan that will not be\npaid by the customer after default occurs including the recovery\nafter prosecution or collection, and the recovery given by the\ncollateral.\nThe cost of defaulting for each customer follows LOSS =\nLGD /C1EAD. In order to determine the cut-off point it is necessary\nto know the accumulated cost for all customers. The \ufb01nal amount\ncorresponds to the set of defaulters whose default probabilities are\nbelow the cut-off point p:\nCp\nLoss\u00bcP\ni2D\u00f0p\u00deLOSS i\njD\u00f0p\u00dejp2f0:05;0:1;...;0:95;1g; \u00f02\u00de\nwhere D(p) is the set of defaulters with estimated probability of de-\nfault less than p,jD(p)jis the number of customers that belong to set\nD(p), and LOSS iis the observed loss for customer i.\nThe cost is divided by jD(p)jso an average cost per loan in that\ncut-off is determined. This way, when applying the model, correc-\ntions can be introduced considering the actual number of loans\nthat are observed \u2013 a new jD(p)j\u2013 tying the cut-off to market\nconditions.4.1.2. Cost of rejecting a good applicant\nThe case of a private loan-granting company rejecting a poten-\ntially good customer leads to", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd": {"__data__": {"id_": "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd", "embedding": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c0b507a-870b-4dbe-8711-79f546a86280", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "4d044a62bbe0ab9cb63fa3f3da4c249157e1417115a99a8c017a61a8fba2359f"}, "2": {"node_id": "463e26d5-5bf8-4df1-8310-da982fbafee3", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "f9c26df367762f3b5955e7bf350fa6f6de200b4b9bbb8528e0f454b72fd23c78"}, "3": {"node_id": "6930ab06-3b63-4130-a205-a8f9663830b7", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "3aba50098488617777a5bbb3cfb2597cf458cc862ee429ff1087c0e742ff4731"}}, "hash": "e6d7347c2dd72f1f2cedd8a685b96bb9201021538bb8848a9d2bbffaf8b4a99c", "text": "company rejecting a poten-\ntially good customer leads to an opportunity cost equivalent to the\ngain or utility the loan would have generated for the \ufb01nancial en-\ntity. This cost has an associated market share loss: if the \ufb01nancial\npolicy has been too restrictive and many potentially good borrow-\ners have been rejected, the company exposes itself to a commercial\nrisk by reaching fewer customers than would have been possible.\nIn the case of a state-owned institution, however, the cost of re-\njected loans has to consider the pro\ufb01t a rejected loan could gener-\nate for the organization (opportunity cost) plus the bene\ufb01t for\nsociety that is not going to occur since the credit is not granted.\nIn general, this bene\ufb01t is not relevant for private organizations\nwhen determining their cut-off points. Furthermore, it could be ar-\ngued that the pro\ufb01ts that any institution (public or private) per-ceives is at the expense of the borrower, so the net social bene\ufb01t\nof pro\ufb01ts is zero, therefore should not be considered.\nIn order to estimate this lost bene\ufb01t for society, we compared\nthe average income of applicants who didreceive loans from the\ninstitution with average income of entrepreneurs who did not re-\nceive any loan. A previous study provided by the government insti-\ntution analyzed a sample of 1010 entrepreneurs who received a\nloan and a sample of 500 workers who did not receive any loan\n(control group).\nThe survey was conducted in the year 2005, considering\ngranted loans between 2000 and 2003 for the study group. These\ngranted loans were repaid before the survey was taken. The survey\nwas controlled by region and activity by using cluster sampling,\nbut also another variables were studied in order to validate the\nsampling process, such as age, sex, size of the family group, ethnic\ngroup, and educational level. No signi\ufb01cant differences were found\nfor these variables.\nThe average annual income obtained by the customers (study\ngroup) was 4869 EUR, which is 1287 EUR higher than the average\nof the control group (3582 EUR). Additionally, the customers who\nreceived a loan were asked to estimate their expected loss in terms\nof annual income in case they would not have received credit.\nThose who did not receive a loan were asked to estimate how\nmuch additional income they would have perceived. The averages\nof both estimations are similar to the difference of the average an-\nnual income for both groups (1314 EUR and 1287 EUR respec-\ntively), making the estimation robust. Table 1 summarizes the\nresults of this analysis.\nAccording to this information, the average income of applicants\nwith access to a loan is 26.44% higher than the ones without this\nform of \ufb01nancial help. Given that the vast majority of these entre-\npreneurs without access to loans do not get credit from private\n\ufb01nancial institutions and tend to reduce their production level,we consider this percentage to be a valid estimate for the opportu-\nnity cost of not receiving a loan.\nWe assume the opportunity cost as a percentage of the amount\nof the loan for several reasons: First, the annual income and the\namount of the granted loan are highly correlated for micro-entre-\npreneurs, as we could corroborate for a similar universe of the pri-\nvate bank\u2019s customers. This insight is relevant since the variable\nTable 1\nComparison of annual income for customers and control group.\nWith\nloanWithout\nloanDiff./avg. Percentage\nAvg. annual income 4869 EUR 3582 EUR 1287\nEUR26.44%\nEst. loss of income\n(if loan not granted)1247 EUR", "start_char_idx": 3398, "end_char_idx": 6912, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6930ab06-3b63-4130-a205-a8f9663830b7": {"__data__": {"id_": "6930ab06-3b63-4130-a205-a8f9663830b7", "embedding": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c0b507a-870b-4dbe-8711-79f546a86280", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "4d044a62bbe0ab9cb63fa3f3da4c249157e1417115a99a8c017a61a8fba2359f"}, "2": {"node_id": "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd", "node_type": null, "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}, "hash": "e6d7347c2dd72f1f2cedd8a685b96bb9201021538bb8848a9d2bbffaf8b4a99c"}}, "hash": "3aba50098488617777a5bbb3cfb2597cf458cc862ee429ff1087c0e742ff4731", "text": "loss of income\n(if loan not granted)1247 EUR 1314\nEUR26.97%\nEst. additional income\n(if loan granted)1377 EUR360 C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366", "start_char_idx": 6917, "end_char_idx": 7106, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "727b5520-6845-40b1-b734-0a38b196bf67": {"__data__": {"id_": "727b5520-6845-40b1-b734-0a38b196bf67", "embedding": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "16607738c46feb2729281ad64320d54d7616c90418c7ec57d4f072461a97b1b9"}, "3": {"node_id": "0c4d7c05-41d8-415d-9814-2916239e0a89", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "6efef63ce1c601cfc462a4b9172f8b90fe0ae1ebdcdc7239c409a038f50a1cbb"}}, "hash": "af9de27c0a155201aebcfa5d1efa482055037b503d5f06b59f4fac241068f820", "text": "\u2018\u2018income\u2019\u2019 is not properly available for all applicants. Secondly, we\nwant to provide a comparison between both costs (monetary loss,\nwhich depends on the amount of the loan, and opportunity cost) in\nterms of the same variable. Finally, it is more intuitive to estimate\nthe pro\ufb01t a good payer may generate (and therefore a social bene-\n\ufb01t) as a proportion of the money that he/she receives (the amount\nof the loan), instead of the income he/she already has.\nThe cost of rejecting a good applicant per cut-off point pis:\nCp\nRG\u00bc0:2644 Lp;p2f0:05;0:1;...;0:95;1g; \u00f03\u00de\nwhere Lpis the average amount of the loans granted for a given cut-\noffp. It is important to notice that for this particular application no\ninterest rate is considered, and only the effect of the in\ufb02ation is\ncharged for repayment. In a general case, the interest should be sub-\ntracted from the pro\ufb01t generated by the loan.\n4.1.3. Cut-off point construction\nGiven the results of the previous subsections, i.e. knowing the\nvalues of Cp\nLossand Cp\nRG, it is possible to construct the \ufb01nal cut-off\npoint. The optimal value will be simply the one that minimizes\nthe total cost in the test dataset, given by:\nPmin\u00bcargminp\u00f0Cp\nLoss/C1jD\u00f0p\u00dej \u00feCp\nRG/C1\u00f0 jG\u00f01\u00dej /C0 j G\u00f0p\u00dej\u00de\u00de \u00f0 4\u00de\nSimilarly to Eq. (2),G(p) represents the set of customers with prob-\nability of default less than p,s ojG(1)jrepresents the total number of\ncustomers. For Eq. (4)we assume a Benthamite welfare function,\ncommonly used for social economics without assuming an uneven\ndistribution of goods, in which an Euro of foregone bene\ufb01t to soci-\nety has the same weight as an Euro of loss from default for the\n\ufb01nancial institution, since the Euro foregone should have been used\nfor a new loan or for improvement of the welfare of a borrower, the\ndamage is indistinct ( Arrow and Debreu, 2002 ).\nPminis the cut-off point that minimizes the total cost of mis-\nclassi\ufb01cation. It should be noted that this expression is valid only\nif the value of the objective variable used to estimate the logisticregression model is 1 in case the loan defaults, as is recommended\nwhen constructing scorecards ( Anderson, 2007 ).\n4.2. Model follow-up\nAs in many other projects, the users of our credit scoring sys-\ntems asked for a way to update the respective models. This practi-\ncal need has also been recognized in the literature. According to\nThomas et al. (2002) , a statistical model has an average lifetime\nof two years before it begins to lose predictive capacity, which di-\nrectly translates into losses due to a higher default rate as well ashigher provisions. A way to prevent the described loss in predictive\ncapacity would therefore be attractive for any credit-granting com-\npany, making model follow-up a challenge and an interesting re-\nsearch opportunity.\nThe need for follow-up in credit scoring also has a very strong\nregulation component: credit risk models must be \ufb01rst approved\nby the national supervisor before \ufb01rst use, and this procedure\ncan be time consuming, expensive (since resources must be allo-\ncated), and overall stressing the operations of the credit risk area.\nThese facts block simply updating the model every six months,\nand incentives the institution to wait as much as possible to\nchange the model, usually when losses are already being", "start_char_idx": 0, "end_char_idx": 3281, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c4d7c05-41d8-415d-9814-2916239e0a89": {"__data__": {"id_": "0c4d7c05-41d8-415d-9814-2916239e0a89", "embedding": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "16607738c46feb2729281ad64320d54d7616c90418c7ec57d4f072461a97b1b9"}, "2": {"node_id": "727b5520-6845-40b1-b734-0a38b196bf67", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "af9de27c0a155201aebcfa5d1efa482055037b503d5f06b59f4fac241068f820"}, "3": {"node_id": "8bddc558-f028-4026-814c-7f84ef496937", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "644ca6ff5f22d5f692253bff6e150a91fdc67205473bf2beae8c0ae0eee6e43f"}}, "hash": "6efef63ce1c601cfc462a4b9172f8b90fe0ae1ebdcdc7239c409a038f50a1cbb", "text": "wait as much as possible to\nchange the model, usually when losses are already being perceived.\nCorrect follow-up should help in avoiding these losses from taking\nplace. Additionally, the developing a credit scoring model is not\njust related to estimating the coef\ufb01cients, since implementing a\nnew model stresses much more than just the risk analysis depart-\nment in the company. The costs saved in training, systems imple-\nmentation time and resources, and other overhead costs that arisewhen a new model is implemented encourage the use of follow-up\ntechniques over implementing a new model from scratch.\nSimilar to model construction, model follow-up must be easily\nunderstandable by its users and easily applicable. Additionally, it\nmust provide an accurate picture of the population shifts. We ap-\nplied the following three approaches for model follow-up:\n1.Variables\u2019 discriminatory capacity : Over time, variables can lose\ntheir discriminatory capacity. This was measured indepen-\ndently of the respective model, using the same procedures per-\nformed for variable selection. We applied K\u2013S or\nv2-tests using\nthe predicted result as the splitting variable on each of the\nregressors in the model. This way we can measure the discrim-\ninatory capacity of the variables on a monthly or weekly basis\nusing currently granted loans, closely following the marketmovement and providing early alerts if necessary.\n2.Discriminatory capacity of the model : The model may also lose\ndiscriminatory capacity as the market changes. Unfortunately,\nthis can only be determined exactly once a certain loan has\nbeen paid off or \u2013 in the opposite case \u2013 the respective customer\ndefaults. We applied the following alternatives which can be\nconsidered in order to measure the model\u2019s performance peri-\nodically. We compute a score for each customer who defaulted\nor paid off the loan each month. Next a confusion matrix is con-\nstructed and percentages of false positives and false negatives\nare calculated. If any of these measures is above a prede\ufb01ned\nthreshold, for example ten percent of the reported test accu-\nracy, then an alert is given. This test allows following the perfor-\nmance of the model closely.\n3.Change of distribution of the variables : The most challenging test,\nhowever, was to detect whether or not a signi\ufb01cant change\noccurred in the distribution of the variables. The underlying\nquestion is if a small change in the variables\u2019 distribution is a\nrisk for model performance. Any model should allow for certain\nvariations in terms of the estimated coef\ufb01cients, basically\nbecause a statistically correct sample of a population may also\nrepresent a slightly different one. Therefore standard K\u2013S and\nv2-tests turned out not to be useful, since they are too sensitive\nto small variations. We solved this problem by constructing an\nempirical test as described in the following subsection using the\nmodel coef\ufb01cients and their standard deviations as proxies. If a\nvariable jhas an associated estimated coef\ufb01cient ^bjand an esti-\nmated standard deviation ^rj, it follows from the asymptotically\nnormal behavior of bcoef\ufb01cients in a logistic regression model\n(Hosmer and Lemeshow, 2000 ), that a 95% interval of con\ufb01-\ndence for the population parameter bjis:\n^bj/C01:96^rj6bj6^bj\u00fe1:96^rjj\u00bc1;...;N \u00f05\u00de\nThe follow-up problem consists of measuring the shift between\ntwo different data sets: the original one used to construct the logis-\ntic regression model, and a second one with new cases. Formally,\nwe have:\n/C15Original data set xand estimated parameters ^bjassociated with\nvariable j,j=1 , ...,N.\n/C15New data set x0, with new cases", "start_char_idx": 3212, "end_char_idx": 6844, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8bddc558-f028-4026-814c-7f84ef496937": {"__data__": {"id_": "8bddc558-f028-4026-814c-7f84ef496937", "embedding": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5ac57d1d-b8f7-49b6-a56d-e65f186921e0", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "16607738c46feb2729281ad64320d54d7616c90418c7ec57d4f072461a97b1b9"}, "2": {"node_id": "0c4d7c05-41d8-415d-9814-2916239e0a89", "node_type": null, "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}, "hash": "6efef63ce1c601cfc462a4b9172f8b90fe0ae1ebdcdc7239c409a038f50a1cbb"}}, "hash": "644ca6ff5f22d5f692253bff6e150a91fdc67205473bf2beae8c0ae0eee6e43f", "text": ", ...,N.\n/C15New data set x0, with new cases x0\ni\u00bcx0\ni1;...;x0\niN/C0/C1\n, and observed\noutputs y0\ni;i\u00bc1;...;NCwhere NCis the number of new cases.\n/C15Estimated default probabilities for the new cases\npx0\ni/C0/C1\n\u00f0i\u00bc1;...;NC\u00deobtained from the original models.\nThe respective literature provides some approximations to\nsolve this particular problem. The closest model to the one pre-\nsented here was presented by Zeira et al. (2005) . It takes into ac-\ncount a measure of the shift, not just if a shift has occurred. TheC. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366 361", "start_char_idx": 6870, "end_char_idx": 7469, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63c546c7-41a0-49fc-9c62-23f2a98a1ec6": {"__data__": {"id_": "63c546c7-41a0-49fc-9c62-23f2a98a1ec6", "embedding": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "a7635fe3b2b42806180a00b9e5b8443847ae6c40a0768a0623454f3937659ff3"}, "3": {"node_id": "829c1fcf-275d-4e8f-9680-c71989401abe", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "b3332f9c350242900582e14d7f504061a0285906499c4e3983ffc04fa6f0d6ff"}}, "hash": "f1ec381cb46ffbc86303abc2ff64b0373ae3a3e2e37fde0694b264a3a0926ce8", "text": "authors developed a statistical test for general models considering\nthe output errors, assuming that they distribute normally, and that\nthe variables are identically distributed. A variation of this ap-\nproach, proposed by Cieslak and Chawla (2007) , considers model\nevaluation in two steps: global discriminatory capacity of the mod-\nel, and changes in the distribution of the variables instead of mea-\nsuring them independently. Castermans et al. (2010) recently\ndeveloped a framework for monitoring and validating the use of\nrisk models within the context of Basel II accord. They consider a\nstability index to monitor the changes in the distribution of vari-\nables and a discrimination index for global performance.\n4.2.1. Statistical test for model follow-up\nThe basic idea of the test we developed is to check whether the\nnew coef\ufb01cients ^b0\njestimated from the new dataset \u00f0x0\ni1;...;\nx0\niN;y0\ni\u00de2R\u00f0N\u00fe1\u00de\u00f0i\u00bc1;...;NC) are still within the con\ufb01dence inter-\nval belonging to the original estimators ^bjof the variables.\nSince the variables of the new dataset are constructed the same\nway as those from the original one (a necessary condition forapplying the model), estimating the new coef\ufb01cients ^b\n0\njis straight-\nforward and can be done at a very low computational cost.\nWith the new estimators ^b0\nj, and their standard deviations \u00f0^r0\nj\u00de\nthat are obtained from the new dataset, a new statistic for the pop-\nulation values of the variable b0\njcan be constructed ( Greene, 1993 ).\nIf the new sample is large enough (necessary condition for estimat-\ning the logistic regression parameters), and considering the nor-\nmality of the coef\ufb01cients, the following is ful\ufb01lled:\n^b0\nj/C0bref\n^r0\nj,tNC/C0N; \u00f06\u00de\nwhere the statistic has a t-distribution with NC/C0Ndegrees of free-\ndom. The scalar brefrepresents the assumption about the population\nparameter. We constructed a statistical test to verify whether or not\nthe estimated new parameters are still within the con\ufb01dence inter-\nvals obtained for the original dataset considering two one-sided\nhypothesis tests, as shown in (7).\nH0:^b0\nj\u00bcblo\nH1:^b0\nj<bloandH0:^b0\nj\u00bcbup;\nH1:^b0\nj>bup\u00f07\u00de\nwhere blo:\u00bc^bj/C01:96^rjand bup:\u00bc^bj\u00fe1:96^rj.\nThe test checks whether or not the new parameters ^b0\njare still\nwithin the respective bounds determined by the previous model.\nIn this case, i.e. the null hypothesis H0is not rejected, the distribu-\ntions of the variables did not change signi\ufb01cantly, which recon-\n\ufb01rms the model\u2019s stability. To apply this test, the number of\ncases must be suf\ufb01cient to ensure normal distribution of the beta\ncoef\ufb01cients, which should be given if the procedure is conducted\nevery of three to six months.\nOur approach differs from the introduced models in several\npoints: \ufb01rst, by being model-dependent (uses the information\nand variability of the original model), it shows a more business-ori-\nented focus on the concept drift approach. The other methodolo-\ngies focus on whether a change occurred, or on the absolute\nstrength of this change, not in if the change proposed is relevant\nfor the model, as we believe should be. This fact ties the usefulness\nof our approach tightly to credit scoring, where logistic regression\nis", "start_char_idx": 0, "end_char_idx": 3194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "829c1fcf-275d-4e8f-9680-c71989401abe": {"__data__": {"id_": "829c1fcf-275d-4e8f-9680-c71989401abe", "embedding": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "a7635fe3b2b42806180a00b9e5b8443847ae6c40a0768a0623454f3937659ff3"}, "2": {"node_id": "63c546c7-41a0-49fc-9c62-23f2a98a1ec6", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "f1ec381cb46ffbc86303abc2ff64b0373ae3a3e2e37fde0694b264a3a0926ce8"}, "3": {"node_id": "91ec94eb-0a66-42fc-bb91-773947f3a7b1", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "a761ca40d599e32958c9d36c43c49caf3adcf2a3a5ad7b9019678234364fef6b"}}, "hash": "b3332f9c350242900582e14d7f504061a0285906499c4e3983ffc04fa6f0d6ff", "text": "usefulness\nof our approach tightly to credit scoring, where logistic regression\nis widely used and the changes on variables can be much more\nsudden, in contrast with the other methods that present a more\ngeneral approach. Another difference, contrasting Hellinger\u2019s dis-\ntance for concept drift ( Cieslak and Chawla, 2007 ) to our method,\nis that the former is only relevant for categorical variables, whereas\nour approach is applicable for both continuous and binary vari-\nables. The use of K\u2013S and v2measures, also common practice\naccording to the respective literature, is not recommended, sincethey are known to be very sensitive to small changes in the vari-\nables\u2019 distributions, and are therefore prone to false alarms. Finally,\na difference between our approach and the one by Castermans\net al. (2010) is that the bounds on the \u2018\u2018danger zone\u2019\u2019 of the latter\nare de\ufb01ned focusing on a percentage on the entropy in the compar-\nison of the distributions, which is somewhat arbitrary, whereas our\napproach is tied to a hard bound on the limits given by the cer-\ntainty on the original parameter estimation, which we believe is\nan advantage.\n5. Results\nIn this section we present the results we obtained applying the\nproposed methodology to the governmental organization we\nworked with. The methodology employed, however, is generic\nand has also been used in our projects for the private sector. In par-\nticular we analyzed two different datasets with loans that ranged\nfrom one to \ufb01ve years duration and amounts that varied between\nEUR 175 and EUR 17,500. Both datasets present an average granted\nloan of EUR 1500.\nThe \ufb01rst data set (Universe U2) contains 41,200 long-term loans\nfor new customers during a period of 12 years (1996\u20132007), and\npresents a high number of defaulters, with a default rate of\n26.2%. The second dataset (Universe U4) contains 110,000 long-\nterm loans for renewing customers during the same period, and\nshows a default rate of 17.9%.\nThe following subsections present results that were obtained in\neach one of the steps of the proposed methodology as described\nabove, paying special attention to the results achieved by our new-\nly developed steps for cut-off point construction and model follow-\nup.\n5.1. Variable selection\nThe repository created for datasets U2 and U4 initially con-\ntained more than 100 potential input variables. The goal of variable\nselection is to identify no more than 10\u201315 variables for model\nconstruction.\nVariables commonly used in literature can be divided into three\ndifferent groups: socio-demographic variables (customer provided,\nage, income, etc.), internal data (evolution of previous loans, other\nproducts, etc.), and external data (outstanding debts, checking ac-\ncounts, etc.) ( Anderson, 2007 ). All three were present in the origi-\nnal dataset, and all classical indicators of debt evolution were built,\nif possible. However, since the segment we analyzed did not have\naccess to \ufb01nancial services previously, the borrowers did not pos-\nsess common debts and income variables. This fact made it evenmore dif\ufb01cult to develop the respective scoring systems than those\npresented in literature, and made most of \u2018\u2018normal\u2019\u2019 variables\nuseless.\nUsing simple \ufb01lter methods for feature selection, we removed\nhighly concentrated variables (i.e. more than 99% of cases that have\nthe same feature value) and obviously irrelevant ones detected by\nK\u2013S and\nv2-tests, reducing the number of remaining variables to\nfewer than 100. Subsequently, we applied forward selection and\nbackward elimination for logistic regression obtaining a manage-\nable number of features for each", "start_char_idx": 3122, "end_char_idx": 6747, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "91ec94eb-0a66-42fc-bb91-773947f3a7b1": {"__data__": {"id_": "91ec94eb-0a66-42fc-bb91-773947f3a7b1", "embedding": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eae96a5e-8f42-4a52-bfae-1a9e0905f458", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "a7635fe3b2b42806180a00b9e5b8443847ae6c40a0768a0623454f3937659ff3"}, "2": {"node_id": "829c1fcf-275d-4e8f-9680-c71989401abe", "node_type": null, "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}, "hash": "b3332f9c350242900582e14d7f504061a0285906499c4e3983ffc04fa6f0d6ff"}}, "hash": "a761ca40d599e32958c9d36c43c49caf3adcf2a3a5ad7b9019678234364fef6b", "text": "for logistic regression obtaining a manage-\nable number of features for each model.\nDuring this variable selection process we maintained very close\ninteraction with our customers, in particular with business experts\nand future users in order to assure suitable input variables for the\nrespective models. This interaction is of utmost importance, since\nit adds business knowledge to the selection process and assures\nmodel acceptance by the intended users. However, in some cases\nwe obtained surprising results, such as the case of the income var-\niable, which was expected to be an important input. But as already362 C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366", "start_char_idx": 6744, "end_char_idx": 7438, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25c627e2-0801-4294-a18b-157a0edf0b48": {"__data__": {"id_": "25c627e2-0801-4294-a18b-157a0edf0b48", "embedding": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "1f90fef31d93f56b1cae303d08eed6e3ec52be026fd567bc051075b23dedc032"}, "3": {"node_id": "2946aa2b-e791-47ea-b86e-9cb9ce6eea43", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "3c5ad694f290f4385da1a577705dc1d79ce4f01c9a94cc27a98b00b12d2663b6"}}, "hash": "6def2e744f0851283d6eb8dde1688e229ea4dda69b1da757ffe19f67e81a759d", "text": "mentioned, income seems not to be a very good variable when it\ncomes to predicting micro-entrepreneurs\u2019 paying behavior using\ncredit scoring models. Our analyses con\ufb01rmed this assumption.\nThis is completely different from, for example, the mass consumer\nsegment, in which the income varies much more, and is a relevant\nvariable, especially when constructing indicators associating it\nwith debt.\nThe \ufb01nal variables selected for U4 are divided into two groups.\nThe socio-economic variables include Economic Activity (Activity),\nthe sector of the economy that the customer is immersed in\n(through his/her job or company). The large number of activities\nwas clustered to diminish the deviation and to improve interpreta-\ntion of the variable, bringing the 47 different activities into three\nhomogeneous groups of activities (Activity_A, Activity_B, and\nActivity_C); the ownership of housing (Ownership), whether thecustomer owns, rents, or has other types of agreements in his/\nher current home. Four classes are recognized: Owner, Tenant\n(Rent), Shares Tenant (Share), or others; the number of productive\nproperties the borrower controls, i.e. properties that are necessary\nto develop the respective activities and therefore different from\nownership, clusterized into zero, one, or more (NumProp_One,\nNumprop_More),; a clusterization of the regions in the country\n(Region_A, Region_B, Region_C). Finally, the age of the customer\nin years, normalized to [0,1] (Age), or transformed using the natu-\nral logarithm (LogAge) is included. It is known that in general age\nshould be treated as a discrete variable to account for non-linear-\nities. However, in our case the range of ages was much more re-\nstricted than in normal consumer loans and the behavior was\nmuch more linear regarding age. For simplicity reasons we used\nthis justi\ufb01cation and treated age as linear variable.\nThe second group characterize the credit history of the cus-\ntomer, including the number of current or parallel loans (Num-\nCurr), the number of closed loans (NumClosed), the average term\nfor all loans granted previously (AvgTerm), a binary variable indi-\ncating whether the customer has been in arrears for any of his/\nher past loans (PrevArr), the percentage of the paid installments\nof previous loans that have been in arrears (PercArr), and the max-\nimum number of days that the customer was in arrears for any pre-\nvious installment (MaxArr).\n5.2. Model results\nApplying the methodology for credit scoring described above,\nwe obtained a logistic regression model for each one of the two\ndatasets, U2 and U4. Table 2 displays the respective model param-\neters. The \ufb01nal models are characterized by 41,200 cases and 10\nvariables (in the case of U2), and 110,000 cases and 13 variables\n(in the case of U4). In both instances we used approximately 80%of cases for model construction and the remaining 20% for model\nevaluation.\nTo evaluate the obtained models, common accuracy measures\nwere estimated for both datasets, with an unsurprising result: it\nis much more dif\ufb01cult to construct a logistic regression model for\nnew customers than for renewing ones. The evaluation dataset\nassociated with new customers (U2) presented an Area Under\nthe Curve (AUC) of 0.6314, and a K\u2013S maximum distance of\n0.1991, while the evaluation dataset associated with renewing cus-\ntomers (U4) presented an AUC of 0.7795 with a K\u2013S maximum dis-\ntance of 0.4204. This represents a 15% increase in AUC and more\nthan double K\u2013S maximum difference between the two datasets.\nIt is interesting to note that this result is very much in line with\nconsumer credit scoring models, with similar adjustments to", "start_char_idx": 0, "end_char_idx": 3650, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2946aa2b-e791-47ea-b86e-9cb9ce6eea43": {"__data__": {"id_": "2946aa2b-e791-47ea-b86e-9cb9ce6eea43", "embedding": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "1f90fef31d93f56b1cae303d08eed6e3ec52be026fd567bc051075b23dedc032"}, "2": {"node_id": "25c627e2-0801-4294-a18b-157a0edf0b48", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "6def2e744f0851283d6eb8dde1688e229ea4dda69b1da757ffe19f67e81a759d"}, "3": {"node_id": "c4b7764a-daea-409f-ae9d-701d43e8cbf9", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "b0da5e89929a0f009998f79cf92e8e44873b41cdf029ec550411b30ce4f720b0"}}, "hash": "3c5ad694f290f4385da1a577705dc1d79ce4f01c9a94cc27a98b00b12d2663b6", "text": "that this result is very much in line with\nconsumer credit scoring models, with similar adjustments to those\nobserved in practice. The corresponding ROC curves are shown in\nFig. 1 .\nWhen studying the accuracy on a case-by-case basis, the mod-\nels present 74.8% accuracy for non-defaulters, and 65.7% fordefaulters from dataset U2, and 78.2% accuracy for non-defaulters,\nand 76.0% for defaulters from dataset U4, assuming a cut-off point\nof 0.5. Again, the overall measure accuracy provides better results\nfor known customers (U4) than for new ones (U2). As can be ob-\nserved from Tables 3 and 4 , however, the proposed models identify\nbetter among defaulters in U2, a result which is intuitive, since the\nbehavior of good payers is easily observed during repayment, and\nthis additional information allows for better discrimination of good\npayers. Furthermore, in our credit scoring projects we observed\nthat good payers in general show a much more homogeneous\nbehavior than defaulters which present a multiplicity of reasons\nto default.\nAnalyzing the obtained results reveals that the most important\nissue for determining paying behavior is the way a customer han-\ndles his/her budget, which is re\ufb02ected in variables associated with\nthe number of days in arrears that the customer has.\n5.3. Results of cut-off point construction\nAs was explained above, for cut-off point construction we need\ntables that consist of the solicited loans, the estimated defaultprobability, and the loss incurred when defaulting. To determine\ncut-off points, the calculated default probability is replaced by\nthe closest upper value of the 0.05 intervals used. That is, if the\nestimated probability is, for example, 0.543, it is replaced by\n0.55. Segmenting the cut-off values allows grouping the loans\nand analyzing the effects of changing the cut-off policy over a\nbatch of loans, instead of on a per-loan basis, which greatly simpli-\n\ufb01es the analysis.\nTo construct the tables as shown e.g. in Table 3 , we \ufb01rst identify\nthe customers in each 0.05 interval and then calculate accuracy in\nthe respective intervals. This is done separately for Good Custom-\ners (column 2 of each table) and for Defaulters (column 3 of each\ntable). The total number of correctly classi\ufb01ed cases is presented\nin column 4; see e.g.\nTable 3 .\nThe subsequent columns are used to estimate the related costs\nof rejecting a good applicant and accepting a bad applicant, respec-\ntively. Column 5 presents the average credit amount solicited by\ngood payers who would be rejected if the corresponding cut-offTable 2\nParameters obtained for logistic regression model. New customers (left) and renewingcustomers (right).\nVariable b S.E. Sig.\nNew customers (U2)\nOwnership_Owner /C0.421 .044 .000\nOwnership_Let /C0.071 .057 .216\nOwnership_Share .184 .147 .210LogAge /C0.342 .047 .000\nNumProp_One .614 .058 .000NumProp_More .111 .066 .092\nActivity_A .320 .052 .000\nActivity_B /C0.022 .054 .677\nRegion_A .093 .038 .015Region_B /C0.569 .044 .000\nRenewing customers (U4)\nRegion_A .092 .037 .011\nRegion_B /C0.238 .032 .000\nOwnership_Owner /C0.293 .037 .000\nOwnership_Let .076 .051 .139Ownership_Share .354 .114 .002NumProp_One .497 .034 .000NumProp_More .196 .035 .000LogAge /C0.436 .046", "start_char_idx": 3564, "end_char_idx": 6793, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4b7764a-daea-409f-ae9d-701d43e8cbf9": {"__data__": {"id_": "c4b7764a-daea-409f-ae9d-701d43e8cbf9", "embedding": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8379f36a-f221-43f2-b1fd-ceb2784fdca2", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "1f90fef31d93f56b1cae303d08eed6e3ec52be026fd567bc051075b23dedc032"}, "2": {"node_id": "2946aa2b-e791-47ea-b86e-9cb9ce6eea43", "node_type": null, "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}, "hash": "3c5ad694f290f4385da1a577705dc1d79ce4f01c9a94cc27a98b00b12d2663b6"}}, "hash": "b0da5e89929a0f009998f79cf92e8e44873b41cdf029ec550411b30ce4f720b0", "text": ".196 .035 .000LogAge /C0.436 .046 .000\nNumClosed /C0.090 .004 .000\nNumCurr /C0.034 .013 .007\nPrevArr 1.493 .032 .000PercArr .089 .018 .000MaxArr .001 .000 .000C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366 363", "start_char_idx": 6847, "end_char_idx": 7087, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0fec8e8d-56cf-4489-8de0-b9b7a6028d34": {"__data__": {"id_": "0fec8e8d-56cf-4489-8de0-b9b7a6028d34", "embedding": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cf803f2-ac0e-4462-8207-dff6f8aec554", "node_type": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "hash": "592ce880082b8232655d7189c080b73d1ebb6abd1067e2ee96b38a079f39bbc5"}, "3": {"node_id": "424857bc-8bdc-4854-983b-0d9c8c3b7da6", "node_type": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "hash": "09e68ad0705ff06826f3ef9eb73a4795ae606703dc25694bd81535dbe634c30a"}}, "hash": "bf019081944e9b0eff0198394b851875d53de0000d88cde1c579a8de7c50687e", "text": "point were used. Column 6 was calculated using Eq. (2), considering\nthe average observed loss per-customer up to that cut-off point.\nSome considerations are relevant: \ufb01rst, collateral is not consid-\nered in this segment since the institution would incur in important\nsocial and direct costs when trying to recover such items. Conse-\nquently, there is no active policy for their collection. Second, the\nexposure of the loan considers only the amount due (amortiza-tion), not the interest collected. This is done to ensure transparency\nin the reported losses, since interest rates may vary from borrower\nto borrower.\nColumns 7 and 8 present, for each possible cut-off point, the\ncost of rejected loans and the cost of accepted loans, respectively.\nThe cost of rejected loans is obtained by multiplying the cost of\nrejecting a good applicant (0.2644 /C1Column 5) and the number of\ngood applicants that would have been rejected for a given cut-off\npoint (from Column 2, all good borrowers -last row- minus the va-\nlue for the cut-off point). On the other hand, the cost of accepted\nloans is calculated by multiplying the cost of accepting a bad payer,\nfrom Column 6, by the number of bad payers that would have been\naccepted in that cut-off (from Column 3, all defaulters -last row-\nminus the value for the cut-off point).\nThe total cost (Column 9) is \ufb01nally obtained by adding the cost\nof rejected loans and the cost of accepted loans (Column 7 + Col-\numn 8). Both resulting tables are presented in Tables 3 and 4 ,\nrespectively.\nThese tables reveal very interesting insights into the respective\nbusiness. First, the proposed cut-off points are reasonable in the\nlight of the risk associated with each of the two datasets. For\nnew customers (U2), the total cost is the dominating factor taking\ninto account that it is very dif\ufb01cult to determine the correct class of\na \ufb01rst-time customer and that very high default rates have been\nobtained for this segment. Consequently, the suggestion is a very\nconservative 0.55 cut-off point, which translates into an accep-\ntance rate (coverage) of only 45%. Evaluating the remaining re-\nquests carefully by a committee of experts considering the cut-\noff point associated with accuracy (0.95) is recommended.\nThe cut-off points for dataset U4 present a different picture,\nsince for renewing customers much more information is available\nand the respective segment presents lower default rates. The cut-\noff point that minimizes total costs is 0.7 which translates into adirect acceptance rate of 85%. Loans with calculated reject proba-\nbility greater than the cut-off point that maximizes accuracy\n(0.8) are rejected directly, leaving only a 7% of solicited loans to a\ncommittee.\nIt is also interesting to analyze the expected savings this policy\nwould have generated. The current policy has a cost of 8,770,881\nEUR for dataset U2, and 17,861,211 EUR for dataset U4 (consider-\ning only costs associated with loss). A conservative calculation\ncan be performed to estimate the savings of using the models, con-\nsidering that the cut-off point that minimizes costs is used, i.e.\nthere is no committee.\nUnder this setting, the savings for our governmental institution,\nand therefore for society, when using the models in dataset U2 are\n8,770,881\u20137,213,219 = 1,557,662 EUR, which is equivalent to 4.2\nFig. 1. ROC curves for the two datasets: new customers (U2, AUC = 0.6314) and renewing customers (U4, AUC = 0.7795).\nTable 4\nCut-off", "start_char_idx": 0, "end_char_idx": 3463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "424857bc-8bdc-4854-983b-0d9c8c3b7da6": {"__data__": {"id_": "424857bc-8bdc-4854-983b-0d9c8c3b7da6", "embedding": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9cf803f2-ac0e-4462-8207-dff6f8aec554", "node_type": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "hash": "592ce880082b8232655d7189c080b73d1ebb6abd1067e2ee96b38a079f39bbc5"}, "2": {"node_id": "0fec8e8d-56cf-4489-8de0-b9b7a6028d34", "node_type": null, "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}, "hash": "bf019081944e9b0eff0198394b851875d53de0000d88cde1c579a8de7c50687e"}}, "hash": "09e68ad0705ff06826f3ef9eb73a4795ae606703dc25694bd81535dbe634c30a", "text": "(U4, AUC = 0.7795).\nTable 4\nCut-off table for renewing customers (U4). All monetary amounts in EUR thousands.\nCut-offCorrectly classi\ufb01ed Avg.\namountAvg.\nlossCost\n(good)Cost\n(def.)Total\ncost\nGood Defaulter Total\n0.4 53,805 15,563 69,368 1666 620 15,936 2510 18,447\n0.45 60,374 14,473 74,847 1711 641 13,393 3296 16,690\n0.5 66,129 13,428 79,557 1740 665 10,976 4115 15,0910.55 71,090 12,292 83,382 1765 690 8820 5050 13,8700.6 75,243 11,093 86,336 1788 727 6969 6191 13,1600.65 78,890 9738 88,628 1811 738 5311 7292 12,604\n0.7 82,004 8,312 90,316 1,876 746 3,957 8,432 12,390\n0.75 84,642 6685 91,327 1980 747 2796 9659 12,456\n0.8 86,737 5054 91,791 2118 766 1818 11,158 12,977\n0.85 88,418 3358 91,776 2295 797 950 12,950 13,901\n0.9 89,491 1650 91,141 2414 838 315 15,056 15,3710.95 89,894 578 90,472 2107 887 50 16,880 16,931\n1 89,985 0 89,985 0 911 0 17,861 17,861Total 89,985 19,614 109,599 \u2013\u2013 \u2013 \u2013 \u2013Table 3\nCut-off table for new customers (U2). Cut-off points maximizing accuracy (0.95) andminimizing total cost (0.55) are marked as bold , respectively. All monetary amounts\nin EUR thousands.\nCut-offCorrectly classi\ufb01ed Avg.\namountAvg.\nlossCost\n(good)Cost\n(def.)Total\ncost\nGood Defaulter Total\n0.4 5847 9828 15,675 967 1789 6279 1731 8010\n0.45 8504 9239 17,743 900 1614 5208 2513 77220.5 11,768 8458 20,226 831 1402 4093 3278 7371\n0.55 15,340 7457 22,797 784 1226 3120 4092 7213\n0.6 18,880 6176 25,056 740 1102 2254 5092 73460.65 22,142 4786 26,928 696 1009 1519 6062 7581\n0.7 25,234 3304 28,538 644 941 878 7052 7931\n0.75 27,787 1916 29,703 575 886 396 7870 82670.8 29,404 790 30,194 493 848 129 8481 86100.85 30,180 153 30,333 354 820 20 8725 87450.9 30,376 7 30,383 243 813 1 8770 8771\n0.95 30,396 0 30,396 0 812 0 8770 8770\nTotal 30,396 10,796 41,192 \u2013\u2013 \u2013 \u2013 \u2013364 C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366", "start_char_idx": 3428, "end_char_idx": 5272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b6644d8-c129-402d-a4f4-f3e127145b3d": {"__data__": {"id_": "2b6644d8-c129-402d-a4f4-f3e127145b3d", "embedding": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "904052d6-e23e-4a7a-895d-934074a819dc", "node_type": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "hash": "d46bc8d7ddcd98800e3829a37273e8fa34a3f7efdd930db3e09f008764a4ba6f"}, "3": {"node_id": "a9429596-81c8-4c43-8b73-a125392a8be6", "node_type": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "hash": "d6a5bb57608b50ffd73a88497608986431dfe406042203ef8251fbb662b1eaa0"}}, "hash": "c0af2dc69ebfeb46f21f4a6309e6f5a8382c38c9528278465d05cf7b42cc2284", "text": "EUR per loan granted. The savings in U4 are 17,861,211 /C0\n12,390,124 = 5,471,087, or a staggering 49.9 EUR per loan.\nThere is no need to emphasize the usefulness of the proposed\nmethodology and of the use of credit scoring models in general,\nconsidering the substantial savings that are produced by the use\nof risk assessment models.\n5.4. Follow-up results\nIn order to study the performance of the proposed follow-up\nmethodology, we constructed models for datasets U2 and U4 usingthe loans granted during the period 2000\u20132004, and then we ana-\nlyzed the models\u2019 changes for the period 2005\u20132007. We divided\nthe sample into an 80 percent training set for both periods, and a\ntest set consisting of 20 percent of the data, constructing two test\nsets and two training sets. We trained four models, one for each\nperiod, and for each dataset (U2\u2013U4). Fig. 2 shows an example of\nhow a particular variable (number of current loans from U4)\nchanges its distribution in a period of three years, affecting its\nin\ufb02uence in the model.To motivate the effects of concept drift we estimated the loss in\ndiscriminatory capacity applying the model with \u2018\u2018old\u2019\u2019 data to the\nnew test set: the AUC of the model built with new data corre-\nsponds to 0.6404, and to 0.5924 using the original model, repre-\nsenting a drop of eight percent in AUC. Finally, from the work of\nBlochlinger and Leippold (2006) it can be deduced that a drop of\n0.01 in the K\u2013S statistic can translate in a loss of up to two percent\nin the net utility of the lender, and in this case the statistic goes\ndown from 0.2359 to 0.1622, representing 75 base points less, or\nup to 15 percent loss of utility for the lender, which can have cat-\nastrophic effects. We believe this re\ufb02ects the strong need for prop-\ner model follow-up.\nThe results obtained for datasets U2 and U4 using the test pro-\nposed in Eq. (7)are shown in Tables 5 and 6 , respectively. For each\nvariable we computed the coef\ufb01cients ^b\n0for the period 2005\u20132007,\nits standard deviation ^r0, the coef\ufb01cients ^bfor period 2000\u20132004,\nits lower bound bloand upper bound bup,t h e t-statistics tloand tup\nfor both hypothesis tests presented in Eq. (7)and the one-sided P val-\nuesploand pup. These latter values can be interpreted as the probabil-\nity that the statistic ^b0would differ as much as the boundariesOutstandin g LoansPercentage\n0 1 2 3 4 5 6 7 8 9 10 11 12 1310% 20% 30% 40%Non\u2212Defaulters\nDefaulters\nOutstandin g LoansPercentage\n0123456789 1 010% 20% 30% 40%Non\u2212DefaultersDefaulters(a) (b)\nFig. 2. Distribution of variable NumCurr for U4, period 2000\u20132004 (left) and 2005\u20132007 (right).\nTable 5\nFollow-up results, new customers (U2).\nVariable ^b0 ^r0 ^b blo bup tlo tup plo pup\nOwnership_Owner .283 .096 .559 .416 .701 /C01.378 /C04.347 .084 1.000\nOwnership_Let .216 .424 .527 .021 1.033 .461 /C01.926 1.000 1.000\nOwnership_Share .167 .114 .367 .216 .518 /C0.426 /C03.083 .335 1.000\nLogAge /C0.141 .119 /C0.659 /C0.821 /C0.496 5.707 2.984 1.000 .001\nNumProp_One /C0.295", "start_char_idx": 0, "end_char_idx": 2991, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a9429596-81c8-4c43-8b73-a125392a8be6": {"__data__": {"id_": "a9429596-81c8-4c43-8b73-a125392a8be6", "embedding": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "904052d6-e23e-4a7a-895d-934074a819dc", "node_type": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "hash": "d46bc8d7ddcd98800e3829a37273e8fa34a3f7efdd930db3e09f008764a4ba6f"}, "2": {"node_id": "2b6644d8-c129-402d-a4f4-f3e127145b3d", "node_type": null, "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}, "hash": "c0af2dc69ebfeb46f21f4a6309e6f5a8382c38c9528278465d05cf7b42cc2284"}}, "hash": "d6a5bb57608b50ffd73a88497608986431dfe406042203ef8251fbb662b1eaa0", "text": "2.984 1.000 .001\nNumProp_One /C0.295 .102 /C01.108 /C01.267 /C0.948 9.512 6.394 1.000 .000\nNumProp_More /C0.795 .224 /C01.953 /C02.310 /C01.597 6.757 3.579 1.000 .000\nActivity_A .212 .090 .169 .035 .303 1.966 /C01.003 1.000 1.000\nActivity_B .171 .101 /C0.546 /C0.693 /C0.400 8.519 5.632 1.000 .000\nRegion_A /C0.167 .093 /C0.440 /C0.609 /C0.270 4.782 1.117 1.000 .132\nRegion_B .015 .082 /C0.100 /C0.219 .019 2.860 /C0.041 1.000 1.000\nTable 6\nFollow-up results, renewing customers (U4).\nVariable ^b0 ^r0 ^b blo bup tlo tup plo pup\nRegion_A .112 .078 .121 /C0.005 .247 1.492 /C01.725 .000 1.000\nRegion_B /C0.197 .075 /C0.135 /C0.273 .003 1.010 /C02.649 1.000 1.000\nOwnership_Owner .268 .094 .689 .530 .848 /C02.775 /C06.154 .003 1.000\nOwnership_Let .379 .293 .919 .494 1.345 /C0.391 /C03.294 .348 1.000\nOwnership_Share .122 .095 .632 .488 .775 /C03.852 /C06.874 .000 1.000\nNumProp_One .086 .074 /C0.687 /C0.808 /C0.566 12.045 8.790 1.000 .000\nNumProp_More /C0.018 .084 /C01.143 /C01.286 /C01.000 15.047 11.654 1.000 .000\nLogAge /C0.397 .126 /C0.636 /C0.814 /C0.458 3.309 .488 1.000 .313\nNumClosed /C0.067 .007 /C0.114 /C0.131 /C0.096 9.536 4.307 1.000 .000\nNumCurr /C0.038 .035 .104 .057 .152 /C02.659 /C05.358 .004 1.000\nPrevArr 1.351 .093 1.877 1.751 2.004 /C04.322 /C07.051 .000 1.000\nPercArr /C0.023 .026 .077 /C0.001 .154 /C0.837 /C06.702 .201 1.000\nMaxArr .001 .000 .002 .002 .002 /C03.832 /C07.236 .000 1.000C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366 365", "start_char_idx": 2955, "end_char_idx": 4448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5abf8005-f0f5-4f14-aa1f-f1e79bd64672": {"__data__": {"id_": "5abf8005-f0f5-4f14-aa1f-f1e79bd64672", "embedding": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc341b8-7c7c-42bd-a397-6656a6719120", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "72c15fd4f6ca4f253958e308dfc2107e14e17de1a4d175249f9012996582f370"}, "3": {"node_id": "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "61cb705ddc2895cea945c7a4391b65c609ce7d933ea68c540e04e37d530499c8"}}, "hash": "434c5b635ce63c64e77f775c23e7fb587bb2f699f4985e631ae8fc7ae26f63a4", "text": "obtained from the original dataset in the direction speci\ufb01ed by the\nhypothesis just by chance, even though the parameter is actually\nwithin these boundaries (assuming that the null hypothesis is true).\nP values below 0.05 can be considered low enough to af\ufb01rm that\nthe new statistic differs signi\ufb01cantly from the original one.\nIt can be concluded from these experiments that the models\npresent an important loss in performance, especially regarding\nprediction of non-defaulters. One of the main reasons for this\nchange can be seen in the variables, where four out of ten present\ncritical changes in their distribution for new customers (Age, Num-\nber of Properties in two levels, and Activity), affecting the perfor-\nmance of the model. The change is even greater for the universe\nof renewing customers, where nine out of thirteen variables pres-\nent signi\ufb01cant changes in comparison with the original data set.\nFurthermore, many of the affected variables become irrelevant inthe new data set, which is a clear sign that the models need to\nbe re-adjusted. These results underline the potential of the model\nfollow-up methodology proposed in this paper.\n6. Conclusions and future work\nGranting loans to micro-entrepreneurs is a very important busi-\nness opportunity in developing countries. As a country develops,\ngranting such loans is slowly moving away from public institutions\nand is being considered a real business opportunity for private\norganizations, such as banks. The high risk, however, associated\nwith micro-entrepreneurs is one of the main problems that hinders\nthe expansion of this type of loans, and consequently a faster\ndevelopment of the respective economies. This explains the utmost\nimportance of adequate risk assessment models that are tailored to\nthe particular characteristics of micro-entrepreneurs.\nIn this paper, we have described the successful adaptations of\nthe standard KDD process to the particular needs of public and pri-\nvate \ufb01nancial institutions that grant loans to micro-entrepreneurs,\nusing logistic regression as the classi\ufb01cation method. This proce-\ndure provides good results and a useful interpretation of the\nrespective input variables.\nTraditional credit scoring variab l e ss u c ha s\u2018 \u2018 i n c o m e \u2019 \u2019w e r en o tr e l -\nevant. Micro-entrepreneurs have similar incomes and therefore this\nvariable does not discriminate. We also tried related variables using\nincome, such as income/debt, etc. which neither discriminated well.\nAnother important conclusion is to use variables that can be\ncorroborated. Ambiguous variables describing characteristics that\ncannot be proven easily are not useful since customers and/or\nsalespersons know how to answer certain questions in order to im-\nprove their chances of getting the loan.\nThe proposed cut-off point methodology explicitly quanti\ufb01es\nthe lost bene\ufb01t for society when a loan for a good customer is\nnot granted. Our numerical experiments underline the potential\nimpact such a methodology might generate, and help in quantify-\ning the bene\ufb01t of using statistical models in practice, with impor-\ntant savings when compared to the absence of such models.\nMicro-entrepreneurs represent a very volatile market, and are\ntherefore very sensitive to changes in economic conditions, which\nmakes the nature of the respective credit granting operations very\ndynamic and subject to constant change. This is re\ufb02ected by shift-\ning risk factors. Tools that detect such shifts are very attractive for\npractitioners but not always available in standard solutions for\ncredit scoring. In this paper we introduced statistical tests for mod-\nel follow-up that were developed in our projects and provided\nexcellent results.\nBy using adequate methodologies for credit risk management,\nthe market of loans for micro-entrepreneurs will continue to grow\nas more private", "start_char_idx": 0, "end_char_idx": 3839, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d": {"__data__": {"id_": "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d", "embedding": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc341b8-7c7c-42bd-a397-6656a6719120", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "72c15fd4f6ca4f253958e308dfc2107e14e17de1a4d175249f9012996582f370"}, "2": {"node_id": "5abf8005-f0f5-4f14-aa1f-f1e79bd64672", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "434c5b635ce63c64e77f775c23e7fb587bb2f699f4985e631ae8fc7ae26f63a4"}, "3": {"node_id": "cbca2dff-8b71-46ca-8612-3a76543a55f9", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "5a74d703ba40118eb7ccf6dfd5bcf64dd8fbbe1dfdf86a823f1ee875b081adf3"}}, "hash": "61cb705ddc2895cea945c7a4391b65c609ce7d933ea68c540e04e37d530499c8", "text": "market of loans for micro-entrepreneurs will continue to grow\nas more private companies will offer such loans; see e.g. Kim and\nSohn (2007) and Kim and Sohn (2010) for a similar situation. This,in turn, will foster their capacity to innovate and generate growth.\nOR-methodologies will contribute to a sustainable development of\nthe respective countries as has been shown already for many other\ncases in White et al. (2011) .\nAcknowledgments\nThe \ufb01rst author acknowledges CONICYT for the Grants that sup-\nports this work (AT-24110006, NAC-DOC: 21090573). All authors\nacknowledge the support of the institution which provided the\ndata. The work reported in this paper has been partially funded\nby the Institute of Complex Engineering Systems (ICM: P-05-004-\nF, CONICYT: FBO16) and the Finance Center of the Department of\nIndustrial Engineering, Universidad de Chile, with the support of\nbank Bci.\nReferences\nAnderson, R., 2007. The Credit Scoring Toolkit. Oxford University Press.\nArrow, K.J., Debreu, G., 2002. Landmark Papers in General Equilibrium Theory,\nSocial Choice and Welfare. Edward Elgar Publishing.\nBaesens, B., Van Gestel, T., Viaene, S., Stepanova, M., Suykens, J., Vanthienen, J., 2003.\nBenchmarking state-of-the-art classi\ufb01cation algorithms for credit scoring. The\nJournal of the Operational Research Society 54, 627\u2013636.\nBellotti, T., Crook, J., 2008. Credit scoring with macroeconomic variables using\nsurvival analysis. Journal of the Operational Research Society 60, 1699\u20131707.\nBlochlinger, A., Leippold, M., 2006. Economic bene\ufb01t of powerful credit scoring.\nJournal of Banking & Finance 30, 851\u2013873.\nBrown, I., Mues, C., 2012. An experimental comparison of classi\ufb01cation algorithms\nfor imbalanced credit scoring data sets. Expert Systems with Applications 39,3446\u20133453.\nCastermans, G., Hamers, B., Van Gestel, T., Baesens, B., 2010. An overview and\nframework for PD backtesting and benchmarking. The Journal of the\nOperational Research Society 61, 359\u2013373.\nCieslak, D., Chawla, N., 2007. Detecting fractures in classi\ufb01er performance. In:\nProceedings of the Seventh IEEE International Conference on Data Mining.\nDepartment of Computer Science and Engineering. University of Notredame, pp.123\u2013132.\nInstituto Nacional de Estad\u0131 \u00b4sticas, I., 2002. Primera encuesta de las micro, peque\u00f1as\ny medianas empresas [\ufb01rst survey of micro, small, and medium companies]\n(retrieved 14.02.12).\nFayyad, U., Piatetsky-Shapiro, G., Smyth, P., 1996. The KDD process for extracting\nuseful knowledge from volumes of data. Communications of the ACM 39, 27\u2013\n34.\nFinlay, S., 2011. Multiple classi\ufb01er architectures and their application to credit risk\nassessment. European Journal of Operational Research 210, 368\u2013378.\nGreene, W.H., 1993. Econometric Analysis. Prentice Hall.Hand, D., Henley, W., 1997. Statistical classi\ufb01cation methods in consumer credit\nscoring: a review. Journal of the Royal Statistical Society Association 160, 523\u2013\n541.\nHosmer, D., Lemeshow, H., 2000. Applied Logistic Regression. John Wiley & Sons.\nKim, H.S., Sohn, S.Y., 2007. Random effects logistic regression model for default\nprediction of technology credit garantee fund. European Journal of Operational\nResearch 183, 472\u2013478.\nKim, H.S., Sohn,", "start_char_idx": 3773, "end_char_idx": 6987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cbca2dff-8b71-46ca-8612-3a76543a55f9": {"__data__": {"id_": "cbca2dff-8b71-46ca-8612-3a76543a55f9", "embedding": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cfc341b8-7c7c-42bd-a397-6656a6719120", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "72c15fd4f6ca4f253958e308dfc2107e14e17de1a4d175249f9012996582f370"}, "2": {"node_id": "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d", "node_type": null, "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}, "hash": "61cb705ddc2895cea945c7a4391b65c609ce7d933ea68c540e04e37d530499c8"}}, "hash": "5a74d703ba40118eb7ccf6dfd5bcf64dd8fbbe1dfdf86a823f1ee875b081adf3", "text": "Operational\nResearch 183, 472\u2013478.\nKim, H.S., Sohn, S.Y., 2010. Support vector machines for default prediction of SMEs\nbased on technology credit. European Journal of Operational Research 201,\n838\u2013846.\nDivisi\u00f3n Empresas de Menor Tama\u00f1o, M.d.E., 2009. Encuesta longitudinal de\nempresas [longitudinal survey of companies] (retrieved 14.02.12).\nMinisterio de Econom\u0131 \u00b4a. 2012. Segunda encuesta de microemprendimiento [second\nmicro-entrepreneurship survey] (retrieved 05.05.12).\nOzdemir, B., Miu, P., 2009. Basel II Implementation. McGraw-Hill.\nSchreiner, M., 2000. Credit scoring for micro\ufb01nance \u2013 can it work? Journal of\nMicro\ufb01nance 2, 105\u2013119.\nSetiono, R., Baesens, B., Mues, C., 2009. A note on knowledge discovery using neural\nnetworks and its application to credit card screening. European Journal ofOperational Research 192, 326\u2013332.\nSuperintendencia de Bancos e Instituciones Financieras. 2008. Compendio de\nNormas Contables [Compendium of Accounting Rules]. SBIF.\nThomas, L., Crook, J., Edelman, D., 2002. Credit Scoring and its Applications. SIAM.\nTong, E.N.C., Mues, C., Thomas, L.C., 2012. Mixture cure models: if and when\nborrowers default. European Journal of Operations Research 218, 132\u2013139.\nVan Gool, J., Verbeke, W., Sercu, P., Baesens, B., 2011. Credit scoring for\nmicro\ufb01nance: is it worth it? International Journal of Finance & Economics\n(24.01.11).\nWhite, L., Smith, H., Currie, C., 2011. OR in developing countries: a review. European\nJournal of Operational Research 208, 1\u201311.\nZeira, G., Last, M., Maimon, O., 2005. Advanced Techniques in Knowledge Discovery\nand Data Mining. Segmentation on Continuous Data Streams Based on a Change\nDetection Methodology. Springer, pp. 103\u2013126.366 C. Bravo et al. / European Journal of Operational Research 227 (2013) 358\u2013366", "start_char_idx": 7003, "end_char_idx": 8782, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8c42a8c-dd74-4138-af03-344135c7165a": {"__data__": {"id_": "c8c42a8c-dd74-4138-af03-344135c7165a", "embedding": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05100dcf-aae1-4774-a8fa-1153b914e00b", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "3": {"node_id": "fd983614-a094-4a87-abbd-4dfe91740d3d", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "0c928a8294d4358629697539a5977c257cbe0ffba4ecda86a2fe37f39803eca8"}}, "hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236", "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005 645\nSurvey of Clustering Algorithms\nRui Xu , Student Member, IEEE and Donald Wunsch II , Fellow, IEEE\nAbstract\u2014 Data analysis plays an indispensable role for un-\nderstanding various phenomena. Cluster analysis, primitiveexploration with little or no prior knowledge, consists of researchdeveloped across a wide variety of communities. The diversity,on one hand, equips us with many tools. On the other hand,the profusion of options causes confusion. We survey clusteringalgorithms for data sets appearing in statistics, computer science,and machine learning, and illustrate their applications in somebenchmark data sets, the traveling salesman problem, and bioin-formatics, a new \ufb01eld attracting intensive efforts. Several tightlyrelated topics, proximity measure, and cluster validation, are also\ndiscussed.\nIndex Terms\u2014 Adaptive resonance theory (ART), clustering,\nclustering algorithm, cluster validation, neural networks, prox-imity, self-organizing feature map (SOFM).\nI. INTRODUCTION\nWE ARE living in a world full of data. Every day, people\nencounter a large amount of information and store or\nrepresent it as data, for further analysis and management. Oneof the vital means in dealing with these data is to classify or\ngroup them into a set of categories or clusters. Actually, as one\nof the most primitive activities of human beings [14], classi-\ufb01cation plays an important and indispensable role in the longhistory of human development. In order to learn a new object\nor understand a new phenomenon, people always try to seek\nthe features that can describe it, and further compare it withother known objects or phenomena, based on the similarity ordissimilarity, generalized as proximity, according to some cer-\ntain standards or rules. \u201cBasically, classi\ufb01cation systems are ei-\nther supervised or unsupervised, depending on whether they as-sign new inputs to one of a \ufb01nite number of discrete supervised\nclasses or unsupervised categories, respectively [38], [60], [75].\nIn supervised classi\ufb01cation, the mapping from a set of input datavectors (\n, where\n is the input space dimensionality), to\na \ufb01nite set of discrete class labels (\n , where\n is\nthe total number of class types), is modeled in terms of some\nmathematical function\n , where\n is a vector of\nadjustable parameters. The values of these parameters are de-termined (optimized) by an inductive learning algorithm (also\ntermed inducer), whose aim is to minimize an empirical risk\nfunctional (related to an inductive principle) on a \ufb01nite data setof input\u2013output examples,\n, where\n is\nthe \ufb01nite cardinality of the available representative data set [38],\nManuscript received March 31, 2003; revised September 28, 2004. This work\nwas supported in part by the National Science Foundation and in part by the\nM. K. Finley Missouri Endowment.\nThe authors are with the Department of Electrical and Computer Engineering,\nUniversity of Missouri-Rolla, Rolla, MO 65409 USA (e-mail: rxu@umr.edu;\ndwunsch@ece.umr.edu).\nDigital Object Identi\ufb01er 10.1109/TNN.2005.845141[60], [167]. When the inducer reaches convergence or termi-\nnates, an induced classi\ufb01er is generated [167].\nIn unsupervised classi\ufb01cation, called clustering or ex-\nploratory data analysis, no labeled data are available [88],[150]. The goal of clustering is to separate a \ufb01nite unlabeleddata set into a \ufb01nite and discrete set of \u201cnatural,\u201d hidden data\nstructures, rather than", "start_char_idx": 0, "end_char_idx": 3458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fd983614-a094-4a87-abbd-4dfe91740d3d": {"__data__": {"id_": "fd983614-a094-4a87-abbd-4dfe91740d3d", "embedding": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "05100dcf-aae1-4774-a8fa-1153b914e00b", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "2": {"node_id": "c8c42a8c-dd74-4138-af03-344135c7165a", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236"}}, "hash": "0c928a8294d4358629697539a5977c257cbe0ffba4ecda86a2fe37f39803eca8", "text": "and discrete set of \u201cnatural,\u201d hidden data\nstructures, rather than provide an accurate characterization\nof unobserved samples generated from the same probabilitydistribution [23], [60]. This can make the task of clustering falloutside of the framework of unsupervised predictive learning\nproblems, such as vector quantization [60] (see Section II-C),\nprobability density function estimation [38] (see Section II-D),[60], and entropy maximization [99]. It is noteworthy thatclustering differs from multidimensional scaling (perceptual\nmaps), whose goal is to depict all the evaluated objects in a\nway that minimizes the topographical distortion while using asfew dimensions as possible. Also note that, in practice, many(predictive) vector quantizers are also used for (nonpredictive)\nclustering analysis [60].\nNonpredictive clustering is a subjective process in nature,\nwhich precludes an absolute judgment as to the relative ef\ufb01-cacy of all clustering techniques [23], [152]. As pointed out by\nBacker and Jain [17], \u201cin cluster analysis a group of objects is\nsplit up into a number of more or less homogeneous subgroupson the basis of an often subjectively chosen measure of sim-ilarity (i.e., chosen subjectively based on its ability to create\n\u201cinteresting\u201d clusters), such that the similarity between objects\nwithin a subgroup is larger than the similarity between objectsbelonging to different subgroups\u201d\u201d\n1.\nClustering algorithms partition data into a certain number\nof clusters (groups, subsets, or categories). There is no univer-\nsally agreed upon de\ufb01nition [88]. Most researchers describe acluster by considering the internal homogeneity and the externalseparation [111], [124], [150], i.e., patterns in the same cluster\nshould be similar to each other, while patterns in different clus-\nters should not. Both the similarity and the dissimilarity shouldbe examinable in a clear and meaningful way. Here, we givesome simple mathematical descriptions of several types of clus-\ntering, based on the descriptions in [124].\nGiven a set of input patterns\n,\nwhere\n and each measure\nis said to be a feature (attribute, dimension, or variable).\n\u2022 (Hard) partitional clustering attempts to seek a\n -par-\ntition of\n , such that\n1)\n ;\n2)\n ;\n3)\n and\n .\n1The preceding quote is taken verbatim from verbiage suggested by the\nanonymous associate editor, a suggestion which we gratefully acknowledge.\n1045-9227/$20.00 \u00a9 2005 IEEE", "start_char_idx": 3392, "end_char_idx": 5813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "962f9d3f-47d2-4b75-80d2-c6e383155097": {"__data__": {"id_": "962f9d3f-47d2-4b75-80d2-c6e383155097", "embedding": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0285f646-8be1-4699-a3f0-38b1829c02bc", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "3": {"node_id": "cb9c835c-b4cb-46f9-9190-781b786f60c6", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "6d4aa1864edb5c87fdc00c976c5fc7f7e86f40d01ace69e99a9fe207f6a1fd1e"}}, "hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f", "text": "646 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 1. Clustering procedure. The typical cluster analysis consists of four steps with a feedback pathway. These steps are closely related to each oth er and affect\nthe derived clusters.\n\u2022) Hierarchical clustering attempts to construct a\ntree-like nested structure partition of\n, such that\n, and\n imply\n or\n for all\n.\nFor hard partitional clustering, each pattern only belongs to\none cluster. However, a pattern may also be allowed to belongto all clusters with a degree of membership,\n, which\nrepresents the membership coef \ufb01cient of the\n th object in the\nth cluster and satis \ufb01es the following two constraints:\nand\nas introduced in fuzzy set theory [293]. This is known as fuzzy\nclustering, reviewed in Section II-G.\nFig. 1 depicts the procedure of cluster analysis with four basic\nsteps.\n1) Feature selection or extraction . As pointed out by Jain\net al. [151], [152] and Bishop [38], feature selection\nchoosesdistinguishingfeaturesfromasetofcandidates,\nwhile feature extraction utilizes some transformationsto generate useful and novel features from the originalones. Both are very crucial to the effectiveness of clus-\ntering applications. Elegant selection of features can\ngreatly decrease the workload and simplify the subse-quentdesignprocess.Generally,idealfeaturesshouldbeof use in distinguishing patterns belonging to different\nclusters, immune to noise, easy to extract and interpret.\nWe elaborate the discussion on feature extraction inSection II-L, in the context of data visualization anddimensionality reduction. More information on feature\nselection can be found in [38], [151], and [250].\n2) Clustering algorithm design or selection . The step is\nusually combined with the selection of a correspondingproximity measure and the construction of a criterion\nfunction. Patterns are grouped according to whether\nthey resemble each other. Obviously, the proximitymeasure directly affects the formation of the resultingclusters. Almost all clustering algorithms are explicitly\nor implicitly connected to some de \ufb01nition of proximity\nmeasure. Some algorithms even work directly on theproximity matrix, as de \ufb01ned in Section II-A. Once\na proximity measure is chosen, the construction of aclustering criterion function makes the partition of\nclusters an optimization problem, which is well de \ufb01ned\nmathematically, and has rich solutions in the literature.Clusteringisubiquitous,andawealthofclusteringalgo-\nrithmshasbeendevelopedto solvedifferentproblemsin\nspeci \ufb01c\ufb01elds.However,thereisnoclusteringalgorithm\nthat can be universally used to solve all problems. \u201cIt has\nbeen very dif \ufb01cult to develop a uni \ufb01ed framework for\nreasoning about it (clustering) at a technical level, and\nprofoundly diverse approaches to clustering \u201d[166], as\nproved through an impossibility theorem. Therefore, itis important to carefully investigate the characteristics\nof the problem at hand, in order to select or design an\nappropriate clustering strategy.\n3) Cluster validation . Given a data set, each clustering\nalgorithm can always generate a division, no matter\nwhether the structure exists or not. Moreover, different\napproaches usually lead to different clusters; and evenfor the same algorithm, parameter identi \ufb01cation or\nthe presentation order of input patterns may affect the\n\ufb01nal results. Therefore, effective evaluation standards\nand criteria are important to provide the users with adegree of con \ufb01dence for the clustering results derived\nfrom the used algorithms. These assessments should\nbe", "start_char_idx": 0, "end_char_idx": 3560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb9c835c-b4cb-46f9-9190-781b786f60c6": {"__data__": {"id_": "cb9c835c-b4cb-46f9-9190-781b786f60c6", "embedding": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0285f646-8be1-4699-a3f0-38b1829c02bc", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "2": {"node_id": "962f9d3f-47d2-4b75-80d2-c6e383155097", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f"}}, "hash": "6d4aa1864edb5c87fdc00c976c5fc7f7e86f40d01ace69e99a9fe207f6a1fd1e", "text": "clustering results derived\nfrom the used algorithms. These assessments should\nbe objective and have no preferences to any algorithm.\nAlso, they should be useful for answering questionslike how many clusters are hidden in the data, whetherthe clusters obtained are meaningful or just an artifact\nof the algorithms, or why we choose some algorithm\ninstead of another. Generally, there are three categoriesof testing criteria: external indices, internal indices,and relative indices. These are de \ufb01ned on three types\nof clustering structures, known as partitional clus-\ntering, hierarchical clustering, and individual clusters[150]. Tests for the situation, where no clusteringstructure exists in the data, are also considered [110],\nbut seldom used, since users are con \ufb01dent of the pres-\nence of clusters. External indices are based on someprespeci \ufb01ed structure, which is the re \ufb02ection of prior\ninformation on the data, and used as a standard to\nvalidate the clustering solutions. Internal tests are not\ndependent on external information (prior knowledge).On the contrary, they examine the clustering structuredirectly from the original data. Relative criteria place", "start_char_idx": 3480, "end_char_idx": 4647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "85703d6a-0930-4931-8f74-2ee7d93f5bd5": {"__data__": {"id_": "85703d6a-0930-4931-8f74-2ee7d93f5bd5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18393476-d62c-410e-b17f-f795cc1ec376", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "3": {"node_id": "0dee2263-8357-4a25-b57d-59371e5a7f68", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1217385778dbc19d0fbff61e0eeb33a019196a9da911aa415b0fd0a92c797588"}}, "hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 647\nthe emphasis on the comparison of different clustering\nstructures, in order to provide a reference, to decidewhich one may best reveal the characteristics of the\nobjects. We will not survey the topic in depth and refer\ninterested readers to [74], [110], and [150]. However,we will cover more details on how to determine thenumber of clusters in Section II-M. Some more recent\ndiscussion can be found in [22], [37], [121], [180],\nand [181]. Approaches for fuzzy clustering validityare reported in [71], [104], [123], and [220].\n4) Results interpretation. The ultimate goal of clustering\nis to provide users with meaningful insights from the\noriginal data, so that they can effectively solve theproblems encountered. Experts in the relevant \ufb01elds in-\nterpret the data partition. Further analyzes, even exper-\niments, may be required to guarantee the reliability of\nextracted knowledge.\nNote that the \ufb02ow chart also includes a feedback pathway.\nClusteranalysisisnotaone-shotprocess.Inmanycircumstances,\nit needs a series of trials and repetitions. Moreover, there are no\nuniversal and effective criteria to guide the selection of featuresand clustering schemes. Validation criteria provide some insightson the quality of clustering solutions. But even how to choose the\nappropriate criterion is still a problem requiring more efforts.\nClustering has been applied in a wide variety of \ufb01elds,\nranging from engineering (machine learning, arti \ufb01cial intelli-\ngence, pattern recognition, mechanical engineering, electrical\nengineering), computer sciences (web mining, spatial database\nanalysis, textual document collection, image segmentation),life and medical sciences (genetics, biology, microbiology,paleontology, psychiatry, clinic, pathology), to earth sciences\n(geography. geology, remote sensing), social sciences (soci-\nology, psychology, archeology, education), and economics(marketing, business) [88], [127]. Accordingly, clustering isalso known as numerical taxonomy, learning without a teacher\n(or unsupervised learning), typological analysis and partition.\nThe diversity re \ufb02ects the important position of clustering in\nscienti \ufb01c research. On the other hand, it causes confusion, due\nto the differing terminologies and goals. Clustering algorithms\ndeveloped to solve a particular problem, in a specialized \ufb01eld,\nusually make assumptions in favor of the application of interest.These biases inevitably affect performance in other problemsthat do not satisfy these premises. For example, the\n-means\nalgorithm is based on the Euclidean measure and, hence, tends\nto generate hyperspherical clusters. But if the real clusters arein other geometric forms,\n-means may no longer be effective,\nand we need to resort to other schemes. This situation also\nholds true for mixture-model clustering, in which a model is \ufb01t\nto data in advance.\nClustering has a long history, with lineage dating back to Aris-\ntotle [124]. General references on clustering techniques include\n[14], [75], [77], [88], [111], [127], [150], [161], [259]. Important\nsurvey papers on clustering techniques also exist in the literature.Starting from a statistical pattern recognition viewpoint, Jain,Murty,andFlynnreviewedtheclusteringalgorithmsandotherim-\nportant issues related to cluster analysis [152], while Hansen and\nJaumard described the clustering problems under a mathematicalprogramming scheme [124]. Kolatch and He investigated appli-cationsofclusteringalgorithmsforspatialdatabasesystems[171]\nand information retrieval", "start_char_idx": 0, "end_char_idx": 3551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0dee2263-8357-4a25-b57d-59371e5a7f68": {"__data__": {"id_": "0dee2263-8357-4a25-b57d-59371e5a7f68", "embedding": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "18393476-d62c-410e-b17f-f795cc1ec376", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "2": {"node_id": "85703d6a-0930-4931-8f74-2ee7d93f5bd5", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5"}}, "hash": "1217385778dbc19d0fbff61e0eeb33a019196a9da911aa415b0fd0a92c797588", "text": "information retrieval [133], respectively. Berkhin further ex-panded the topic to the whole \ufb01eld of data mining [33]. Murtagh\nreported the advances in hierarchical clustering algorithms [210]\nandBaraldisurveyedseveralmodelsforfuzzyandneuralnetworkclustering [24]. Some more survey papers can also be found in[25], [40], [74], [89], and [151]. In addition to the review papers,\ncomparative research on clustering algorithms is also signi \ufb01cant.\nRauber, Paralic, and Pampalk presented empirical results for \ufb01ve\ntypical clustering algorithms [231]. Wei, Lee, and Hsu placed theemphasisonthecomparisonoffastalgorithmsforlargedatabases\n[280]. Scheunders compared several clustering techniques for\ncolor image quantization, with emphasis on computational timeandthepossibilityofobtainingglobaloptima[239].Applicationsand evaluations of different clustering algorithms for the analysis\nof gene expression data from DNA microarray experiments were\ndescribed in [153], [192], [246], and [271]. Experimental evalua-tionondocumentclusteringtechniques,basedonhierarchicaland\n-means clustering algorithms, were summarized by Steinbach,\nKarypis, and Kumar [261].\nIn contrast to the above, the purpose of this paper is to pro-\nvide a comprehensive and systematic description of the in \ufb02u-\nential and important clustering algorithms rooted in statistics,computer science, and machine learning, with emphasis on new\nadvances in recent years.\nThe remainder of the paper is organized as follows. In Sec-\ntion II, we review clustering algorithms, based on the natures\nof generated clusters and techniques and theories behind them.Furthermore, we discuss approaches for clustering sequentialdata, large data sets, data visualization, and high-dimensional\ndata through dimension reduction. Two important issues on\ncluster analysis, including proximity measure and how tochoose the number of clusters, are also summarized in thesection. This is the longest section of the paper, so, for conve-\nnience, we give an outline of Section II in bullet form here:\nII. Clustering Algorithms\n\u2022 A. Distance and Similarity Measures\n(See also Table I)\n\u2022 B. Hierarchical\n\u2014 Agglomerative\nSingle linkage, complete linkage, group average\nlinkage, median linkage, centroid linkage, Ward \u2019s\nmethod, balanced iterative reducing and clusteringusing hierarchies (BIRCH), clustering using rep-resentatives (CURE), robust clustering using links(ROCK)\n\u2014 Divisive\ndivisive analysis (DIANA), monothetic analysis\n(MONA)\n\u2022 C. Squared Error-Based (Vector Quantization)\n\u2014\n -means, iterative self-organizing data analysis\ntechnique (ISODATA), genetic\n -means algorithm\n(GKA), partitioning around medoids (PAM)\n\u2022 D. pdf Estimation via Mixture Densities\n\u2014 Gaussian mixture density decomposition (GMDD),\nAutoClass\n\u2022 E. Graph Theory-Based\n\u2014 Chameleon, Delaunay triangulation graph (DTG),\nhighly connected subgraphs (HCS), clustering iden-", "start_char_idx": 3530, "end_char_idx": 6409, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5555f92c-7851-49e7-8f37-6e1e27ed8519": {"__data__": {"id_": "5555f92c-7851-49e7-8f37-6e1e27ed8519", "embedding": null, "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8f4ef6c-2bd8-408e-8924-a330d3e0c356", "node_type": null, "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}, "hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d"}}, "hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d", "text": "648 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nTABLE I\nSIMILARITY AND DISSIMILARITY MEASURE FOR QUANTITATIVE FEATURES\nti\ufb01cation via connectivity kernels (CLICK), cluster\naf\ufb01nity search technique (CAST)\n\u2022 F. Combinatorial Search Techniques-Based\n\u2014 Genetically guided algorithm (GGA), TS clustering,\nSA clustering\n\u2022 G. Fuzzy\n\u2014 Fuzzy\n -means (FCM), mountain method (MM), pos-\nsibilistic\n -means clustering algorithm (PCM), fuzzy\n-shells (FCS)\n\u2022 H. Neural Networks-Based\n\u2014 Learning vector quantization (LVQ), self-organizing\nfeature map (SOFM), ART, simpli \ufb01ed ART (SART),\nhyperellipsoidal clustering network (HEC), self-split-ting competitive learning network (SPLL)\n\u2022 I. Kernel-Based\n\u2014 Kernel\n -means, support vector clustering (SVC)\n\u2022 J. Sequential Data\n\u2014 Sequence Similarity\n\u2014 Indirect sequence clustering\n\u2014 Statistical sequence clustering\n\u2022 K. Large-Scale Data Sets (See also Table II)\n\u2014 CLARA, CURE, CLARANS, BIRCH, DBSCAN,\nDENCLUE, WaveCluster, FC, ART\n\u2022 L. Data visualization and High-dimensional Data\n\u2014 PCA, ICA, Projection pursuit, Isomap, LLE,\nCLIQUE, OptiGrid, ORCLUS\n\u2022 M. How Many Clusters?Applications in two benchmark data sets, the traveling\nsalesman problem, and bioinformatics are illustrated in Sec-tion III. We conclude the paper in Section IV .\nII. C\nLUSTERING ALGORITHMS\nDifferent starting points and criteria usually lead to different\ntaxonomies of clustering algorithms [33], [88], [124], [150],[152], [171]. A rough but widely agreed frame is to classify\nclustering techniques as hierarchical clustering and parti-\ntional clustering, based on the properties of clusters generated[88], [152]. Hierarchical clustering groups data objects witha sequence of partitions, either from singleton clusters to a\ncluster including all individuals or vice versa, while partitional\nclustering directly divides data objects into some prespeci \ufb01ed\nnumber of clusters without the hierarchical structure. Wefollow this frame in surveying the clustering algorithms in the\nliterature. Beginning with the discussion on proximity measure,\nwhich is the basis for most clustering algorithms, we focus onhierarchical clustering and classical partitional clustering algo-rithms in Section II-B \u2013D. Starting from part E, we introduce\nand analyze clustering algorithms based on a wide variety of\ntheories and techniques, including graph theory, combinato-rial search techniques, fuzzy set theory, neural networks, andkernels techniques. Compared with graph theory and fuzzy set", "start_char_idx": 0, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "353d37da-bb3c-447d-b3d5-5274abeb6317": {"__data__": {"id_": "353d37da-bb3c-447d-b3d5-5274abeb6317", "embedding": null, "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b071042-368b-4574-916c-56171c9bcede", "node_type": null, "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}, "hash": "eb4549aa3fe8970830ceb7ede336adc8a4e61c273f25cdb0e04a9a9db070c96a"}}, "hash": "493f7df431511224349c07bd3d8f2485415412f03d4482c9997130b8f9b2065a", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 649\nTABLE II\nCOMPUTATIONAL COMPLEXITY OF CLUSTERING ALGORITHMS\ntheory, which had already been widely used in cluster analysis\nbefore the 1980s, the other techniques have been \ufb01nding their\napplications in clustering just in the recent decades. In spite ofthe short history, much progress has been achieved. Note that\nthese techniques can be used for both hierarchical and parti-\ntional clustering. Considering the more frequent requirement oftackling sequential data sets, large-scale, and high-dimensionaldata sets in many current applications, we review clustering\nalgorithms for them in the following three parts. We focus\nparticular attention on clustering algorithms applied in bioin-formatics. We offer more detailed discussion on how to identifyappropriate number of clusters, which is particularly important\nin cluster validity, in the last part of the section.\nA. Distance and Similarity Measures\nIt is natural to ask what kind of standards we should use to\ndetermine the closeness, or how to measure the distance (dis-\nsimilarity) or similarity between a pair of objects, an object anda cluster, or a pair of clusters. In the next section on hierarchicalclustering, we will illustrate linkage metrics for measuring prox-\nimity between clusters. Usually, a prototype is used to represent\na cluster so that it can be further processed like other objects.Here, we focus on reviewing measure approaches between in-dividuals due to the previous consideration.A data object is described by a set of features, usually repre-\nsented as a multidimensional vector. The features can be quan-titative or qualitative, continuous or binary, nominal or ordinal,\nwhich determine the corresponding measure mechanisms.\nA distance or dissimilarity function on a data set\nis de\ufb01ned\nto satisfy the following conditions.\n1) Symmetry.\n ;\n2) Positivity.\n for all\n and\n .\nIf conditions\n3) Triangle inequality.\nfor all\n and\nand (4) Re \ufb02exivity.\n also\nhold, it is called a metric.\nLikewise, a similarity function is de \ufb01ned to satisfy the con-\nditions in the following.\n1) Symmetry.\n ;\n2) Positivity.\n , for all\n and\n .\nIf it also satis \ufb01es conditions\n3)\nfor all\n and\nand (4)\n , it is called a simi-\nlarity metric.\nFor a data set with\n input patterns, we can de \ufb01ne an\nsymmetric matrix, called proximity matrix, whose\n th\nelement represents the similarity or dissimilarity measure for\nthe\nth and\n th patterns\n .\nTypically, distance functions are used to measure continuous\nfeatures, while similarity measures are more important for qual-\nitative variables. We summarize some typical measures for con-tinuous features in Table I. The selection of different measuresis problem dependent. For binary features, a similarity measure\nis commonly used (dissimilarity measures can be obtained by\nsimply using\n). Suppose we use two binary sub-\nscripts to count features in two objects.\n and\n represent\nthe number of simultaneous absence or presence of features in\ntwo objects, and\n and\n count the features present only in\none object. Then two types of commonly used similarity mea-sures for data points\nand\n are illustrated in the following.\n\u2022\nsimple matching coef \ufb01cient\nRogers and Tanimoto measure.\nGower and Legendre measure\nThese measures compute the match between two objects\ndirectly. Unmatched pairs are weighted based on their\ncontribution to the similarity.\n\u2022\nJaccard coef \ufb01cient\nSokal and Sneath measure.\nGower and Legendre measure", "start_char_idx": 0, "end_char_idx": 3464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b2203988-5ab2-4c03-8306-0deea9280a12": {"__data__": {"id_": "b2203988-5ab2-4c03-8306-0deea9280a12", "embedding": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "919ef11e-1fa6-4961-8a5e-3f2ba143981a", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "3": {"node_id": "cfd6fe0d-bc37-4e17-be7b-c55d3fff9cde", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "3fa99ab3db4dcf95cc276b945ad3d13dc0cac7ac894a33a6d0866eb5d9d60e96"}}, "hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a", "text": "650 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nThese measures focus on the co-occurrence features while\nignoring the effect of co-absence.\nFor nominal features that have more than two states, a simple\nstrategy needs to map them into new binary features [161], while\na more effective method utilizes the matching criterion\nwhere\nif\nand\n do not match\nif\nand\n match\n[88]. Ordinal features order multiple states according to some\nstandard and can be compared by using continuous dissimi-larity measures discussed in [161]. Edit distance for alphabetic\nsequences is discussed in Section II-J. More discussion on se-\nquences and strings comparisons can be found in [120] and[236].\nGenerally, for objects consisting of mixed variables, we can\nmap all these variables into the interval (0, 1) and use mea-\nsures like the Euclidean metric. Alternatively, we can trans-form them into binary variables and use binary similarity func-tions. The drawback of these methods is the information loss.\nA more powerful method was described by Gower in the formof\n, where\n indicates the\nsimilarity for the\n th feature and\n is a 0 \u20131 coef \ufb01cient based\non whether the measure of the two objects is missing [88], [112].\nB. Hierarchical Clustering\nHierarchical clustering (HC) algorithms organize data into a\nhierarchical structure according to the proximity matrix. The re-\nsults of HC are usually depicted by a binary tree or dendrogram.The root node of the dendrogram represents the whole data set\nand each leaf node is regarded as a data object. The interme-\ndiate nodes, thus, describe the extent that the objects are prox-imal to each other; and the height of the dendrogram usuallyexpresses the distance between each pair of objects or clusters,\nor an object and a cluster. The ultimate clustering results can\nbe obtained by cutting the dendrogram at different levels. Thisrepresentation provides very informative descriptions and visu-alization for the potential data clustering structures, especially\nwhen real hierarchical relations exist in the data, like the data\nfrom evolutionary research on different species of organizms.HC algorithms are mainly classi \ufb01ed as agglomerative methods\nand divisive methods. Agglomerative clustering starts with\nclusters and each of them includes exactly one object. A seriesof merge operations are then followed out that \ufb01nally lead all\nobjects to the same group. Divisive clustering proceeds in anopposite way. In the beginning, the entire data set belongs to\na cluster and a procedure successively divides it until all clus-\nters are singleton clusters. For a cluster with\nobjects, there\nare\n possible two-subset divisions, which is very ex-\npensive in computation [88]. Therefore, divisive clustering is\nnot commonly used in practice. We focus on the agglomera-\ntive clustering in the following discussion and some of divisiveclustering applications for binary data can be found in [88]. Twodivisive clustering algorithms, named MONA and DIANA, are\ndescribed in [161].The general agglomerative clustering can be summarized by\nthe following procedure.\n1) Start with\nsingleton clusters. Calculate the prox-\nimity matrix for the\n clusters.\n2) Search the minimal distance\nwhere\n is the distance function discussed be-\nfore, in the proximity matrix, and combine cluster\nand\n to form a new cluster.\n3) Update the proximity matrix by computing the dis-\ntances between the new cluster and the other clusters.\n4) Repeat steps 2) \u20133) until all objects are in the same\ncluster.\nBased on the different de \ufb01nitions for distance between two\nclusters, there are many agglomerative", "start_char_idx": 0, "end_char_idx": 3610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cfd6fe0d-bc37-4e17-be7b-c55d3fff9cde": {"__data__": {"id_": "cfd6fe0d-bc37-4e17-be7b-c55d3fff9cde", "embedding": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "919ef11e-1fa6-4961-8a5e-3f2ba143981a", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "2": {"node_id": "b2203988-5ab2-4c03-8306-0deea9280a12", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a"}}, "hash": "3fa99ab3db4dcf95cc276b945ad3d13dc0cac7ac894a33a6d0866eb5d9d60e96", "text": "de \ufb01nitions for distance between two\nclusters, there are many agglomerative clustering algorithms.\nThe simplest and most popular methods include single linkage\n[256] and complete linkage technique [258]. For the singlelinkage method, the distance between two clusters is deter-mined by the two closest objects in different clusters, so it\nis also called nearest neighbor method. On the contrary, the\ncomplete linkage method uses the farthest distance of a pair ofobjects to de \ufb01ne inter-cluster distance. Both the single linkage\nand the complete linkage method can be generalized by the\nrecurrence formula proposed by Lance and Williams [178] as\nwhere\n is the distance function and\n , and\n are\ncoef\ufb01cients that take values dependent on the scheme used.\nThe formula describes the distance between a cluster\n and a\nnew cluster formed by the merge of two clusters\n and\n . Note\nthat when\n , and\n , the formula\nbecomes\nwhich corresponds to the single linkage method. When\nand\n , the formula is\nwhich corresponds to the complete linkage method.\nSeveral more complicated agglomerative clustering algo-\nrithms, including group average linkage, median linkage,centroid linkage, and Ward \u2019s method, can also be constructed\nby selecting appropriate coef \ufb01cients in the formula. A detailed\ntable describing the coef \ufb01cient values for different algorithms\nis offered in [150] and [210]. Single linkage, complete linkageand average linkage consider all points of a pair of clusters,\nwhen calculating their inter-cluster distance, and are also called\ngraph methods. The others are called geometric methods sincethey use geometric centers to represent clusters and determinetheir distances. Remarks on important features and properties\nof these methods are summarized in [88]. More inter-cluster", "start_char_idx": 3535, "end_char_idx": 5314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "946381a4-0c98-48f8-95af-2a5a5d4bcafe": {"__data__": {"id_": "946381a4-0c98-48f8-95af-2a5a5d4bcafe", "embedding": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77519e60-e254-4414-a4d8-dd26e47c0923", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "3": {"node_id": "6aa5575b-33d5-4c9f-a765-f8a9516cbed0", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "3f5c7db8c90488be866bb0e0fa9c83c03343d6e4570e9a11c0458dff517910d2"}}, "hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 651\ndistance measures, especially the mean-based ones, were intro-\nduced by Yager, with further discussion on their possible effectto control the hierarchical clustering process [289].\nThe common criticism for classical HC algorithms is that they\nlack robustness and are, hence, sensitive to noise and outliers.Once an object is assigned to a cluster, it will not be considered\nagain, which means that HC algorithms are not capable of cor-\nrecting possible previous misclassi \ufb01cation. The computational\ncomplexity for most of HC algorithms is at least\nand\nthis high cost limits their application in large-scale data sets.\nOther disadvantages of HC include the tendency to form spher-\nical shapes and reversal phenomenon, in which the normal hier-archical structure is distorted.\nIn recent years, with the requirement for handling large-scale\ndata sets in data mining and other \ufb01elds, many new HC tech-\nniques have appeared and greatly improved the clustering per-formance. Typical examples include CURE [116], ROCK [117],\nChameleon [159], and BIRCH [295].\nThe main motivations of BIRCH lie in two aspects, the ability\nto deal with large data sets and the robustness to outliers [295].\nIn order to achieve these goals, a new data structure, clustering\nfeature (CF) tree, is designed to store the summaries of theoriginal data. The CF tree is a height-balanced tree, with eachinternal vertex composed of entries de \ufb01ned as\nchild\n, where\n is a representation of the cluster\n and\nis de \ufb01ned as\n , where\n is the number of\ndata objects in the cluster,\n is the linear sum of the objects,\nand SS is the squared sum of the objects, child\n is a pointer to the\nth child node, and\n is a threshold parameter that determines\nthe maximum number of entries in the vertex, and each leafcomposed of entries in the form of\n, where\nis the threshold parameter that controls the maximum number ofentriesintheleaf.Moreover,theleavesmustfollowtherestrictionthat the diameter\nof each entry in the leaf is less than a threshold\n . The CF\ntree structure captures the important clustering information of\nthe original data while reducing the required storage. Outliersare eliminated from the summaries by identifying the objectssparsely distributed in the feature space. After the CF tree is\nbuilt, an agglomerative HC is applied to the set of summaries to\nperform global clustering. An additional step may be performedto re \ufb01ne the clusters. BIRCH can achieve a computational\ncomplexity of\n.\nNoticing the restriction of centroid-based HC, which is\nunable to identify arbitrary cluster shapes, Guha, Rastogi, andShim developed a HC algorithm, called CURE, to explore more\nsophisticated cluster shapes [116]. The crucial feature of CURE\nlies in the usage of a set of well-scattered points to representeach cluster, which makes it possible to \ufb01nd rich cluster shapes\nother than hyperspheres and avoids both the chaining effect\n[88] of the minimum linkage method and the tendency to favor\nclusters with similar sizes of centroid. These representativepoints are further shrunk toward the cluster centroid accordingto an adjustable parameter in order to weaken the effects of\noutliers. CURE utilizes random sample (and partition) strategy\nto reduce computational complexity. Guha et al. also proposed\nanother agglomerative HC algorithm, ROCK, to group datawith qualitative attributes [117]. They used a novel measure\u201clink\u201dto describe the relation between a pair of objects and their\ncommon neighbors. Like CURE, a random sample strategy isused to", "start_char_idx": 0, "end_char_idx": 3562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6aa5575b-33d5-4c9f-a765-f8a9516cbed0": {"__data__": {"id_": "6aa5575b-33d5-4c9f-a765-f8a9516cbed0", "embedding": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77519e60-e254-4414-a4d8-dd26e47c0923", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "2": {"node_id": "946381a4-0c98-48f8-95af-2a5a5d4bcafe", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4"}}, "hash": "3f5c7db8c90488be866bb0e0fa9c83c03343d6e4570e9a11c0458dff517910d2", "text": "and their\ncommon neighbors. Like CURE, a random sample strategy isused to handle large data sets. Chameleon is constructed from\ngraph theory and will be discussed in Section II-E.\nRelative hierarchical clustering (RHC) is another exploration\nthat considers both the internal distance (distance between apair of clusters which may be merged to yield a new cluster)\nand the external distance (distance from the two clusters to the\nrest), and uses the ratio of them to decide the proximities [203].Leung et al. showed an interesting hierarchical clustering based\non scale-space theory [180]. They interpreted clustering using\na blurring process, in which each datum is regarded as a light\npoint in an image, and a cluster is represented as a blob. Liand Biswas extended agglomerative HC to deal with both nu-meric and nominal data. The proposed algorithm, called simi-\nlarity-based agglomerative clustering (SBAC), employs a mixed\ndata measure scheme that pays extra attention to less commonmatches of feature values [183]. Parallel techniques for HC arediscussed in [69] and [217], respectively.\nC. Squared Error\u2014Based Clustering (Vector Quantization)\nIn contrast to hierarchical clustering, which yields a succes-\nsive level of clusters by iterative fusions or divisions, partitionalclustering assigns a set of objects into\nclusters with no hier-\narchical structure. In principle, the optimal partition, based on\nsome speci \ufb01c criterion, can be found by enumerating all pos-\nsibilities. But this brute force method is infeasible in practice,due to the expensive computation [189]. Even for a small-scale\nclustering problem (organizing 30 objects into 3 groups), the\nnumber of possible partitions is\n. Therefore, heuristic\nalgorithms have been developed in order to seek approximatesolutions.\nOne of the important factors in partitional clustering is the\ncriterion function [124]. The sum of squared error function isone of the most widely used criteria. Suppose we have a set ofobjects\n, and we want to organize them\ninto\n subsets\n . The squared error criterion\nthen is de \ufb01ned as\nwhere\na partition matrix;\nif\n cluster\notherwisewith\ncluster prototype or centroid (means) matrix;\nsample mean for the\n th cluster;\nnumber of objects in the\n th cluster.\nNote the relation between the sum of squared error criterion\nand the scatter matrices de \ufb01ned in multiclass discriminant anal-\nysis [75],", "start_char_idx": 3489, "end_char_idx": 5875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25996ad9-fc2c-4de5-88b5-e76be56d489e": {"__data__": {"id_": "25996ad9-fc2c-4de5-88b5-e76be56d489e", "embedding": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "701e5445-9e31-4562-aa98-1f374562f74f", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "3": {"node_id": "00ae5058-315c-4129-89c9-019e4da2e62f", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "f4e77c46edf379b161f3cdc5fa533264e9a9b420440624a1768a2d6d39ff1751"}}, "hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef", "text": "652 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nwhere\ntotal scatter matrix;\nwithin-class scatter matrix;\nbetween-class scatter matrix; and\nmean vector for the whole data set.\nIt is not dif \ufb01cult to see that the criterion based on the trace\nof\n is the same as the sum of squared error criterion. To\nminimize the squared error criterion is equivalent to minimizing\nthe trace of\n or maximizing the trace of\n . We can obtain\na rich class of criterion functions based on the characteristics of\nand\n [75].\nThe\n -means algorithm is the best-known squared\nerror-based clustering algorithm [94], [191].\n1) Initialize a\n -partition randomly or based on some\nprior knowledge. Calculate the cluster prototype ma-\ntrix\n .\n2) Assign each object in the data set to the nearest cluster\n, i.e.\nif\nfor\n and\n3) Recalculate the cluster prototype matrix based on the\ncurrent partition.\n4) Repeat steps 2) \u20133) until there is no change for each\ncluster.\nThe\n -means algorithm is very simple and can be easily\nimplemented in solving many practical problems. It can workvery well for compact and hyperspherical clusters. The time\ncomplexity of\n-means is\n . Since\n and\n are usu-\nally much less than\n -means can be used to cluster large\ndata sets. Parallel techniques for\n -means are developed that\ncan largely accelerate the algorithm [262]. The drawbacks of\n-means are also well studied, and as a result, many variants of\n-means have appeared in order to overcome these obstacles.\nWe summarize some of the major disadvantages with the pro-posed improvement in the following.\n1) There is no ef \ufb01cient and universal method for iden-\ntifying the initial partitions and the number of clus-ters\n. The convergence centroids vary with different\ninitial points. A general strategy for the problem is\nto run the algorithm many times with random initialpartitions. Pe \u00f1a, Lozano, and Larra \u00f1aga compared the\nrandom method with other three classical initial parti-\ntion methods by Forgy [94], Kaufman [161], and Mac-\nQueen [191], based on the effectiveness, robustness,and convergence speed criteria [227]. According totheir experimental results, the random and Kaufman \u2019s\nmethod work much better than the other two under the\n\ufb01rst two criteria and by further considering the conver-\ngence speed, they recommended Kaufman \u2019s method.\nBradley and Fayyad presented a re \ufb01nement algorithm\nthat\ufb01rst utilizes\n-means\n times to\n random sub-\nsets from the original data [43]. The set formed fromthe union of the solution (centroids of the\nclusters)of the\n subsets is clustered\n times again, setting\neach subset solution as the initial guess. The startingpoints for the whole data are obtained by choosing the\nsolution with minimal sum of squared distances. Likas,\nVlassis, and Verbeek proposed a global\n-means algo-\nrithm consisting of a series of\n -means clustering pro-\ncedures with the number of clusters varying from 1 to\n[186]. After \ufb01nding the centroid for only one cluster\nexisting, at each\n , the previous\ncentroids are \ufb01xed and the new centroid is selected by\nexamining all data points. The authors claimed that the\nalgorithm is independent of the initial partitions and\nprovided accelerating strategies. But the problem oncomputational complexity exists, due to the require-ment for executing\n-means\n times for each value\nof\n.\nAn interesting technique, called ISODATA, devel-\noped by Ball and", "start_char_idx": 0, "end_char_idx": 3375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "00ae5058-315c-4129-89c9-019e4da2e62f": {"__data__": {"id_": "00ae5058-315c-4129-89c9-019e4da2e62f", "embedding": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "701e5445-9e31-4562-aa98-1f374562f74f", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "2": {"node_id": "25996ad9-fc2c-4de5-88b5-e76be56d489e", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef"}}, "hash": "f4e77c46edf379b161f3cdc5fa533264e9a9b420440624a1768a2d6d39ff1751", "text": "interesting technique, called ISODATA, devel-\noped by Ball and Hall [21], deals with the estimationof\n. ISODATA can dynamically adjust the number of\nclusters by merging and splitting clusters according to\nsome prede \ufb01ned thresholds (in this sense, the problem\nof identifying the initial number of clusters becomesthat of parameter (threshold) tweaking). The new\nis\nused as the expected number of clusters for the next it-\neration.\n2) The iteratively optimal procedure of\n -means cannot\nguarantee convergence to a global optimum. The sto-\nchastic optimal techniques, like simulated annealing\n(SA) and genetic algorithms (also see part II.F), can\ufb01nd the global optimum with the price of expensive\ncomputation. Krishna and Murty designed new opera-\ntors in their hybrid scheme, GKA, in order to achieve\nglobal search and fast convergence [173]. The de \ufb01ned\nbiased mutation operator is based on the Euclideandistance between an object and the centroids and aims\nto avoid getting stuck in a local optimum. Another\noperator, the\n-means operator (KMO), replaces the\ncomputationally expensive crossover operators andalleviates the complexities coming with them. An\nadaptive learning rate strategy for the online mode\n-means is illustrated in [63]. The learning rate is\nexclusively dependent on the within-group variationsand can be adjusted without involving any user activi-\nties. The proposed enhanced LBG (ELBG) algorithm\nadopts a roulette mechanism typical of genetic algo-rithms to become near-optimal and therefore, is notsensitive to initialization [222].\n3)\n-means is sensitive to outliers and noise. Even if an\nobject is quite far away from the cluster centroid, it isstill forced into a cluster and, thus, distorts the clustershapes. ISODATA [21] and PAM [161] both consider\nthe effect of outliers in clustering procedures. ISO-\nDATA gets rid of clusters with few objects. The split-ting operation of ISODATA eliminates the possibilityof elongated clusters typical of\n-means. PAM utilizes\nreal data points (medoids) as the cluster prototypes and\navoids the effect of outliers. Based on the same con-sideration, a\n-medoids algorithm is presented in [87]", "start_char_idx": 3313, "end_char_idx": 5467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e0bed58-196c-48cf-9fd7-2cea388f8377": {"__data__": {"id_": "6e0bed58-196c-48cf-9fd7-2cea388f8377", "embedding": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89236ff8-14bb-4e0d-b565-9320faa60783", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "3": {"node_id": "63936964-2624-4e35-9380-a5b697e69a19", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "b906b56c9733c2eb9ccd4d7cff132330e7d1c92e12db643ea94015f5538a9661"}}, "hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 653\nby searching the discrete 1-medians as the cluster cen-\ntroids.\n4) The de \ufb01nition of \u201cmeans \u201dlimits the application\nonly to numerical variables. The\n -medoids algo-\nrithm mentioned previously is a natural choice, whenthe computation of means is unavailable, since themedoids do not need any computation and always exist\n[161]. Huang [142] and Gupta et al. [118] de \ufb01ned dif-\nferent dissimilarity measures to extend\n-means\nto categorical variables. For Huang \u2019s method, the\nclustering goal is to minimize the cost function\n, where\nand\nwith a set of\n -dimensional vectors\n, where\n .\nEach vector\n is known as a mode and is de \ufb01ned to\nminimize the sum of distances\n . The\nproposed\n -modes algorithm operates in a similar\nway as\n -means.\nSeveral recent advances on\n -means and other squared-error\nbased clustering algorithms with their applications can be foundin [125], [155], [222], [223], [264], and [277].\nD. Mixture Densities-Based Clustering (pdf Estimation via\nMixture Densities)\nIn the probabilistic view, data objects are assumed to be gen-\nerated according to several probability distributions. Data pointsin different clusters were generated by different probability dis-tributions. They can be derived from different types of density\nfunctions (e.g., multivariate Gaussian or\n-distribution), or the\nsame families, but with different parameters. If the distributionsare known, \ufb01nding the clusters of a given data set is equivalent\nto estimating the parameters of several underlying models. Sup-\npose the prior probability (also known as mixing probability)\nfor cluster\n (here,\n is assumed to\nbe known and methods for estimating\n are discussed in Sec-\ntion II-M) and the conditional probability density\n(also known as component density), where\n is the unknown\nparameter vector, are known. Then, the mixture probability den-sity for the whole data set is expressed as\nwhere\n , and\n . As long as\nthe parameter vector\n is decided, the posterior probability for\nassigning a data point to a cluster can be easily calculated withBayes \u2019s theorem. Here, the mixtures can be constructed with\nany types of components, but more commonly, multivariateGaussian densities are used due to its complete theory and\nanalytical tractability [88], [297].\nMaximum likelihood (ML) estimation is an important statis-\ntical approach for parameter estimation [75] and it considers the\nbest estimate as the one that maximizes the probability of gen-erating all the observations, which is given by the joint densityfunction\nor, in a logarithm form\nThe best estimate can be achieved by solving the log-likelihood\nequations\n .\nUnfortunately, since the solutions of the likelihood equa-\ntions cannot be obtained analytically in most circumstances[90], [197], iteratively suboptimal approaches are required to\napproximate the ML estimates. Among these methods, the\nexpectation-maximization (EM) algorithm is the most popular[196]. EM regards the data set as incomplete and divideseach data point\ninto two parts\n , where\nrepresents the observable features and\nis the missing data, where\n chooses value 1 or 0 according\nto whether\n belongs to the component\n or not. Thus, the\ncomplete data log-likelihood is\nThe standard EM algorithm generates a series of parameter\nestimates\n , where\n represents the reaching of\nthe convergence criterion, through the following steps:\n1) initialize\n and set\n ;\n2) e-step:", "start_char_idx": 0, "end_char_idx": 3432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63936964-2624-4e35-9380-a5b697e69a19": {"__data__": {"id_": "63936964-2624-4e35-9380-a5b697e69a19", "embedding": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "89236ff8-14bb-4e0d-b565-9320faa60783", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "2": {"node_id": "6e0bed58-196c-48cf-9fd7-2cea388f8377", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2"}}, "hash": "b906b56c9733c2eb9ccd4d7cff132330e7d1c92e12db643ea94015f5538a9661", "text": "steps:\n1) initialize\n and set\n ;\n2) e-step: Compute the expectation of the complete data\nlog-likelihood\n3) m-step: Select a new parameter estimate that maxi-\nmizes the\n -function,\n ;\n4) Increase\n ; repeat steps 2) \u20133) until the conver-\ngence condition is satis \ufb01ed.\nThe major disadvantages for EM algorithm are the sensitivity\nto the selection of initial parameters, the effect of a singular co-\nvariance matrix, the possibility of convergence to a local op-\ntimum, and the slow convergence rate [96], [196]. Variants ofEM for addressing these problems are discussed in [90] and[196].\nA valuable theoretical note is the relation between the EM\nalgorithm and the\n-means algorithm. Celeux and Govaert\nproved that classi \ufb01cation EM (CEM) algorithm under a spher-\nical Gaussian mixture is equivalent to the\n -means algorithm\n[58].", "start_char_idx": 3389, "end_char_idx": 4215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "afcc21c7-378b-4a88-9173-0ba1e0f53b3c": {"__data__": {"id_": "afcc21c7-378b-4a88-9173-0ba1e0f53b3c", "embedding": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a776b20-f761-45dc-8e59-47d70437a52a", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "3": {"node_id": "83c51dca-eaa2-46c6-91dd-40b7f5e000b8", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "737d61d54f7037e93d50b154c805df3d32db1c3d895e19d75825637620285cf0"}}, "hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d", "text": "654 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFraley and Raftery described a comprehensive mix-\nture-model based clustering scheme [96], which was im-plemented as a software package, known as MCLUST [95]. In\nthis case, the component density is multivariate Gaussian, with\na mean vector\nand a covariance matrix\n as the parameters\nto be estimated. The covariance matrix for each component canfurther be parameterized by virtue of eigenvalue decomposi-\ntion, represented as\n, where\n is a scalar,\n is\nthe orthogonal matrix of eigenvectors, and\n is the diagonal\nmatrix based on the eigenvalues of\n [96]. These three elements\ndetermine the geometric properties of each component. After\nthe maximum number of clusters and the candidate models are\nspeci \ufb01ed, an agglomerative hierarchical clustering was used to\nignite the EM algorithm by forming an initial partition, whichincludes at most the maximum number of clusters, for each\nmodel. The optimal clustering result is achieved by checking\nthe Bayesian information criterion (BIC) value discussed inSection II-M. GMDD is also based on multivariate Gaussiandensities and is designed as a recursive algorithm that sequen-\ntially estimates each component [297]. GMDD views data\npoints that are not generated from a distribution as noise andutilizes an enhanced model- \ufb01tting estimator to construct each\ncomponent from the contaminated model. AutoClass considers\nmore families of probability distributions (e.g., Poisson and\nBernoulli) for different data types [59]. A Bayesian approach isused in AutoClass to \ufb01nd out the optimal partition of the given\ndata based on the prior probabilities. Its parallel realization is\ndescribed in [228]. Other important algorithms and programs\ninclude Multimix [147], EM based mixture program (EMMIX)[198], and Snob [278].\nE. Graph Theory-Based Clustering\nThe concepts and properties of graph theory [126] make it\nvery convenient to describe clustering problems by means ofgraphs. Nodes\nof a weighted graph\n correspond to data\npoints in the pattern space and edges\n re\ufb02ect the proximities\nbetween each pair of data points. If the dissimilarity matrix is\nde\ufb01ned as\nif\notherwise\nwhere\n is a threshold value, the graph is simpli \ufb01ed to an\nunweighted threshold graph. Both the single linkage HC and\nthe complete linkage HC can be described on the basis ofthe threshold graph. Single linkage clustering is equivalent toseeking maximally connected subgraphs (components) while\ncomplete linkage clustering corresponds to \ufb01nding maximally\ncomplete subgraphs (cliques) [150]. Jain and Dubes illustratedand discussed more applications of graph theory (e.g., Hubert \u2019s\nalgorithm and Johnson \u2019s algorithm) for hierarchical clustering\nin [150]. Chameleon [159] is a newly developed agglomerative\nHC algorithm based on the\n-nearest-neighbor graph, in which\nan edge is eliminated if both vertices are not within the\nclosest points related to each other. At the \ufb01rst step, Chameleon\ndivides the connectivity graph into a set of subclusters with the\nminimal edge cut. Each subgraph should contain enough nodesin order for effective similarity computation. By combiningboth the relative interconnectivity and relative closeness, whichmake Chameleon \ufb02exible enough to explore the characteristics\nof potential clusters, Chameleon merges these small subsetsand, thus, comes up with the ultimate clustering solutions.\nHere, the relative interconnectivity (or closeness) is obtained\nby normalizing the sum of weights (or average weight) of", "start_char_idx": 0, "end_char_idx": 3514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "83c51dca-eaa2-46c6-91dd-40b7f5e000b8": {"__data__": {"id_": "83c51dca-eaa2-46c6-91dd-40b7f5e000b8", "embedding": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a776b20-f761-45dc-8e59-47d70437a52a", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "2": {"node_id": "afcc21c7-378b-4a88-9173-0ba1e0f53b3c", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d"}}, "hash": "737d61d54f7037e93d50b154c805df3d32db1c3d895e19d75825637620285cf0", "text": "is obtained\nby normalizing the sum of weights (or average weight) of theedges connecting the two clusters over the internal connectivity(or closeness) of the clusters. DTG is another important graph\nrepresentation for HC analysis. Cherng and Lo constructed a\nhypergraph (each edge is allowed to connect more than twovertices) from the DTG and used a two-phase algorithm that issimilar to Chameleon to \ufb01nd clusters [61]. Another DTG-based\napplication, known as AMOEBA algorithm, is presented in\n[86].\nGraph theory can also be used for nonhierarchical clusters.\nZahn \u2019s clustering algorithm seeks connected components as\nclusters by detecting and discarding inconsistent edges in the\nminimum spanning tree [150]. Hartuv and Shamir treated clus-ters as HCS, where \u201chighly connected \u201dmeans the connectivity\n(the minimum number of edges needed to disconnect a graph)\nof the subgraph is at least half as great as the number of the\nvertices [128]. A minimum cut (mincut) procedure, whichaims to separate a graph with a minimum number of edges, isused to \ufb01nd these HCSs recursively. Another algorithm, called\nCLICK, is based on the calculation of the minimum weight\ncut to form clusters [247]. Here, the graph is weighted and theedge weights are assigned a new interpretation, by combiningprobability and graph theory. The edge weight between node\nand\n is de \ufb01ned as shown in\nbelong to the same cluster\ndoes not belong to the same cluster\nwhere\n represents the similarity between the two nodes.\nCLICK further assumes that the similarity values within clus-ters and between clusters follow Gaussian distributions with\ndifferent means and variances, respectively. Therefore, the\nprevious equation can be rewritten by using Bayes \u2019theorem as\nwhere\n is the prior probability that two objects belong to\nthe same cluster and\n are the means and\nvariances for between-cluster similarities and within-clusters\nsimilarities, respectively. These parameters can be estimatedeither from prior knowledge, or by using parameter estimationmethods [75]. CLICK recursively checks the current subgraph,\nand generates a kernel list, which consists of the components\nsatisfying some criterion function. Subgraphs that include onlyone node are regarded as singletons, and are separated forfurther manipulation. Using the kernels as the basic clusters,\nCLICK carries out a series of singleton adoptions and cluster\nmerge to generate the resulting clusters. Additional heuristicsare provided to accelerate the algorithm performance.\nSimilarly, CAST considers a probabilistic model in designing\na graph theory-based clustering algorithm [29]. Clusters are\nmodeled as corrupted clique graphs, which, in ideal conditions,are regarded as a set of disjoint cliques. The effect of noiseis incorporated by adding or removing edges from the ideal", "start_char_idx": 3446, "end_char_idx": 6252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "506d1584-d7bb-41fd-9985-e2bc26359c10": {"__data__": {"id_": "506d1584-d7bb-41fd-9985-e2bc26359c10", "embedding": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fce819b-421d-49b9-b995-40ee1ba0d387", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "3": {"node_id": "ef0c1f7c-31c4-478d-8778-1ea866949bf8", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "b90fba686d681124163d2b53f31c1bdaeed47296cdb5e1cfcef57df6b3c7fb7f"}}, "hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 655\nmodel, with a probability\n . Proofs were given for recovering\nthe uncorrupted graph with a high probability. CAST is theheuristic implementation of the original theoretical version.\nCAST creates clusters sequentially, and each cluster begins\nwith a random and unassigned data point. The relation betweena data point\nand a cluster\n being built is determined by the\naf\ufb01nity, de \ufb01ned as\n , and the af \ufb01nity threshold\nparameter\n . When\n , it means that the data point is\nhighly related to the cluster and vice versa. CAST alternatelyadds high af \ufb01nity data points or deletes low af \ufb01nity data points\nfrom the cluster until no more changes occur.\nF . Combinatorial Search Techniques-Based Clustering\nThe basic object of search techniques is to \ufb01nd the global\nor approximate global optimum for combinatorial optimization\nproblems, which usually have NP-hard complexity and need to\nsearch an exponentially large solution space. Clustering can beregarded as a category of optimization problems. Given a set ofdata points\n, clustering algorithms aim\nto organize them into\n subsets\n that optimize\nsome criterion function. The possible partition for\n points into\nclusters is given by the formula [189]\nAs shown before, even for small\n and\n , the computa-\ntional complexity is extremely expensive, not to mention thelarge-scale clustering problems frequently encountered in recentdecades. Simple local search techniques, like hill-climbing al-\ngorithms, are utilized to \ufb01nd the partitions, but they are easily\nstuck in local minima and therefore cannot guarantee optimality.More complex search methods (e.g., evolutionary algorithms(EAs) [93], SA [165], and Tabu search (TS) [108] are known as\nstochastic optimization methods, while deterministic annealing\n(DA) [139], [234] is the most typical deterministic search tech-nique) can explore the solution space more \ufb02exibly and ef \ufb01-\nciently.\nInspired by the natural evolution process, evolutionary com-\nputation, which consists of genetic algorithms (GAs), evolutionstrategies (ESs), evolutionary programming (EP), and geneticprogramming (GP), optimizes a population of structure by using\na set of evolutionary operators [93]. An optimization function,\ncalled the \ufb01tness function, is the standard for evaluating the opti-\nmizing degree of the population, in which each individual has itscorresponding \ufb01tness. Selection, recombination, and mutation\nare the most widely used evolutionary operators. The selection\noperator ensures the continuity of the population by favoring thebest individuals in the next generation. The recombination andmutation operators support the diversity of the population by ex-\nerting perturbations on the individuals. Among many EAs, GAs\n[140] are the most popular approaches applied in cluster anal-ysis. In GAs, each individual is usually encoded as a binary bitstring, called a chromosome. After an initial population is gener-\nated according to some heuristic rules or just randomly, a series\nof operations, including selection, crossover and mutation, areiteratively applied to the population until the stop condition issatis\ufb01ed.Hall, \u00d6zyurt, and Bezdek proposed a GGA that can be re-\ngarded as a general scheme for center-based (hard or fuzzy)clustering problems [122]. Fitness functions are reformulated\nfrom the standard sum of squared error criterion function in\norder to adapt the change of the construction of the optimiza-tion problem (only the prototype matrix is needed)\nfor hard clustering\nfor fuzzy clustering\nwhere\n , is the distance between\nthe\nth", "start_char_idx": 0, "end_char_idx": 3583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ef0c1f7c-31c4-478d-8778-1ea866949bf8": {"__data__": {"id_": "ef0c1f7c-31c4-478d-8778-1ea866949bf8", "embedding": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4fce819b-421d-49b9-b995-40ee1ba0d387", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "2": {"node_id": "506d1584-d7bb-41fd-9985-e2bc26359c10", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6"}}, "hash": "b90fba686d681124163d2b53f31c1bdaeed47296cdb5e1cfcef57df6b3c7fb7f", "text": "fuzzy clustering\nwhere\n , is the distance between\nthe\nth cluster and the\n th data object, and\n is the fuzzi \ufb01cation\nparameter.\nGGA proceeds with the following steps.\n1) Choose appropriate parameters for the algorithm. Ini-\ntialize the population randomly with\n individuals,\neach of which represents a\n prototype matrix and\nis encoded as gray codes. Calculate the \ufb01tness value for\neach individual.\n2) Use selection (tournament selection) operator to\nchoose parental members for reproduction.\n3) Use crossover (two-point crossover) and mutation (bit-\nwise mutation) operator to generate offspring from theindividuals chosen in step 2).\n4) Determine the next generation by keeping the individ-\nuals with the highest \ufb01tness.\n5) Repeat steps 2) \u20134) until the termination condition is\nsatis\ufb01ed.\nOther GAs-based clustering applications have appeared\nbased on a similar framework. They are different in themeaning of an individual in the population, encoding methods,\ufb01tness function de \ufb01nition, and evolutionary operators [67],\n[195], [273]. The algorithm CLUSTERING in [273] includes\na heuristic scheme for estimating the appropriate number ofclusters in the data. It also uses a nearest-neighbor algorithmto divide data into small subsets, before GAs-based clustering,\nin order to reduce the computational complexity. GAs are very\nuseful for improving the performance of\n-means algorithms.\nBabu and Murty used GAs to \ufb01nd good initial partitions [15].\nKrishna and Murty combined GA with\n -means and devel-\noped GKA algorithm that can \ufb01nd the global optimum [173].\nAs indicated in Section II-C, the algorithm ELBG uses theroulette mechanism to address the problems due to the badinitialization [222]. It is worthwhile to note that ELBG are\nequivalent to another algorithm, fully automatic clustering\nsystem (FACS) [223], in terms of quantization level detection.The difference lies in the input parameters employed (ELBGadopts the number of quantization levels, while FACS uses\nthe desired distortion error). Except the previous applications,\nGAs can also be used for hierarchical clustering. Lozano andLarra \u00f1ag discussed the properties of ultrametric distance [127]\nand reformulated the hierarchical clustering as an optimization", "start_char_idx": 3527, "end_char_idx": 5750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a3dc5b0-d106-49a9-b6e4-1b195fde88a5": {"__data__": {"id_": "8a3dc5b0-d106-49a9-b6e4-1b195fde88a5", "embedding": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b0ac33d-3b6a-4845-8f1c-8475b84ee395", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "3": {"node_id": "3f2f4ede-3176-49ec-b114-961912cc699c", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "4ff09ae5ca032bf8fc345244862cc9d54eb92331aa373eb0d4ef2f341bf77ec0"}}, "hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c", "text": "656 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nproblem that tries to \ufb01nd the closest ultrametic distance for a\ngiven dissimilarity with Euclidean norm [190]. They suggestedan order-based GA to solve the problem. Clustering algorithms\nbased on ESs and EP are described and analyzed in [16] and\n[106], respectively.\nTS is a combinatory search technique that uses the tabu list to\nguide the search process consisting of a sequence of moves. The\ntabu list stores part or all of previously selected moves according\nto the speci \ufb01ed size. These moves are forbidden in the current\nsearch and are called tabu. In the TS clustering algorithm devel-oped by Al-Sultan [9], a set of candidate solutions are generated\nfrom the current solution with some strategy. Each candidate so-\nlution represents the allocations of\ndata objects in\n clusters.\nThe candidate with the optimal cost function is selected as thecurrent solution and appended to the tabu list, if it is not already\nin the tabu list or meets the aspiration criterion, which can over-\nrule the tabu restriction. Otherwise, the remaining candidatesare evaluated in the order of their cost function values, until allthese conditions are satis \ufb01ed. When all the candidates are tabu, a\nnew set of candidate solutions are created followed by the same\nsearch process. The search process proceeds until the maximumnumber of iterations is reached. Sung and Jin \u2019s method includes\nmore elaborate search processes with the packing and releasing\nprocedures [266]. They also used a secondary tabu list to keep\nthe search from trapping into the potential cycles. A fuzzy ver-sion of TS clustering can be found in [72].\nSA is also a sequential and global search technique and is mo-\ntivated by the annealing process in metallurgy [165]. SA allowsthe search process to accept a worse solution with a certain prob-ability. The probability is controlled by a parameter, known as\ntemperature\nand is usually expressed as\n , where\nis the change of the energy (cost function). The tempera-\nture\n goes through an annealing schedule from initial high to\nultimate low values, which means that SA attempts to explore\nsolution space more completely at high temperatures while fa-\nvors the solutions that lead to lower energy at low temperatures.SA-based clustering was reported in [47] and [245]. The formerillustrated an application of SA clustering to evaluate different\nclustering criteria and the latter investigated the effects of input\nparameters to the clustering performance.\nHybrid approaches that combine these search techniques are\nalso proposed. A tabu list is used in a GA clustering algorithm to\npreserve the variety of the population and avoid repeating com-\nputation [243]. An application of SA for improving TS was re-ported in [64]. The algorithm further reduces the possible movesto local optima.\nThe main drawback that plagues the search techniques-based\nclustering algorithms is the parameter selection. More oftenthan not, search techniques introduce more parameters thanother methods (like\n-means). There are no theoretic guide-\nlines to select the appropriate and effective parameters. Hall\net al. provided some methods for setting parameters in their\nGAs-based clustering framework [122], but most of thesecriteria are still obtained empirically. The same situation exists\nfor TS and SA clustering [9], [245]. Another problem is the\ncomputational complexity paid for the convergence to globaloptima. High computational requirement limits their applica-tions in large-scale data sets.G. Fuzzy Clustering\nExcept for GGA, the clustering techniques we have discussed\nso far are referred to as hard or crisp clustering, which means\nthat each object is assigned to only one", "start_char_idx": 0, "end_char_idx": 3735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f2f4ede-3176-49ec-b114-961912cc699c": {"__data__": {"id_": "3f2f4ede-3176-49ec-b114-961912cc699c", "embedding": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6b0ac33d-3b6a-4845-8f1c-8475b84ee395", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "2": {"node_id": "8a3dc5b0-d106-49a9-b6e4-1b195fde88a5", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c"}}, "hash": "4ff09ae5ca032bf8fc345244862cc9d54eb92331aa373eb0d4ef2f341bf77ec0", "text": "crisp clustering, which means\nthat each object is assigned to only one cluster. For fuzzy clus-tering, this restriction is relaxed, and the object can belong to all\nof the clusters with a certain degree of membership [293]. This\nis particularly useful when the boundaries among the clustersare not well separated and ambiguous. Moreover, the member-ships may help us discover more sophisticated relations between\na given object and the disclosed clusters.\nFCM is one of the most popular fuzzy clustering algorithms\n[141]. FCM can be regarded as a generalization of ISODATA[76] and was realized by Bezdek [35]. FCM attempts to \ufb01nd a\npartition (\nfuzzy clusters) for a set of data points\nwhile minimizing the cost function\nwhere\nis the fuzzy partition matrix and\nis the membership coef \ufb01cient of the\nth object in the\n th cluster;\nis the cluster prototype (mean or center)\nmatrix;\nis the fuzzi \ufb01cation parameter and usually\nis set to 2 [129];\nis the distance measure between\n and\n.\nWe summarize the standard FCM as follows, in which the\nEuclidean or\n norm distance function is used.\n1) Select appropriate values for\n , and a small positive\nnumber\n . Initialize the prototype matrix\n randomly.\nSet step variable\n .\n2) Calculate (at\n ) or update (at\n ) the member-\nship matrix\n by\nfor\n and\n3) Update the prototype matrix\n by\nfor\n4) Repeat steps 2) \u20133) until\n .\nNumerous FCM variants and other fuzzy clustering algo-\nrithms have appeared as a result of the intensive investigation\non the distance measure functions, the effect of weightingexponent on fuzziness control, the optimization approaches forfuzzy partition, and improvements of the drawbacks of FCM\n[84], [141].\nLike its hard counterpart, FCM also suffers from the presence\nof noise and outliers and the dif \ufb01culty to identify the initial par-\ntitions. Yager and Filev proposed a MM in order to estimate the", "start_char_idx": 3665, "end_char_idx": 5525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b7d98762-2f56-4370-b6be-ce980d1ec451": {"__data__": {"id_": "b7d98762-2f56-4370-b6be-ce980d1ec451", "embedding": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ed6bb-179c-49bf-95fe-8d948b9527d2", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "3": {"node_id": "35f0cb4a-44da-4a09-a945-5903a16263e6", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "97b648859bc2a226adf9017753a4e605f4ebf8adff469f8c650ac28c874667b0"}}, "hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 657\ncenters of clusters [290]. Candidate centers consist of a set of\nvertices that are formed by building a grid on the pattern space.The mountain function for a vertex\nis de \ufb01ned as\nwhere\n is the distance between the\n th data object and\nthe\nth node, and\n is a positive constant. Therefore, the closer\na data object is to a vertex, the more the data object contributes\nto the mountain function. The vertex\n with the maximum\nvalue of mountain function\n is selected as the \ufb01rst center.\nA procedure, called mountain destruction, is performed to get\nrid of the effects of the selected center. This is achieved by sub-\ntracting the mountain function value for each of the rest ver-tices with an amount dependent on the current maximum moun-tain function value and the distance between the vertex and the\ncenter. The process iterates until the ratio between the current\nmaximum and\nis below some threshold. The connection\nof MM with several other fuzzy clustering algorithms was fur-ther discussed in [71]. Gath and Geva described an initialization\nstrategy of unsupervised tracking of cluster prototypes in their\n2-layer clustering scheme, in which FCM and fuzzy ML esti-mation are effectively combined [102].\nKersten suggested that city block distance (or\nnorm) could\nimprove the robustness of FCM to outliers [163]. Furthermore,\nHathaway, Bezdek, and Hu extended FCM to a more universalcase by using Minkowski distance (or\nnorm,\n ) and\nseminorm\n for the models that operate either di-\nrectly on the data objects or indirectly on the dissimilarity mea-\nsures [130]. According to their empirical results, the object databased models, with\nand\n norm, are recommended. They\nalso pointed out the possible improvement of models for other\nnorm with the price of more complicated optimization oper-\nations. PCM is another approach for dealing with outliers [175].Under this model, the memberships are interpreted by a possi-bilistic view, i.e., \u201cthe compatibilities of the points with the class\nprototypes \u201d[175]. The effect of noise and outliers is abated with\nthe consideration of typicality. In this case, the \ufb01rst condition for\nthe membership coef \ufb01cient described in Section I is relaxed to\n. Accordingly, the cost function is reformu-\nlated as\nwhere\n are some positive constants. The additional term\ntends to give credits to memberships with large values. A\nmodi \ufb01ed version in order to \ufb01nd appropriate clusters is pro-\nposed in [294]. Dav \u00e9and Krishnapuram further elaborated\nthe discussion on fuzzy clustering robustness and indicated itsconnection with robust statistics [71]. Relations among some\nwidely used fuzzy clustering algorithms were discussed and\ntheir similarities to some robust statistical methods were alsoreviewed. They reached a uni \ufb01ed framework as the conclusion\nfor the previous discussion and proposed generic algorithms\nfor robust clustering.The standard FCM alternates the calculation of the member-\nship and prototype matrix, which causes a computational burdenfor large-scale data sets. Kolen and Hutcheson accelerated the\ncomputation by combining updates of the two matrices [172].\nHung and Yang proposed a method to reduce computationaltime by identifying more accurate cluster centers [146]. FCMvariants were also developed to deal with other data types, such\nas symbolic data [81] and data with missing values [129].\nA family of fuzzy\n-shells algorithms has also appeared to de-\ntect different types of cluster shapes, especially contours (lines,circles, ellipses, rings, rectangles, hyperbolas) in", "start_char_idx": 0, "end_char_idx": 3574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35f0cb4a-44da-4a09-a945-5903a16263e6": {"__data__": {"id_": "35f0cb4a-44da-4a09-a945-5903a16263e6", "embedding": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2c4ed6bb-179c-49bf-95fe-8d948b9527d2", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "2": {"node_id": "b7d98762-2f56-4370-b6be-ce980d1ec451", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b"}}, "hash": "97b648859bc2a226adf9017753a4e605f4ebf8adff469f8c650ac28c874667b0", "text": "ellipses, rings, rectangles, hyperbolas) in a two-dimen-\nsional data space. They use the \u201cshells \u201d(curved surfaces [70])\nas the cluster prototypes instead of points or surfaces in tra-ditional fuzzy clustering algorithms. In the case of FCS [36],[70], the proposed cluster prototype is represented as a\n-di-\nmensional hyperspherical shell\n (\n for circles),\nwhere\n is the center, and\n is the radius. A dis-\ntance function is de \ufb01ned as\n to\nmeasure the distance from a data object\n to the prototype\n .\nSimilarly, other cluster shapes can be achieved by de \ufb01ning ap-\npropriate prototypes and corresponding distance functions, ex-ample including fuzzy\n-spherical shells (FCSS) [176], fuzzy\n-rings (FCR) [193], fuzzy\n -quadratic shells (FCQS) [174], and\nfuzzy\n -rectangular shells (FCRS) [137]. See [141] for further\ndetails.\nFuzzy set theories can also be used to create hierarchical\ncluster structure. Geva proposed a hierarchical unsupervised\nfuzzy clustering (HUFC) algorithm [104], which can effec-\ntively explore data structure at different levels like HC, whileestablishing the connections between each object and cluster inthe hierarchy with the memberships. This design makes HUFC\novercome one of the major disadvantages of HC, i.e., HC\ncannot reassign an object once it is designated into a cluster.Fuzzy clustering is also closely related to neural networks [24],and we will see more discussions in the following section.\nH. Neural Networks-Based Clustering\nNeural networks-based clustering has been dominated by\nSOFMs and adaptive resonance theory (ART), both of whichare reviewed here, followed by a brief discussion of otherapproaches.\nIn competitive neural networks, active neurons reinforce\ntheir neighborhood within certain regions, while suppressingthe activities of other neurons (so-called on-center/off-surroundcompetition). Typical examples include LVQ and SOFM [168],\n[169]. Intrinsically, LVQ performs supervised learning, and is\nnot categorized as a clustering algorithm [169], [221]. But itslearning properties provide an insight to describe the potentialdata structure using the prototype vectors in the competitive\nlayer. By pointing out the limitations of LVQ, including sen-\nsitivity to initiation and lack of a de \ufb01nite clustering object,\nPal, Bezdek, and Tsao proposed a general LVQ algorithmfor clustering, known as GLVQ [221] (also see [157] for its\nimproved version GLVQ-F). They constructed the clustering\nproblem as an optimization process based on minimizing aloss function, which is de \ufb01ned on the locally weighted error\nbetween the input pattern and the winning prototype. They also\nshowed the relations between LVQ and the online\n-means", "start_char_idx": 3531, "end_char_idx": 6203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "060d849e-db0f-46ab-af16-6cbf6f335f10": {"__data__": {"id_": "060d849e-db0f-46ab-af16-6cbf6f335f10", "embedding": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c35f800b-c539-4a91-ac9d-1e568091bcfc", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "3": {"node_id": "d0103b5f-a473-4495-a086-1256179309af", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "2769b2877636983ba4a919a012e6ff35a0de00417b639595b63e544232e239ed"}}, "hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38", "text": "658 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nalgorithm. Soft LVQ algorithms, e.g., fuzzy algorithms for\nLVQ (FALVQ), were discussed in [156].\nThe objective of SOFM is to represent high-dimensional\ninput patterns with prototype vectors that can be visualized in\na usually two-dimensional lattice structure [168], [169]. Eachunit in the lattice is called a neuron, and adjacent neurons areconnected to each other, which gives the clear topology of\nhow the network \ufb01ts itself to the input space. Input patterns\nare fully connected to all neurons via adaptable weights, andduring the training process, neighboring input patterns areprojected into the lattice, corresponding to adjacent neurons. In\nthis sense, some authors prefer to think of SOFM as a method\nto displaying latent data structure in a visual way rather than aclustering approach [221]. Basic SOFM training goes throughthe following steps.\n1) De \ufb01ne the topology of the SOFM; Initialize the proto-\ntype vectors\nrandomly.\n2) Present an input pattern\n to the network; Choose\nthe winning node\n that is closest to\n , i.e.,\n.\n3) Update prototype vectors\nwhere\n is the neighborhood function that is often\nde\ufb01ned as\nwhere\n is the monotonically decreasing learning\nrate,\n represents the position of corresponding neuron,\nand\n is the monotonically decreasing kernel width\nfunction, or\nif node\n belongs to the neighborhood\nof the winning node\notherwise\n4) Repeat steps 2) \u20133) until no change of neuron position\nthat is more than a small positive number is observed.\nWhile SOFM enjoy the merits of input space density ap-\nproximation and independence of the order of input patterns, a\nnumber of user-dependent parameters cause problems when ap-plied in real practice. Like the\n-means algorithm, SOFM need\nto prede \ufb01ne the size of the lattice, i.e., the number of clusters,\nwhich is unknown for most circumstances. Additionally, trained\nSOFM may be suffering from input space density misrepresen-tation [132], where areas of low pattern density may be over-rep-resented and areas of high density under-represented. Kohonen\nreviewed a variety of variants of SOFM in [169], which improve\ndrawbacks of basic SOFM and broaden its applications. SOFMcan also be integrated with other clustering approaches (e.g.,\n-means algorithm or HC) to provide more effective and faster\nclustering. [263] and [276] illustrate two such hybrid systems.\nART was developed by Carpenter and Grossberg, as a so-\nlution to the plasticity and stability dilemma [51], [53], [113].ART can learn arbitrary input patterns in a stable, fast, andself-organizing way, thus, overcoming the effect of learning in-\nstability that plagues many other competitive networks. ART isnot, as is popularly imagined, a neural network architecture. It\nis a learning theory, that resonance in neural circuits can trigger\nfast learning. As such, it subsumes a large family of currentand future neural networks architectures, with many variants.ART1 is the \ufb01rst member, which only deals with binary input\npatterns [51], although it can be extended to arbitrary input\npatterns by a variety of coding mechanisms. ART2 extendsthe applications to analog input patterns [52] and ART3 intro-duces a new mechanism originating from elaborate biological\nprocesses to achieve more ef \ufb01cient parallel search in hierar-\nchical structures [54]. By incorporating two ART modules,which receive input patterns\nART\n and corresponding labels\nART\n , respectively, with an", "start_char_idx": 0, "end_char_idx": 3472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0103b5f-a473-4495-a086-1256179309af": {"__data__": {"id_": "d0103b5f-a473-4495-a086-1256179309af", "embedding": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c35f800b-c539-4a91-ac9d-1e568091bcfc", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "2": {"node_id": "060d849e-db0f-46ab-af16-6cbf6f335f10", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38"}}, "hash": "2769b2877636983ba4a919a012e6ff35a0de00417b639595b63e544232e239ed", "text": "patterns\nART\n and corresponding labels\nART\n , respectively, with an inter-ART module, the resulting\nARTMAP system can be used for supervised classi \ufb01cations\n[56]. The match tracking strategy ensures the consistency ofcategory prediction between two ART modules by dynamicallyadjusting the vigilance parameter of ART\n. Also see fuzzy\nARTMAP in [55]. A similar idea, omitting the inter-ART\nmodule, is known as LAPART [134].\nThe basic ART1 architecture consists of two-layer nodes, the\nfeature representation \ufb01eld\n and the category representation\n\ufb01eld\n . They are connected by adaptive weights, bottom-up\nweight matrix\n and top-down weight matrix\n . The pro-\ntotypes of clusters are stored in layer\n . After it is activated\naccording to the winner-takes-all competition, an expectation\nis re\ufb02ected in layer\n , and compared with the input pattern.\nThe orienting subsystem with the speci \ufb01ed vigilance parameter\ndetermines whether the expectation and the\ninput are closely matched, and therefore controls the generation\nof new clusters. It is clear that the larger\n is, the more clusters\nare generated. Once weight adaptation occurs, both bottom-upand top-down weights are updated simultaneously. This is calledresonance, from which the name comes. The ART1 algorithm\ncan be described as follows.\n1) Initialize weight matrices\nand\n as\n, where\n are sorted in a descending order and sat-\nis\ufb01es\n for\n and any\nbinary input pattern\n , and\n ;\n2) For a new pattern\n , calculate the input from layer\nto layer\n as\nif\nis an uncommitted node\n\ufb01rst activated\nif\nis a committed node\nwhere\n represents the logic AND operation.\n3) Activate layer\n by choosing node\n with the winner-\ntakes-all rule\n .\n4) Compare the expectation from layer\n with the input\npattern. If\n , go to step 5a), other-\nwise go to step 5b).", "start_char_idx": 3405, "end_char_idx": 5196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e45e714b-bb58-428c-8583-971665ec4fc5": {"__data__": {"id_": "e45e714b-bb58-428c-8583-971665ec4fc5", "embedding": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faf94140-9051-486b-bd76-9b19bbc6de76", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "3": {"node_id": "d0a1f3ca-4ae2-471f-a970-47b832c5f4f2", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "8b8702fdaf808caaa4a3954d598d6bf6de9c4edc5ea0e5cb467d5feece2e0d0e"}}, "hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 659\n5)\na) Update the corresponding weights for the active\nnode as\n new\n old\nold\n and\n new\n old\n.;\nb) Send a reset signal to disable the current active node\nby the orienting subsystem and return to step 3).\n6) Present another input pattern, return to step 2) until all\npatterns are processed.\nNote the relation between ART network and other clustering\nalgorithms described in traditional and statistical language.\nMoore used several clustering algorithms to explain the clus-tering behaviors of ART1 and therefore induced and proved anumber of important properties of ART1, notably its equiva-\nlence to varying\n-means clustering [204]. She also showed\nhow to adapt these algorithms under the ART1 framework. In[284] and [285], the ease with which ART may be used forhierarchical clustering is also discussed.\nFuzzy ART (FA) bene \ufb01ts the incorporation of fuzzy set theory\nand ART [57]. FA maintains similar operations to ART1 and\nuses the fuzzy set operators to replace the binary operators, sothat it can work for all real data sets. FA exhibits many desirablecharacteristics such as fast and stable learning and atypical pat-\ntern detection. Huang et al. investigated and revealed more prop-\nerties of FA classi \ufb01ed as template, access, reset, and the number\nof learning epochs [143]. The criticisms for FA are mostly fo-cused on its inef \ufb01ciency in dealing with noise and the de \ufb01-\nciency of hyperrectangular representation for clusters in many\ncircumstances [23], [24], [281]. Williamson described GaussianART (GA) to overcome these shortcomings [281], in which eachcluster is modeled with Gaussian distribution and represented as\na hyperellipsoid geometrically. GA does not inherit the of \ufb02ine\nfast learning property of FA, as indicated by Anagnostopoulos et\nal.[13], who proposed different ART architectures: hypersphere\nART (HA) [12] for hyperspherical clusters and ellipsoid ART\n(EA) [13] for hyperellipsoidal clusters, to explore a more ef \ufb01-\ncient representation of clusters, while keeping important prop-erties of FA. Baraldi and Alpaydin proposed SART followingtheir general ART clustering networks frame, which is described\nthrough a feedforward architecture combined with a match com-\nparison mechanism [23]. As speci \ufb01c examples, they illustrated\nsymmetric fuzzy ART (SFART) and fully self-organizing SART(FOSART) networks. These networks outperform ART1 and FA\naccording to their empirical studies [23].\nIn addition to these, many other neural network architectures\nare developed for clustering. Most of these architectures uti-lize prototype vectors to represent clusters, e.g., cluster detec-\ntion and labeling network (CDL) [82], HEC [194], and SPLL\n[296]. HEC uses a two-layer network architecture to estimatethe regularized Mahalanobis distance, which is equated to theEuclidean distance in a transformed whitened space. CDL is\nalso a two-layer network with an inverse squared Euclidean\nmetric. CDL requires the match between the input patterns andthe prototypes above a threshold, which is dynamically adjusted.SPLL emphasizes initiation independent and adaptive genera-\ntion of clusters. It begins with a random prototype in the input\nspace and iteratively chooses and divides prototypes until no fur-ther split is available. The divisibility of a prototype is based onthe consideration that each prototype represents only one natural\nFig. 2. ART1 architecture. Two layers are included in the attentional\nsubsystem, connected via", "start_char_idx": 0, "end_char_idx": 3500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0a1f3ca-4ae2-471f-a970-47b832c5f4f2": {"__data__": {"id_": "d0a1f3ca-4ae2-471f-a970-47b832c5f4f2", "embedding": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "faf94140-9051-486b-bd76-9b19bbc6de76", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "2": {"node_id": "e45e714b-bb58-428c-8583-971665ec4fc5", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46"}}, "hash": "8b8702fdaf808caaa4a3954d598d6bf6de9c4edc5ea0e5cb467d5feece2e0d0e", "text": "Two layers are included in the attentional\nsubsystem, connected via bottom-up and top-down adaptive weights. Theirinteractions are controlled by the orienting subsystem through a vigilance\nparameter.\ncluster, instead of the combinations of several clusters. Simpson\nemployed hyperbox fuzzy sets to characterize clusters [100],\n[249]. Each hyperbox is delineated by a min and max point, and\ndata points build their relations with the hyperbox through themembership function. The learning process experiences a se-ries of expansion and contraction operations, until all clusters\nare stable.\nI. Kernel-Based Clustering\nKernel-based learning algorithms [209], [240], [274] are\nbased on Cover \u2019s theorem. By nonlinearly transforming a set\nof complex and nonlinearly separable patterns into a higher-di-\nmensional feature space, we can obtain the possibility to\nseparate these patterns linearly [132]. The dif \ufb01culty of curse\nof dimensionality can be overcome by the kernel trick, arisingfrom Mercer \u2019s theorem [132]. By designing and calculating\nan inner-product kernel, we can avoid the time-consuming,\nsometimes even infeasible process to explicitly describe thenonlinear mapping and compute the corresponding points in\nthe transformed space.\nIn [241], Sch \u00f6lkopf, Smola, and M \u00fcller depicted a kernel-\n-means algorithm in the online mode. Suppose we have a set of\npatterns\n and a nonlinear map\n. Here,\n represents a feature space with arbitrarily high di-\nmensionality. The object of the algorithm is to \ufb01nd\n centers so\nthat we can minimize the distance between the mapped patterns\nand their closest center\nwhere\n is the center for the\n th cluster and lies in a span of\n, and\n is the inner-\nproduct kernel.\nDe\ufb01ne the cluster assignment variable\nif\n belongs to cluster\notherwise.", "start_char_idx": 3433, "end_char_idx": 5209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c38c6520-a97c-4a99-bdb9-5548c7fc6665": {"__data__": {"id_": "c38c6520-a97c-4a99-bdb9-5548c7fc6665", "embedding": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a08f6232-65ec-47b4-8564-c6274517669d", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "3": {"node_id": "b4ebd8f0-dbbc-4820-9b83-a51aff565db9", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "e71746930c2e328fb173ed955569eacad948b4761ea723327bf0f6ca538223b4"}}, "hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b", "text": "660 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nThen the kernel-\n -means algorithm can be formulated as the\nfollowing.\n1) Initialize the centers\n with the \ufb01rst\n , ob-\nservation patterns;\n2) Take a new pattern\n and calculate\n as\nshown in the equation at the bottom of the page.\n3) Update the mean vector\n whose corresponding\nis 1\nwhere\n .\n4) Adapt the coef \ufb01cients\n for each\n as\nfor\nfor\n5) Repeat steps 2) \u20134) until convergence is achieved.\nTwo variants of kernel-\n -means were introduced in [66],\nmotivated by SOFM and ART networks. These variants con-\nsider effects of neighborhood relations, while adjusting the\ncluster assignment variables, and use a vigilance parameter tocontrol the process of producing mean vectors. The authors alsoillustrated the application of these approaches in case based\nreasoning systems.\nAn alternative kernel-based clustering approach is in [107].\nThe problem was formulated to determine an optimal partition\nto minimize the trace of within-group scatter matrix in the\nfeature space\nwhere\n,\nand\n is the total number of patterns in the\n th cluster.\nNote that the kernel function utilized in this case is the radial\nbasis function (RBF) and\n can be interpreted as a mea-\nsure of the denseness for the\n th cluster.\nBen-Hur et al. presented a new clustering algorithm, SVC,\nin order to \ufb01nd a set of contours used as the cluster bound-\naries in the original data space [31], [32]. These contours can\nbe formed by mapping back the smallest enclosing sphere in\nthe transformed feature space. RBF is chosen in this algorithm,and, by adjusting the width parameter of RBF, SVC can form ei-\nther agglomerative or divisive hierarchical clusters. When somepoints are allowed to lie outside the hypersphere, SVC can deal\nwith outliers effectively. An extension, called multiple spheres\nsupport vector clustering, was proposed in [62], which combinesthe concept of fuzzy membership.\nKernel-based clustering algorithms have many advantages.\n1) It is more possible to obtain a linearly separable hyper-\nplane in the high-dimensional, or even in \ufb01nite feature\nspace.\n2) They can form arbitrary clustering shapes other than\nhyperellipsoid and hypersphere.\n3) Kernel-based clustering algorithms, like SVC, have the\ncapability of dealing with noise and outliers.\n4) For SVC, there is no requirement for prior knowledge\nto determine the system topological structure. In [107],the kernel matrix can provide the means to estimate thenumber of clusters.\nMeanwhile, there are also some problems requiring further\nconsideration and investigation. Like many other algorithms,how to determine the appropriate parameters, for example, thewidth of Gaussian kernel, is not trivial. The problem of compu-\ntational complexity may become serious for large data sets.\nThe process of constructing the sum-of-squared clustering\nalgorithm [107] and\n-means algorithm [241] presents a good\nexample to reformulate more powerful nonlinear versions\nfor many existing linear algorithms, provided that the scalar\nproduct can be obtained. Theoretically, it is important to investi-gate whether these nonlinear variants can keep some useful andessential properties of the original algorithms and how Mercer\nkernels contribute to the improvement of the algorithms. The\neffect of different types of kernel functions, which are rich inthe literature, is also an interesting topic for further exploration.\nJ. Clustering Sequential Data\nSequential data are sequences with variable length and many\nother distinct characteristics, e.g.,", "start_char_idx": 0, "end_char_idx": 3533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b4ebd8f0-dbbc-4820-9b83-a51aff565db9": {"__data__": {"id_": "b4ebd8f0-dbbc-4820-9b83-a51aff565db9", "embedding": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a08f6232-65ec-47b4-8564-c6274517669d", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "2": {"node_id": "c38c6520-a97c-4a99-bdb9-5548c7fc6665", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b"}}, "hash": "e71746930c2e328fb173ed955569eacad948b4761ea723327bf0f6ca538223b4", "text": "data are sequences with variable length and many\nother distinct characteristics, e.g., dynamic behaviors, timeconstraints, and large volume [120], [265]. Sequential data can\nbe generated from: DNA sequencing, speech processing, text\nmining, medical diagnosis, stock market, customer transactions,web data mining, and robot sensor analysis, to name a few [78],[265]. In recent decades, sequential data grew explosively. For\nexample, in genetics, the recent statistics released on October\n15, 2004 (Release 144.0) shows that there are 43 194 602655 bases from 38 941 263 sequences in GenBank database[103] and release 45.0 of SWISSPROT on October 25, 2004\ncontains 59 631 787 amino acids in 163 235 sequence entries\n[267]. Cluster analysis explores potential patterns hidden in thelarge number of sequential data in the context of unsupervisedlearning and therefore provides a crucial way to meet the cur-\nrent challenges. Generally, strategies for sequential clustering\nmostly fall into three categories.\nif\notherwise", "start_char_idx": 3447, "end_char_idx": 4463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15525363-3d90-43cd-a223-601934df3bcf": {"__data__": {"id_": "15525363-3d90-43cd-a223-601934df3bcf", "embedding": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1223bf80-dba8-4575-939c-432249e037ac", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "3": {"node_id": "f96e73ad-9c95-49fa-8f50-e3e7b617fa03", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "94fddf28ce654e61aa9b5e59bf760a85075bff4bb9e2023501af9089e49e3567"}}, "hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 661\n1) Sequence Similarity: The \ufb01rst scheme is based on the\nmeasure of the distance (or similarity) between each pair of se-quences. Then, proximity-based clustering algorithms, either hi-\nerarchical or partitional, can group sequences. Since many se-\nquential data are expressed in an alphabetic form, like DNAor protein sequences, conventional measure methods are inap-propriate. If a sequence comparison is regarded as a process of\ntransforming a given sequence to another with a series of substi-\ntution, insertion, and deletion operations, the distance betweenthe two sequences can be de \ufb01ned by virtue of the minimum\nnumber of required operations. A common analysis processes is\nalignment, illustrated in Fig. 3. The de \ufb01ned distance is known\nas edit distance or Levenshtein distance [120], [236]. These editoperations are weighted (punished or rewarded) according tosome prior domain knowledge and the distance herein is equiva-\nlent to the minimum cost to complete the transformation. In this\nsense, the similarity or distance between two sequences can bereformulated as an optimal alignment problem, which \ufb01ts well\nin the framework of dynamic programming.\nGiven two sequences,\nand\n, the basic dynamic program-\nming-based sequence alignment algorithm, also known asthe Needleman-Wunsch algorithm, can be depicted by the\nfollowing recursive equation [78], [212]:\nwhere\n is de \ufb01ned as the best alignment score be-\ntween sequence segment\n of\n and\nof\n , and\n ,o r\nrepresent the cost for aligning\n to\n , aligning\n to\na gap (denoted as\n ), or aligning\n to a gap, respectively. The\ncomputational results for each position at\n and\n are recorded\nin an array with a pointer that stores current optimal operationsand provides an effective path in backtracking the alignment.\nThe Needleman-Wunsch algorithm considers the comparison\nof the whole length of two sequences and therefore performsa global optimal alignment. However, it is also important to\ufb01nd local similarity among sequences in many circumstances.\nThe Smith-Waterman algorithm achieves that by allowing the\nbeginning of a new alignment during the recursive computa-tion, and the stop of an alignment anywhere in the dynamicprogramming matrix [78], [251]. This change is summarized\nin the following:\nFor both the global and local alignment algorithms, the com-\nputation complexity is\n , which is very expensive, es-\npecially for a clustering problem that requires an all-against-all\npairwise comparison. A wealth of speed-up methods has been\ndeveloped to improve the situation [78], [120]. We will seemore discussion in Section III-E in the context of biologicalsequences analysis. Other examples include applications for\nspeech recognition [236] and navigation pattern mining [131].\nFig. 3. Illustration of a sequence alignment. Series of edit operations\nis performed to change the sequence CLUSTERING into the sequence\nCLASSIFICATION.\n2) Indirect Sequence Clustering: The second approach\nemploys an indirect strategy, which begins with the extraction\nof a set of features from the sequences. All the sequencesare then mapped into the transformed feature space, whereclassical vector space-based clustering algorithms can be\nused to form clusters. Obviously, feature extraction becomes\nthe essential factor that decides the effectiveness of thesealgorithms. Guralnik and Karypis discussed the potential de-pendency between two sequential patterns and suggested both\nthe global and the local approaches to prune the initial feature\nsets in order to better represent sequences in the new featurespace [119]. Morzy et al. utilized the sequential patterns as\nthe basic element in the agglomerative", "start_char_idx": 0, "end_char_idx": 3702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f96e73ad-9c95-49fa-8f50-e3e7b617fa03": {"__data__": {"id_": "f96e73ad-9c95-49fa-8f50-e3e7b617fa03", "embedding": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1223bf80-dba8-4575-939c-432249e037ac", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "2": {"node_id": "15525363-3d90-43cd-a223-601934df3bcf", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0"}}, "hash": "94fddf28ce654e61aa9b5e59bf760a85075bff4bb9e2023501af9089e49e3567", "text": "et al. utilized the sequential patterns as\nthe basic element in the agglomerative hierarchical clustering\nand de \ufb01ned a co-occurrence measure, as the standard of fusion\nof smaller clusters [207]. These methods greatly reduce thecomputational complexities and can be applied to large-scalesequence databases. However, the process of feature selection\ninevitably causes the loss of some information in the original\nsequences and needs extra attention.\n3) Statistical Sequence Clustering: Typically, the \ufb01rst two\napproaches are used to deal with sequential data composed of\nalphabets, while the third paradigm, which aims to construct\nstatistical models to describe the dynamics of each group of se-quences, can be applied to numerical or categorical sequences.The most important method is hidden Markov models (HMMs)\n[214], [219], [253], which \ufb01rst gained its popularity in the appli-\ncation of speech recognition [229]. A discrete HMM describesan unobservable stochastic process consisting of a set of states,each of which is related to another stochastic process that emits\nobservable symbols. Therefore, the HMM is completely speci-\n\ufb01ed by the following.\n1) A \ufb01nite set\nwith\n states.\n2) A discrete set\n with\n observa-\ntion symbols.\n3) A state transition distribution\n , where\nth state at time\n th state at time\n4) A symbol emission distribution\n , where\nat\n th state at\n5) An initial state distribution\n , where\nth state at\nAfter an initial state is selected according to the initial dis-\ntribution\n , a symbol is emitted with emission distribution\n .\nThe next state is decided by the state transition distribution\nand it also generates a symbol based on\n . The process repeats\nuntil reaching the last state. Note that the procedure generates", "start_char_idx": 3621, "end_char_idx": 5364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "532cb39b-74f8-401f-ad08-4af5fe2da04e": {"__data__": {"id_": "532cb39b-74f8-401f-ad08-4af5fe2da04e", "embedding": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "3": {"node_id": "39ea1c51-14b2-4f40-8115-117c27a1f724", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "7efa399dff6278a0cc6a664985a863da9d8ac889c280b567406da66b348811ca"}}, "hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead", "text": "662 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\na sequence of symbol observations instead of states, which is\nwhere the name \u201chidden \u201dcomes from. HMMs are well founded\ntheoretically [229]. Dynamic programming and EM algorithm\nare developed to solve the three basic problems of HMMs as the\nfollowing.\n1) Likelihood (forward or backward algorithm). Com-\npute the probability of an observation sequence given\na model.\n2) State interpretation (Vertbi algorithm). Find an op-\ntimal state sequence by optimizing some criterion\nfunction given the observation sequence and the\nmodel.\n3) Parameter estimation (Baum \u2013Welch algorithm). De-\nsign suitable model parameters to maximize the prob-\nability of observation sequence under the model.\nThe equivalence between an HMM and a recurrent\nback-propagation network was elucidated in [148], and auniversal framework was constructed to describe both the\ncomputational and the structural properties of the HMM and\nthe neural network.\nSmyth proposed an HMM-based clustering model, which,\nsimilar to the theories introduced in mixture densities-based\nclustering, assumes that each cluster is generated based on some\nprobability distribution [253]. Here, HMMs are used rather thanthe common Gaussian or\n-distribution. In addition to the form\nof\ufb01nite mixture densities, the mixture model can also be de-\nscribed by means of an HMM with the transition matrix\nwhere\n is the transition distribution for the\n th\ncluster. The initial distribution of the HMM is determined basedon the prior probability for each cluster. The basic learning\nprocess starts with a parameter initialization scheme to form\na rough partition with the log-likelihood of each sequenceserving as the distance measure. The partition is further re-\ufb01ned by training the overall HMM over all sequences with\nthe classical EM algorithm. A Monte-Carlo cross validation\nmethod was used to estimate the possible number of clusters.An application with a modi \ufb01ed HMM model that considers\nthe effect of context for clustering facial display sequences is\nillustrated in [138]. Oates et al. addressed the initial problem by\npregrouping the sequences with the agglomerative hierarchicalclustering, which operates on the proximity matrix determinedby the dynamic time warping (DTW) technique [214]. The area\nformed between one original sequence and a new sequence,\ngenerated by warping the time dimension of another originalsequence, re \ufb02ects the similarity of the two sequences. Li and\nBiswas suggested several objective criterion functions based\non posterior probability and information theory for structural\nselection of HMMs and cluster validity [182]. More recentadvances on HMMs and other related topics are reviewed in[30].\nOther model-based sequence clustering includes mixtures of\n\ufb01rst-order Markov chain [255] and a linear model like autore-\ngressive moving average (ARMA) model [286]. Usually, theyare combined with EM for parameter estimation [286]. Smyth\n[255] and Cadez et al. [50] further generalize a universal prob-\nabilistic framework to model mixed data measurement, which\nincludes both conventional static multivariate vectors and dy-\nnamic sequence data.\nThe paradigm models clusters directly from original data\nwithout additional process that may cause information loss.\nThey provide more intuitive ways to capture the dynamics\nof data and more \ufb02exible means to deal with variable length\nsequences. However, determining the number of model com-ponents remains a complicated and uncertain process [214],\n[253]. Also, the model selected is required to have suf \ufb01cient\ncomplexity, in order to interpret the characteristics of data.\nK. Clustering Large-Scale Data", "start_char_idx": 0, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "39ea1c51-14b2-4f40-8115-117c27a1f724": {"__data__": {"id_": "39ea1c51-14b2-4f40-8115-117c27a1f724", "embedding": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "2": {"node_id": "532cb39b-74f8-401f-ad08-4af5fe2da04e", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead"}}, "hash": "7efa399dff6278a0cc6a664985a863da9d8ac889c280b567406da66b348811ca", "text": "in order to interpret the characteristics of data.\nK. Clustering Large-Scale Data Sets\nScalability becomes more and more important for clustering\nalgorithms with the increasing complexity of data, mainly man-ifesting in two aspects: enormous data volume and high dimen-sionality. Examples, illustrated in the sequential clustering sec-\ntion, are just some of the many applications that require this ca-\npability. With the further advances of database and Internet tech-nologies, clustering algorithms will face more severe challengesin handling the rapid growth of data. We summarize the com-\nputational complexity of some typical and classical clustering\nalgorithms in Table II with several newly proposed approachesspeci \ufb01cally designed to deal with large-scale data sets. Several\npoints can be generalized through the table.\n1) Obviously, classical hierarchical clustering algo-\nrithms, including single-linkage, complete linkage,\naverage linkage, centroid linkage and median linkage,are not appropriate for large-scale data sets due to thequadratic computational complexities in both execu-\ntion time and store space.\n2)\n-means algorithm has a time complexity of\nand space complexity of\n . Since\n is usu-\nally much larger than both\n and\n , the complexity be-\ncomes near linear to the number of samples in the data\nsets.\n -means algorithm is effective in clustering large-\nscale data sets, and efforts have been made in order toovercome its disadvantages [142], [218].\n3) Many novel algorithms have been developed to cluster\nlarge-scale data sets, especially in the context of datamining [44], [45], [85], [135], [213], [248]. Many ofthem can scale the computational complexity linearly\nto the input size and demonstrate the possibility of han-\ndling very large data sets.\na) Random sampling approach, e.g., CLARA clus-\ntering large applications (CLARA) [161] and CURE\n[116]. The key point lies that the appropriate sample\nsizes can effectively maintain the important geomet-rical properties of clusters. Furthermore, Chernoffboundscanprovideestimationforthelowerboundof\nthe minimum sample size, given the low probability\nthat points in each cluster are missed in the sampleset [116]. CLARA represents each cluster with a", "start_char_idx": 3604, "end_char_idx": 5828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01813f96-3e63-47ed-8779-f0ca8a50b3db": {"__data__": {"id_": "01813f96-3e63-47ed-8779-f0ca8a50b3db", "embedding": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7906c566-aa7c-4424-90f1-0c02cb5faa53", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "3": {"node_id": "99e41b42-5242-4e72-8c10-73986a9a27f1", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "191ed8bc2708d7eaf6cd911e79488cd5dd96e34229f0e76856c065df0d4f0738"}}, "hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 663\nmedoid while CURE chooses a set of well-scattered\nand center-shrunk points.\nb) Randomized search approach, e.g., clustering\nlarge applications based on randomized search\n(CLARANS) [213]. CLARANS sees the clusteringas a search process in a graph, in which each nodecorresponds to a set of\nmedoids. It begins with an\narbitrary node as the current node and examines a set\nof neighbors, de \ufb01ned as the node consisting of only\none different data object, to seek a better solution,i.e., any neighbor, with a lower cost, becomes the\ncurrent node. If the maximum number of neighbors,\nspeci \ufb01ed by the user, has been reached, the current\nnode is accepted as a winning node. This processiterates several times as speci \ufb01ed by users. Though\nCLARANS achieves better performance than algo-\nrithms like CLARA, the total computational time isstill quadratic, which makes CLARANS not quiteeffective in very large data sets.\nc) Condensation-based approach, e.g., BIRCH [295].\nBIRCH generates and stores the compact sum-maries of the original data in a CF tree, as discussedin Section II-B. This new data structure ef \ufb01ciently\ncaptures the clustering information and largely\nreduces the computational burden. BIRCH wasgeneralized into a broader framework in [101] withtwo algorithms realization, named as BUBBLE and\nBUBBLE-FM.\nd) Density-based approach, e.g., density based spatial\nclustering of applications with noise (DBSCAN)[85] and density-based clustering (DENCLUE)\n[135]. DBSCAN requires that the density in a\nneighborhood for an object should be high enoughif it belongs to a cluster. DBSCAN creates a newcluster from a data object by absorbing all objects in\nits neighborhood. The neighborhood needs to sat-\nisfy a user-speci \ufb01ed density threshold. DBSCAN\nuses a\n-tree structure for more ef \ufb01cient queries.\nDENCLUE seeks clusters with local maxima of\nthe overall density function, which re \ufb02ects the\ncomprehensive in \ufb02uence of data objects to their\nneighborhoods in the corresponding data space.\ne) Grid-based approach, e.g., WaveCluster [248] and\nfractal clustering (FC) [26]. WaveCluster assigns\ndataobjectstoasetofunitsdividedintheoriginalfea-ture space, and employs wavelet transforms on theseunits, to map objects into the frequency domain. The\nkey idea is that clusters can be easily distinguished in\nthe transformed space. FC combines the concepts ofboth incremental clustering and fractal dimension.Data objects are incrementally added to the clusters,\nspeci \ufb01ed through an initial process, and represented\nas cells in a grid, with the condition that the fractaldimension of cluster needs to keep relatively stable.\n4) Most algorithms listed previously lack the capability of\ndealing with data with high dimensionality. Their per-\nformances degenerate with the increase of dimension-ality. Some algorithms, like FC and DENCLUE, haveshown some successful applications in such cases, but\nthese are still far from completely effective.\nIn addition to the aforementioned approaches, several other\ntechniquesalsoplaysigni \ufb01cantrolesinclusteringlarge-scaledata\nsets. Parallel algorithms can more effectively use computational\nresources,andgreatlyimproveoverallperformanceinthecontext\nofboth time andspace complexity [69], [217], [262]. Incrementalclustering techniques do not require the storage of the entire dataset, and can handle it in", "start_char_idx": 0, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "99e41b42-5242-4e72-8c10-73986a9a27f1": {"__data__": {"id_": "99e41b42-5242-4e72-8c10-73986a9a27f1", "embedding": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7906c566-aa7c-4424-90f1-0c02cb5faa53", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "2": {"node_id": "01813f96-3e63-47ed-8779-f0ca8a50b3db", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c"}}, "hash": "191ed8bc2708d7eaf6cd911e79488cd5dd96e34229f0e76856c065df0d4f0738", "text": "techniques do not require the storage of the entire dataset, and can handle it in a one-pattern-at-a-time way. If the pat-\ntern displays enough closeness to a cluster according to some\nprede \ufb01ned criteria, it is assigned to the cluster. Otherwise, a new\ncluster is created to represent the object. A typical example isthe ART family [51] \u2013[53] discussed in Section II-H. Most incre-\nmental clustering algorithms are dependent on the order of the\ninput patterns [51], [204]. Bradley, Fayyad, and Reina proposeda scalable clustering framework, considering seven relevant im-portant characteristics in dealing with large databases [44]. Ap-\nplications of the framework were illustrated for the\n-means al-\ngorithm and EM mixture models [44], [45].\nL. Exploratory Data Visualization and High-Dimensional\nData Analysis Through Dimensionality Reduction\nFor most of the algorithms summarized in Table II, although\nthey can deal with large-scale data, they are not suf \ufb01cient for\nanalyzing high-dimensional data. The term, \u201ccurse of dimen-\nsionality, \u201dwhich was \ufb01rst used by Bellman to indicate the ex-\nponential growth of complexity in the case of multivariate func-\ntion estimation under a high dimensionality situation [28], isgenerally used to describe the problems accompanying high di-mensional spaces [34], [132]. It is theoretically proved that the\ndistance between the nearest points is no different from that\nof other points when the dimensionality of the space is highenough [34]. Therefore, clustering algorithms that are based onthe distance measure may no longer be effective in a high dimen-\nsional space. Fortunately, in practice, many high-dimensional\ndata usually have an intrinsic dimensionality that is much lowerthan the original dimension [60]. Dimension reduction is impor-tant in cluster analysis, which not only makes the high-dimen-\nsional data addressable and reduces the computational cost, but\nprovides users with a clearer picture and visual examination ofthe data of interest. However, dimensionality reduction methodsinevitably cause some loss of information, and may damage the\ninterpretability of the results, even distort the real clusters.\nOne natural strategy for dimensionality reduction is to\nextract important components from original data, which cancontribute to the division of clusters. Principle component\nanalysis (PCA) or Karhunen-Lo \u00e9ve transformation is one of\nthe typical approaches, which is concerned with constructinga linear combination of a set of vectors that can best describethe variance of data. Given the\ninput pattern matrix\n, the linear mapping\nprojects\n into a low-dimensional\nsubspace, where\n is the resulting\n matrix and\n is the\nprojection matrix whose columns are the eigenvectors\nthat correspond to the\n largest eigenvalues of the\n co-\nvariance matrix\n , calculated from the whole data set (hence,\nthe column vectors of\n are orthonormal). PCA estimates the\nmatrix\n while minimizing the sum of squares of the error", "start_char_idx": 3305, "end_char_idx": 6275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a71a76c7-1e7b-4e8c-b48c-c39635de9755": {"__data__": {"id_": "a71a76c7-1e7b-4e8c-b48c-c39635de9755", "embedding": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "551278d9-b81e-4bb4-ad7e-9a814ccd95a8", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "3": {"node_id": "dd17a6f4-7ea6-4a76-a6c6-7fa3e8a6629f", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "b6b29f8cb0a6a1d57cfb05647a5fb8bea427f30613b6fd3e4adda298d8892b33"}}, "hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b", "text": "664 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nof approximating the input vectors. In this sense, PCA can\nbe realized through a three-layer neural network, called anauto-associative multilayer perceptron, with linear activation\nfunctions [19], [215]. In order to extract more complicated\nnonlinear data structure, nonlinear PCA was developed and oneof the typical examples is kernel PCA. As methods discussedin Section II-I, kernel PCA \ufb01rst maps the input patterns into a\nfeature space. The similar steps are then applied to solve the\neigenvalue problem with the new covariance matrix in the fea-ture space. In another way, extra hidden layers with nonlinearactivation functions can be added into the auto-associative\nnetwork for this purpose [38], [75].\nPCA is appropriate for Gaussian distributions since it relies on\nsecond-order relationships in the covariance matrix, Other linear\ntransforms, like independent component analysis (ICA) and pro-jection pursuit, which use higher order statistical information,are more suited for non-Gaussian distributions [60], [151]. The\nbasic goal of ICA is to \ufb01nd the components that are most statis-\ntically independent from each other [149], [154]. In the contextof blind source separation, ICA aims to separate the independentsource signals from the mixed observation signal. This problem\ncan be formulated in several different ways [149], and one of\nthe simplest form (without considering noise) is represented as\n, where\n is the\n -dimensional observable vector,\nis the\n -dimensional source vector assumed to be statistically\nindependent, and\n is a nonsingular\n mixing matrix. ICA\ncan also be realized by virtueof multilayer perceptrons, and[158]illustrates one of such examples. The proposed ICA networkincludes whitening, separation, and basis vectors estimation\nlayers, with corresponding learning algorithms. The authors\nalso indicated its connection to the auto-associative multilayerperceptron. Projection pursuit is another statistical technique forseeking low-dimensional projection structures for multivariate\ndata [97], [144]. Generally, projection pursuit regards the normal\ndistribution as the least interesting projections and optimizessome certain indices that measure the degree of nonnormality[97]. PCA can be considered as a special example of projection\npursuit, as indicated in [60]. More discussions on the relations\namong PCA, ICA, projection pursuit, and other relevant tech-niques are offered in [149] and [158].\nDifferent from PCA, ICA, and projection pursuit, Multidi-\nmensional scaling (MDS) is a nonlinear projection technique\n[75], [292]. The basic idea of MDS lies in \ufb01tting original mul-\ntivariate data into a low-dimensional structure while aiming tomaintain the proximity information. The distortion is measuredthrough some criterion functions, e.g., in the sense of sum\nof squared error between the real distance and the projection\ndistance. The isometric feature mapping (Isomap) algorithmis another nonlinear technique, based on MDS [270]. Isomapestimates the geodesic distance between a pair of points, which\nis the shortest path between the points on a manifold, by virtue\nof the measured input-space distances, e.g., the Euclideandistance usually used. This extends the capability of MDS toexplore more complex nonlinear structures in the data. Locally\nlinear embedding (LLE) algorithm addresses the nonlinear\ndimensionality reduction problem from a different startingpoint [235]. LLE emphasizes the local linearity of the manifoldand assumes that the local relations in the original data space(\n-dimensional) are also preserved in the projected low-di-\nmensional space (\n -dimensional). This is represented through\na weight matrix, describing how each", "start_char_idx": 0, "end_char_idx": 3754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd17a6f4-7ea6-4a76-a6c6-7fa3e8a6629f": {"__data__": {"id_": "dd17a6f4-7ea6-4a76-a6c6-7fa3e8a6629f", "embedding": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "551278d9-b81e-4bb4-ad7e-9a814ccd95a8", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "2": {"node_id": "a71a76c7-1e7b-4e8c-b48c-c39635de9755", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b"}}, "hash": "b6b29f8cb0a6a1d57cfb05647a5fb8bea427f30613b6fd3e4adda298d8892b33", "text": "space (\n -dimensional). This is represented through\na weight matrix, describing how each point is related to the\nreconstruction of another data point. Therefore, the procedure\nfor dimensional reduction can be constructed as the problemthat \ufb01nding\n-dimensional vectors\n so that the criterion\nfunction\n is minimized. Another inter-\nesting nonlinear dimensionality reduction approach, known as\nLaplace eigenmap algorithm, is presented in [27].\nAs discussed in Section II-H, SOFM also provide good visu-\nalization for high-dimensional input patterns [168]. SOFM map\ninput patterns into a one or usually two dimensional lattice struc-\nture, consisting of nodes associated with different clusters. Anapplication for clustering of a large set of documental data isillustrated in [170], in which 6 840 568 patent abstracts were\nprojected onto a SOFM with 1 002 240 nodes.\nSubspace-based clustering addresses the challenge by ex-\nploring the relations of data objects under different combina-tions of features. clustering in quest (CLIQUE) [3] employs a\nbottom-up scheme to seek dense rectangular cells in all sub-\nspaces with high density of points. Clusters are generated as theconnected components in a graph whose vertices stand for thedense units. The resulting minimal description of the clusters is\nobtained through the merge of these rectangles. OptiGrid [136] is\ndesigned to obtain an optimal grid-partitioning. This is achievedby constructing the best cutting hyperplanes through a set ofprojections. The time complexity for OptiGrid is in the interval\nof\nand\n . ORCLUS (arbitrarily ORiented\nprojected CLUster generation) [2] de \ufb01nes a generalized pro-\njected cluster as a densely distributed subset of data objects ina subspace, along with a subset of vectors that represent the\nsubspace. The dimensionality of the subspace is prespeci \ufb01ed by\nusers as an input parameter, and several strategies are proposedin guidance of its selection. The algorithm begins with a setof randomly selected\nseeds with the full dimensionality.\nThis dimensionality and the number of clusters are decayed\naccording to some factors at each iteration, until the numberof clusters reaches the prede \ufb01ned values. Each repetition con-\nsists of three basic operations, known as assignment, vector\n\ufb01nding, and merge. ORCLUS has the overall time complexity of\nand space complexity of\n .\nObviously, the scalability to large data sets relies on the numberof initial seeds\n. A generalized subspace clustering model,\npCluster was proposed in [279]. These pClusters are formed\nby a depth- \ufb01rst clustering algorithm. Several other interesting\napplications, including a Clindex (CLustering for INDEXing)scheme and wavelet transform, are shown in [184] and [211],\nrespectively.\nM. How Many Clusters?\nThe clustering process partitions data into an appropriate\nnumber of subsets. Although for some applications, users candetermine the number of clusters,\n, in terms of their expertise,\nunder more circumstances, the value of\n is unknown and\nneeds to be estimated exclusively from the data themselves.\nMany clustering algorithms ask\n to be provided as an input\nparameter, and it is obvious that the quality of resulting clustersis largely dependent on the estimation of\n. A division with", "start_char_idx": 3666, "end_char_idx": 6915, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7c1381a-a188-4635-ac50-fc52a6c4541b": {"__data__": {"id_": "c7c1381a-a188-4635-ac50-fc52a6c4541b", "embedding": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "3": {"node_id": "58dfe800-c747-4d60-97bd-beddd57d59f1", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "3fd6f5f7a68261dfb98c13d000ce8f6ae5e5f7733e0855b8381bb67b7294208f"}}, "hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 665\ntoo many clusters complicates the result, therefore, makes it\nhard to interpret and analyze, while a division with too fewclusters causes the loss of information and misleads the \ufb01nal\ndecision. Dubes called the problem of determining the number\nof clusters \u201cthe fundamental problem of cluster validity \u201d[74].\nA large number of attempts have been made to estimate the\nappropriate\nand some of representative examples are illus-\ntrated in the following.\n1) Visualization of the data set . For the data points that\ncan be effectively projected onto a two-dimensional\nEuclidean space, which are commonly depicted with\na histogram or scatterplot, direct observations can pro-vide good insight on the value of\n. However, the com-\nplexity of most real data sets restricts the effectiveness\nof the strategy only to a small scope of applications.\n2) Construction of certain indices (or stopping rules) .\nThese indices usually emphasize the compactness ofintra-cluster and isolation of inter-cluster and consider\nthe comprehensive effects of several factors, including\nthe de \ufb01ned squared error, the geometric or statistical\nproperties of the data, the number of patterns, the dis-similarity (or similarity), and the number of clusters.\nMilligan and Cooper compared and ranked 30 indices\naccording to their performance over a series of arti \ufb01-\ncial data sets [202]. Among these indices, the Cali \u00f1ski\nand Harabasz index [74] achieve the best performance\nand can be represented as\nCH\nwhere\n is the total number of patterns and\nand\n are the trace of the between and within\nclass scatter matrix, respectively. The\n that maxi-\nmizes the value of CH\n is selected as the optimal.\nIt is worth noting that these indices may be data de-pendent. The good performance of an index for cer-tain data does not guarantee the same behavior with\ndifferent data. As pointed out by Everitt, Landau, and\nLeese, \u201cit is advisable not to depend on a single rule\nfor selecting the number of groups, but to synthesizethe results of several techniques \u201d[88].\n3) Optimization of some criterion functions under prob-\nabilistic mixture-model framework . In a statistical\nframework, \ufb01nding the correct number of clusters\n(components)\n, is equivalent to \ufb01tting a model with\nobserved data and optimizing some criterion [197].\nUsually, the EM algorithm is used to estimate themodel parameters for a given\n, which goes through\na prede \ufb01ned range of values. The value of\n that\nmaximizes (or minimizes) the de \ufb01ned criterion is\nregarded as optimal. Smyth presented a Monte-Carlocross-validation method, which randomly divides datainto training and test sets\ntimes according to a cer-\ntain fraction\n (\n works well from the empirical\nresults) [252]. The\n is selected either directly based\non the criterion function or some posterior probabili-ties calculated.A large number of criteria, which combine concepts\nfrom information theory, have been proposed in theliterature. Typical examples include,\n\u2022 Akaike \u2019s information criterion (AIC) [4], [282]\nAIC\nwhere\n is the total number of patterns,\n is the\nnumber of parameters for each cluster,\n is the total\nnumber of parameters estimated, and\n is the max-\nimum log-likelihood.\n is selected with the minimum\nvalue of AIC\n .\n\u2022 Bayesian inference criterion (BIC) [226], [242]\nBIC\nis selected with the maximum value of BIC\n .\nMore criteria, such as minimum description length\n(MDL) [114], [233], minimum message length (MML)\n[114], [216], cross", "start_char_idx": 0, "end_char_idx": 3492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "58dfe800-c747-4d60-97bd-beddd57d59f1": {"__data__": {"id_": "58dfe800-c747-4d60-97bd-beddd57d59f1", "embedding": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "2": {"node_id": "c7c1381a-a188-4635-ac50-fc52a6c4541b", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487"}}, "hash": "3fd6f5f7a68261dfb98c13d000ce8f6ae5e5f7733e0855b8381bb67b7294208f", "text": "[233], minimum message length (MML)\n[114], [216], cross validation-based information crite-\nrion (CVIC) [254] and covariance in \ufb02ation criterion\n(CIC) [272], with their characteristics, are summarizedin [197]. Like the previous discussion for validation\nindex, there is no criterion that is superior to others in\ngeneral case. The selection of different criteria is stilldependent on the data at hand.\n4) Other heuristic approaches based on a variety of tech-\nniques and theories . Girolami performed eigenvalue\ndecomposition on the kernel matrix in the high-dimen-sional feature space and used the dominant\ncompo-\nnents in the decomposition summation as an indication\nof the possible existence of\n clusters [107]. Kothari\nand Pitts described a scale-based method, in which thedistance from a cluster centroid to other clusters inits neighborhood is considered (added as a regulariza-\ntion term in the original squared error criterion, Sec-\ntion II-C) [160]. The neighborhood of clusters work asa scale parameter and the\nthat is persistent in the\nlargest interval of the neighborhood parameter is re-\ngarded as the optimal.\nBesides the previous methods, constructive clustering algo-\nrithms can adaptively and dynamically adjust the number of\nclusters rather than use a prespeci \ufb01ed and \ufb01xed number. ART\nnetworks generate a new cluster, only when the match betweenthe input pattern and the expectation is below some prespeci \ufb01ed\ncon\ufb01dence value [51]. A functionally similar mechanism is\nused in the CDL network [82]. The robust competitive clus-\ntering algorithm (RCA) describes a competitive agglomerationprocess that progresses in stages, and clusters that lose in thecompetition are discarded, and absorbed into other clusters [98].\nThis process is generalized in [42], which attains the number\nof clusters by balancing the effect between the complexityand the \ufb01delity. Another learning scheme, SPLL iteratively\ndivides cluster prototypes from a single prototype until no\nmore prototypes satisfy the split criterion [296]. Several other\nconstructive clustering algorithms, including the FACS andplastic neural gas, can be accessed in [223] and [232], re-spectively. Obviously, the problem of determining the number", "start_char_idx": 3437, "end_char_idx": 5654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd3a61aa-aa78-4316-9ef8-3bf28352765b": {"__data__": {"id_": "dd3a61aa-aa78-4316-9ef8-3bf28352765b", "embedding": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86deeb8e-300b-4588-b9c4-3a595f07a249", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "3": {"node_id": "18a240bd-af7f-43c0-a008-f23c6eae1915", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "2d315cb4c8018dfb1779a7fe4b3d7fabf24dde95b43dd77e9423a39cd742925b"}}, "hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9", "text": "666 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nof clusters is converted into a parameter selection problem,\nand the resulting number of clusters is largely dependent onparameter tweaking.\nIII. A\nPPLICATIONS\nWe illustrate applications of clustering techniques in three as-\npects. The \ufb01rst is for two classical benchmark data sets that are\nwidely used in pattern recognition and machine learning. Then,we show an application of clustering for the traveling salesmanproblem. The last topic is on bioinformatics. We deal with clas-\nsical benchmarks in Sections III-A and III-B and the traveling\nsalesman problem in Section III-C. A more extensive discussionof bioinformatics is in Sections III-D and III-E.\nA. Benchmark Data Sets \u2014I\nRIS\nThe iris data set [92] is one of the most popular data\nsets to examine the performance of novel methods in pat-tern recognition and machine learning. It can be down-loaded from the UCI Machine Learning Repository at\nhttp://www.ics.uci.edu/~mlearn/MLRepository.html. There are\nthree categories in the data set (i.e., iris setosa, iris versicolorand iris virginical), each having 50 patterns with four features[i.e., sepal length (SL), sepal width (SW), petal length (PL),\nand petal width (PW)]. Iris setosa can be linearly separated\nfrom iris versicolor and iris virginical, while iris versicolor andiris virginical are not linearly separable (see Fig. 4(a), in whichonly three features are used). Fig. 4(b) depicts the clustering\nresult with a standard\n-means algorithm. It is clear to see that\n-means can correctly differentiate iris setosa from the other\ntwo iris plants. But for iris versicolor and virginical, there exist16 misclassi \ufb01cations. This result is similar to those (around\n15 errors) obtained from other classical clustering algorithms\n[221]. Table III summarizes some of the clustering resultsreported in the literature. From the table, we can see that manynewly developed approaches can greatly improve the clustering\nperformance on iris data set (around 5 misclassi \ufb01cations); some\neven can achieve 100% accuracy. Therefore, the data can bewell classi \ufb01ed with appropriate methods.\nB. Benchmark Data Sets \u2014M\nUSHROOM\nUnlike the iris data set, all of the features of the mushroom\ndata set, which can also be accessible at the UCI Machine\nLearning Repository, are nominal rather than numerical. These23 species of gilled mushrooms are categorized as either edible\nor poisonous. The total number of instances is 8 124 with 4\n208 being edible and 3 916 poisonous. The 22 features aresummarized in Table IV with corresponding possible values.Table V illustrates some experimental results in the literature.\nAs indicated in [117] and [277], traditional clustering strategies,\nlike\n-means and hierarchical clustering, work poorly on the\ndata set. The accuracy for\n -means is just around 69% [277]\nand the clusters formed by classical HC are mixed with nearly\nsimilar proportion of both edible and poisonous objects [117].\nThe results reported in the newly developed algorithms, whichare speci \ufb01cally used for tackling categorical or mixture data,\ngreatly improve the situation [117], [183]. The algorithm ROCK\nFig. 4. (a) Iris data sets. There are three iris categories, each having 50 samples\nwith 4 features. Here, only three features are used: PL, PW, and SL. (b) /75-means\nclustering result with 16 classi \ufb01cation errors observed.\nTABLE III\nSOME CLUSTERING RESULTS FOR THE", "start_char_idx": 0, "end_char_idx": 3439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "18a240bd-af7f-43c0-a008-f23c6eae1915": {"__data__": {"id_": "18a240bd-af7f-43c0-a008-f23c6eae1915", "embedding": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86deeb8e-300b-4588-b9c4-3a595f07a249", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "2": {"node_id": "dd3a61aa-aa78-4316-9ef8-3bf28352765b", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9"}}, "hash": "2d315cb4c8018dfb1779a7fe4b3d7fabf24dde95b43dd77e9423a39cd742925b", "text": "errors observed.\nTABLE III\nSOME CLUSTERING RESULTS FOR THE IRISDATASET\ndivides objects into 21 clusters with most of them (except one)\nconsisting of only one category, which increases the accuracy\nalmost to 99%. The algorithm SBAC works on a subset of\n200 randomly selected objects, 100 for each category and thegeneral results show the correct partition of 3 clusters (two foredible mushrooms, one for poisonous ones). In both studies, the", "start_char_idx": 3381, "end_char_idx": 3821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "335b5d31-c8fa-4764-a2fc-2c7235adf685": {"__data__": {"id_": "335b5d31-c8fa-4764-a2fc-2c7235adf685", "embedding": null, "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b3c4f649-1a73-4279-ae68-a261bb02aa64", "node_type": null, "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}, "hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f"}}, "hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 667\nTABLE IV\nFEATURES FOR THE MUSHROOM DATASET\nconstitution of each feature for generated clusters is also illus-\ntrated and it is observed that some features, like cap-shape andring-type, represent themselves identically for both categories\nand, thus, suggest poor performance of traditional approaches.\nMeanwhile, feature odor shows good discrimination for thedifferent types of mushrooms. Usually, value almond, anise,or none indicates the edibility of mushrooms, while value\npungent, foul, or \ufb01shy means the high possibility of presence\nof poisonous contents in the mushrooms.\nC. Traveling Salesman Problem\nThe traveling salesman problem (TSP) is one of the most\nstudied examples in an important class of problems known as\nNP-complete problems. Given a complete undirected graph\n, where\n is a set of vertices and\n is a set of\nedges each relating two vertices with an associated nonnegativeinteger cost, the most general form of the TSP is equivalent\nto\ufb01nding any Hamiltonian cycle, which is a tour over\nthat\nbegins and ends at the same vertex and visits other verticesexactly once. The more common form of the problem is theTABLE V\nSOME CLUSTERING RESULTS FOR THE MUSHROOM DATASET\noptimization problem of trying to \ufb01nd the shortest Hamiltonian\ncycle, and in particular, the most common is the Euclidean\nversion, where the vertices and edges all lie in the plane.Mulder and Wunsch applied a divide-and-conquer clusteringtechnique, with ART networks, to scale the problem to a mil-\nlion cities [208]. The divide and conquer paradigm gives the\n\ufb02exibility to hierarchically break large problems into arbitrarily\nsmall clusters depending on what tradeoff between accuracy\nand speed is desired. In addition, the subproblems provide an\nexcellent opportunity to take advantage of parallel systemsfor further optimization. As the \ufb01rst stage of the algorithm,\nthe ART network is used to sort the cities into clusters. The\nvigilance parameter is used to set a maximum distance from the\ncurrent pattern. A vigilance parameter between 0 and 1 is usedas a percentage of the global space to determine the vigilancedistance. Values were chosen based on the desired number and\nsize of individual clusters. The clusters were then each passed to\na version of the Lin-Kernighan (LK) algorithm [187]. The laststep combines the subtours back into one complete tour. Tourswith good quality for city levels up to 1 000 000 were obtained\nwithin 25 minutes on a 2 GHz AMD Athlon MP processor with\n512 M of DDR RAM. Fig. 5 shows the visualizing results for 1000, 10 000, and 1 000 000 cities, respectively.\nIt is worthwhile to emphasize the relation between the TSP\nand very large-scale integrated (VLSI) circuit clustering, which\npartitions a sophisticated system into smaller and simpler sub-circuits to facilitate the circuit design. The object of the par-titions is to minimize the number of connections among the\ncomponents. One strategy for solving the problem is based on\ngeometric representations, either linear or multidimensional [8].Alpert and Kahng considered a solution to the problem as the\u201cinverse \u201dof the divide-and-conquer TSP method and used a\nlinear tour of the modules to form the subcircuit partitions [7].\nThey adopted the space \ufb01lling curve heuristic for the TSP to con-\nstruct the tour so that connected modules are still close in thegenerated tour. A dynamic programming method was used to\ngenerate the resulting partitions. More detailed discussion on\nVLSI circuit clustering can be found in the survey by Alpertand Kahng [7].", "start_char_idx": 0, "end_char_idx": 3579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fb4ec6a4-a3ae-418b-84b5-4e6f9d38d8f3": {"__data__": {"id_": "fb4ec6a4-a3ae-418b-84b5-4e6f9d38d8f3", "embedding": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "088ffcb3-8104-413c-9cd0-af1d1c3a7445", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "3": {"node_id": "ae0974cb-1360-421d-aa06-4fa9fd539828", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "eab61a3759eb69baf37d87d0da47fa8f71f5e24845eba382b9e927e3655e3e5f"}}, "hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b", "text": "668 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 5. Clustering divide-and-conquer TSP resulting tours for (a) 1 k, (b) 10 k,\n(c) 1 M cities. The clustered LK algorithm achieves a signi \ufb01cant speedup and\nshows good scalability.\nD. Bioinformatics \u2014Gene Expression Data\nRecently, advances in genome sequencing projects and DNA\nmicroarray technologies have been achieved. The \ufb01rst draft of\nthe human genome sequence project was completed in 2001,\nseveral years earlier than expected [65], [275]. The genomic se-\nquence data for other organizms (e.g., Drosophila melanogaster\nandEscherichia coli ) are also abundant. DNA microarray tech-\nnologies provide an effective and ef \ufb01cient way to measure gene\nexpression levels of thousands of genes simultaneously under\ndifferent conditions and tissues, which makes it possible to in-vestigate gene activities from the angle of the whole genome[79], [188]. With sequences and gene expression data in hand,\nto investigate the functions of genes and identify their roles in\nthe genetic process become increasingly important. Analyzesunder traditional laboratory techniques are time-consuming andexpensive. They fall far behind the explosively increasing gen-\neration of new data. Among the large number of computational\nmethods used to accelerate the exploration of life science, clus-tering can reveal the hidden structures of biological data, and isparticularly useful for helping biologists investigate and under-\nstand the activities of uncharacterized genes and proteins and\nfurther, the systematic architecture of the whole genetic net-work. We demonstrate the applications of clustering algorithmsin bioinformatics from two aspects. The \ufb01rst part is based on\nthe analysis of gene expression data generated from DNA mi-\ncroarray technologies. The second part describes clustering pro-cesses that directly work on linear DNA or protein sequences.The assumption is that functionally similar genes or proteins\nusually share similar patterns or primary sequence structures.DNA microarray technologies generate many gene ex-\npression pro \ufb01les. Currently, there are two major microarray\ntechnologies based on the nature of the attached DNA: cDNAwith length varying from several hundred to thousand bases,\nor oligonucleotides containing 20 \u201330 bases. For cDNA tech-\nnologies, a DNA microarray consists of a solid substrate to\nwhich a large amount of cDNA clones are attached according\nto a certain order [79]. Fluorescently labeled cDNA, obtainedfrom RNA samples of interest through the process of reverse\ntranscription, is hybridized with the array. A reference sample\nwith a different \ufb02uorescent label is also needed for comparison.\nImage analysis techniques are then used to measure the \ufb02uores-\ncence of each dye, and the ratio re \ufb02ects relative levels of gene\nexpression. For a high-density oligonucleotide microarray,\noligonucleotides are \ufb01xed on a chip through photolithography\nor solid-phase DNA synthesis [188]. In this case, absolute\ngene expression levels are obtained. After the normalization\nof the \ufb02uorescence intensities, the gene expression pro \ufb01les\nare represented as a matrix\n, where\n is the ex-\npression level of the\n th gene in the\n th condition, tissue, or\nexperimental stage. Gene expression data analysis consists of a\nthree-level framework based on the complexity, ranging from\nthe investigation of single gene activities to the inference of the\nentire genetic network [20]. The intermediate level explores\nthe relations and interactions between genes under differentconditions, and attracts more attention currently. Generally,\ncluster analysis of gene expression data is composed of two\naspects:", "start_char_idx": 0, "end_char_idx": 3678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae0974cb-1360-421d-aa06-4fa9fd539828": {"__data__": {"id_": "ae0974cb-1360-421d-aa06-4fa9fd539828", "embedding": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "088ffcb3-8104-413c-9cd0-af1d1c3a7445", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "2": {"node_id": "fb4ec6a4-a3ae-418b-84b5-4e6f9d38d8f3", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b"}}, "hash": "eab61a3759eb69baf37d87d0da47fa8f71f5e24845eba382b9e927e3655e3e5f", "text": "Generally,\ncluster analysis of gene expression data is composed of two\naspects: clustering genes [80], [206], [260], [268], [283], [288]\nor clustering tissues or experiments [5], [109], [238].\nResults of gene clustering may suggest that genes in the same\ngroup have similar functions, or they share the same transcrip-tional regulation mechanism. Cluster analysis, for groupingfunctionally similar genes, gradually became popular after\nthe successful application of the average linkage hierarchical\nclustering algorithm for the expression data of budding yeastSaccharomyces cerevisiae and reaction of human \ufb01broblasts to\nserum by Eisen et al. [80]. They used the Pearson correlation\ncoef\ufb01cient to measure the similarity between two genes, and\nprovided a very informative visualization of the clustering re-sults. Their results demonstrate that functionally similar genestend to reside in the same clusters formed by their expression\npattern, even under a relatively small set of conditions. Herwig\net al. developed a variant of\n-means algorithm to cluster a set\nof 2 029 human cDNA clones and adopted mutual informationas the similarity measure [230]. Tomayo et al. [268] made\nuse of SOFM to cluster gene expression data and its applica-\ntion in hematopoietic differentiation provided new insight forfurther research. Graph theories based clustering algorithms,like CAST [29] and CLICK [247], showed very promising\nperformances in tackling different types of gene expression\ndata. Since many genes usually display more than one function,fuzzy clustering may be more effective in exposing these rela-tions [73]. Gene expression data is also important to elucidate\nthe genetic regulation mechanism in a cell. By examining\nthe corresponding DNA sequences in the control regions of acluster of co-expressed genes, we may identify potential shortand consensus sequence patterns, known as motifs, and further\ninvestigate their interaction with transcriptional binding factors,", "start_char_idx": 3599, "end_char_idx": 5569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2743dcc2-71bb-4d6d-af3f-0f9e878efdb3": {"__data__": {"id_": "2743dcc2-71bb-4d6d-af3f-0f9e878efdb3", "embedding": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab26705-5314-421d-8a00-aa0ceb0d94ea", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "3": {"node_id": "40085889-33eb-4c53-9be0-99b3346eb316", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "62353d53f7e685c0db92ab6a0a879d06caa8554d7086b9d297210a11e77d20c3"}}, "hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 669\nleading to different gene activities. Spellman et al. clustered\n800 genes according to their expression during the yeast cellcycle [260]. Analyzes of 8 major gene clusters unravel the\nconnection between co-expression and co-regulation. Tavazoie\net al. partitioned 3 000 genes into 30 clusters with the\n-means\nalgorithm [269]. For each cluster, 600 base pairs upstreamsequences of the genes were searched for potential motifs. 18\nmotifs were found from 12 clusters in their experiments and 7\nof them can be veri \ufb01ed according to previous empirical results\nin the literature. A more comprehensive investigation can befound in [206].\nAs to another application, clustering tissues or experiments\nare valuable in identifying samples that are in the different dis-ease states, discovering, or predicting different cancer types, andevaluating the effects of novel drugs and therapies [5], [109],\n[238]. Golub et al. described the restriction of traditional cancer\nclassi \ufb01cation methods, which are mostly dependent on mor-\nphological appearance of tumors, and divided cancer classi \ufb01-\ncation into two challenges: class discovery and class predic-\ntion. They utilized SOFM to discriminate two types of human\nacute leukemias: acute myeloid leukemia (AML) and acute lym-phoblastic leukemia (ALL) [109]. According to their results,two subsets of ALL, with different origin of lineage, can be\nwell separated. Alon et al. performed a two-way clustering for\nboth tissues and genes and revealed the potential relations, rep-resented as visualizing patterns, among them [6]. Alizadeh et\nal.demonstrated the effectiveness of molecular classi \ufb01cation of\ncancers by their gene expression pro \ufb01les and successfully dis-\ntinguished two molecularly distinct subtypes of diffuse largeB-cell lymphoma, which cause high percentage failure in clin-ical treatment [5]. Furthermore, Scherf et al. constructed a gene\nexpression database to study the relationship between genes and\ndrugs for 60 human cancer cell lines, which provides an im-portant criterion for therapy selection and drug discovery [238].Other applications of clustering algorithms for tissue classi \ufb01-\ncation include: mixtures of multivariate Gaussian distributions\n[105], ellipsoidal ART [287], and graph theory-based methods[29], [247]. In most of these applications, important genes thatare tightly related to the tumor types are identi \ufb01ed according to\ntheir expression differentiation under different cancerous cate-\ngories, which are in accord with our prior recognition of rolesof these genes, to a large extent [5], [109]. For example, Alon et\nal.found that 5 of 20 statistically signi \ufb01cant genes were muscle\ngenes, and the corresponding muscle indices provided an expla-\nnation for false classi \ufb01cations [6].\nFig. 7 illustrates an application of hierarchical clustering\nand SOFM for gene expression data. This data set is on the\ndiagnostic research of small round blue-cell tumors (SRBCT \u2019s)\nof childhood and consists of 83 samples from four categories,known as Burkitt lymphomas (BL), the Ewing family of tumors(EWS), neuroblastoma (NB), and rhabdomyosarcoma (RMS),\nand 5 non-SRBCT samples [164]. Gene expression levels of\n6 567 genes were measured using cDNA microarray for eachsample, 2 308 of which passed the \ufb01lter and were kept for fur-\nther analyzes. These genes are further ranked according to the\nscores calculated by some criterion functions", "start_char_idx": 0, "end_char_idx": 3454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "40085889-33eb-4c53-9be0-99b3346eb316": {"__data__": {"id_": "40085889-33eb-4c53-9be0-99b3346eb316", "embedding": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bab26705-5314-421d-8a00-aa0ceb0d94ea", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "2": {"node_id": "2743dcc2-71bb-4d6d-af3f-0f9e878efdb3", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba"}}, "hash": "62353d53f7e685c0db92ab6a0a879d06caa8554d7086b9d297210a11e77d20c3", "text": "genes are further ranked according to the\nscores calculated by some criterion functions [109]. Generally,\nthese criterion functions attempt to seek a subset of genes thatcontribute most to the discrimination of different cancer types.This can be regarded as a feature selection process. However,\nproblems like how many genes are really required, and whetherthese genes selected are really biologically meaningful, are\nstill not answered satisfactorily. Hierarchical clustering was\nperformed by the program CLUSTER and the results werevisualized by the program TreeView, developed by Eisen inStanford University. Fig. 7(a) and (b) depicts the clustering\nresults for both the top 100 genes, selected by the Fisher\nscores, and the samples. Graphic visualization is achieved byassociating each data point with a certain color according to thecorresponding scale. Some clustering patterns are clearly dis-\nplayed in the image. Fig. 7(c) depicts a 5-by-5 SOFM topology\nfor all genes, with each cluster represented by the centroid(mean) for each feature (sample). 25 clusters are generatedand the number of genes in each cluster is also indicated.\nThe software package GeneCluster, developed by Whitehead\nInstitute/MIT Center for Genome Research (WICGR), wasused in this analysis.\nAlthough clustering techniques have already achieved many\nimpressive results in the analysis of gene expression data, thereare still many problems that remain open. Gene expression data\nsets usually are characterized as\n1) small set samples with high-dimensional features;\n2) high redundancy;\n3) inherent noise;4) sparsity of the data.\nMost of the published data sets include usually less than 20\nsamples for each tumor type, but with as many as thousands ofgene measures [80], [109], [238], [268]. This is partly causedby the lag of experimental condition (e.g., sample collection), in\ncontrast to the rapid advancement of microarray and sequencing\ntechnologies. In order to evaluate existing algorithms morereasonably and develop more effective new approaches, moredata with enough samples or more conditional observations are\nneeded. But from the trend of gene chip technologies, which\nalso follows Moore \u2019s law for semiconductor chips [205], the\ncurrent status will still exist for a long time. This problem ismore serious in the application of gene expression data for\ncancer research, in which clustering algorithms are required to\nbe capable of effectively \ufb01nding potential patterns under a large\nnumber of irrelevant factors, as a result of the introduction oftoo many genes. At the same time, feature selection, which is\nalso called informative gene selection in the context, also plays\na very important role. Without any doubt, clustering algorithmsshould be feasible in both time and space complexity. Due tothe nature of the manufacture process of the microarray chip,\nnoise can be inevitably introduced into the expression data\nduring different stages. Accordingly, clustering algorithmsshould have noise and outlier detection mechanisms in order toremove their effects. Furthermore, different algorithms usually\nform different clusters for the same data set, which is a general\nproblem in cluster analysis. How to evaluate the quality of thegenerated clusters of genes, and how to choose appropriatealgorithms for a speci \ufb01ed application, are particularly crucial\nfor gene expression data research, because sometimes, even\nbiologists cannot identify the real patterns from the artifacts ofthe clustering algorithms, due to the limitations of biological", "start_char_idx": 3367, "end_char_idx": 6907, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15e4f284-42a6-43ad-9dab-4c7e823b1ad9": {"__data__": {"id_": "15e4f284-42a6-43ad-9dab-4c7e823b1ad9", "embedding": null, "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "55016afa-ef68-433b-ab36-85af350cde22", "node_type": null, "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}, "hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d"}}, "hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d", "text": "670 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 6. Basic procedure of cDNA microarray technology [68]. Fluorescently labeled cDNAs, obtained from target and reference samples through revers e\ntranscription, are hybridized with the microarray, which is comprised of a large amount of cDNA clones. Image analysis measures the ratio of the two dy es.\nComputational methods, e.g., hierarchical clustering, further disclose the relations among genes and corresponding conditions.\nFig. 7. Hierarchical and SOFM clustering of SRBCT \u2019s gene expression data set. (a) Hierarchical clustering result for the 100 selected genes under 83 tissue\nsamples. The gene expression matrix is visualized through a color scale. (b) Hierarchical clustering result for the 83 tissue samples. Here, the dime nsion is 100 as\n100 genes are selected like in (a). (c) SOFM clustering result for the 2308 genes. A 5 /25 SOFM is used and 25 clusters are formed. Each cluster is represented by\nthe average values.", "start_char_idx": 0, "end_char_idx": 1004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1af0d32-7d3c-4e55-a822-953be8f57a9e": {"__data__": {"id_": "c1af0d32-7d3c-4e55-a822-953be8f57a9e", "embedding": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "702049fa-a8c4-4f11-98b8-f3cdabe21869", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "3": {"node_id": "b13b3430-9d01-4bab-bbe1-075bee6e9263", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "fc5bef94211ae1dafe9dfc6cd28b0c001b047ed9145ae0f6249ccd8c58b5cbf4"}}, "hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 671\nknowledge. Some recent results can be accessed in [29], [247],\nand [291].\nE. Bioinformatics \u2014DNA or Protein Sequences Clustering\nDNA (deoxyribonucleic acid) is the hereditary material ex-\nisting in all living cells. A DNA molecule is a double helix con-\nsisting of two strands, each of which is a linear sequence com-posed of four different nucleotides \u2014adenine, guanine, thymine,\nand cytosine, abbreviated as the letters A, G, T, and C, respec-\ntively. Each letter in a DNA sequence is also called a base.\nProteins determine most of cells \u2019structures, functions, prop-\nerties, and regulatory mechanisms. The primary structure of aprotein is also a linear and alphabetic chain with the difference\nthat each unit represents an amino acid, which has twenty types\nin total. Proteins are encoded by certain segments of DNA se-quences through a two-stage process (transcription and trans-lation). These segments are known as genes or coding regions.\nInvestigation of the relations between DNA and proteins, as well\nas their own functions and properties, is one of the important re-search directions in both genetics and bioinformatics.\nThe similarity between newly sequenced genes or proteins\nand annotated genes or proteins usually offers a cue to identifytheir functions. Searching corresponding databases for a new\nDNA or protein sequence has already become routine in genetic\nresearch. In contrast to sequence comparison and search, clusteranalysis provides a more effective means to discover compli-cated relations among DNA and protein sequences. We summa-\nrize the following clustering applications for DNA and protein\nsequences:\n1) function recognition of uncharacterized genes or pro-\nteins [119];\n2) structure identi \ufb01cation of large-scale DNA or protein\ndatabases [237], [257];\n3) redundancy decrease of large-scale DNA or protein\ndatabases [185];\n4) domain identi \ufb01cation [83], [115];\n5) expressed sequence tag (EST) clustering [49], [200].\nAs described in Section II-J, classical dynamic programming\nalgorithms for global and local sequence alignment are too in-\ntensive in computational complexity. This becomes worse be-cause of the existence of a large volume of nucleic acids andamino acids in the current DNA or protein databases, e.g., bac-\nteria genomes are from 0.5 to 10 Mbp, fungi genomes range\nfrom 10 to 50 Mbp, while the human genome is around 3 310Mbp [18] (Mbp means million base pairs). Thus, conventionaldynamic programming algorithms are computationally infea-\nsible. In practice, sequence comparison or proximity measure\nis achieved via some heuristics. Well-known examples includeBLAST and FASTA with many variants [10], [11], [224]. Thekey idea of these methods is to identify regions that may have\npotentially high matches, with a list of prespeci \ufb01ed high-scoring\nwords, at an early stage. Therefore, further search only needs tofocus on these regions with expensive but accurate algorithms.Recognizing the bene \ufb01t coming from the separation of word\nmatching and sequence alignment to computational burden re-\nduction, Miller, Gurd, and Brass described three algorithms fo-cusing on speci \ufb01c problems [199]. The implementation of thescheme for large database vs. database comparison exhibits an\napparent improvement in computation time. Kent and Zahler de-signed a three-pass algorithm, called wobble aware bulk aligner\n(WABA) [162], for aligning large-scale genomic sequences", "start_char_idx": 0, "end_char_idx": 3461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b13b3430-9d01-4bab-bbe1-075bee6e9263": {"__data__": {"id_": "b13b3430-9d01-4bab-bbe1-075bee6e9263", "embedding": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "702049fa-a8c4-4f11-98b8-f3cdabe21869", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "2": {"node_id": "c1af0d32-7d3c-4e55-a822-953be8f57a9e", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394"}}, "hash": "fc5bef94211ae1dafe9dfc6cd28b0c001b047ed9145ae0f6249ccd8c58b5cbf4", "text": "aligner\n(WABA) [162], for aligning large-scale genomic sequences of\ndifferent species, which employs a seven-state pairwise hiddenMarkov model [78] for more effective alignments. In [201],Miller summarized the current research status of genomic se-\nquence comparison and suggested valuable directions for fur-\nther research efforts.\nMany clustering techniques have been applied to organize\nDNA or protein sequence data. Some directly operate on a\nproximity measure; some are based on feature extraction,\nwhile others are constructed on statistical models. Somervuoand Kohonen illustrated an application of SOFM to clusterprotein sequences in SWISSPROT database [257]. FASTA\nwas used to calculate the sequence similarity. The resulting\ntwo-dimensional SOFM provides a visualized representationof the relations within the entire sequence database. Basedon the similarity measure of gapped BLAST, Sasson et al.\nutilized an agglomerative hierarchical clustering paradigm to\ncluster all protein sequences in SWISSPROT [237]. The effectsof four merging rules, different from the interpretation ofcluster centers, on the resulting protein clusters were examined.\nThe advantages as well as the potential risk of the concept,\ntransitivity, were also elucidated in the paper. According tothe transitivity relation, two sequences that do not show highsequence similarity by virtue of direct comparison, may be\nhomologous (having a common ancestor) if there exists an\nintermediate sequence similar to both of them. This makes itpossible to detect remote homologues that can not be observedby similarity comparison. However, unrelated sequences may\nbe clustered together due to the effects of these intermediate\nsequences [237]. Bolten et al. addressed the problem with the\nconstruction a directed graph, in which each protein sequencecorresponds to a vertex and edges are weighted based on the\nalignment score between two sequences and self alignment\nscore of each sequence [41]. Clusters were formed throughthe search of strongly connected components (SCCs), each ofwhich is a maximal subset of vertices and for each pair of ver-\ntices\nand\n in the subset, there exist two directed paths from\nto\nand vice versa. A minimum normalized cut algorithm for\ndetecting protein families and a minimum spanning tree (MST)application for seeking domain information were presented in\n[1] and [115], respectively. In contrast with the aforementioned\nproximity-based methods, Guralnik and Karypis transformedprotein or DNA sequences into a new feature space, basedon the detected subpatterns working as the sequence features,\nand clustered with the\n-means algorithm [119]. The method\nis immune from all-against-all expensive sequence compar-ison and suitable for analyzing large-scale databases. Kroghdemonstrated the power of hidden Markov models (HMMs)\nin biological sequences modeling and clustering of protein\nfamilies [177]. Fig. 8 depicts a typical structure of HMM, inwhich match states (abbreviated with letter M), insert states (I)and delete states (D) are represented as rectangles, diamonds,\nand circles, respectively [78], [177]. These states correspond\nto substitution, insertion, and deletion in edit operations. Forconvenience, a begin state and an end state are added to the", "start_char_idx": 3397, "end_char_idx": 6661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "67defbe9-d008-41d5-ba59-33112621edca": {"__data__": {"id_": "67defbe9-d008-41d5-ba59-33112621edca", "embedding": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3249eb0-b356-4531-9bf2-ade92df3a8d6", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "3": {"node_id": "f54b8e69-215b-4141-a228-78566b779997", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "7509af604895c8619373d8549c1d9aa022ada9622c1938abebae728637d9699e"}}, "hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334", "text": "672 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 8. HMM architecture [177]. There are three different states, match (M),\ninsert (I), and delete (D), corresponding to substitution, insertion, and deletion\noperation, respectively. A begin (B) and end (E) state are also introduced to\nrepresent the start and end of the process. This process goes through a series of\nstates according to the transition probability, and emits either 4-letter nucleotide\nor 20-letter amino acid alphabet based on the emission probability.\nmodel, denoted by letter B and E. Letters, either from the 4-letter\nnucleotide alphabet or from 20-letter amino acid alphabet,\nare generated from match and insert states according to some\nemission probability distributions. Delete states do not produceany symbols, and are used to skip the match states.\nHMMs are\nrequiredinorderto describe\n clusters,orfamilies (subfamilies),\nwhich are regarded as a mixture model and proceeded with\nan EM learning algorithm similar to single HMM case. Anexample for clustering subfamilies of 628 globins shows theencouraging results. Further discussion can be found in [78]\nand [145].\nIV . C\nONCLUSION\nAs an important tool for data exploration, cluster analysis\nexamines unlabeled data, by either constructing a hierarchicalstructure, or forming a set of groups according to a prespeci \ufb01ed\nnumber. This process includes a series of steps, ranging from\npreprocessing and algorithm development, to solution validityand evaluation. Each of them is tightly related to each otherand exerts great challenges to the scienti \ufb01c disciplines. Here, we\nplace the focus on the clustering algorithms and review a wide\nvariety of approaches appearing in the literature. These algo-rithms evolve from different research communities, aim to solvedifferent problems, and have their own pros and cons. Though\nwe have already seen many examples of successful applications\nof cluster analysis, there still remain many open problems dueto the existence of many inherent uncertain factors. These prob-lems have already attracted and will continue to attract intensive\nefforts from broad disciplines. We summarize and conclude the\nsurvey with listing some important issues and research trendsfor cluster algorithms.\n1) There is no clustering algorithm that can be univer-\nsally used to solve all problems. Usually, algorithmsare designed with certain assumptions and favor sometype of biases. In this sense, it is not accurate to say\n\u201cbest\u201din the context of clustering algorithms, although\nsome comparisons are possible. These comparisons aremostly based on some speci \ufb01c applications, under cer-\ntain conditions, and the results may become quite dif-\nferent if the conditions change.\n2) New technology has generated more complex and\nchallenging tasks, requiring more powerful clusteringalgorithms. The following properties are important to\nthe ef \ufb01ciency and effectiveness of a novel algorithm.\nI) generate arbitrary shapes of clusters rather than be\ncon\ufb01ned to some particular shape;\nII) handle large volume of data as well as high-dimen-\nsional features with acceptable time and storagecomplexities;\nIII) detect and remove possible outliers and noise;\nIV) decrease the reliance of algorithms on users-de-\npendent parameters;\nV) have the capability of dealing with newly occur-\nring data without relearning from the scratch;\nVI) be immune to the effects of order of input patterns;VII) provide some insight for the number of potential\nclusters without prior knowledge;\nVIII) show good data visualization and provide users\nwith results that can simplify further analysis;\nIX) be capable of handling both numerical and", "start_char_idx": 0, "end_char_idx": 3669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f54b8e69-215b-4141-a228-78566b779997": {"__data__": {"id_": "f54b8e69-215b-4141-a228-78566b779997", "embedding": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f3249eb0-b356-4531-9bf2-ade92df3a8d6", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "2": {"node_id": "67defbe9-d008-41d5-ba59-33112621edca", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334"}}, "hash": "7509af604895c8619373d8549c1d9aa022ada9622c1938abebae728637d9699e", "text": "further analysis;\nIX) be capable of handling both numerical and nom-\ninal data or be easily adaptable to some other data\ntype.\nOf course, some more detailed requirements for spe-ci\ufb01c applications will affect these properties.\n3) At the preprocessing and post-processing phase, fea-\nture selection/extraction (as well as standardization\nand normalization) and cluster validation are as impor-tant as the clustering algorithms. Choosing appropriateand meaningful features can greatly reduce the burden\nof subsequent designs and result evaluations re \ufb02ect\nthe degree of con \ufb01dence to which we can rely on the\ngenerated clusters. Unfortunately, both processes lackuniversal guidance. Ultimately, the tradeoff among\ndifferent criteria and methods is still dependent on the\napplications themselves.\nA\nCKNOWLEDGMENT\nThe authors would like to thank the Eisen Laboratory in Stan-\nford University for use of their CLUSTER and TreeView soft-\nware and Whitehead Institute/MIT Center for Genome Research\nfor use of their GeneCluster software. They would also like tothank S. Mulder for the part on the traveling salesman problemand also acknowledge extensive comments from the reviewers\nand the anonymous associate editor.\nR\nEFERENCES\n[1] F. Abascal and A. Valencia, \u201cClustering of proximal sequence space\nfor the identi \ufb01cation of protein families, \u201dBioinformatics , vol. 18, pp.\n908\u2013921, 2002.\n[2] C. Aggarwal and P. Yu, \u201cRede \ufb01ning clustering for high-dimensional ap-\nplications, \u201dIEEE Trans. Knowl. Data Eng. , vol. 14, no. 2, pp. 210 \u2013225,\nFeb. 2002.\n[3] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan, \u201cAutomatic\nsubspace clustering of high dimensional data for data mining applica-\ntions, \u201dinProc. ACM SIGMOD Int. Conf. Management of Data , 1998,\npp. 94 \u2013105.\n[4] H. Akaike, \u201cA new look at the statistical model identi \ufb01cation, \u201dIEEE\nTrans. Autom. Control , vol. AC-19, no. 6, pp. 716 \u2013722, Dec. 1974.\n[5] A. Alizadeh et al. ,\u201cDistinct types of diffuse large B-cell Lymphoma\nidenti \ufb01ed by gene expression pro \ufb01ling,\u201dNature , vol. 403, pp. 503 \u2013511,\n2000.\n[6] U. Alon, N. Barkai, D. Notterman, K. Gish, S. Ybarra, D. Mack, and\nA. Levine, \u201cBroad patterns of gene expression revealed by clustering\nanalysis of tumor and normal colon tissues probed by oligonucleotidearrays, \u201dProc. Nat. Acad. Sci. USA , pp. 6745 \u20136750, 1999.", "start_char_idx": 3606, "end_char_idx": 5927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc770eee-2b46-49c3-b4f9-cfd34c9e0bb5": {"__data__": {"id_": "cc770eee-2b46-49c3-b4f9-cfd34c9e0bb5", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a512369-3428-41ba-881b-94bacaafa392", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "3": {"node_id": "4517a38b-d3fb-47cd-a1e9-42bf07b04f51", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "1c1d5e0391af521e3bbad2265e841974f49a0747185829f6e4cd8c359c102f25"}}, "hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 673\n[7] C. Alpert and A. Kahng, \u201cMulti-way partitioning via space \ufb01lling curves\nand dynamic programming, \u201dinProc. 31st ACM/IEEE Design Automa-\ntion Conf. , 1994, pp. 652 \u2013657.\n[8] ,\u201cRecent directions in netlist partitioning: A survey, \u201dVLSI J. , vol.\n19, pp. 1 \u201381, 1995.\n[9] K. Al-Sultan, \u201cA Tabu search approach to the clustering problem, \u201dPat-\ntern Recognit. , vol. 28, no. 9, pp. 1443 \u20131451, 1995.\n[10] S. Altschul et al. ,\u201cGapped BLAST and PSI-BLAST: A new generation\nof protein database search programs, \u201dNucleic Acids Res. , vol. 25, pp.\n3389 \u20133402, 1997.\n[11] S. Altschul et al. ,\u201cBasic local alignment search tool, \u201dJ. Molec. Biol. ,\nvol. 215, pp. 403 \u2013410, 1990.\n[12] G. Anagnostopoulos and M. Georgiopoulos, \u201cHypersphere ART and\nARTMAP for unsupervised and supervised incremental learning, \u201din\nProc. IEEE-INNS-ENNS Int. Joint Conf. Neural Networks (IJCNN \u201900),\nvol. 6, Como, Italy, pp. 59 \u201364.\n[13] ,\u201cEllipsoid ART and ARTMAP for incremental unsupervised and\nsupervised learning, \u201dinProc. IEEE-INNS-ENNS Int. Joint Conf. Neural\nNetworks (IJCNN \u201901), vol. 2, Washington, DC, 2001, pp. 1221 \u20131226.\n[14] M. Anderberg, Cluster Analysis for Applications . New York: Aca-\ndemic, 1973.\n[15] G. Babu and M. Murty, \u201cA near-optimal initial seed value selection in\n/75-means algorithm using a genetic algorithm, \u201dPattern Recognit. Lett. ,\nvol. 14, no. 10, pp. 763 \u2013769, 1993.\n[16] ,\u201cClustering with evolution strategies, \u201dPattern Recognit. , vol. 27,\nno. 2, pp. 321 \u2013329, 1994.\n[17] E. Backer and A. Jain, \u201cA clustering performance measure based on\nfuzzy set decomposition, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol.\nPAMI-3, no. 1, pp. 66 \u201375, Jan. 1981.\n[18] P. Baldi and S. Brunak, Bioinformatics: The Machine Learning Ap-\nproach , 2nd ed. Cambridge, MA: MIT Press, 2001.\n[19] P. Baldi and K. Hornik, \u201cNeural networks and principal component anal-\nysis: Learning from examples without local minima, \u201dNeural Netw. , vol.\n2, pp. 53 \u201358, 1989.\n[20] P. Baldi and A. Long, \u201cA Bayesian framework for the analysis of mi-\ncroarray expression data: Regularized t-test and statistical inferences of\ngene changes, \u201dBioinformatics , vol. 17, pp. 509 \u2013519, 2001.\n[21] G. Ball and D. Hall, \u201cA clustering technique for summarizing multi-\nvariate data, \u201dBehav. Sci. , vol. 12, pp. 153 \u2013155, 1967.\n[22] S. Bandyopadhyay and U. Maulik, \u201cNonparametric genetic clustering:\nComparison of validity indices, \u201dIEEE Trans. Syst., Man, Cybern. C,\nAppl. Rev. , vol. 31, no. 1, pp. 120 \u2013125, Feb.", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4517a38b-d3fb-47cd-a1e9-42bf07b04f51": {"__data__": {"id_": "4517a38b-d3fb-47cd-a1e9-42bf07b04f51", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a512369-3428-41ba-881b-94bacaafa392", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "cc770eee-2b46-49c3-b4f9-cfd34c9e0bb5", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f"}, "3": {"node_id": "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "b431683b355c46a836cc849285d0f6ea68603b7b9562c3d6fe9485f24eb2d1d0"}}, "hash": "1c1d5e0391af521e3bbad2265e841974f49a0747185829f6e4cd8c359c102f25", "text": "Rev. , vol. 31, no. 1, pp. 120 \u2013125, Feb. 2001.\n[23] A. Baraldi and E. Alpaydin, \u201cConstructive feedforward ART clustering\nnetworks \u2014Part I and II, \u201dIEEE Trans. Neural Netw. , vol. 13, no. 3, pp.\n645\u2013677, May 2002.\n[24] A. Baraldi and P. Blonda, \u201cA survey of fuzzy clustering algorithms for\npattern recognition \u2014Part I and II, \u201dIEEE Trans. Syst., Man, Cybern. B,\nCybern. , vol. 29, no. 6, pp. 778 \u2013801, Dec. 1999.\n[25] A. Baraldi and L. Schenato, \u201cSoft-to-hard model transition in clustering:\nA review, \u201d, Tech. Rep. TR-99-010, 1999.\n[26] D. Barbar \u00e1and P. Chen, \u201cUsing the fractal dimension to cluster datasets, \u201d\ninProc. 6th ACM SIGKDD Int. Conf. Knowledge Discovery and Data\nMining , 2000, pp. 260 \u2013264.\n[27] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps and spectral techniques\nfor embedding and clustering, \u201dinAdvances in Neural Information\nProcessing Systems , T. G. Dietterich, S. Becker, and Z. Ghahramani,\nEds. Cambridge, MA: MIT Press, 2002, vol. 14.\n[28] R. Bellman, Adaptive Control Processes: A Guided Tour . Princeton,\nNJ: Princeton Univ. Press, 1961.\n[29] A. Ben-Dor, R. Shamir, and Z. Yakhini, \u201cClustering gene expression\npatterns, \u201dJ. Comput. Biol. , vol. 6, pp. 281 \u2013297, 1999.\n[30] Y . Bengio, \u201cMarkovian models for sequential data, \u201dNeural Comput.\nSurv. , vol. 2, pp. 129 \u2013162, 1999.\n[31] A. Ben-Hur, D. Horn, H. Siegelmann, and V . Vapnik, \u201cSupport vector\nclustering, \u201dJ. Mach. Learn. Res. , vol. 2, pp. 125 \u2013137, 2001.\n[32] ,\u201cA support vector clustering method, \u201dinProc. Int. Conf. Pattern\nRecognition , vol. 2, 2000, pp. 2724 \u20132727.\n[33] P. Berkhin. (2001) Survey of clustering data mining techniques. [On-\nline]. Available: http://www.accrue.com/products/rp_cluster_review.pdf\nhttp://citeseer.nj.nec.com/berkhin02survey.html\n[34] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft, \u201cWhen is nearest\nneighbor meaningful, \u201dinProc. 7th Int. Conf. Database Theory , 1999,\npp. 217 \u2013235.\n[35] J. Bezdek, Pattern Recognition with Fuzzy Objective Function Algo-\nrithms . New York: Plenum, 1981.[36] J. Bezdek and R. Hathaway, \u201cNumerical convergence and interpretation\nof the fuzzy /99-shells clustering algorithms, \u201dIEEE Trans. Neural Netw. ,\nvol. 3, no. 5, pp. 787 \u2013793, Sep. 1992.\n[37] J. Bezdek and N. Pal, \u201cSome new indexes of cluster validity, \u201dIEEE\nTrans. Syst., Man, Cybern. B, Cybern. , vol. 28, no. 3, pp. 301 \u2013315, Jun.\n1998.\n[38] C. Bishop, Neural Networks for Pattern Recognition . New York: Ox-\nford Univ. Press, 1995.\n[39] L. Bobrowski and J.", "start_char_idx": 2494, "end_char_idx": 4968, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4": {"__data__": {"id_": "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a512369-3428-41ba-881b-94bacaafa392", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "4517a38b-d3fb-47cd-a1e9-42bf07b04f51", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "1c1d5e0391af521e3bbad2265e841974f49a0747185829f6e4cd8c359c102f25"}, "3": {"node_id": "810113cd-21c4-4998-991c-be7e184bb99d", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "1cd7251ada1dee452caebd4791b0ee767e2ca3bf17f1d25fad94deff946d00d6"}}, "hash": "b431683b355c46a836cc849285d0f6ea68603b7b9562c3d6fe9485f24eb2d1d0", "text": "Univ. Press, 1995.\n[39] L. Bobrowski and J. Bezdek, \u201cc-Means clustering with the /108\nand /108\nnorms, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 21, no. 3, pp. 545 \u2013554,\nMay-Jun. 1991.\n[40] H. Bock, \u201cProbabilistic models in cluster analysis, \u201dComput. Statist.\nData Anal. , vol. 23, pp. 5 \u201328, 1996.\n[41] E. Bolten, A. Sxhliep, S. Schneckener, D. Schomburg, and R. Schrader,\n\u201cClustering protein sequences \u2014Structure prediction by transitive ho-\nmology, \u201dBioinformatics , vol. 17, pp. 935 \u2013941, 2001.\n[42] N. Boujemaa, \u201cGeneralized competitive clustering for image segmen-\ntation, \u201dinProc. 19th Int. Meeting North American Fuzzy Information\nProcessing Soc. (NAFIPS \u201900), Atlanta, GA, 2000, pp. 133 \u2013137.\n[43] P. Bradley and U. Fayyad, \u201cRe\ufb01ning initial points for /75-means clus-\ntering, \u201dinProc. 15th Int. Conf. Machine Learning , 1998, pp. 91 \u201399.\n[44] P. Bradley, U. Fayyad, and C. Reina, \u201cScaling clustering algorithms to\nlarge databases, \u201dinProc. 4th Int. Conf. Knowledge Discovery and Data\nMining (KDD \u201998), 1998, pp. 9 \u201315.\n[45] ,\u201cClustering very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[46] ,\u201cClustering very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[47] D. Brown and C. Huntley, \u201cA practical application of simulated an-\nnealing to clustering, \u201dPattern Recognit. , vol. 25, no. 4, pp. 401 \u2013412,\n1992.\n[48] C. Burges, \u201cA tutorial on support vector machines for pattern recogni-\ntion,\u201dData Mining Knowl. Discov. , vol. 2, pp. 121 \u2013167, 1998.\n[49] J. Burke, D. Davison, and W. Hide, \u201cd2Cluster: A validated method for\nclustering EST and full-length cDNA sequences, \u201dGenome Res. , vol. 9,\npp. 1135 \u20131142, 1999.\n[50] I. Cadez, S. Gaffney, and P. Smyth, \u201cA general probabilistic framework\nfor clustering individuals and objects, \u201dinProc. 6th ACM SIGKDD Int.\nConf. Knowledge Discovery and Data Mining , 2000, pp. 140 \u2013149.\n[51] G. Carpenter and S. Grossberg, \u201cA massively parallel architecture for\na self-organizing neural pattern recognition machine, \u201dComput. Vis.\nGraph. Image Process. , vol. 37, pp. 54 \u2013115, 1987.\n[52] ,\u201cART2: Self-organization of stable category recognition codes for\nanalog input patterns, \u201dAppl. Opt. , vol. 26, no. 23, pp. 4919 \u20134930, 1987.\n[53] ,\u201cThe ART of adaptive pattern recognition by a self-organizing\nneural network, \u201dIEEE Computer , vol. 21, no. 3, pp. 77 \u201388, Mar. 1988.\n[54] ,\u201cART3: Hierarchical search using chemical transmitters in self-\norganizing pattern recognition architectures, \u201dNeural Netw. , vol.", "start_char_idx": 4964, "end_char_idx": 7551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "810113cd-21c4-4998-991c-be7e184bb99d": {"__data__": {"id_": "810113cd-21c4-4998-991c-be7e184bb99d", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1a512369-3428-41ba-881b-94bacaafa392", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "b431683b355c46a836cc849285d0f6ea68603b7b9562c3d6fe9485f24eb2d1d0"}}, "hash": "1cd7251ada1dee452caebd4791b0ee767e2ca3bf17f1d25fad94deff946d00d6", "text": "pattern recognition architectures, \u201dNeural Netw. , vol. 3, no.\n23, pp. 129 \u2013152, 1990.\n[55] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen,\n\u201cFuzzy ARTMAP: A neural network architecture for incremental super-\nvised learning of analog multidimensional maps, \u201dIEEE Trans. Neural\nNetw. , vol. 3, no. 5, pp. 698 \u2013713, 1992.\n[56] G. Carpenter, S. Grossberg, and J. Reynolds, \u201cARTMAP: Supervised\nreal-time learning and classi \ufb01cation of nonstationary data by a self-or-\nganizing neural network, \u201dNeural Netw. , vol. 4, no. 5, pp. 169 \u2013181, 1991.\n[57] G. Carpenter, S. Grossberg, and D. Rosen, \u201cFuzzy ART: Fast stable\nlearning and categorization of analog patterns by an adaptive resonance\nsystem, \u201dNeural Netw. , vol. 4, pp. 759 \u2013771, 1991.\n[58] G. Celeux and G. Govaert, \u201cA classi \ufb01cation EM algorithm for clustering\nand two stochastic versions, \u201dComput. Statist. Data Anal. , vol. 14, pp.\n315\u2013332, 1992.\n[59] P. Cheeseman and J. Stutz, \u201cBayesian classi \ufb01cation (AutoClass):\nTheory and results, \u201dinAdvances in Knowledge Discovery and Data\nMining , U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,\nEds. Menlo Park, CA: AAAI Press, 1996, pp. 153 \u2013180.\n[60] V . Cherkassky and F. Mulier, Learning From Data: Concepts, Theory,\nand Methods . New York: Wiley, 1998.\n[61] J. Cherng and M. Lo, \u201cA hypergraph based clustering algorithm for spa-\ntial data sets, \u201dinProc. IEEE Int. Conf. Data Mining (ICDM \u201901), 2001,\npp. 83 \u201390.\n[62] J. Chiang and P. Hao, \u201cA new kernel-based fuzzy clustering approach:\nSupport vector clustering with cell growing, \u201dIEEE Trans. Fuzzy Syst. ,\nvol. 11, no. 4, pp. 518 \u2013527, Aug. 2003.\n[63] C. Chinrungrueng and C. S \u00e9quin, \u201cOptimal adaptive /75-means algo-\nrithm with dynamic adjustment of learning rate, \u201dIEEE Trans. Neural\nNetw. , vol. 6, no. 1, pp. 157 \u2013169, Jan. 1995.", "start_char_idx": 7533, "end_char_idx": 9353, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae71050f-527b-4443-a07a-b5f231c8e5d7": {"__data__": {"id_": "ae71050f-527b-4443-a07a-b5f231c8e5d7", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "3": {"node_id": "8446f964-cbd4-4078-8677-8cf38f63f83e", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "24991b881fe6f7bc872d17b93f2e1ce5bbe684c10780b04b2f7ab273d6567537"}}, "hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737", "text": "674 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[64] S. Chu and J. Roddick, \u201cA clustering algorithm using the Tabu search\napproach with simulated annealing, \u201dinData Mining II \u2014Proceedings\nof Second International Conference on Data Mining Methods and\nDatabases , N. Ebecken and C. Brebbia, Eds, Cambridge, U.K., 2000,\npp. 515 \u2013523.\n[65] I. H. G. S. Consortium, \u201cInitial sequencing and analysis of the human\ngenome, \u201dNature , vol. 409, pp. 860 \u2013921, 2001.\n[66] J. Corchado and C. Fyfe, \u201cA comparison of kernel methods for instan-\ntiating case based reasoning systems, \u201dComput. Inf. Syst. , vol. 7, pp.\n29\u201342, 2000.\n[67] M. Cowgill, R. Harvey, and L. Watson, \u201cA genetic algorithm approach\nto cluster analysis, \u201dComput. Math. Appl. , vol. 37, pp. 99 \u2013108, 1999.\n[68] C. Cummings and D. Relman, \u201cUsing DNA microarray to study host-\nmicrobe interactions, \u201dGenomics , vol. 6, no. 5, pp. 513 \u2013525, 2000.\n[69] E. Dahlhaus, \u201cParallel algorithms for hierarchical clustering and appli-\ncations to split decomposition and parity graph recognition, \u201dJ. Algo-\nrithms , vol. 36, no. 2, pp. 205 \u2013240, 2000.\n[70] R. Dav \u00e9,\u201cAdaptive fuzzy /99-shells clustering and detection of ellipses, \u201d\nIEEE Trans. Neural Netw. , vol. 3, no. 5, pp. 643 \u2013662, Sep. 1992.\n[71] R. Dav \u00e9and R. Krishnapuram, \u201cRobust clustering methods: A uni \ufb01ed\nview, \u201dIEEE Trans. Fuzzy Syst. , vol. 5, no. 2, pp. 270 \u2013293, May 1997.\n[72] M. Delgado, A. Sk \u00e1rmeta, and H. Barber \u00e1,\u201cA Tabu search approach\nto the fuzzy clustering problem, \u201dinProc. 6th IEEE Int. Conf. Fuzzy\nSystems , vol. 1, 1997, pp. 125 \u2013130.\n[73] D. Demb \u00e9l\u00e9and P. Kastner, \u201cFuzzy /99-means method for clustering mi-\ncroarray data, \u201dBioinformatics , vol. 19, no. 8, pp. 973 \u2013980, 2003.\n[74] Handbook of Pattern Recognition and Computer Vision , C. Chen, L.\nPau, and P. Wang, Eds., World Scienti \ufb01c, Singapore, 1993, pp. 3 \u201332. R.\nDubes, \u201cCluster analysis and related issue \u201d.\n[75] R. Duda, P. Hart, and D. Stork, Pattern Classi\ufb01cation , 2nd ed. New\nYork: Wiley, 2001.\n[76] J. Dunn, \u201cA fuzzy relative of the ISODATA process and its use in de-\ntecting compact well separated clusters, \u201dJ. Cybern. , vol. 3, no. 3, pp.\n32\u201357, 1974.\n[77] B. Duran and P. Odell, Cluster Analysis: A Survey . New York:\nSpringer-Verlag, 1974.\n[78] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison, Biological Sequence\nAnalysis: Probabilistic Models of Proteins and Nucleic Acids . Cam-\nbridge, U.K.: Cambridge Univ. Press, 1998.\n[79] M. Eisen and P. Brown, \u201cDNA", "start_char_idx": 0, "end_char_idx": 2471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8446f964-cbd4-4078-8677-8cf38f63f83e": {"__data__": {"id_": "8446f964-cbd4-4078-8677-8cf38f63f83e", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "ae71050f-527b-4443-a07a-b5f231c8e5d7", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737"}, "3": {"node_id": "97664fa0-9070-4279-9f21-065320de12c7", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "96aa01133ac86a98be7ec1a9af3f0a87bc64c63ddb2cc0933fb2c4b33419ce20"}}, "hash": "24991b881fe6f7bc872d17b93f2e1ce5bbe684c10780b04b2f7ab273d6567537", "text": "Press, 1998.\n[79] M. Eisen and P. Brown, \u201cDNA arrays for analysis of gene expression, \u201d\nMethods Enzymol. , vol. 303, pp. 179 \u2013205, 1999.\n[80] M. Eisen, P. Spellman, P. Brown, and D. Botstein, \u201cCluster analysis and\ndisplay of genome-wide expression patterns, \u201dinProc. Nat. Acad. Sci.\nUSA, vol. 95, 1998, pp. 14 863 \u201314 868.\n[81] Y . El-Sonbaty and M. Ismail, \u201cFuzzy clustering for symbolic data, \u201dIEEE\nTrans. Fuzzy Syst. , vol. 6, no. 2, pp. 195 \u2013204, May 1998.\n[82] T. Eltoft and R. deFigueiredo, \u201cA new neural network for cluster-de-\ntection-and-labeling, \u201dIEEE Trans. Neural Netw. , vol. 9, no. 5, pp.\n1021 \u20131035, Sep. 1998.\n[83] A. Enright and C. Ouzounis, \u201cGeneRAGE: A robust algorithm for se-\nquence clustering and domain detection, \u201dBioinformatics , vol. 16, pp.\n451\u2013457, 2000.\n[84] S. Eschrich, J. Ke, L. Hall, and D. Goldgof, \u201cFast accurate fuzzy clus-\ntering through data reduction, \u201dIEEE Trans. Fuzzy Syst. , vol. 11, no. 2,\npp. 262 \u2013270, Apr. 2003.\n[85] M. Ester, H. Kriegel, J. Sander, and X. Xu, \u201cA density-based algorithm\nfor discovering clusters in large spatial databases with noise, \u201dinProc.\n2nd Int. Conf. Knowledge Discovery and Data Mining (KDD\u201996) , 1996,\npp. 226 \u2013231.\n[86] V . Estivill-Castro and I. Lee, \u201cAMOEBA: Hierarchical clustering\nbased on spatial proximity using Delaunay diagram, \u201dinProc. 9th Int.\nSymp. Spatial Data Handling (SDH\u201999) , Beijing, China, 1999, pp.\n7a.26 \u20137a.41.\n[87] V . Estivill-Castro and J. Yang, \u201cA fast and robust general purpose clus-\ntering algorithm, \u201dinProc. 6th Paci\ufb01c Rim Int. Conf. Arti\ufb01cial Intelli-\ngence (PRICAI\u201900) , R. Mizoguchi and J. Slaney, Eds., Melbourne, Aus-\ntralia, 2000, pp. 208 \u2013218.\n[88] B. Everitt, S. Landau, and M. Leese, Cluster Analysis . London:\nArnold, 2001.\n[89] D. Fasulo, \u201cAn analysis of recent work on clustering algorithms, \u201dDept.\nComput. Sci. Eng., Univ. Washington, Seattle, WA, Tech. Rep. 01-03-02,\n1999.\n[90] M. Figueiredo and A. Jain, \u201cUnsupervised learning of \ufb01nite mixture\nmodels, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 3, pp.\n381\u2013396, Mar. 2002.\n[91] D. Fisher, \u201cKnowledge acquisition via incremental conceptual clus-\ntering, \u201dMach. Learn. , vol. 2, pp. 139 \u2013172, 1987.[92] R. Fisher, \u201cThe use of multiple measurements in taxonomic problems, \u201d\nAnnu. Eugenics , pt. II, vol. 7, pp. 179 \u2013188, 1936.\n[93] D. Fogel, \u201cAn introduction to simulated evolutionary optimization, \u201d\nIEEE Trans. Neural Netw. , vol. 5, no. 1,", "start_char_idx": 2434, "end_char_idx": 4858, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97664fa0-9070-4279-9f21-065320de12c7": {"__data__": {"id_": "97664fa0-9070-4279-9f21-065320de12c7", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "8446f964-cbd4-4078-8677-8cf38f63f83e", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "24991b881fe6f7bc872d17b93f2e1ce5bbe684c10780b04b2f7ab273d6567537"}, "3": {"node_id": "d630ecdd-40eb-4969-b37e-c977e1cd9d17", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "268b1d9f30d597a60011fb647fa326a7624baa6ff21ea50b021a7acd29920ac0"}}, "hash": "96aa01133ac86a98be7ec1a9af3f0a87bc64c63ddb2cc0933fb2c4b33419ce20", "text": "Trans. Neural Netw. , vol. 5, no. 1, pp. 3 \u201314, Jan. 1994.\n[94] E. Forgy, \u201cCluster analysis of multivariate data: Ef \ufb01ciency vs. inter-\npretability of classi \ufb01cations, \u201dBiometrics , vol. 21, pp. 768 \u2013780, 1965.\n[95] C. Fraley and A. Raftery, \u201cMCLUST: Software for model-based cluster\nanalysis, \u201dJ. Classi\ufb01cat. , vol. 16, pp. 297 \u2013306, 1999.\n[96] ,\u201cModel-Based clustering, discriminant analysis, and density esti-\nmation, \u201dJ. Amer. Statist. Assoc. , vol. 97, pp. 611 \u2013631, 2002.\n[97] J. Friedman, \u201cExploratory projection pursuit, \u201dJ. Amer. Statist. Assoc. ,\nvol. 82, pp. 249 \u2013266, 1987.\n[98] H. Frigui and R. Krishnapuram, \u201cA robust competitive clustering algo-\nrithm with applications in computer vision, \u201dIEEE Trans. Pattern Anal.\nMach. Intell. , vol. 21, no. 5, pp. 450 \u2013465, May 1999.\n[99] B. Fritzke. (1997) Some competitive learning methods. [Online]. Avail-\nable: http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/re-\nsearch/gsn/JavaPaper\n[100] B. Gabrys and A. Bargiela, \u201cGeneral fuzzy min-max neural network for\nclustering and classi \ufb01cation, \u201dIEEE Trans. Neural Netw. , vol. 11, no. 3,\npp. 769 \u2013783, May 2000.\n[101] V . Ganti, R. Ramakrishnan, J. Gehrke, A. Powell, and J. French, \u201cClus-\ntering large datasets in arbitrary metric spaces, \u201dinProc. 15th Int. Conf.\nData Engineering , 1999, pp. 502 \u2013511.\n[102] I. Gath and A. Geva, \u201cUnsupervised optimal fuzzy clustering, \u201dIEEE\nTrans. Pattern Anal. Mach. Intell. , vol. 11, no. 7, pp. 773 \u2013781, Jul. 1989.\n[103] GenBank Release Notes 144.0 .\n[104] A. Geva, \u201cHierarchical unsupervised fuzzy clustering, \u201dIEEE Trans.\nFuzzy Syst. , vol. 7, no. 6, pp. 723 \u2013733, Dec. 1999.\n[105] D. Ghosh and A. Chinnaiyan, \u201cMixture modeling of gene expression\ndata from microarray experiments, \u201dBioinformatics , vol. 18, no. 2, pp.\n275\u2013286, 2002.\n[106] A. Ghozeil and D. Fogel, \u201cDiscovering patterns in spatial data using\nevolutionary programming, \u201dinProc. 1st Annu. Conf. Genetic Program-\nming , 1996, pp. 512 \u2013520.\n[107] M. Girolami, \u201cMercer kernel based clustering in feature space, \u201dIEEE\nTrans. Neural Netw. , vol. 13, no. 3, pp. 780 \u2013784, May 2002.\n[108] F. Glover, \u201cTabu search, part I, \u201dORSA J. Comput. , vol. 1, no. 3, pp.\n190\u2013206, 1989.\n[109] T. Golub, D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov,\nH. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloom \ufb01eld, and E.\nLander, \u201cMolecular classi \ufb01cation of cancer: Class", "start_char_idx": 4868, "end_char_idx": 7246, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d630ecdd-40eb-4969-b37e-c977e1cd9d17": {"__data__": {"id_": "d630ecdd-40eb-4969-b37e-c977e1cd9d17", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24a3dd37-d46d-40cf-a715-1dca69ce6102", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "97664fa0-9070-4279-9f21-065320de12c7", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "96aa01133ac86a98be7ec1a9af3f0a87bc64c63ddb2cc0933fb2c4b33419ce20"}}, "hash": "268b1d9f30d597a60011fb647fa326a7624baa6ff21ea50b021a7acd29920ac0", "text": "\u201cMolecular classi \ufb01cation of cancer: Class discovery and class\nprediction by gene expression monitoring, \u201dScience , vol. 286, pp.\n531\u2013537, 1999.\n[110] A. Gordon, \u201cCluster validation, \u201dinData Science, Classi\ufb01cation, and Re-\nlated Methods , C. Hayashi, N. Ohsumi, K. Yajima, Y . Tanaka, H. Bock,\nand Y . Bada, Eds. New York: Springer-Verlag, 1998, pp. 22 \u201339.\n[111] ,Classi\ufb01cation , 2nd ed. London, U.K.: Chapman & Hall, 1999.\n[112] J. Gower, \u201cA general coef \ufb01cient of similarity and some of its properties, \u201d\nBiometrics , vol. 27, pp. 857 \u2013872, 1971.\n[113] S. Grossberg, \u201cAdaptive pattern recognition and universal encoding II:\nFeedback, expectation, olfaction, and illusions, \u201dBiol. Cybern. , vol. 23,\npp. 187 \u2013202, 1976.\n[114] P. Gr \u00fcnwald, P. Kontkanen, P. Myllym \u00e4ki, T. Silander, and H. Tirri,\n\u201cMinimum encoding approaches for predictive modeling, \u201dinProc. 14th\nInt. Conf. Uncertainty in AI (UAI\u201998) , 1998, pp. 183 \u2013192.\n[115] X. Guan and L. Du, \u201cDomain identi \ufb01cation by clustering sequence\nalignments, \u201dBioinformatics , vol. 14, pp. 783 \u2013788, 1998.\n[116] S. Guha, R. Rastogi, and K. Shim, \u201cCURE: An ef \ufb01cient clustering algo-\nrithm for large databases, \u201dinProc. ACM SIGMOD Int. Conf. Manage-\nment of Data , 1998, pp. 73 \u201384.\n[117] ,\u201cROCK: A robust clustering algorithm for categorical attributes, \u201d\nInf. Syst. , vol. 25, no. 5, pp. 345 \u2013366, 2000.\n[118] S. Gupata, K. Rao, and V . Bhatnagar, \u201c /75-means clustering algorithm\nfor categorical attributes, \u201dinProc. 1st Int. Conf. Data Warehousing and\nKnowledge Discovery (DaWaK\u201999) , Florence, Italy, 1999, pp. 203 \u2013208.\n[119] V . Guralnik and G. Karypis, \u201cA scalable algorithm for clustering sequen-\ntial data, \u201dinProc. 1st IEEE Int. Conf. Data Mining (ICDM\u201901) , 2001,\npp. 179 \u2013186.\n[120] D. Gus \ufb01eld,Algorithms on Strings, Trees, and Sequences: Computer\nScience and Computational Biology . Cambridge, U.K.: Cambridge\nUniv. Press, 1997.\n[121] M. Halkidi, Y . Batistakis, and M. Vazirgiannis, \u201cCluster validity\nmethods: Part I & II, \u201dSIGMOD Record , vol. 31, no. 2 \u20133, 2002.\n[122] L. Hall, I. \u00d6zyurt, and J. Bezdek, \u201cClustering with a genetically opti-\nmized approach, \u201dIEEE Trans. Evol. Comput. , vol. 3, no. 2, pp. 103 \u2013112,\n1999.", "start_char_idx": 7233, "end_char_idx": 9420, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "47bd0dde-315d-489d-ab4d-fac58b5b7527": {"__data__": {"id_": "47bd0dde-315d-489d-ab4d-fac58b5b7527", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4432947-93c5-44a6-9506-440f898f3273", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "3": {"node_id": "41b4e924-ed5e-42db-a942-13397fba5eef", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "024de7916ef0c21911a1e13244c30f772c231b50272374313bf78e97a5de0815"}}, "hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 675\n[123] R. Hammah and J. Curran, \u201cValidity measures for the fuzzy cluster anal-\nysis of orientations, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no.\n12, pp. 1467 \u20131472, Dec. 2000.\n[124] P. Hansen and B. Jaumard, \u201cCluster analysis and mathematical program-\nming, \u201dMath. Program. , vol. 79, pp. 191 \u2013215, 1997.\n[125] P. Hansen and N. Mladenovi \u00e6,\u201cJ-means: A new local search heuristic\nfor minimum sum of squares clustering, \u201dPattern Recognit. , vol. 34, pp.\n405\u2013413, 2001.\n[126] F. Harary, Graph Theory . Reading, MA: Addison-Wesley, 1969.\n[127] J. Hartigan, Clustering Algorithms . New York: Wiley, 1975.\n[128] E. Hartuv and R. Shamir, \u201cA clustering algorithm based on graph con-\nnectivity, \u201dInf. Process. Lett. , vol. 76, pp. 175 \u2013181, 2000.\n[129] R. Hathaway and J. Bezdek, \u201cFuzzy /99-means clustering of incomplete\ndata,\u201dIEEE Trans. Syst., Man, Cybern. , vol. 31, no. 5, pp. 735 \u2013744,\n2001.\n[130] R. Hathaway, J. Bezdek, and Y . Hu, \u201cGeneralized fuzzy /99-means clus-\ntering strategies using /76\nnorm distances, \u201dIEEE Trans. Fuzzy Syst. , vol.\n8, no. 5, pp. 576 \u2013582, Oct. 2000.\n[131] B. Hay, G. Wets, and K. Vanhoof, \u201cClustering navigation patterns on a\nwebsite using a sequence alignment method, \u201dinProc. Intelligent Tech-\nniques for Web Personalization: 17th Int. Joint Conf. Arti \ufb01cial Intelli-\ngence , vol. s.l, 2001, pp. 1 \u20136, 200.\n[132] S. Haykin, Neural Networks: A Comprehensive Foundation , 2nd\ned. Englewood Cliffs, NJ: Prentice-Hall, 1999.\n[133] Q. He, \u201cA review of clustering algorithms as applied to IR, \u201dUniv. Illinois\nat Urbana-Champaign, Tech. Rep. UIUCLIS-1999/6+IRG, 1999.\n[134] M. Healy, T. Caudell, and S. Smith, \u201cA neural architecture for pattern\nsequence veri \ufb01cation through inferencing, \u201dIEEE Trans. Neural Netw. ,\nvol. 4, no. 1, pp. 9 \u201320, Jan. 1993.\n[135] A. Hinneburg and D. Keim, \u201cAn ef \ufb01cient approach to clustering in large\nmultimedia databases with noise, \u201dinProc. 4th Int. Conf. Knowledge\nDiscovery and Data Mining (KDD \u201998), 1998, pp. 58 \u201365.\n[136] ,\u201cOptimal grid-clustering: Toward breaking the curse of dimen-\nsionality in high-dimensional clustering, \u201dinProc. 25th VLDB Conf. ,\n1999, pp. 506 \u2013517.\n[137] F. Hoeppner, \u201cFuzzy shell clustering algorithms in image processing:\nFuzzy /99-rectangular and 2-rectangular shells, \u201dIEEE Trans. Fuzzy Syst. ,\nvol. 5, no. 4, pp. 599 \u2013613, Nov. 1997.\n[138] J. Hoey, \u201cClustering contextual facial display sequences, \u201dinProc. 5th\nIEEE Int. Conf. Automatic Face and", "start_char_idx": 0, "end_char_idx": 2499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "41b4e924-ed5e-42db-a942-13397fba5eef": {"__data__": {"id_": "41b4e924-ed5e-42db-a942-13397fba5eef", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4432947-93c5-44a6-9506-440f898f3273", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "47bd0dde-315d-489d-ab4d-fac58b5b7527", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491"}, "3": {"node_id": "2066e1f2-0af8-47e7-8dd9-0fe23c989ace", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "cdb815738c0be0bfff719b14516ba743bf55a850cc7f4a79a188d0f551f76b4a"}}, "hash": "024de7916ef0c21911a1e13244c30f772c231b50272374313bf78e97a5de0815", "text": "\u201dinProc. 5th\nIEEE Int. Conf. Automatic Face and Gesture Recognition (FGR \u201902),\n2002, pp. 354 \u2013359.\n[139] T. Hofmann and J. Buhmann, \u201cPairwise data clustering by deterministic\nannealing, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 1, pp.\n1\u201314, Jan. 1997.\n[140] J. Holland, Adaption in Natural and Arti \ufb01cial Systems . Ann Arbor, MI:\nUniv. Michigan Press, 1975.\n[141] F. H \u00f6ppner, F. Klawonn, and R. Kruse, Fuzzy Cluster Analysis: Methods\nfor Classi \ufb01cation, Data Analysis, and Image Recognition . New York:\nWiley, 1999.\n[142] Z. Huang, \u201cExtensions to the /75-means algorithm for clustering large\ndata sets with categorical values, \u201dData Mining Knowl. Discov. , vol. 2,\npp. 283 \u2013304, 1998.\n[143] J. Huang, M. Georgiopoulos, and G. Heileman, \u201cFuzzy ART properties, \u201d\nNeural Netw. , vol. 8, no. 2, pp. 203 \u2013213, 1995.\n[144] P. Huber, \u201cProjection pursuit, \u201dAnn. Statist. , vol. 13, no. 2, pp. 435 \u2013475,\n1985.\n[145] R. Hughey and A. Krogh, \u201cHidden Markov models for sequence anal-\nysis: Extension and analysis of the basic method, \u201dCABIOS , vol. 12, no.\n2, pp. 95 \u2013107, 1996.\n[146] M. Hung and D. Yang, \u201cAn ef \ufb01cient fuzzy /99-means clustering algo-\nrithm, \u201dinProc. IEEE Int. Conf. Data Mining , 2001, pp. 225 \u2013232.\n[147] L. Hunt and J. Jorgensen, \u201cMixture model clustering using the MUL-\nTIMIX program, \u201dAustralia and New Zealand J. Statist. , vol. 41, pp.\n153\u2013171, 1999.\n[148] J. Hwang, J. Vlontzos, and S. Kung, \u201cA systolic neural network archi-\ntecture for hidden Markov models, \u201dIEEE Trans. Acoust., Speech, Signal\nProcess. , vol. 37, no. 12, pp. 1967 \u20131979, Dec. 1989.\n[149] A. Hyv \u00e4rinen, \u201cSurvey of independent component analysis, \u201dNeural\nComput. Surv. , vol. 2, pp. 94 \u2013128, 1999.\n[150] A. Jain and R. Dubes, Algorithms for Clustering Data . Englewood\nCliffs, NJ: Prentice-Hall, 1988.\n[151] A. Jain, R. Duin, and J. Mao, \u201cStatistical pattern recognition: A review, \u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1, pp. 4 \u201337, 2000.\n[152] A. Jain, M. Murty, and P. Flynn, \u201cData clustering: A review, \u201dACM\nComput. Surv. , vol. 31, no. 3, pp. 264 \u2013323, 1999.[153] D. Jiang, C. Tang, and A. Zhang, \u201cCluster analysis for gene expression\ndata: A survey, \u201dIEEE Trans. Knowl. Data Eng. , vol. 16, no. 11, pp.\n1370 \u20131386, Nov. 2004.\n[154] C. Jutten and J. Herault, \u201cBlind separation of sources, Part I: An adaptive\nalgorithms based on neuromimetic architecture, \u201dSignal Process. , vol.\n24, no. 1, pp. 1 \u201310, 1991.\n[155] T. Kanungo,", "start_char_idx": 2459, "end_char_idx": 4906, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2066e1f2-0af8-47e7-8dd9-0fe23c989ace": {"__data__": {"id_": "2066e1f2-0af8-47e7-8dd9-0fe23c989ace", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4432947-93c5-44a6-9506-440f898f3273", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "41b4e924-ed5e-42db-a942-13397fba5eef", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "024de7916ef0c21911a1e13244c30f772c231b50272374313bf78e97a5de0815"}, "3": {"node_id": "014cd91d-0ed6-4168-aab3-68a1d818a97f", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "8c81f81d1782c54b1db102338c9ba9f5ab57865b3b0db2f726418f6268e47cb4"}}, "hash": "cdb815738c0be0bfff719b14516ba743bf55a850cc7f4a79a188d0f551f76b4a", "text": "pp. 1 \u201310, 1991.\n[155] T. Kanungo, D. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A.\nWu, \u201cAn ef \ufb01cient /75-means clustering algorithm: Analysis and imple-\nmentation, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp.\n881\u2013892, Jul. 2000.\n[156] N. Karayiannis, \u201cA methodology for construction fuzzy algorithms for\nlearning vector quantization, \u201dIEEE Trans. Neural Netw. , vol. 8, no. 3,\npp. 505 \u2013518, May 1997.\n[157] N. Karayiannis, J. Bezdek, N. Pal, R. Hathaway, and P. Pai, \u201cRepairs\nto GLVQ: A new family of competitive learning schemes, \u201dIEEE Trans.\nNeural Netw. , vol. 7, no. 5, pp. 1062 \u20131071, Sep. 1996.\n[158] J. Karhunen, E. Oja, L. Wang, R. Vig \u00e1rio, and J. Joutsensalo, \u201cA class\nof neural networks for independent component analysis, \u201dIEEE Trans.\nNeural Netw. , vol. 8, no. 3, pp. 486 \u2013504, May 1997.\n[159] G. Karypis, E. Han, and V . Kumar, \u201cChameleon: Hierarchical clustering\nusing dynamic modeling, \u201dIEEE Computer , vol. 32, no. 8, pp. 68 \u201375,\nAug. 1999.\n[160] R. Kathari and D. Pitts, \u201cOn\ufb01nding the number of clusters, \u201dPattern\nRecognit. Lett. , vol. 20, pp. 405 \u2013416, 1999.\n[161] L. Kaufman and P. Rousseeuw, Finding Groups in Data: An Introduction\nto Cluster Analysis : Wiley, 1990.\n[162] W. Kent and A. Zahler, \u201cConservation, regulation, synteny, and introns\nin a large-scale C. Briggsae \u2014C. elegans genomic alignment, \u201dGenome\nRes., vol. 10, pp. 1115 \u20131125, 2000.\n[163] P. Kersten, \u201cImplementation issues in the fuzzy /99-medians clustering\nalgorithm, \u201dinProc. 6th IEEE Int. Conf. Fuzzy Systems , vol. 2, 1997, pp.\n957\u2013962.\n[164] J. Khan, J. Wei, M. Ringn \u00e9r, L. Saal, M. Ladanyi, F. Westermann, F.\nBerthold, M. Schwab, C. Antonescu, C. Peterson, and P. Meltzer, \u201cClas-\nsi\ufb01cation and diagnostic prediction of cancers using gene expression\npro\ufb01ling and arti \ufb01cial neural networks, \u201dNature Med. , vol. 7, no. 6, pp.\n673\u2013679, 2001.\n[165] S. Kirkpatrick, C. Gelatt, and M. Vecchi, \u201cOptimization by simulated\nannealing, \u201dScience , vol. 220, no. 4598, pp. 671 \u2013680, 1983.\n[166] J. Kleinberg, \u201cAn impossibility theorem for clustering, \u201dinProc. 2002\nConf. Advances in Neural Information Processing Systems , vol. 15,\n2002, pp. 463 \u2013470.\n[167] R. Kohavi, \u201cA study of cross-validation and bootstrap for accuracy es-\ntimation and model selection, \u201dinProc. 14th Int. Joint Conf. Arti \ufb01cial\nIntelligence , 1995, pp. 338 \u2013345.\n[168] T. Kohonen, \u201cThe self-organizing map, \u201dProc. IEEE , vol. 78, no. 9,", "start_char_idx": 4919, "end_char_idx": 7338, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "014cd91d-0ed6-4168-aab3-68a1d818a97f": {"__data__": {"id_": "014cd91d-0ed6-4168-aab3-68a1d818a97f", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4432947-93c5-44a6-9506-440f898f3273", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "2066e1f2-0af8-47e7-8dd9-0fe23c989ace", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "cdb815738c0be0bfff719b14516ba743bf55a850cc7f4a79a188d0f551f76b4a"}}, "hash": "8c81f81d1782c54b1db102338c9ba9f5ab57865b3b0db2f726418f6268e47cb4", "text": "map, \u201dProc. IEEE , vol. 78, no. 9, pp.\n1464 \u20131480, Sep. 1990.\n[169] ,Self-Organizing Maps , 3rd ed. New York: Springer-Verlag,\n2001.\n[170] T. Kohonen, S. Kaski, K. Lagus, J. Saloj \u00e4rvi, J. Honkela, V . Paatero, and\nA. Saarela, \u201cSelf organization of a massive document collection, \u201dIEEE\nTrans. Neural Netw. , vol. 11, no. 3, pp. 574 \u2013585, May 2000.\n[171] E. Kolatch. (2001) Clustering algorithms for spatial databases: A\nSurvey. [Online]. Available: http://citeseer.nj.nec.com/436 843.html\n[172] J. Kolen and T. Hutcheson, \u201cReducing the time complexity of the\nfuzzy /99-means algorithm, \u201dIEEE Trans. Fuzzy Syst. , vol. 10, no. 2, pp.\n263\u2013267, Apr. 2002.\n[173] K. Krishna and M. Murty, \u201cGenetic /75-means algorithm, \u201dIEEE Trans.\nSyst., Man, Cybern. B, Cybern. , vol. 29, no. 3, pp. 433 \u2013439, Jun. 1999.\n[174] R. Krishnapuram, H. Frigui, and O. Nasraoui, \u201cFuzzy and possiblistic\nshell clustering algorithms and their application to boundary detection\nand surface approximation \u2014Part I and II, \u201dIEEE Trans. Fuzzy Syst. , vol.\n3, no. 1, pp. 29 \u201360, Feb. 1995.\n[175] R. Krishnapuram and J. Keller, \u201cA possibilistic approach to clustering, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 1, no. 2, pp. 98 \u2013110, Apr. 1993.\n[176] R. Krishnapuram, O. Nasraoui, and H. Frigui, \u201cThe fuzzy /99spherical\nshells algorithm: A new approach, \u201dIEEE Trans. Neural Netw. , vol. 3,\nno. 5, pp. 663 \u2013671, Sep. 1992.\n[177] A. Krogh, M. Brown, I. Mian, K. Sj \u00f6lander, and D. Haussler, \u201cHidden\nMarkov models in computational biology: Applications to protein mod-\neling, \u201dJ. Molec. Biol. , vol. 235, pp. 1501 \u20131531, 1994.\n[178] G. Lance and W. Williams, \u201cA general theory of classi \ufb01cation sorting\nstrategies: 1. Hierarchical systems, \u201dComput. J. , vol. 9, pp. 373 \u2013380,\n1967.\n[179] M. Law and J. Kwok, \u201cRival penalized competitive learning for model-\nbased sequence clustering, \u201dinProc. 15th Int. Conf. Pattern Recognition ,\nvol. 2, 2000, pp. 195 \u2013198.", "start_char_idx": 7333, "end_char_idx": 9243, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33599ae9-74bc-484d-8322-7cc5901e47fd": {"__data__": {"id_": "33599ae9-74bc-484d-8322-7cc5901e47fd", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41166736-536f-4f15-919b-2bde324e2364", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "3": {"node_id": "040d2a55-9557-4afb-b016-49b0c056b485", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "9fbb62f2620c75edc2dcf0b8d1be4e66b79c72d606a4858bd5326e4be9e6a65d"}}, "hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7", "text": "676 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[180] Y . Leung, J. Zhang, and Z. Xu, \u201cClustering by scale-space \ufb01ltering, \u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 12, pp. 1396 \u20131410,\nDec. 2000.\n[181] E. Levine and E. Domany, \u201cResampling method for unsupervised es-\ntimation of cluster validity, \u201dNeural Comput. , vol. 13, pp. 2573 \u20132593,\n2001.\n[182] C. Li and G. Biswas, \u201cTemporal pattern generation using hidden\nMarkov model based unsupervised classi \ufb01cation, \u201dinAdvances in\nIntelligent Data Analysis . ser. Lecture Notes in Computer Science, D.\nHand, K. Kok, and M. Berthold, Eds. New York: Springer-Verlag,\n1999, vol. 1642.\n[183] ,\u201cUnsupervised learning with mixed numeric and nominal data, \u201d\nIEEE Trans. Knowl. Data Eng. , vol. 14, no. 4, pp. 673 \u2013690, Jul.-Aug.\n2002.\n[184] C. Li, H. Garcia-Molina, and G. Wiederhold, \u201cClustering for approxi-\nmate similarity search in high-dimensional spaces, \u201dIEEE Trans. Knowl.\nData Eng. , vol. 14, no. 4, pp. 792 \u2013808, Jul.-Aug. 2002.\n[185] W. Li, L. Jaroszewski, and A. Godzik, \u201cClustering of highly homologous\nsequences to reduce the size of large protein databases, \u201dBioinformatics ,\nvol. 17, pp. 282 \u2013283, 2001.\n[186] A. Likas, N. Vlassis, and J. Verbeek, \u201cThe global /75-means clustering\nalgorithm, \u201dPattern Recognit. , vol. 36, no. 2, pp. 451 \u2013461, 2003.\n[187] S. Lin and B. Kernighan, \u201cAn effective heuristic algorithm for the trav-\neling salesman problem, \u201dOperat. Res. , vol. 21, pp. 498 \u2013516, 1973.\n[188] R. Lipshutz, S. Fodor, T. Gingeras, and D. Lockhart, \u201cHigh density\nsynthetic oligonucleotide arrays, \u201dNature Genetics , vol. 21, pp. 20 \u201324,\n1999.\n[189] G. Liu, Introduction to Combinatorial Mathematics . New York: Mc-\nGraw-Hill, 1968.\n[190] J. Lozano and P. Larra \u00f1aga, \u201cApplying genetic algorithms to search for\nthe best hierarchical clustering of a dataset, \u201dPattern Recognit. Lett. , vol.\n20, pp. 911 \u2013918, 1999.\n[191] J. MacQueen, \u201cSome methods for classi \ufb01cation and analysis of mul-\ntivariate observations, \u201dinProc. 5th Berkeley Symp. , vol. 1, 1967, pp.\n281\u2013297.\n[192] S. C. Madeira and A. L. Oliveira, \u201cBiclustering algorithms for biolog-\nical data analysis: A survey, \u201dIEEE/ACM Trans. Computat. Biol. Bioin-\nformatics , vol. 1, no. 1, pp. 24 \u201345, Jan. 2004.\n[193] Y . Man and I. Gath, \u201cDetection and separation of ring-shaped clusters\nusing fuzzy clustering, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 16,\nno. 8, pp. 855 \u2013861, Aug. 1994.\n[194] J. Mao and A. Jain, \u201cA self-organizing network for hyperellipsoidal", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "040d2a55-9557-4afb-b016-49b0c056b485": {"__data__": {"id_": "040d2a55-9557-4afb-b016-49b0c056b485", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41166736-536f-4f15-919b-2bde324e2364", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "33599ae9-74bc-484d-8322-7cc5901e47fd", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7"}, "3": {"node_id": "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "a3c10ef2dd766733a67071d28b495ffe6420400f50a85fdb1e4b1357dfaac4a5"}}, "hash": "9fbb62f2620c75edc2dcf0b8d1be4e66b79c72d606a4858bd5326e4be9e6a65d", "text": "A. Jain, \u201cA self-organizing network for hyperellipsoidal clus-\ntering (HEC), \u201dIEEE Trans. Neural Netw. , vol. 7, no. 1, pp. 16 \u201329, Jan.\n1996.\n[195] U. Maulik and S. Bandyopadhyay, \u201cGenetic algorithm-based clustering\ntechnique, \u201dPattern Recognit. , vol. 33, pp. 1455 \u20131465, 2000.\n[196] G. McLachlan and T. Krishnan, The EM Algorithm and Exten-\nsions . New York: Wiley, 1997.\n[197] G. McLachlan and D. Peel, Finite Mixture Models . New York: Wiley,\n2000.\n[198] G. McLachlan, D. Peel, K. Basford, and P. Adams, \u201cThe EMMIX soft-\nware for the \ufb01tting of mixtures of normal and t-components, \u201dJ. Statist.\nSoftware , vol. 4, 1999.\n[199] C. Miller, J. Gurd, and A. Brass, \u201cA RAPID algorithm for sequence data-\nbase comparisons: Application to the identi \ufb01cation of vector contami-\nnation in the EMBL databases, \u201dBioinformatics , vol. 15, pp. 111 \u2013121,\n1999.\n[200] R. Miller et al. ,\u201cA comprehensive approach to clustering of expressed\nhuman gene sequence: The sequence tag alignment and consensus\nknowledge base, \u201dGenome Res. , vol. 9, pp. 1143 \u20131155, 1999.\n[201] W. Miller, \u201cComparison of genomic DNA sequences: Solved and un-\nsolved problems, \u201dBioinformatics , vol. 17, pp. 391 \u2013397, 2001.\n[202] G. Milligan and M. Cooper, \u201cAn examination of procedures for deter-\nmining the number of clusters in a data set, \u201dPsychometrika , vol. 50, pp.\n159\u2013179, 1985.\n[203] R. Mollineda and E. Vidal, \u201cA relative approach to hierarchical\nclustering, \u201dinPattern Recognition and Applications, Frontiers in\nArti\ufb01cial Intelligence and Applications , M. Torres and A. Sanfeliu,\nEds. Amsterdam, The Netherlands: IOS Press, 2000, vol. 56, pp.\n19\u201328.\n[204] B. Moore, \u201cART1 and pattern clustering, \u201dinProc. 1988 Connectionist\nModels Summer School , 1989, pp. 174 \u2013185.\n[205] S. Moore, \u201cMaking chips to probe genes, \u201dIEEE Spectr. , vol. 38, no. 3,\npp. 54 \u201360, Mar. 2001.\n[206] Y . Moreau, F. Smet, G. Thijs, K. Marchal, and B. Moor, \u201cFunctional\nbioinformatics of microarray data: From expression to regulation, \u201dProc.\nIEEE , vol. 90, no. 11, pp. 1722 \u20131743, Nov. 2002.[207] T. Morzy, M. Wojciechowski, and M. Zakrzewicz, \u201cPattern-oriented\nhierarchical clustering, \u201dinProc. 3rd East Eur. Conf. Advances in\nDatabases and Information Systems , 1999, pp. 179 \u2013190.\n[208] S. Mulder and D. Wunsch, \u201cMillion city traveling salesman problem\nsolution by divide and conquer clustering with adaptive resonance neural\nnetworks, \u201dNeural Netw. , vol. 16, pp. 827 \u2013832, 2003.\n[209] K. M \u00fcller, S. Mika, G. R \u00e4tsch, K. Tsuda, and B. Sch \u00f6lkopf, \u201cAn intro-\nduction to kernel-based learning algorithms, \u201dIEEE Trans. Neural Netw.", "start_char_idx": 2470, "end_char_idx": 5050, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8": {"__data__": {"id_": "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41166736-536f-4f15-919b-2bde324e2364", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "040d2a55-9557-4afb-b016-49b0c056b485", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "9fbb62f2620c75edc2dcf0b8d1be4e66b79c72d606a4858bd5326e4be9e6a65d"}, "3": {"node_id": "398aac6d-f3ec-4a24-955b-3efea64e7cb9", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "c7c19ed2aabd602e26d6a8ce44bfefd678d9adaf541cabf55fe55bf5d310d6f2"}}, "hash": "a3c10ef2dd766733a67071d28b495ffe6420400f50a85fdb1e4b1357dfaac4a5", "text": "to kernel-based learning algorithms, \u201dIEEE Trans. Neural Netw. ,\nvol. 12, no. 2, pp. 181 \u2013201, Mar. 2001.\n[210] F. Murtagh, \u201cA survey of recent advances in hierarchical clustering al-\ngorithms, \u201dComput. J. , vol. 26, no. 4, pp. 354 \u2013359, 1983.\n[211] F. Murtagh and M. Berry, \u201cOvercoming the curse of dimensionality in\nclustering by means of the wavelet transform, \u201dComput. J. , vol. 43, no.\n2, pp. 107 \u2013120, 2000.\n[212] S. Needleman and C. Wunsch, \u201cA general method applicable to the\nsearch for similarities in the amino acid sequence of two proteins, \u201dJ.\nMolec. Biol. , vol. 48, pp. 443 \u2013453, 1970.\n[213] R. Ng and J. Han, \u201cCLARANS: A method for clustering objects for\nspatial data mining, \u201dIEEE Trans. Knowl. Data Eng. , vol. 14, no. 5, pp.\n1003 \u20131016, Sep.-Oct. 2002.\n[214] T. Oates, L. Firoiu, and P. Cohen, \u201cUsing dynamic time warping to boot-\nstrap HMM-based clustering of time series, \u201dinSequence Learning . ser.\nLNAI 1828, R. Sun and C. Giles, Eds. Berlin, Germany: Springer-Verlag, 2000, pp. 35 \u201352.\n[215] E. Oja, \u201cPrincipal components minor components, and linear neural net-\nworks, \u201dNeural Netw. , vol. 5, pp. 927 \u2013935, 1992.\n[216] J. Oliver, R. Baxter, and C. Wallace, \u201cUnsupervised learning using\nMML, \u201dinProc. 13th Int. Conf. Machine Learning (ICML \u201996), Lorenza,\nSaitta, 1996, pp. 364 \u2013372.\n[217] C. Olson, \u201cParallel algorithms for hierarchical clustering, \u201dParallel\nComput. , vol. 21, pp. 1313 \u20131325, 1995.\n[218] C. Ordonez and E. Omiecinski, \u201cEf\ufb01cient disk-based K-means clus-\ntering for relational databases, \u201dIEEE Trans. Knowl. Data Eng. , vol. 16,\nno. 8, pp. 909 \u2013921, Aug. 2004.\n[219] L. Owsley, L. Atlas, and G. Bernard, \u201cSelf-organizing feature maps\nand hidden Markov models for machine-tool monitoring, \u201dIEEE Trans.\nSignal Process. , vol. 45, no. 11, pp. 2787 \u20132798, Nov. 1997.\n[220] N. Pal and J. Bezdek, \u201cOn cluster validity for the fuzzy\n/99-means model, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 3, no. 3, pp. 370 \u2013379, Aug. 1995.\n[221] N. Pal, J. Bezdek, and E. Tsao, \u201cGeneralized clustering networks and\nKohonen \u2019s self-organizing scheme, \u201dIEEE Trans. Neural Netw. , vol. 4,\nno. 4, pp. 549 \u2013557, Jul. 1993.\n[222] G. Patan \u00e8and M. Russo, \u201cThe enhanced-LBG algorithm, \u201dNeural Netw. ,\nvol. 14, no. 9, pp. 1219 \u20131237, 2001.\n[223] ,\u201cFully automatic clustering system, \u201dIEEE Trans. Neural Netw. ,\nvol. 13, no. 6, pp. 1285 \u20131298, Nov. 2002.\n[224] W. Pearson, \u201cImproved tools for biological sequence comparison, \u201dProc.\nNat. Acad.", "start_char_idx": 5046, "end_char_idx": 7488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "398aac6d-f3ec-4a24-955b-3efea64e7cb9": {"__data__": {"id_": "398aac6d-f3ec-4a24-955b-3efea64e7cb9", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41166736-536f-4f15-919b-2bde324e2364", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "a3c10ef2dd766733a67071d28b495ffe6420400f50a85fdb1e4b1357dfaac4a5"}}, "hash": "c7c19ed2aabd602e26d6a8ce44bfefd678d9adaf541cabf55fe55bf5d310d6f2", "text": "tools for biological sequence comparison, \u201dProc.\nNat. Acad. Sci. , vol. 85, pp. 2444 \u20132448, 1988.\n[225] D. Peel and G. McLachlan, \u201cRobust mixture modeling using the t-dis-\ntribution, \u201dStatist. Comput. , vol. 10, pp. 339 \u2013348, 2000.\n[226] D. Pelleg and A. Moore, \u201cX-means: Extending /75-means with ef \ufb01cient\nestimation of the number of clusters, \u201dinProc. 17th Int. Conf. Machine\nLearning (ICML \u201900), 2000, pp. 727 \u2013734.\n[227] J. Pe \u00f1a, J. Lozano, and P. Larra \u00f1aga,\u201cAn empirical comparison of four\ninitialization methods for the /75-means algorithm, \u201dPattern Recognit.\nLett., vol. 20, pp. 1027 \u20131040, 1999.\n[228] C. Pizzuti and D. Talia, \u201cP-AutoClass: Scalable parallel clustering for\nmining large data sets, \u201dIEEE Trans. Knowl. Data Eng. , vol. 15, no. 3,\npp. 629 \u2013641, May-Jun. 2003.\n[229] L. Rabiner, \u201cA tutorial on hidden Markov models and selected applica-\ntions in speech recognition, \u201dProc. IEEE , vol. 77, no. 2, pp. 257 \u2013286,\nFeb. 1989.\n[230] Ralf-Herwig, A. Poustka, C. M \u00fcller, C. Bull, H. Lehrach, and\nJ. O \u2019Brien, \u201cLarge-scale clustering of cDNA- \ufb01ngerprinting data, \u201d\nGenome Res. , pp. 1093 \u20131105, 1999.\n[231] A. Rauber, J. Paralic, and E. Pampalk, \u201cEmpirical evaluation of clus-\ntering algorithms, \u201dJ. Inf. Org. Sci. , vol. 24, no. 2, pp. 195 \u2013209, 2000.\n[232] S. Ridella, S. Rovetta, and R. Zunino, \u201cPlastic algorithm for adaptive\nvector quantization, \u201dNeural Comput. Appl. , vol. 7, pp. 37 \u201351, 1998.\n[233] J. Rissanen, \u201cFisher information and stochastic complexity, \u201dIEEE\nTrans. Inf. Theory , vol. 42, no. 1, pp. 40 \u201347, Jan. 1996.\n[234] K. Rose, \u201cDeterministic annealing for clustering, compression, classi \ufb01-\ncation, regression, and related optimization problems, \u201dProc. IEEE , vol.\n86, no. 11, pp. 2210 \u20132239, Nov. 1998.", "start_char_idx": 7484, "end_char_idx": 9224, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "968838ad-f5fc-4a41-99f3-062c6abed929": {"__data__": {"id_": "968838ad-f5fc-4a41-99f3-062c6abed929", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f20d19db-543f-4d8e-b038-4715499fb0a0", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "3": {"node_id": "87f6e9dc-a389-4d72-857e-ba0221e36cdb", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "3483267210b60f2f0b11d4b78863afe4c8c7c7721a5d3dcdd51b052da437a0fb"}}, "hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 677\n[235] S. Roweis and L. Saul, \u201cNonlinear dimensionality reduction by locally\nlinear embedding, \u201dScience , vol. 290, no. 5500, pp. 2323 \u20132326, 2000.\n[236] D. Sankoff and J. Kruskal, Time Warps, String Edits, and Macro-\nmolecules: The Theory and Practice of Sequence Comparison . Stan-\nford, CA: CSLI Publications, 1999.\n[237] O. Sasson, N. Linial, and M. Linial, \u201cThe metric space of pro-\nteins \u2014Comparative study of clustering algorithms, \u201dBioinformatics ,\nvol. 18, pp. s14 \u2013s21, 2002.\n[238] U. Scherf, D. Ross, M. Waltham, L. Smith, J. Lee, L. Tanabe, K. Kohn,\nW. Reinhold, T. Myers, D. Andrews, D. Scudiero, M. Eisen, E. Sausville,\nY . Pommier, D. Botstein, P. Brown, and J. Weinstein, \u201cA gene expression\ndatabase for the molecular pharmacology of cancer, \u201dNature Genetics ,\nvol. 24, no. 3, pp. 236 \u2013244, 2000.\n[239] P. Scheunders, \u201cA comparison of clustering algorithms applied to color\nimage quantization, \u201dPattern Recognit. Lett. , vol. 18, pp. 1379 \u20131384,\n1997.\n[240] B. Sch \u00f6lkopf and A. Smola, Learning with Kernels: Support Vector Ma-\nchines, Regularization, Optimization, and Beyond . Cambridge, MA:\nMIT Press, 2002.\n[241] B. Sch \u00f6lkopf, A. Smola, and K. M \u00fcller,\u201cNonlinear component analysis\nas a kernel eigenvalue problem, \u201dNeural Computat. , vol. 10, no. 5, pp.\n1299 \u20131319, 1998.\n[242] G. Schwarz, \u201cEstimating the dimension of a model, \u201dAnn. Statist. , vol.\n6, no. 2, pp. 461 \u2013464, 1978.\n[243] G. Scott, D. Clark, and T. Pham, \u201cA genetic clustering algorithm guided\nby a descent algorithm, \u201dinProc. Congr. Evolutionary Computation , vol.\n2, Piscataway, NJ, 2001, pp. 734 \u2013740.\n[244] P. Sebastiani, M. Ramoni, and P. Cohen, \u201cSequence learning via\nBayesian clustering by dynamics, \u201dinSequence Learning . ser. LNAI\n1828, R. Sun and C. Giles, Eds. Berlin, Germany: Springer-Verlag,\n2000, pp. 11 \u201334.\n[245] S. Selim and K. Alsultan, \u201cA simulated annealing algorithm for the clus-\ntering problems, \u201dPattern Recognit. , vol. 24, no. 10, pp. 1003 \u20131008,\n1991.\n[246] R. Shamir and R. Sharan, \u201cAlgorithmic approaches to clustering gene\nexpression data, \u201dinCurrent Topics in Computational Molecular Bi-\nology , T. Jiang, T. Smith, Y . Xu, and M. Zhang, Eds. Cambridge, MA:\nMIT Press, 2002, pp. 269 \u2013300.\n[247] R. Sharan and R. Shamir, \u201cCLICK: A clustering algorithm with appli-\ncations to gene expression analysis, \u201dinProc. 8th Int. Conf. Intelligent\nSystems for Molecular Biology , 2000, pp. 307 \u2013316.\n[248] G. Sheikholeslami, S. Chatterjee, and A. Zhang, \u201cWaveCluster: A multi-\nresolution clustering approach", "start_char_idx": 0, "end_char_idx": 2565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87f6e9dc-a389-4d72-857e-ba0221e36cdb": {"__data__": {"id_": "87f6e9dc-a389-4d72-857e-ba0221e36cdb", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f20d19db-543f-4d8e-b038-4715499fb0a0", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "968838ad-f5fc-4a41-99f3-062c6abed929", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7"}, "3": {"node_id": "5dcc2ded-551c-4808-8bba-58debc0e37c8", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "b577a0ce16d6b1d2c533bccd72250c444b030020c41878d5a3905a02da17acb9"}}, "hash": "3483267210b60f2f0b11d4b78863afe4c8c7c7721a5d3dcdd51b052da437a0fb", "text": "Zhang, \u201cWaveCluster: A multi-\nresolution clustering approach for very large spatial databases, \u201dinProc.\n24th VLDB Conf. , 1998, pp. 428 \u2013439.\n[249] P. Simpson, \u201cFuzzy min-max neural networks \u2014Part 2: Clustering, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 1, no. 1, pp. 32 \u201345, Feb. 1993.\n[250] Handbook of Pattern Recognition and Computer Vision , C. Chen, L.\nPau, and P. Wang, Eds., World Scienti \ufb01c, Singapore, 1993, pp. 61 \u2013124.\nJ. Sklansky and W. Siedlecki, \u201cLarge-scale feature selection \u201d.\n[251] T. Smith and M. Waterman, \u201cNew stratigraphic correlation techniques, \u201d\nJ. Geology , vol. 88, pp. 451 \u2013457, 1980.\n[252] P. Smyth, \u201cClustering using Monte Carlo cross-validation, \u201dinProc. 2nd\nInt. Conf. Knowledge Discovery and Data Mining , 1996, pp. 126 \u2013133.\n[253] ,\u201cClustering sequences with hidden Markov models, \u201dinAdvances\nin Neural Information Processing , M. Mozer, M. Jordan, and T. Petsche,\nEds. Cambridge, MA: MIT Press, 1997, vol. 9, pp. 648 \u2013654.\n[254] ,\u201cModel selection for probabilistic clustering using cross validated\nlikelihood, \u201dStatist. Comput. , vol. 10, pp. 63 \u201372, 1998.\n[255] ,\u201cProbabilistic model-based clustering of multivariate and sequen-\ntial data, \u201dinProc. 7th Int. Workshop on Arti \ufb01cial Intelligence and Sta-\ntistics , 1999, pp. 299 \u2013304.\n[256] P. Sneath, \u201cThe application of computers to taxonomy, \u201dJ. Gen. Micro-\nbiol., vol. 17, pp. 201 \u2013226, 1957.\n[257] P. Somervuo and T. Kohonen, \u201cClustering and visualization of large pro-\ntein sequence databases by means of an extension of the self-organizingmap, \u201dinLNAI 1967 , 2000, pp. 76 \u201385.\n[258] T. Sorensen, \u201cA method of establishing groups of equal amplitude in\nplant sociology based on similarity of species content and its application\nto analyzes of the vegetation on Danish commons, \u201dBiologiske Skrifter ,\nvol. 5, pp. 1 \u201334, 1948.\n[259] H. Sp \u00e4th,Cluster Analysis Algorithms . Chichester, U.K.: Ellis Hor-\nwood, 1980.\n[260] P. Spellman, G. Sherlock, M. Ma, V . Iyer, K. Anders, M. Eisen, P. Brown,\nD. Botstein, and B. Futcher, \u201cComprehensive identi \ufb01cation of cell cycle-\nregulated genes of the Yeast Saccharomyces Cerevisiae by microarray\nhybridization, \u201dMol. Biol. Cell , vol. 9, pp. 3273 \u20133297, 1998.\n[261] \u201cTech. Rep. 00 \u2013034,\u201dUniv. Minnesota, Minneapolis, 2000.[262] K. Stoffel and A. Belkoniene, \u201cParallel\n/75-means clustering for\nlarge data sets, \u201dinProc. EuroPar \u201999 Parallel Processing , 1999, pp.\n1451 \u20131454.\n[263] M. Su and H. Chang, \u201cFast self-organizing feature map algorithm, \u201dIEEE\nTrans. Neural Netw. , vol. 11, no. 3, pp. 721 \u2013733, May 2000.\n[264] M. Su and C. Chou, \u201cA modi \ufb01ed version of the /75-means algorithm", "start_char_idx": 2511, "end_char_idx": 5115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5dcc2ded-551c-4808-8bba-58debc0e37c8": {"__data__": {"id_": "5dcc2ded-551c-4808-8bba-58debc0e37c8", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f20d19db-543f-4d8e-b038-4715499fb0a0", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "87f6e9dc-a389-4d72-857e-ba0221e36cdb", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "3483267210b60f2f0b11d4b78863afe4c8c7c7721a5d3dcdd51b052da437a0fb"}, "3": {"node_id": "7a7c5737-1d34-46b6-87dc-29d12d88bcb4", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "f7e62202ae4994ad9982e3d1bf85ab302af545267a33f57e8119c681adb4d0be"}}, "hash": "b577a0ce16d6b1d2c533bccd72250c444b030020c41878d5a3905a02da17acb9", "text": "\u201cA modi \ufb01ed version of the /75-means algorithm with\na distance based on cluster symmetry, \u201dIEEE Trans. Pattern Anal. Mach.\nIntell. , vol. 23, no. 6, pp. 674 \u2013680, Jun. 2001.\n[265] R. Sun and C. Giles, \u201cSequence learning: Paradigms, algorithms, and\napplications, \u201dinLNAI 1828 , . Berlin, Germany, 2000.\n[266] C. Sung and H. Jin, \u201cA Tabu-search-based heuristic for clustering, \u201dPat-\ntern Recognit. , vol. 33, pp. 849 \u2013858, 2000.\n[267] SWISS-PROT Protein Knowledgebase Release 45.0 Statistics .\n[268] P. Tamayo, D. Slonim, J. Mesirov, Q. Zhu, S. Kitareewan, E. Dmitro-\nvsky, E. Lander, and T. Golub, \u201cInterpreting patterns of gene expression\nwith self-organizing maps: Methods and application to hematopoietic\ndifferentiation, \u201dProc. Nat. Acad. Sci. , pp. 2907 \u20132912, 1999.\n[269] S. Tavazoie, J. Hughes, M. Campbell, R. Cho, and G. Church, \u201cSys-\ntematic determination of genetic network architecture, \u201dNature Genetics ,\nvol. 22, pp. 281 \u2013285, 1999.\n[270] J. Tenenbaum, V . Silva, and J. Langford, \u201cA global geometric frame-\nwork for nonlinear dimensionality reduction, \u201dScience , vol. 290, pp.\n2319 \u20132323, 2000.\n[271] R. Tibshirani, T. Hastie, M. Eisen, D. Ross, D. Botstein, and P. Brown,\n\u201cClustering methods for the analysis of DNA microarray data, \u201dDept.\nStatist., Stanford Univ., Stanford, CA, Tech. Rep..\n[272] R. Tibshirani and K. Knight, \u201cThe covariance in \ufb02ation criterion for\nadaptive model selection, \u201dJ. Roy. Statist. Soc. B , vol. 61, pp. 529 \u2013546,\n1999.\n[273] L. Tseng and S. Yang, \u201cA genetic approach to the automatic clustering\nproblem, \u201dPattern Recognit. , vol. 34, pp. 415 \u2013424, 2001.\n[274] V . Vapnik, Statistical Learning Theory . New York: Wiley, 1998.\n[275] J. Venter et al. ,\u201cThe sequence of the human genome, \u201dScience , vol. 291,\npp. 1304 \u20131351, 2001.\n[276] J. Vesanto and E. Alhoniemi, \u201cClustering of the self-organizing map, \u201d\nIEEE Trans. Neural Netw. , vol. 11, no. 3, pp. 586 \u2013600, May 2000.\n[277] K. Wagstaff, S. Rogers, and S. Schroedl, \u201cConstrained /75-means clus-\ntering with background knowledge, \u201dinProc. 8th Int. Conf. Machine\nLearning , 2001, pp. 577 \u2013584.\n[278] C. Wallace and D. Dowe, \u201cIntrinsic classi \ufb01cation by MML \u2014The SNOB\nprogram, \u201dinProc. 7th Australian Joint Conf. Arti \ufb01cial Intelligence ,\n1994, pp. 37 \u201344.\n[279] H. Wang, W. Wang, J. Yang, and P. Yu, \u201cClustering by pattern similarity\nin large data sets, \u201dinProc. ACM SIGMOD Int. Conf. Management of\nData , 2002, pp. 394 \u2013405.\n[280] C. Wei, Y . Lee, and C. Hsu, \u201cEmpirical comparison of fast clustering\nalgorithms for large data sets,", "start_char_idx": 5132, "end_char_idx": 7660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a7c5737-1d34-46b6-87dc-29d12d88bcb4": {"__data__": {"id_": "7a7c5737-1d34-46b6-87dc-29d12d88bcb4", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f20d19db-543f-4d8e-b038-4715499fb0a0", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "5dcc2ded-551c-4808-8bba-58debc0e37c8", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "b577a0ce16d6b1d2c533bccd72250c444b030020c41878d5a3905a02da17acb9"}}, "hash": "f7e62202ae4994ad9982e3d1bf85ab302af545267a33f57e8119c681adb4d0be", "text": "comparison of fast clustering\nalgorithms for large data sets, \u201dinProc. 33rd Hawaii Int. Conf. System\nSciences , Maui, HI, 2000, pp. 1 \u201310.\n[281] J. Williamson, \u201cGaussian ARTMAP: A neural network for fast incre-\nmental learning of noisy multidimensional maps, \u201dNeural Netw. , vol. 9,\nno. 5, pp. 881 \u2013897, 1996.\n[282] M. Windham and A. Culter, \u201cInformation ratios for validating mixture\nanalysis, \u201dJ. Amer. Statist. Assoc. , vol. 87, pp. 1188 \u20131192, 1992.\n[283] S. Wu, A. W.-C. Liew, H. Yan, and M. Yang, \u201cCluster analysis of\ngene expression data based on self-splitting and merging competitive\nlearning, \u201dIEEE Trans. Inf. Technol. Biomed. , vol. 8, no. 1, pp. 5 \u201315,\nJan. 2004.\n[284] D. Wunsch, \u201cAn optoelectronic learning machine: Invention, experi-\nmentation, analysis of \ufb01rst hardware implementation of the ART1 neural\nnetwork, \u201dPh.D. dissertation, Univ. Washington, Seattle, WA, 1991.\n[285] D. Wunsch, T. Caudell, C. Capps, R. Marks, and R. Falk, \u201cAn optoelec-\ntronic implementation of the adaptive resonance neural network, \u201dIEEE\nTrans. Neural Netw. , vol. 4, no. 4, pp. 673 \u2013684, Jul. 1993.\n[286] Y . Xiong and D. Yeung, \u201cMixtures of ARMA models for model-based\ntime series clustering, \u201dinProc. IEEE Int. Conf. Data Mining , 2002, pp.\n717\u2013720.\n[287] R. Xu, G. Anagnostopoulos, and D. Wunsch, \u201cTissue classi \ufb01cation\nthrough analysis of gene expression data using a new family of ARTarchitectures, \u201dinProc. Int. Joint Conf. Neural Networks (IJCNN \u201902),\nvol. 1, 2002, pp. 300 \u2013304.\n[288] Y . Xu, V . Olman, and D. Xu, \u201cClustering gene expression data using\ngraph-theoretic approach: An application of minimum spanning trees, \u201d\nBioinformatics , vol. 18, no. 4, pp. 536 \u2013545, 2002.", "start_char_idx": 7638, "end_char_idx": 9319, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fa30bdbf-604b-4ce6-ae49-75bfa61db9c4": {"__data__": {"id_": "fa30bdbf-604b-4ce6-ae49-75bfa61db9c4", "embedding": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2dccad2-a69d-41e5-8d00-adc812359e09", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "3": {"node_id": "11b43a7e-b2cb-4edb-854f-234a6e38fb90", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "b143a149bccb0c99f5aa8380310d5dc29001d829a439bdcf4fc642b1f2e377a3"}}, "hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac", "text": "678 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[289] R. Yager, \u201cIntelligent control of the hierarchical agglomerative clus-\ntering process, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 30, no. 6, pp.\n835\u2013845, 2000.\n[290] R. Yager and D. Filev, \u201cApproximate clustering via the moun-\ntain method, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 24, no. 8, pp.\n1279 \u20131284, 1994.\n[291] K. Yeung, D. Haynor, and W. Ruzzo, \u201cValidating clustering for gene\nexpression data, \u201dBioinformatics , vol. 17, no. 4, pp. 309 \u2013318, 2001.\n[292] F. Young and R. Hamer, Multidimensional Scaling: History, Theory, and\nApplications . Hillsdale, NJ: Lawrence Erlbaum, 1987.\n[293] L. Zadeh, \u201cFuzzy sets, \u201dInf. Control , vol. 8, pp. 338 \u2013353, 1965.\n[294] J. Zhang and Y . Leung, \u201cImproved possibilistic C-means clustering al-\ngorithms, \u201dIEEE Trans. Fuzzy Syst. , vol. 12, no. 2, pp. 209 \u2013217, Apr.\n2004.\n[295] T. Zhang, R. Ramakrishnan, and M. Livny, \u201cBIRCH: An ef \ufb01cient data\nclustering method for very large databases, \u201dinProc. ACM SIGMOD\nConf. Management of Data , 1996, pp. 103 \u2013114.\n[296] Y . Zhang and Z. Liu, \u201cSelf-splitting competitive learning: A new on-line\nclustering paradigm, \u201dIEEE Trans. Neural Networks , vol. 13, no. 2, pp.\n369\u2013380, Mar. 2002.\n[297] X. Zhuang, Y . Huang, K. Palaniappan, and Y . Zhao, \u201cGaussian mixture\ndensity modeling, decomposition, and applications, \u201dIEEE Trans. Image\nProcess. , vol. 5, no. 9, pp. 1293 \u20131302, Sep. 1996.\nRui Xu (S\u201900) received the B.E. degree in electrical\nengineering from Huazhong University of Science\nand Technology, Wuhan, Hubei, China, in 1997,\nand the M.E. degree in electrical engineering from\nSichuan University, Chengdu, Sichuan, in 2000.\nHe is currently pursuing the Ph.D. degree in the\nDepartment of Electrical and Computer Engineering,\nUniversity of Missouri-Rolla.\nHis research interests include machine learning,\nneural networks, pattern classi \ufb01cation and clustering,\nand bioinformatics.\nMr. Xu is a Student Member of the IEEE Computational Intelligence Society,\nEngineering in Medicine and Biology Society, and the International Society for\nComputational Biology.\nDonald C. Wunsch II (S\u201987\u2013M\u201992\u2013SM\u201994\u2013F\u201905)\nreceived the B.S. degree in applied mathematics\nfrom the University of New Mexico, Albuquerque,\nand the M.S. degree in applied mathematics and\nthe Ph.D. degree in electrical engineering from the\nUniversity of Washington, Seattle.\nHeis the Mary K. Finley Missouri Distinguished\nProfessor of Computer Engineering, University\nof Missouri-Rolla, where he has been since 1999.\nHis prior positions were Associate Professor and\nDirector of the Applied Computational Intelligence\nLaboratory, Texas Tech University, Lubbock; Senior Principal Scientist,\nBoeing; Consultant, Rockwell International; and Technician, International\nLaser Systems. He has well over 200 publications, and has attracted over $5\nmillion in research funding. He has produced eight", "start_char_idx": 0, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "11b43a7e-b2cb-4edb-854f-234a6e38fb90": {"__data__": {"id_": "11b43a7e-b2cb-4edb-854f-234a6e38fb90", "embedding": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b2dccad2-a69d-41e5-8d00-adc812359e09", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "2": {"node_id": "fa30bdbf-604b-4ce6-ae49-75bfa61db9c4", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac"}}, "hash": "b143a149bccb0c99f5aa8380310d5dc29001d829a439bdcf4fc642b1f2e377a3", "text": "publications, and has attracted over $5\nmillion in research funding. He has produced eight Ph.D. recipients \u2014four in\nelectrical engineering, three in computer engineering, and one in computer\nscience.\nDr. Wunsch has received the Halliburton Award for Excellence in Teaching\nand Research, and the National Science Foundation CAREER Award. He served\nas a V oting Member of the IEEE Neural Networks Council, Technical Program\nCo-Chair for IJCNN \u201902, General Chair for IJCNN \u201903, International Neural Net-\nworks Society Board of Governors Member, and is now President of the Inter-national Neural Networks Society.", "start_char_idx": 2814, "end_char_idx": 3424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f6bafeb-afa6-488b-b63f-fd4971a1f0bb": {"__data__": {"id_": "0f6bafeb-afa6-488b-b63f-fd4971a1f0bb", "embedding": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f96d6ac3-6f73-4e3e-ace1-a77a1d5e771e", "node_type": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "text": "Journalof Machine LearningResearch 3(2003)1157-1182 Submitted 11/02;Published3/03\nAnIntroduction to Variableand Feature Selection\nIsabelleGuyon ISABELLE @CLOPINET .COM\nClopinet\n955 Creston Road\nBerkeley, CA 94708-1501, USA\nAndr\u00b4eElisseeff ANDRE@TUEBINGEN .MPG.DE\nEmpirical Inference for Machine Learning and Perception De partment\nMax Planck InstituteforBiological Cybernetics\nSpemannstrasse 38\n72076 T\u00a8ubingen, Germany\nEditor:Leslie Pack Kaelbling\nAbstract\nVariable and feature selection have become the focus of much research in areas of application for\nwhich datasets with tens or hundreds of thousands of variabl es are available. These areas include\ntextprocessingofinternetdocuments,geneexpressionarr ayanalysis,andcombinatorialchemistry.\nThe objective of variable selection is three-fold: improvi ng the prediction performance of the pre-\ndictors,providingfasterandmorecost-effectivepredict ors,andprovidingabetterunderstandingof\nthe underlying process that generated the data. The contrib utions of this special issue cover a wide\nrange of aspects of such problems: providing a better de\ufb01nit ion of the objective function, feature\nconstruction, feature ranking, multivariate feature sele ction, ef\ufb01cient search methods, and feature\nvalidity assessment methods.\nKeywords: Variable selection, feature selection, space dimensional ity reduction, pattern discov-\nery, \ufb01lters, wrappers, clustering, information theory, su pport vector machines, model selection,\nstatistical testing, bioinformatics, computational biol ogy, gene expression, microarray, genomics,\nproteomics, QSAR, text classi\ufb01cation, information retrie val.\n1 Introduction\nAs of 1997, when a special issue on relevance including several pape rs on variable and feature\nselection was published (Blum and Langley, 1997, Kohavi and John, 19 97), few domains explored\nused more than 40 features. The situation has changed considerably in the past few years and, in\nthis special issue, most papers explore domains with hundreds to tens of tho usands of variables or\nfeatures:1Newtechniquesareproposedtoaddressthesechallengingtasksinvolvin gmanyirrelevant\nandredundant variablesandoftencomparably fewtrainingexamples.\nTwoexamplesaretypicalofthenewapplicationdomainsandserveusasillustr ationthroughout\nthis introduction. One is gene selection from microarray data and the other is te xt categorization.\nIn the gene selection problem, the variables are gene expression coef\ufb01c ients corresponding to the\n1. We call \u201cvariable\u201d the \u201craw\u201d input variables and \u201cfeatures\u201d variable s constructed for the input variables. We use\nwithoutdistinctiontheterms\u201cvariable\u201dand\u201cfeature\u201dwhenthereisnoimpac tontheselectionalgorithms,e.g.,when\nfeatures resulting from a pre-processing of input variables are explic itly computed. The distinction is necessary in\nthe case ofkernel methods forwhich features are notexplicitly compute d (seesection 5.3).\nc/circlecopyrt2003Isabelle Guyonand Andr \u00b4eElisseeff.", "start_char_idx": 0, "end_char_idx": 2946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a96cdbe-16ea-487c-aa14-7d502b1f171e": {"__data__": {"id_": "8a96cdbe-16ea-487c-aa14-7d502b1f171e", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "3": {"node_id": "8ac67088-1023-4df3-9a7d-baf022cf4b90", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58"}}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "text": "GUYON AND ELISSEEFF\nabundance of mRNA in a sample (e.g. tissue biopsy), for a number of patients . A typical clas-\nsi\ufb01cation task is to separate healthy patients from cancer patients, based on their gene expression\n\u201cpro\ufb01le\u201d. Usually fewer than 100 examples (patients) are available altogeth er for training and test-\ning. But, the number of variables in the raw data ranges from 6000 to 60,000 . Some initial \ufb01ltering\nusually brings the number of variables to a few thousand. Because the abu ndance of mRNA varies\nbyseveralordersofmagnitudedependingonthegene,thevariablesar eusuallystandardized. Inthe\ntext classi\ufb01cation problem, the documents are represented by a \u201cbag-of- words\u201d, that is a vector of\ndimensionthesizeofthevocabularycontainingwordfrequencycounts(p ropernormalizationofthe\nvariables also apply). Vocabularies of hundreds of thousands of wor ds are common, but an initial\npruning of the most and least frequent words may reduce the effective number of words to 15,000.\nLargedocumentcollections of5000to800,000documents areavailablefor research. Typicaltasks\ninclude the automatic sorting of URLs into a web directory and the detection of un solicited email\n(spam). Foralistofpubliclyavailabledatasetsusedinthisissue,seeTable1 attheendofthepaper.\nTherearemanypotentialbene\ufb01tsofvariableandfeatureselection: facilita tingdatavisualization\nanddataunderstanding,reducingthemeasurementandstoragerequire ments,reducingtrainingand\nutilization times, defying the curse of dimensionality to improve prediction performa nce. Some\nmethods put more emphasis on one aspect than another, and this is another p oint of distinction\nbetweenthisspecialissueandpreviouswork. Thepapersinthisissuefo cusmainlyonconstructing\nand selecting subsets of features that areusefulto build a good predictor. This contrasts with the\nproblemof\ufb01ndingorrankingallpotentiallyrelevantvariables. Selectingth emostrelevantvariables\nisusuallysuboptimalforbuildingapredictor,particularlyifthevariablesare redundant. Conversely,\na subset of useful variables may exclude many redundant, but relevan t, variables. For a discussion\nofrelevance vs.usefulnessandde\ufb01nitionsofthevariousnotionsofrelevance,seethere viewarticles\nof KohaviandJohn(1997)andBlum andLangley(1997).\nThis introduction surveys the papers presented in this special issue. The depth of treatment of\nvarioussubjectsre\ufb02ectstheproportionofpaperscoveringthem: thepro blemofsupervisedlearning\nis treated more extensively than that of unsupervised learning; classi\ufb01ca tion problems serve more\noftenasillustrationthanregressionproblems,andonlyvectorialinputdata isconsidered. Complex-\nity is progressively introduced throughout the sections: The \ufb01rst sectio n starts by describing \ufb01lters\nthat select variables by ranking them with correlation coef\ufb01cients (Section 2). Limitations of such\napproaches are illustrated by a set of constructed examples (Section 3). Subset selection methods\narethenintroduced(Section4). Theseinclude wrappermethods thatassesssubsetsofvariablesac-\ncording to their usefulness to a given predictor. We show how some embedd ed methods implement\nthesameidea,butproceedmoreef\ufb01cientlybydirectlyoptimizingatwo-partob jectivefunctionwith\nagoodness-of-\ufb01ttermandapenaltyforalargenumberofvariables. W ethenturntotheproblemof\nfeature construction, whose goals include increasing the predictor perf ormance and building more\ncompact feature subsets (Section 5). All of the previous steps bene\ufb01t f rom reliably assessing", "start_char_idx": 0, "end_char_idx": 3465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8ac67088-1023-4df3-9a7d-baf022cf4b90": {"__data__": {"id_": "8ac67088-1023-4df3-9a7d-baf022cf4b90", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "2": {"node_id": "8a96cdbe-16ea-487c-aa14-7d502b1f171e", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425"}}, "hash": "cf2905a06d4f5f10c2816c9ae41aab79f870d6e7d9ebe042d7226d9653b33f58", "text": "5). All of the previous steps bene\ufb01t f rom reliably assessing the\nstatistical signi\ufb01cance of the relevance of features. We brie\ufb02y review mode l selection methods and\nstatisticaltestsusedtothateffect(Section6). Finally,weconcludethepap erwithadiscussionsec-\ntion in which we go over more advanced issues (Section 7). Because the or ganization of our paper\ndoes not follow the work \ufb02ow of building a machine learning application, we summa rize the steps\nthatmaybe takentosolveafeatureselectionprobleminacheck list2:\n2. We caution the reader that this check list is heuristic. The only recommen dation that is almost surely valid is to try\nthe simplestthings \ufb01rst.\n1158", "start_char_idx": 3404, "end_char_idx": 4070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "35ab5ff0-9e87-4d1b-9502-358109c33b21": {"__data__": {"id_": "35ab5ff0-9e87-4d1b-9502-358109c33b21", "embedding": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d6bcaba-3b0b-4c08-b13a-7a0f0608a212", "node_type": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n1.Do youhavedomain knowledge? Ifyes,constructabetter setof \u201cad hoc\u201dfeatures.\n2.Are yourfeaturescommensurate? If no,consider normalizingthem.\n3.Doyoususpectinterdependenceoffeatures? Ifyes,expandyourfeaturesetbyconstructing\nconjunctive features or products of features, as much as your compute r resources allow you\n(seeexampleof useinSection4.4).\n4.Do you need to prune the input variables (e.g. for cost, speed or data understanding rea-\nsons)? If no, construct disjunctive features or weighted sums of featu res (e.g. by clustering\nor matrixfactorization,seeSection5).\n5.Doyouneedtoassessfeaturesindividually (e.g. tounderstandtheirin\ufb02uenceonthesystem\nor because their number is so large that you need to do a \ufb01rst \ufb01ltering)? If y es, use a variable\nrankingmethod (Section2andSection7.2);else,doitanyway togetbaselin eresults.\n6.Do youneed apredictor? Ifno,stop.\n7.Do you suspect your data is \u201cdirty\u201d (has a few meaningless input patterns and/or noisy\noutputs or wrong class labels)? If yes, detect the outlier examples using the top ranking\nvariables obtainedinstep5as representation;check and/or discardthem.\n8.Doyouknowwhattotry\ufb01rst? Ifno,usealinearpredictor.3Useaforwardselectionmethod\n(Section 4.2) with the \u201cprobe\u201d method as a stopping criterion (Section 6) or us e the/lscript0-norm\nembedded method (Section 4.3). For comparison, following the ranking of ste p 5, construct\na sequence of predictors of same nature using increasing subsets of fe atures. Can you match\nor improve performance with a smaller subset? If yes, try a non-linear pred ictor with that\nsubset.\n9.Do you have new ideas, time, computational resources, and enoug h examples? If yes,\ncompare several feature selection methods, including your new idea, cor relation coef\ufb01cients,\nbackwardselectionandembeddedmethods(Section4). Uselinearandnon -linearpredictors.\nSelect thebestapproachwithmodel selection(Section6).\n10.Do you want a stable solution (to improve performance and/or understanding)? If yes, sub-\nsampleyour dataand redoyour analysisforseveral\u201cbootstraps\u201d(Se ction7.1).\n2 Variable Ranking\nMany variable selection algorithms include variable ranking as a principal or auxiliary selection\nmechanism because of its simplicity, scalability, and good empirical success. S everal papers in this\nissue use variable ranking as a baseline method (see, e.g., Bekkerman et a l., 2003, Caruana and\nde Sa, 2003, Forman, 2003, Weston et al., 2003). Variable ranking is no t necessarily used to build\npredictors. One of its common uses in the microarray analysis domain is to discov er a set of drug\nleads (see, e.g., et al., 1999): A ranking criterion is used to \ufb01nd genes tha t discriminate between\nhealthy and disease patients; such genes may code for \u201cdrugable\u201d prote ins, or proteins that may\n3. By \u201clinear predictor\u201d we mean linear in the parameters. Feature constr uction may render the predictor non-linear in\nthe inputvariables.\n1159", "start_char_idx": 0, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "95340ff0-7c82-453e-bcf4-dea30b5beb85": {"__data__": {"id_": "95340ff0-7c82-453e-bcf4-dea30b5beb85", "embedding": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a75edbeb-8b17-4b85-aa05-bf0d6cc84390", "node_type": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "text": "GUYON AND ELISSEEFF\nthemselves be used as drugs. Validating drug leads is a labor intensive pro blem in biology that is\noutside of the scope of machine learning, so we focus here on building pre dictors. We consider in\nthis section ranking criteria de\ufb01ned for individual variables, independe ntly of the context of others.\nCorrelationmethodsbelongtothatcategory. Wealsolimitourselvestosuperv isedlearningcriteria.\nWerefer thereader toSection7.2for adiscussionof othertechniques.\n2.1 Principleofthe Method andNotations\nConsider a set of mexamples {xk,yk}(k=1,...m) consisting of ninput variables xk,i(i=1,...n)\nand one output variable yk. Variable ranking makes use of a scoring function S(i)computed from\nthe values xk,iandyk,k=1,...m. By convention, we assume that a high score is indicative of a\nvaluable variable and that we sort variables in decreasing order of S(i). To use variable ranking to\nbuild predictors, nested subsets incorporating progressively more and more variables of decreasing\nrelevance are de\ufb01ned. We postpone until Section 6 the discussion of sele cting an optimum subset\nsize.\nFollowingthe classi\ufb01cationof Kohaviand John(1997),variableranking isa\ufb01ltermethod: itis\napreprocessingstep,independentofthechoiceofthepredictor. Still,u ndercertainindependenceor\northogonality assumptions, it may be optimal with respect to a given predictor. For instance, using\nFisher\u2019scriterion4torankvariablesinaclassi\ufb01cationproblemwherethecovariancematrixisdia g-\nonal is optimum for Fisher\u2019s linear discriminant classi\ufb01er (Duda et al., 2001 ). Even when variable\nranking is not optimal, it may be preferable to other variable subset selection methods because of\nitscomputationalandstatisticalscalability: Computationally,itisef\ufb01cientsinceitr equiresonlythe\ncomputation of nscores and sorting the scores; Statistically, it is robust against over\ufb01tting because\nitintroduces biasbutitmayhave considerablyless variance(Hastieetal., 2001).5\nWeintroducesomeadditionalnotation: Iftheinputvector xcanbeinterpretedastherealization\nof a random vector drawn from an underlying unknown distribution, we d enote by Xithe random\nvariablecorrespondingtothe ithcomponentof x. Similarly, Ywillbetherandomvariableofwhich\nthe outcome yis a realization. We further denote by xithemdimensional vector containing all\nthe realizations of the ithvariable for the training examples, and by ythemdimensional vector\ncontaining allthetargetvalues.\n2.2 Correlation Criteria\nLet us consider \ufb01rst the prediction of a continuous outcome y. The Pearson correlation coef\ufb01cient\nisde\ufb01ned as:\nR(i) =cov(Xi,Y)/radicalbig\nvar(Xi)var(Y), (1)\nwherecovdesignatesthecovariance and varthevariance. Theestimateof R(i)is givenby:\nR(i) =\u2211m\nk=1(xk,i\u2212\u00afxi)(yk\u2212\u00afy)/radicalbig\n\u2211m\nk=1(xk,i\u2212\u00afxi)2\u2211m\nk=1(yk\u2212\u00afy)2, (2)\n4. The ratioof thebetween class variance tothe within-class variance.\n5. The similarity of variable ranking to the ORDERED-FS algorithm (Ng, 1998 ) indicates that its sample complexity\nmay be logarithmic in the number of irrelevant features, compared to a po wer law for \u201cwrapper\u201d subset selection\nmethods. This would mean that variable ranking can tolerate a number of ir relevant variables exponential in the\nnumber of trainingexamples.\n1160", "start_char_idx": 0, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "715b60c0-0a1f-4154-ae38-7ed6b53d77fb": {"__data__": {"id_": "715b60c0-0a1f-4154-ae38-7ed6b53d77fb", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ce75a46-ef3b-4b59-9f97-770e1270b528", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "3": {"node_id": "5d061b8c-a9a9-4c63-9046-778e3d59773d", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb"}}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nwhere the bar notation stands for an average over the index k. This coef\ufb01cient is also the cosine\nbetween vectors xiandy, after they have been centered (their mean subtracted). Although the R(i)\nis derived from R(i)it may be used without assuming that the input values are realizations of a\nrandomvariable.\nIn linear regression, the coef\ufb01cient of determination, which is the square ofR(i), represents the\nfractionofthetotalvariancearoundthemeanvalue \u00af ythatisexplainedbythelinearrelationbetween\nxiandy. Therefore, using R(i)2as a variable ranking criterion enforces a ranking according to\ngoodness oflinear \ufb01tof individualvariables.6\nThe use of R(i)2can be extended to the case of two-class classi\ufb01cation, for which each cla ss\nlabel is mapped to a given value of y, e.g., \u00b11.R(i)2can then be shown to be closely related to\nFisher\u2019s criterion (Furey et al., 2000), to the T-test criterion, and other similar criteria (see, e.g.,\net al., 1999, Tusher et al., 2001, Hastie et al., 2001). As further develo ped in Section 6, the link\nto the T-test shows that the score R(i)may be used as a test statistic to assess the signi\ufb01cance of a\nvariable.\nCorrelation criteria such as R(i)can only detect linear dependencies between variable and tar-\nget. A simple way of lifting this restriction is to make a non-linear \ufb01t of the target with single\nvariablesandrankaccordingtothegoodnessof\ufb01t. Becauseoftherisk ofover\ufb01tting,onecanalter-\nnatively consider using non-linear preprocessing (e.g., squaring, tak ing the square root, the log, the\ninverse, etc.) and then using a simple correlation coef\ufb01cient. Correlation cr iteria are often used for\nmicroarraydataanalysis,as illustratedin thisissuebyWestonet al.(2003).\n2.3 SingleVariable Classi\ufb01ers\nAsalreadymentioned,using R(i)2asarankingcriterionfor regression enforcesarankingaccording\ntogoodnessoflinear\ufb01tofindividualvariables. Onecanextendtothe classi\ufb01cation casetheideaof\nselectingvariablesaccordingtotheirindividualpredictivepower,using ascriteriontheperformance\nofaclassi\ufb01erbuiltwithasinglevariable. Forexample,thevalueofthevariab leitself(oritsnegative,\ntoaccountforclasspolarity)canbeusedasdiscriminantfunction. Aclas si\ufb01erisobtainedbysetting\nathreshold \u03b8onthevalueofthevariable(e.g.,atthemid-pointbetweenthecenterofgrav ityofthe\ntwoclasses).\nThe predictive power of the variable can be measured in terms of error rate . But, various other\ncriteria can be de\ufb01ned that involve false positive classi\ufb01cation rate fprand false negative classi\ufb01-\ncation rate fnr. The tradeoff between fprandfnris monitored in our simple example by varying\nthe threshold \u03b8. ROC curves that plot \u201chit\u201d rate (1-fpr)as a function of \u201cfalse alarm\u201d rate fnrare\ninstrumental in de\ufb01ning criteria such as: The \u201cBreak Even Point\u201d (the hit ra te for a threshold value\ncorrespondingto fpr=fnr)andthe \u201cAreaUnder Curve\u201d(thearea under theROCcurve).\nIn the case where there is a large number of variables that separate the da ta perfectly, ranking\ncriteria based on classi\ufb01cation success rate cannot distinguish between th e top ranking variables.\nOnewillthenprefertouseacorrelationcoef\ufb01cientoranotherstatisticliketh emargin(thedistance\nbetween", "start_char_idx": 0, "end_char_idx": 3198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5d061b8c-a9a9-4c63-9046-778e3d59773d": {"__data__": {"id_": "5d061b8c-a9a9-4c63-9046-778e3d59773d", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9ce75a46-ef3b-4b59-9f97-770e1270b528", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "2": {"node_id": "715b60c0-0a1f-4154-ae38-7ed6b53d77fb", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086"}}, "hash": "c1483ecb0cf89d8a65f07f3137779cd7d512fa594c0fed763fb065f0ed9f74bb", "text": "emargin(thedistance\nbetween theexamples of oppositeclassesthatareclosesttooneanother f oragivenvariable).\n6. Avariantofthisideaistousethemean-squared-error,but,ifthevar iablesarenotoncomparablescales,acomparison\nbetween mean-squared-errors is meaningless. Another variant is to u seR(i)to rank variables, not R(i)2. Positively\ncorrelated variables are then top ranked and negatively correlated var iables bottom ranked. With this method, one\ncan choose asubsetof variables withagiven proportion of positivelya nd negatively correlated variables.\n1161", "start_char_idx": 3171, "end_char_idx": 3721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6062226d-a498-404a-ad40-71d80ab1339c": {"__data__": {"id_": "6062226d-a498-404a-ad40-71d80ab1339c", "embedding": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "de8c06fd-f6b5-4d1c-82fe-63ff05835138", "node_type": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "text": "GUYON AND ELISSEEFF\nThe criteria described in this section extend to the case of binary variables. Forman (2003)\npresentsinthisissueanextensivestudyofsuchcriteriaforbinaryvaria bleswithapplicationsintext\nclassi\ufb01cation.\n2.4 InformationTheoretic RankingCriteria\nSeveral approaches to the variable selection problem using information the oretic criteria have been\nproposed (as reviewed in this issue by Bekkerman et al., 2003, Dhillon et a l., 2003, Forman, 2003,\nTorkkola,2003). Manyrelyonempiricalestimatesofthemutualinformationbe tweeneachvariable\nandthe target:\nI(i) =Z\nxiZ\nyp(xi,y)logp(xi,y)\np(xi)p(y)dxdy, (3)\nwherep(xi)andp(y)are the probability densities of xiandy, andp(xi,y)is the joint density. The\ncriterion I(i)is a measure of dependency between the density of variable xiand the density of the\ntargety.\nThe dif\ufb01culty is that the densities p(xi),p(y)andp(xi,y)are all unknown and are hard to\nestimatefromdata. Thecaseofdiscreteornominalvariablesisprobablyea siestbecausetheintegral\nbecomes asum:\nI(i) =\u2211\nxi\u2211\nyP(X=xi,Y=y)logP(X=xi,Y=y)\nP(X=xi)P(Y=y). (4)\nThe probabilities are then estimated from frequency counts. For example, in a three-class\nproblem, if a variable takes 4 values, P(Y=y)represents the class prior probabilities (3 fre-\nquency counts), P(X=xi)represents the distribution of the input variable (4 frequency counts),\nandP(X=xi,Y=y)istheprobabilityofthejointobservations(12frequencycounts). Thees tima-\ntionobviouslybecomes harder withlarger numbersof classesandvariable values.\nThe case of continuous variables (and possibly continuous targets) is the hardest. One can\nconsider discretizing the variables or approximating their densities with a non- parametric method\nsuch as Parzen windows (see, e.g., Torkkola, 2003). Using the normal distribution to estimate\ndensities would bring us back to estimating the covariance between XiandY, thus giving us a\ncriterionsimilartoacorrelationcoef\ufb01cient.\n3 Small but RevealingExamples\nWe present a series of small examples that outline the usefulness and the limitatio ns of variable\nranking techniques and present several situations in which the variable d ependencies cannot be\nignored.\n3.1 Can PresumablyRedundantVariables Help Each Other?\nOnecommoncriticismofvariablerankingisthatitleadstotheselectionofaredu ndantsubset. The\nsame performance could possibly be achieved with a smaller subset of comple mentary variables.\nStill, one may wonder whether adding presumably redundant variables can result in a performance\ngain.\n1162", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a700928c-a33b-44d5-a855-4bf3702f94f1": {"__data__": {"id_": "a700928c-a33b-44d5-a855-4bf3702f94f1", "embedding": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f8d615b7-a3ff-4a66-9db1-90b424c1f073", "node_type": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22126\u22124\u221220246 \u22125 0 5\u22126\u22124\u221220246\u2212505\n(a) (b)\nFigure1: Informationgainfrompresumablyredundantvariables. (a)Atwoclassproblemwith\nindependently and identically distributed (i.i.d.) variables. Each class has a Ga ussian distribution\nwith no covariance. (b) The same example after a 45 degree rotation showin g that a combination\nof the two variables yields a separation improvement by a factor\u221a\n2. I.i.d. variables are not truly\nredundant.\nConsider the classi\ufb01cation problem of Figure 1. For each class, we drew at random m=100\nexamples,eachofthetwovariablesbeingdrawnindependentlyaccording toanormaldistributionof\nstandarddeviation1. Theclasscentersareplacedatcoordinates(-1; -1)and(1;1). Figure1.ashows\nthescatterplotinthetwo-dimensionalspaceoftheinputvariables. Wealsos howonthesame\ufb01gure\nhistogramsoftheprojectionsoftheexamplesontheaxes. Tofacilitateitsreadin g,thescatterplotis\nshowntwicewithanaxisexchange. Figure1.bshowsthesamescatterplotsaf teraforty\ufb01vedegree\nrotation. Inthisrepresentation,thex-axisprojectionprovidesabettersep arationofthetwoclasses:\nthe standard deviation of both classes is the same, but the distance between c enters in projection is\nnow 2\u221a\n2 instead of 2. Equivalently, if we rescale the x-axis by dividing by\u221a\n2 to obtain a feature\nthat is the average of the two input variables, the distance between centers is still 2, but the within\nclass standard deviation is reduced by a factor\u221a\n2. This is not so surprising, since by averaging n\ni.i.d. random variables we will obtain a reduction of standard deviation by a fa ctor of\u221an.Noise\nreduction and consequently better class separation may be obtain ed by adding variables that\nare presumably redundant. Variables that are independently and identically distributed are not\ntrulyredundant.\n3.2 HowDoes Correlation ImpactVariable Redundancy?\nAnother notion of redundancy is correlation. In the previous example, in s pite of the fact that the\nexamples are i.i.d. with respect to the class conditional distributions, the variab les are correlated\nbecause of the separation of the class center positions. One may wonder h ow variable redundancy\nis affected by adding within-class variable correlation. In Figure 2, the cla ss centers are positioned\n1163", "start_char_idx": 0, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1501de03-1484-4e57-b4af-bbf848b1012c": {"__data__": {"id_": "1501de03-1484-4e57-b4af-bbf848b1012c", "embedding": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "98401388-468a-4828-a610-37ab956db0bb", "node_type": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n(a) (b)\nFigure2: Intra-classcovariance. Inprojectionontheaxes,thedistributionsofthetwovariablesare\nthe same as in the previous example. (a) The class conditional distributions hav e a high covariance\ninthedirectionofthelineofthetwoclasscenters. Thereisnosigni\ufb01cantgain inseparationbyusing\ntwo variables instead of just one. (b) The class conditional distributions ha ve a high covariance in\nthe direction perpendicular to the line of the two class centers. An important se paration gain is\nobtained byusingtwovariables insteadofone.\nsimilarly as in the previous example at coordinates (-1; -1) and (1; 1) but w e have added some\nvariableco-variance. Weconsider twocases:\nIn Figure 2.a, in the direction of the class center line, the standard deviation o f the class condi-\ntional distributions is\u221a\n2, while in the perpendicular direction it is a small value ( \u03b5=1/10). With\nthis construction, as \u03b5goes to zero, the input variables have the same separation power as in the\ncase of the example of Figure 1, with a standard deviation of the class distribu tions of one and a\ndistance of the class centers of 2. But the feature constructed as the sum of the input variables has\nnobetterseparationpower: astandarddeviationof\u221a\n2andaclasscenterseparationof2\u221a\n2(asim-\nple scaling that does not change the separation power). Therefore, in the limit of perfect variable\ncorrelation (zero variance in the direction perpendicular to the class cente r line), single variables\nprovide the same separation as the sum of the two variables. Perfectly correlated variables are\ntrulyredundant in thesensethatnoadditionalinformationis gained byaddingthem.\nIn contrast, in the example of Figure 2.b, the \ufb01rst principal direction of the c ovariance matrices\nof the class conditional densities is perpendicular to the class center line. In th is case, more is\ngainedbyaddingthetwovariablesthanintheexampleofFigure1. Onenotices thatinspiteoftheir\ngreatcomplementarity(inthesensethataperfectseparationcanbeachie vedinthetwo-dimensional\nspace spanned by the two variables), the two variables are (anti-)corre lated. More anti-correlation\nis obtained by making the class centers closer and increasing the ratio of the v ariances of the class\nconditional distributions. Very high variable correlation (or anti-correlation) does not mean\nabsenceof variablecomplementarity.\n1164", "start_char_idx": 0, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c5d0b69-48fb-4116-a3a7-180340fc4f6a": {"__data__": {"id_": "6c5d0b69-48fb-4116-a3a7-180340fc4f6a", "embedding": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "469995a2-27b0-416f-9e96-02aa527d4810", "node_type": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nThe examples of Figure 1 and 2 all have variables with the same distribution of e xamples (in\nprojection on the axis). Therefore, methods that score variables individ ually and independently of\neach other areatlosstodeterminewhichcombination of variableswouldgive bestperformance.\n3.3 Can aVariable thatis Useless byItself beUsefulwith Oth ers?\nOneconcernaboutmultivariatemethodsisthattheyarepronetoover\ufb01tting. Theproblemisaggra-\nvated when the number of variables to select from is large compared to the nu mber of examples.\nIt is tempting to use a variable ranking method to \ufb01lter out the least promising varia bles before us-\ning a multivariate method. Still one may wonder whether one could potentially lose s ome valuable\nvariablesthroughthat \ufb01lteringprocess.\nWe constructed an example in Figure 3.a. In this example, the two class condition al distribu-\ntions have identical covariance matrices, and the principal directions are o riented diagonally. The\nclass centers are separated on one axis, but not on the other. By itself o ne variable is \u201cuseless\u201d.\nStill, the two dimensional separation is better than the separation using the \u201cusef ul\u201d variable alone.\nTherefore, a variable that is completely useless by itself can provide a signi\ufb01cant p erformance\nimprovementwhen takenwith others.\nThe next question is whether two variables that are useless by themselves c an provide a good\nseparation when taken together. We constructed an example of such a cas e, inspired by the famous\nXOR problem.7In Figure 3.b, we drew examples for two classes using four Gaussians pla ced on\nthe corners of a square at coordinates (0; 0), (0; 1), (1; 0), and ( 1; 1). The class labels of these four\n\u201cclumps\u201dareattributedaccordingtothetruthtableofthelogicalXORfunction: f(0;0)=0,f(0;1)=1,\nf(1; 0)=1; f(1; 1)=0. We notice that the projections on the axes provide no class separation. Yet,\nin the two dimensional space the classes can easily be separated (albeit not with a linear decision\nfunction).8Twovariables thatare uselessbythemselvescan beusefultogeth er.\n7. The XOR problem is sometimes referred to as the two-bit parity problem a nd is generalizable to more than two\ndimensions (n-bit parity problem). A related problem is the chessboard p roblem in which the two classes pave\nthe space with squares of uniformly distributed examples with alternating clas s labels. The latter problem is also\ngeneralizable to the multi-dimensional case. Similar examples are used in se veral papers in this issue (Perkins et al.,\n2003, Stoppiglia etal., 2003).\n8. Incidentally, thetwo variables are alsouncorrelated with oneanother.\n1165", "start_char_idx": 0, "end_char_idx": 2666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "deb80a10-df07-48b3-905c-313c4bb4a014": {"__data__": {"id_": "deb80a10-df07-48b3-905c-313c4bb4a014", "embedding": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "56c156b3-8227-4439-8ae8-9562c6d3cad9", "node_type": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22122\u22121 012\u2212505\u22122\u22121012\n\u22120.5 00.511.5 \u22120.5 00.511.5\u22120.500.511.5\u22120.500.511.5\n(a) (b)\nFigure 3: A variable useless by itself can be useful together with others. (a) One variable has\ncompletely overlapping class conditional densities. Still, using it jointly with the othe r variable\nimprovesclassseparabilitycomparedtousingtheothervariablealone. (b)X OR-likeorchessboard-\nlike problems. The classes consist of disjoint clumps such that in projection o n the axes the class\nconditional densities overlap perfectly. Therefore, individual variab les have no separation power.\nStill,takentogether,thevariablesprovidegood classseparability.\n4 Variable Subset Selection\nIn the previous section, we presented examples that illustrate the usefulnes s of selecting subsets\nof variables that together have good predictive power, as opposed to r anking variables according\nto their individual predictive power. We now turn to this problem and outline th e main directions\nthat have been taken to tackle it. They essentially divide into wrappers, \ufb01lter s, and embedded\nmethods. Wrappers utilize the learning machine of interest as a black box to score subsets of\nvariable according to their predictive power. Filtersselect subsets of variables as a pre-processing\nstep, independently of the chosen predictor. Embedded methods perform variable selection in the\nprocessof trainingandareusuallyspeci\ufb01ctogivenlearningmachines.\n4.1 WrappersandEmbeddedMethods\nThe wrapper methodology, recently popularized by Kohavi and John (1 997), offers a simple and\npowerful way to address the problem of variable selection, regardless of the chosen learning ma-\nchine. In fact, the learning machine is considered a perfect black box and the method lends itself\nto the use of off-the-shelf machine learning software packages. In its mos t general formulation, the\nwrapper methodology consists in using the prediction performance of a giv en learning machine to\nassess the relative usefulness of subsets of variables. In practice, o ne needs to de\ufb01ne: (i) how to\nsearch the space of all possible variable subsets; (ii) how to assess the p rediction performance of\na learning machine to guide the search and halt it; and (iii) which predictor to us e. An exhaustive\nsearch can conceivably be performed, if the number of variables is not too large. But, the problem\n1166", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e47ab53c-3b04-43e5-af6b-0ba1a3fce25f": {"__data__": {"id_": "e47ab53c-3b04-43e5-af6b-0ba1a3fce25f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4974486-f0b2-4bd0-bc37-2bf4d107babd", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "3": {"node_id": "4d33b75c-78ed-4dbd-abb7-88a2110a240a", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de"}}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nisknowntobeNP-hard(AmaldiandKann,1998)andthesearchbecomes quicklycomputationally\nintractable. A wide range of search strategies can be used, including bes t-\ufb01rst, branch-and-bound,\nsimulated annealing, genetic algorithms (see Kohavi and John, 1997, for a review). Performance\nassessments are usually done using a validation set or by cross-validation (see Section 6). As il-\nlustrated in this special issue, popular predictors include decision trees, n a\u00a8\u0131ve Bayes, least-square\nlinear predictors,and supportvector machines.\nWrappersareoftencriticizedbecausetheyseemtobea\u201cbruteforce\u201dme thodrequiringmassive\namountsofcomputation,butitisnotnecessarilyso. Ef\ufb01cientsearchstra tegiesmaybedevised. Us-\ningsuchstrategiesdoesnotnecessarilymeansacri\ufb01cingpredictionperf ormance. Infact,itappears\nto be the converse in some cases: coarse search strategies may alleviate the problem of over\ufb01tting,\nas illustrated for instance in this issue by the work of Reunanen (2003). Gr eedy search strategies\nseem to be particularly computationally advantageous and robust against o ver\ufb01tting. They come in\ntwo \ufb02avors: forward selection andbackward elimination . In forward selection, variables are pro-\ngressively incorporated into larger and larger subsets, whereas in ba ckward elimination one starts\nwith the set of all variables and progressively eliminates the least promising o nes.9Both methods\nyieldnestedsubsets of variables.\nBy using the learning machine as a black box, wrappers are remarkably un iversal and simple.\nBut embedded methods that incorporate variable selection as part of the tra ining process may be\nmore ef\ufb01cient in several respects: they make better use of the available da ta by not needing to split\nthetrainingdataintoatrainingandvalidationset;theyreachasolutionfasterby avoidingretraining\na predictor from scratch for every variable subset investigated. Embed ded methods are not new:\ndecision trees such as CART, for instance, have a built-in mechanism to per form variable selection\n(Breiman et al., 1984). The next two sections are devoted to two families of emb edded methods\nillustratedby algorithmspublishedinthisissue.\n4.2 NestedSubsetMethods\nSome embedded methods guide their search by estimating changes in the objectiv e function value\nincurredbymakingmovesinvariablesubsetspace. Combinedwithgreedys earchstrategies(back-\nwardeliminationor forwardselection) theyyieldnestedsubsetsof variables .10\nLet us call sthe number of variables selected at a given algorithm step and J(s)the value of\nthe objective function of the trained learning machine using such a variable s ubset. Predicting the\nchange intheobjectivefunctionis obtainedby:\n1.Finite difference calculation: The difference between J(s)andJ(s+1)orJ(s\u22121)is com-\nputed forthevariables thatarecandidates foraddition or removal.\n2.Quadratic approximation of the cost function: This method was originally proposed to\nprune weights in neural networks (LeCun et al., 1990). It can be used for backward elimi-\nnation of variables, via the pruning of the input variable weights wi. A second order Taylor\nexpansion of Jis made. At the optimum of J, the \ufb01rst-order term can be neglected, yield-\n9. The name greedy comes from the fact that one never revisits forme r decisions to include (or exclude) variables in\nlight of newdecisions.\n10. Thealgorithmspresentedinthissectionandinthefollowinggenerallyben", "start_char_idx": 0, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4d33b75c-78ed-4dbd-abb7-88a2110a240a": {"__data__": {"id_": "4d33b75c-78ed-4dbd-abb7-88a2110a240a", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4974486-f0b2-4bd0-bc37-2bf4d107babd", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "2": {"node_id": "e47ab53c-3b04-43e5-af6b-0ba1a3fce25f", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2"}}, "hash": "6da3542aa966be88bb9bfefc580ff0197389f2eef3a52f81c73f8d0d675823de", "text": "Thealgorithmspresentedinthissectionandinthefollowinggenerallyben e\ufb01tfromvariablenormalization,exceptif\nthey have an internalnormalization mechanism likethe Gram-Schmidtortho gonalization procedure .\n1167", "start_char_idx": 3354, "end_char_idx": 3557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8259bb22-fab5-404b-80f1-36527104ad33": {"__data__": {"id_": "8259bb22-fab5-404b-80f1-36527104ad33", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "750fd658-a6f1-4028-8d02-ad5d9ba341b1", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "3": {"node_id": "7e4bddcf-7e51-49e1-bf41-df22b57639b0", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce"}}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "text": "GUYON AND ELISSEEFF\ning for variable ito the variation DJi= (1/2)\u22022J\n\u2202w2\ni(Dwi)2. The change in weight Dwi=wi\ncorrespondstoremovingvariable i.\n3.Sensitivity of the objective function calculation: The absolute value or the square of the\nderivativeof Jwithrespectto xi(orwithrespectto wi) isused.\nSome training algorithms lend themselves to using \ufb01nite differences (method 1) be cause exact\ndifferencescanbecomputedef\ufb01ciently,withoutretrainingnewmodelsfor eachcandidatevariable.\nSuch is the case for the linear least-square model: The Gram-Schmidt orthog onolization procedure\npermits the performance of forward variable selection by adding at each s tep the variable that most\ndecreasesthemean-squared-error. Twopapersinthisissuearedev otedtothistechnique(Stoppiglia\netal.,2003,RivalsandPersonnaz,2003). Forotheralgorithmslikeker nelmethods,approximations\nof the difference can be computed ef\ufb01ciently. Kernel methods are learnin g machines of the form\nf(x) =\u2211m\nk=1\u03b1kK(x,xk), whereKis the kernel function, which measures the similarity between x\nandxk(Schoelkopf and Smola, 2002). The variation in J(s)is computed by keeping the \u03b1kvalues\nconstant. This procedure originally proposed for SVMs (Guyon et al., 2 002) is used in this issueas\nabaselinemethod (Rakotomamonjy,2003, Westonetal.,2003).\nThe \u201coptimum brain damage\u201d (OBD) procedure (method 2) is mentioned in this iss ue in the\npaper of Rivals and Personnaz (2003). The case of linear predictor sf(x) =w\u00b7x+bis particularly\nsimple. The authors of the OBD algorithm advocate using DJiinstead of the magnitude of the\nweights |wi|as pruning criterion. However, for linear predictors trained with an objec tive function\nJthatisquadraticin withesetwocriteriaareequivalent. Thisisthecase,forinstance,fortheline ar\nleast square model using J=\u2211m\nk=1(w\u00b7xk+b\u2212yk)2and for the linear SVM or optimum margin\nclassi\ufb01er, which minimizes J= (1/2)||w||2, under constraints (Vapnik, 1982). Interestingly, for\nlinear SVMs the \ufb01nite difference method (method 1) and the sensitivity method (me thod 3) also\nboildowntoselectingthevariablewithsmallest |wi|foreliminationateachstep(Rakotomamonjy,\n2003).\nThesensitivityoftheobjectivefunctiontochangesin wi(method3)isusedtodeviseaforward\nselection procedure in one paper presented in this issue (Perkins et al., 2 003). Applications of this\nproceduretoalinearmodelwithacross-entropyobjectivefunctionarep resented. Intheformulation\nproposed, the criterion is the absolute value of\u2202J\n\u2202wi=\u2211m\nk=1\u2202J\n\u2202\u03c1k\u2202\u03c1k\n\u2202wi, where\u03c1k=ykf(xk). In the case\nof the linear model f(x) =w\u00b7x+b, the criterion has a simple geometrical interpretation: it is the\nthedotproductbetweenthegradientoftheobjectivefunctionwithrespe cttothemarginvaluesand\nthevector [\u2202\u03c1k\n\u2202wi=xk,iyk]k=1...m. For thecross-entropylossfunction,wehave:\u2202J\n\u2202\u03c1k=1\n1+e\u03c1k.\nAn interesting variant of the sensitivity analysis method is obtained by replacin g the objective\nfunction by the leave-one-out cross-validation error. For some learning machines and some ob-\njective", "start_char_idx": 0, "end_char_idx": 2985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e4bddcf-7e51-49e1-bf41-df22b57639b0": {"__data__": {"id_": "7e4bddcf-7e51-49e1-bf41-df22b57639b0", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "750fd658-a6f1-4028-8d02-ad5d9ba341b1", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "2": {"node_id": "8259bb22-fab5-404b-80f1-36527104ad33", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668"}}, "hash": "cd462d5862196fac28808445b01d7cf86207c20d0593e2a6c3aa3d4e98347dce", "text": "cross-validation error. For some learning machines and some ob-\njective functions, approximate or exact analytical formulas of the leave-o ne-out error are known.\nIn this issue, the case of the linear least-square model (Rivals and Perso nnaz, 2003) and SVMs\n(Rakotomamonjy, 2003) are treated. Approximations for non-linear least-s quares have also been\ncomputed elsewhere (Monari and Dreyfus, 2000). The proposal of Rakotomamonjy (2003) is to\ntrain non-linear SVMs (Boser et al., 1992, Vapnik, 1998) with a regular tr aining procedure and\nselect features with backward elimination like in RFE (Guyon et al., 2002). Th e variable ranking\ncriterion however is not computed using the sensitivity of the objective func tionJ, but that of a\nleave-one-outbound.\n1168", "start_char_idx": 2914, "end_char_idx": 3675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b28bfdc-5df9-474a-96cd-f349ffc9c5fd": {"__data__": {"id_": "9b28bfdc-5df9-474a-96cd-f349ffc9c5fd", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c5779f8-061a-4543-8c4d-379530914867", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "3": {"node_id": "1b02b20c-795f-4a43-8df6-cad472d2f30a", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2"}}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n4.3 DirectObjective Optimization\nAlotofprogresshasbeenmadeinthisissuetoformalizetheobjectivefunction ofvariableselection\nand\ufb01ndalgorithmstooptimizeit. Generally,theobjectivefunctionconsistsoftwo termsthatcom-\npete with each other: (1) the goodness-of-\ufb01t (to be maximized), and (2) the number of variables\n(to be minimized). This approach bears similarity with two-part objective functio ns consisting of\na goodness-of-\ufb01t term and a regularization term, particularly when the ef fect of the regularization\nterm is to \u201cshrink\u201d parameter space. This correspondence is formally esta blished in the paper of\nWeston et al. (2003) for the particular case of classi\ufb01cation with linear pre dictorsf(x) =w\u00b7x+b,\nin the SVM framework (Boser et al., 1992, Vapnik, 1998). Shrinking reg ularizers of the type\n||w||p\np= (\u2211n\ni=1wp\ni)1/p(/lscriptp-norm) are used. In the limit as p\u21920, the /lscriptp-norm is just the number\nof weights, i.e., the number of variables. Weston et al. proceed with showing that the /lscript0-norm\nformulation of SVMs can be solved approximately with a simple modi\ufb01cation of the va nilla SVM\nalgorithm:\n1. Traina regularlinear SVM (using /lscript1-normor /lscript2-normregularization).\n2. Re-scaletheinputvariablesbymultiplyingthembytheabsolutevaluesofthe componentsof\nthe weightvector wobtained.\n3. Iteratethe\ufb01rst2steps untilconvergence.\nThemethodisreminiscentofbackwardeliminationproceduresbasedonthes mallest |wi|. Variable\nnormalizationis importantforsuchamethodtoworkproperly.\nWeston et al. note that, although their algorithm only approximately minimizes the /lscript0-norm, in\npracticeitmaygeneralizebetterthananalgorithmthatreallydidminimizethe /lscript0-norm,becausethe\nlatterwouldnotprovidesuf\ufb01cientregularization(alotofvarianceremain sbecausetheoptimization\nproblem has multiple solutions). The need for additional regularization is also stressed in the paper\nof Perkins et al. (2003). The authors use a three-part objective fun ction that includes goodness-\nof-\ufb01t, a regularization term ( /lscript1-norm or /lscript2-norm), and a penalty for large numbers of variables\n(/lscript0-norm). The authors propose a computationally ef\ufb01cient forward selectio n method to optimize\nsuchobjective.\nAnother paper in the issue, by Bi et al. (2003), uses /lscript1-norm SVMs, without iterative multi-\nplicative updates. The authors \ufb01nd that, for their application, the /lscript1-norm minimization suf\ufb01ces to\ndriveenoughweightstozero. Thisapproachwasalsotakeninthecontex tofleast-squareregression\nby other authors (Tibshirani, 1994). The number of variables can be fu rther reduced by backward\nelimination.\nTo our knowledge, no algorithm has been proposed to directly minimize the numb er of vari-\nablesfornon-linearpredictors. Instead,severalauthorshavesub stitutedfortheproblemofvariable\nselection that of variable scaling (Jebara and Jaakkola, 2000, Weston e t al., 2000, Grandvalet and\nCanu,2002). Thevariablescalingfactorsare\u201chyper-parameters\u201da djustedbymodelselection. The\nscaling factors obtained are used to assess variable relevance. A varia nt of the method consists\nof adjusting the scaling factors by", "start_char_idx": 0, "end_char_idx": 3166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1b02b20c-795f-4a43-8df6-cad472d2f30a": {"__data__": {"id_": "1b02b20c-795f-4a43-8df6-cad472d2f30a", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9c5779f8-061a-4543-8c4d-379530914867", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "2": {"node_id": "9b28bfdc-5df9-474a-96cd-f349ffc9c5fd", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246"}}, "hash": "4a9d70ca681ae2ea69b149557ea2995eec990db19a7a13feb4f0c70e47bffff2", "text": "A varia nt of the method consists\nof adjusting the scaling factors by gradient descent on a bound of the lea ve-one-out error (Weston\net al., 2000). This method is used as baseline method in the paper of Weston et al. (2003) in this\nissue.\n1169", "start_char_idx": 3097, "end_char_idx": 3341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1cb99a2b-7b48-48b8-8439-eb2fb79c0d2f": {"__data__": {"id_": "1cb99a2b-7b48-48b8-8439-eb2fb79c0d2f", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09bccbc7-34e7-49f5-8649-6ff13b981a15", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "3": {"node_id": "746912eb-1d84-407a-9e3c-ad81a75bd7c6", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1"}}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "text": "GUYON AND ELISSEEFF\n4.4 Filters for SubsetSelection\nSeveral justi\ufb01cations for the use of \ufb01lters for subset selection have bee n put forward in this special\nissue and elsewhere. It is argued that, compared to wrappers, \ufb01lters ar e faster. Still, recently pro-\nposed ef\ufb01cient embedded methods are competitive in that respect. Another a rgument is that some\n\ufb01lters (e.g. those based on mutual information criteria) provide a generic se lection of variables, not\ntunedfor/byagivenlearningmachine. Anothercompellingjusti\ufb01cationisthat\ufb01 lteringcanbeused\nas apreprocessingsteptoreducespacedimensionalityandovercomeove r\ufb01tting.\nIn that respect, it seems reasonable to use a wrapper (or embedded metho d) with a linearpre-\ndictor as a \ufb01lter and then train a more complex non-linear predictor on the resulting variables. An\nexampleofthisapproachisfoundinthepaperofBietal.(2003): alinear /lscript1-normSVMisusedfor\nvariable selection, but a non-linear /lscript1-norm SVM is used for prediction. The complexity of linear\n\ufb01lters can be ramped up by adding to the selection process products of inpu t variables (monomi-\nals of a polynomial) and retaining the variables that are part of any selected monomial. Another\npredictor, e.g., a neural network, is eventually substituted to the polynomial to perform predictions\nusing the selected variables (Rivals and Personnaz, 2003, Stoppiglia et al., 2003). In some cases\nhowever, one may on the contrary want to reduce the complexity of linear \ufb01lte rs to overcome over-\n\ufb01ttingproblems. Whenthenumberofexamplesissmallcomparedtothenumberofv ariables(inthe\ncase of microarray data for instance) one may need to resort to selecting v ariables with correlation\ncoef\ufb01cients (seeSection2.2).\nInformation theoretic \ufb01ltering methods such as Markov blanket11algorithms (Koller and Sa-\nhami,1996)constituteanotherbroadfamily. Thejusti\ufb01cationforclassi\ufb01cation problemsisthatthe\nmeasureofmutualinformationdoesnotrelyonanypredictionprocess,bu tprovidesaboundonthe\nerrorrateusinganyprediction schemeforthe givendistribution. Wedono t haveanyillustrationof\nsuchmethodsinthisissuefortheproblemofvariablesubsetselection. Wer efertheinterestedreader\ntoKollerandSahami(1996)andreferencestherein. However,theuse ofmutualinformationcriteria\nforindividualvariablerankingwascoveredinSection2andapplicationto featureconstructionand\nselectionareillustratedin Section5.\n5 FeatureConstruction and SpaceDimensionality Reduction\nIn some applications, reducing the dimensionality of the data by selecting a subs et of the original\nvariablesmaybeadvantageousforreasonsincludingtheexpenseofmak ing,storingandprocessing\nmeasurements. If these considerations are not of concern, other means of space dimensionality\nreductionshouldalsobeconsidered.\nThe art of machine learning starts with the design of appropriate data repre sentations. Better\nperformance is often achieved using features derived from the origina l input. Building a feature\nrepresentationisanopportunitytoincorporatedomainknowledgeintothedata andcanbeveryap-\nplicationspeci\ufb01c. Nonetheless,thereareanumberofgenericfeatureco nstructionmethods,includ-\ning: clustering;basiclineartransformsoftheinputvariables(PCA/SVD,L DA);moresophisticated\nlinear transforms like spectral transforms (Fourier, Hadamard), wavele t transforms or convolutions\nofkernels;andapplyingsimplefunctionstosubsetsofvariables,likeprod uctstocreatemonomials.\n11. The", "start_char_idx": 0, "end_char_idx": 3397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "746912eb-1d84-407a-9e3c-ad81a75bd7c6": {"__data__": {"id_": "746912eb-1d84-407a-9e3c-ad81a75bd7c6", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09bccbc7-34e7-49f5-8649-6ff13b981a15", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "2": {"node_id": "1cb99a2b-7b48-48b8-8439-eb2fb79c0d2f", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d"}}, "hash": "12562011a2a749dcc28309db5e65f9773f481cbb12ef21d395d179561aa5eff1", "text": "uctstocreatemonomials.\n11. The Markov blanket of a given variable xiis a set of variables not including xithat render xi\u201cunnecessary\u201d. Once\na Markov blanket is found, xican safely be eliminated. Furthermore, in a backward elimination procedu re, it will\nremain unnecessary atlater stages.\n1170", "start_char_idx": 3367, "end_char_idx": 3660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48688efd-4bc2-4ece-b55b-1f4481669419": {"__data__": {"id_": "48688efd-4bc2-4ece-b55b-1f4481669419", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dead74a5-9ffd-4904-a649-10d6e6408d9e", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "3": {"node_id": "c42d5026-9e0a-406c-847e-d1a8a1355344", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be"}}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nTwodistinctgoalsmaybepursuedforfeatureconstruction: achievingbe streconstructionofthe\ndata or being most ef\ufb01cient for making predictions. The \ufb01rst problem is an unsupervised learning\nproblem. Itiscloselyrelatedtothatofdatacompressionandalotofalgorithms areusedacrossboth\n\ufb01elds. The second problem is supervised. Are there reasons to select features in an unsupervised\nmanner when the problem is supervised? Yes, possibly several: Some pro blems, e.g., in text pro-\ncessingapplications,comewithmoreunlabelleddatathanlabelleddata. Also,un supervisedfeature\nselectionis lesspronetoover\ufb01tting.\nInthisissue,fourpapersaddresstheproblemoffeatureconstruction . Allofthemtakeaninfor-\nmation theoretic approach to the problem. Two of them illustrate the use of cluster ing to construct\nfeatures (Bekkerman et al., 2003, Dhillon et al., 2003), one provides a n ew matrix factorization al-\ngorithm (Globerson and Tishby, 2003), and one provides a supervise d means of learning features\nfrom a variety of models (Torkkola, 2003). In addition, two papers whos e main focus is directed\ntovariableselectionalsoaddresstheselectionofmonomialsofapolynomialmode landthehidden\nunits of a neural network (Rivals and Personnaz, 2003, Stoppiglia et a l., 2003), and one paper ad-\ndresses the implicit feature selection in non-linear kernel methods for polyn omial kernels (Weston\netal.,2003).\n5.1 Clustering\nClustering has long been used for feature construction. The idea is to rep lace a group of \u201csimilar\u201d\nvariables by a cluster centroid, which becomes a feature. The most popula r algorithms include\nK-means andhierarchicalclustering. Forareview,see,e.g.,thetextboo k ofDuda etal.(2001).\nClustering is usually associated with the idea of unsupervised learning. It c an be useful to\nintroduce some supervision in the clustering procedure to obtain more discrimin ant features. This\nistheideaofdistributionalclustering(Pereiraetal.,1993),whichisdevelo pedintwopapersofthis\nissue. Distributional clustering is rooted in the information bottleneck (IB) theo ry of Tishby et al.\n(1999). Ifwecall \u02dcXtherandomvariablerepresentingtheconstructedfeatures,theIBmethod seeks\nto minimize the mutual information I(X,\u02dcX), while preserving the mutual information I(\u02dcX,Y). A\nglobalobjectivefunctionisbuiltbyintroducingaLagrangemultiplier \u03b2:J=I(X,\u02dcX)\u2212\u03b2I(\u02dcX,Y). So,\nthemethodsearchesforthesolutionthatachievesthelargestpossiblecomp ression,whileretaining\ntheessentialinformationabout thetarget.\nText processing applications are usual targets for such techniques. P atterns are full documents\nand variables come from a bag-of-words representation: Each variab le is associated to a word and\nis proportional to the fraction of documents in which that word appears. In application to feature\nconstruction, clustering methods group words, not documents. In text ca tegorization tasks, the su-\npervisioncomesfromtheknowledgeofdocumentcategories. Itisintroduc edbyreplacingvariable\nvectorscontainingdocumentfrequencycountsbyshortervariablevec torscontainingdocumentcat-\negoryfrequencycounts,i.e.,thewordsarerepresentedas distribution sover documentcategories.\nThe simplest implementation of this idea is presented in the paper of Dhillon et al. (2 003) in\nthis issue. It uses K-means clustering on variables represented by a vec tor of document category\nfrequencycounts.", "start_char_idx": 0, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c42d5026-9e0a-406c-847e-d1a8a1355344": {"__data__": {"id_": "c42d5026-9e0a-406c-847e-d1a8a1355344", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dead74a5-9ffd-4904-a649-10d6e6408d9e", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "2": {"node_id": "48688efd-4bc2-4ece-b55b-1f4481669419", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5"}}, "hash": "5fccab668a93fdfb21ab40b87c3fd19932863ecce1f0570b9faf0319a014f6be", "text": "clustering on variables represented by a vec tor of document category\nfrequencycounts. The(non-symmetric)similaritymeasureusedistheKullbac k-Leiblerdivergence\nK(xj,\u02dcxi) =exp(\u2212\u03b2\u2211kxk,jln(xk,j/\u02dcxk,i)). In the sum, the index kruns over document categories. A\nmore elaborate approach is taken by Bekkerman et al. (2003) who use a \u201c soft\u201d version of K-means\n(allowing words to belong to several clusters) and who progressively d ivide clusters by varying the\nLagrangemultiplier \u03b2monitoringthetradeoffbetween I(X,\u02dcX)andI(\u02dcX,Y). Inthisway,documents\n1171", "start_char_idx": 3287, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "abb82a42-5205-4f94-bc08-c4cbcc1b2749": {"__data__": {"id_": "abb82a42-5205-4f94-bc08-c4cbcc1b2749", "embedding": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "23c228c6-04d0-4fc9-8a8f-f6c3099ee4fd", "node_type": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "text": "GUYON AND ELISSEEFF\narerepresentedasadistributionoverwordcentroids. Bothmethodsperf ormwell. Bekkermanetal.\nmentionthatfewwordsendupbelongingtoseveralclusters,hintingthat\u201ch ard\u201dclusterassignment\nmaybe suf\ufb01cient.\n5.2 Matrix Factorization\nAnother widely used method of feature construction is singular value decomp osition (SVD). The\ngoal of SVD is to form a set of features that are linear combinations of the o riginal variables,\nwhich provide the best possible reconstruction of the original data in the lea st square sense (Duda\net al., 2001). It is an unsupervised method of feature construction. In th is issue, the paper of\nGloberson and Tishby (2003) presents an information theoretic unsuper vised feature construction\nmethod: suf\ufb01cient dimensionality reduction (SDR). The most informative fea tures are extracted by\nsolving an optimization problem that monitors the tradeoff between data recons truction and data\ncompression, similar to the information bottleneck of Tishby et al. (1999); the f eatures are found\nas Lagrange multipliers of the objective optimized. Non-negative matrices P of dimension (m, n)\nrepresentingthejointdistributionoftworandomvariables(forinstancethe co-occurrenceofwords\nin documents) are considered. The features are extracted by information theoretic I-projections,\nyielding a reconstructed matrix of special exponential form \u02dcP= (1/Z)exp(\u03a6\u03a8). For a set of d\nfeatures,\u03a6isa(m,d+2)matrixwhose (d+1)thcolumnisonesand \u03a8isa(d+2,n)matrixwhose\n(d+2)thcolumnisones,and Zisanormalizationcoef\ufb01cient. SimilarlytoSVD,thesolutionshows\nthesymmetryofthe problemwithrespecttopatterns andvariables.\n5.3 SupervisedFeatureSelection\nWe review three approaches for selecting features in cases where fea tures should be distinguished\nfromvariables becausebothappear simultaneouslyinthesamesystem:\nNested subset methods. A number of learning machines extract features as part of the learn-\ning process. These include neural networks whose internal nodes ar e feature extractors. Thus,\nnode pruning techniques such as OBD LeCun et al. (1990) are feature selection algorithms. Gram-\nSchmidt orthogonalization is presented in this issue as an alternative to OBD (S toppiglia et al.,\n2003).\nFilters.Torkkola (2003) proposes a \ufb01lter method for constructing features usin g a mutual in-\nformation criterion. The author maximizes I(\u03c6,y)formdimensional feature vectors \u03c6and target\nvectorsy.12Modelling the feature density function with Parzen windows allows him to compute\nderivatives \u2202I/\u2202\u03c6ithat are transform independent. Combining them with the transform-depend ent\nderivatives \u2202\u03c6i/\u2202w, he devises a gradient descent algorithm to optimize the parameters wof the\ntransform(thatneed notbelinear):\nwt+1=wt+\u03b7\u2202I\n\u2202w=wt+\u03b7\u2202I\n\u2202\u03c6i\u2202\u03c6i\n\u2202w. (5)\nDirect objective optimization. Kernel methods possess an implicit feature space revealed by\nthe kernel expansion: k(x,x/prime) =\u03c6(x).\u03c6(x/prime), where\u03c6(x)is a feature vector of possibly in\ufb01nite di-\nmension. Selecting these implicit features may improve generalization, but does not change the\n12. Infact, theauthor uses aquadratic measure of divergence instea d of theusualmutual information.\n1172", "start_char_idx": 0, "end_char_idx": 3136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "229f52d5-1f75-4cda-8162-598284efcdce": {"__data__": {"id_": "229f52d5-1f75-4cda-8162-598284efcdce", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5d2c01a-e43d-4c1f-91de-b2e5430014d5", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "3": {"node_id": "c6f518ae-f8e9-49ea-b4bc-d1d9d8e66726", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039"}}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nrunning time or help interpreting the prediction function. In this issue, Weston e t al. (2003) pro-\nposeamethodforselectingimplicitkernelfeaturesinthecaseofthepolynomial kernel,usingtheir\nframeworkof minimizationof the /lscript0-norm.\n6 Validation Methods\nWe group in this section all the issues related to out-of-sample performance pr ediction (generaliza-\ntion prediction) and model selection. These are involved in various aspects of variable and feature\nselection: to determine the number of variables that are \u201csigni\ufb01cant\u201d, to guide and halt the search\nfor good variable subsets, to choose hyperparameters, and to evaluate the \ufb01nal performance of the\nsystem.\nOne should \ufb01rst distinguish the problem of model selection from that of eva luating the \ufb01nal\nperformance of the predictor. For that last purpose, it is important to set aside an independent\ntest set. The remaining data is used both for training and performing model sele ction. Additional\nexperimentalsophisticationcanbeaddedbyrepeatingtheentireexperiment forseveraldrawingsof\nthetestset.13\nTo perform model selection (including variable/feature selection and hype rparameter optimiza-\ntion), the data not used for testing may be further split between \ufb01xed training and validation sets,\nor various methods of cross-validation can be used. The problem is then b rought back to that of\nestimating the signi\ufb01cance of differences in validation errors. For a \ufb01xed v alidation set, statistical\ntests can be used, but their validity is doubtful for cross-validation becau se independence assump-\ntions are violated. For a discussion of these issues, see for instance the w ork of Dietterich (1998)\nand Nadeau and Bengio (2001). If there are suf\ufb01ciently many examples, it may not be necessary to\nsplit the training data: Comparisons of training errors with statistical tests can b e used (see Rivals\nandPersonnaz,2003,inthisissue). Cross-validationcanbeextended totime-seriesdataand,while\ni.i.d.assumptionsdonotholdanymore,itisstillpossibletoestimategeneralizatione rrorcon\ufb01dence\nintervals(seeBengio andChapados,2003, inthisissue).\nChoosing what fraction of the data should be used for training and for va lidation is an open\nproblem. Manyauthorsresorttousingtheleave-one-outcross-valida tionprocedure,eventhoughit\nis known to be a high variance estimator of generalization error (Vapnik, 19 82) and to give overly\noptimistic results, particularly when data are not properly independently and identically sampled\nfrom the \u201dtrue\u201d distribution. The leave-one-out procedure consists of removing one example from\nthetrainingset,constructingthepredictoronthebasisonlyoftheremainingtra iningdata,thentest-\ningontheremovedexample. Inthisfashiononetestsallexamplesofthetraining dataandaverages\ntheresults. Aspreviouslymentioned,thereexistexactorapproximatefor mulasoftheleave-one-out\nerror for a number of learning machines (Monari and Dreyfus, 2000, Rivals and Personnaz, 2003,\nRakotomamonjy, 2003).\nLeave-one-out formulas can be viewed as corrected values of the train ing error. Many other\ntypes of penalization of the training error have been proposed in the literatu re (see, e.g., Vapnik,\n1998, Hastie et al., 2001). Recently, a new family of such methods called \u201cmetr ic-based methods\u201d\nhave been proposed (Schuurmans, 1997). The paper of Bengio and Chapados (2003) in this issue\n13. Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6f518ae-f8e9-49ea-b4bc-d1d9d8e66726": {"__data__": {"id_": "c6f518ae-f8e9-49ea-b4bc-d1d9d8e66726", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5d2c01a-e43d-4c1f-91de-b2e5430014d5", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "2": {"node_id": "229f52d5-1f75-4cda-8162-598284efcdce", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee"}}, "hash": "707d54ce6b04d4cc4c122214e708cdeb69baacb18334967d918c4aca348f9039", "text": "Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc anbecarriedoutasan\u201couterloop\u201d,outsidethe\nfeature/variableselectionprocess,toestimatethe\ufb01nalperformanceof thepredictor. Thiscomputationallyexpensive\nprocedure isusedincases where datais extremely scarce.\n1173", "start_char_idx": 3387, "end_char_idx": 3654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0bcc64a5-3ea5-4b95-b1ae-6815aa0b3b13": {"__data__": {"id_": "0bcc64a5-3ea5-4b95-b1ae-6815aa0b3b13", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b76235e3-4910-4db8-8057-38c074ddc3dd", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "3": {"node_id": "7fab722e-f56f-452f-a4b5-58120c90b568", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5"}}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "text": "GUYON AND ELISSEEFF\nillustrates their application to variable selection. The authors make use of unlab elled data, which\nare readily available in the application considered, time series prediction with a h orizon. Consider\ntwomodels fAandfBtrainedwithnestedsubsetsofvariables A\u2282B. Wecall d(fA,fB)thediscrep-\nancy of the two models. The criterion involves the ratio dU(fA,fB)/dT(fA,fB), wheredU(fA,fB)is\ncomputed with unlabelled data and dT(fA,fB)is computed with training data. A ratio signi\ufb01cantly\nlarger thanonesheds doubtontheusefulnessof thevariablesinsubse tBthat arenotin A.\nFor variable ranking or nested subset ranking methods (Sections 2 and 4 .2), another statisti-\ncal approach can be taken. The idea is to introduce a probe in the data that is a random variable.\nRoughly speaking, variables that have a relevance smaller or equal to tha t of the probe should be\ndiscarded. Bi et al. (2003) consider a very simple implementation of that idea : they introduce in\ntheirdatathreeadditional\u201cfakevariables\u201ddrawnrandomlyfromaGauss iandistributionandsubmit\nthem to their variable selection process with the other \u201ctrue variables\u201d. Subs equently, they discard\nall the variables that are less relevant than one of the three fake variable s (according to their weight\nmagnitude criterion). Stoppiglia et al. (2003) propose a more sophisticated me thod for the Gram-\nSchmidt forward selection method. For a Gaussian distributed probe, they p rovide an analytical\nformula to compute the rank of the probe associated with a given risk of acce pting an irrelevant\nvariable. A non-parametric variant of the probe method consists in creating \u201cfake variables\u201d by\nrandomly shuf\ufb02ing real variable vectors. In a forward selection proce ss, the introduction of fake\nvariables does not disturb the selection because fake variables can be d iscarded when they are en-\ncountered. Atagivenstepintheforwardselectionprocess,letuscall ftthefractionoftruevariables\nselected so far (among all true variables) and ffthe fraction of fake variables encountered (among\nall fake variables). As a halting criterion one can place a threshold on the r atioff/ft, which is an\nupper bound on the fraction of falsely relevant variables in the subset s elected so far. The latter\nmethodhasbeenusedforvariableranking(Tusheretal.,2001). Itspa rametricversionforGaussian\ndistributionsusingtheT statisticas rankingcriterionisnothingbuttheT-test.\n7 Advanced Topics and Open Problems\n7.1 Varianceof Variable SubsetSelection\nMany methods of variable subset selection are sensitive to small perturbatio ns of the experimental\nconditions. If the data has redundant variables, different subsets of variables with identical predic-\ntive power may be obtained according to initial conditions of the algorithm, remov al or addition of\na few variables or training examples, or addition of noise. For some applicatio ns, one might want\nto purposely generate alternative subsets that can be presented to a sub sequent stage of processing.\nStill one might \ufb01nd this variance undesirable because (i) variance is often th e symptom of a \u201cbad\u201d\nmodel that does not generalize well; (ii) results are not reproducible; an d (iii) one subset fails to\ncapturethe\u201cwhole picture\u201d.\nOnemethodto\u201cstabilize\u201dvariableselectionexploredinthisissueistousesever al\u201cbootstraps\u201d\n(Bi et al., 2003). The variable selection process is repeated with sub-sa mples of the training data.\nThe union of the subsets of variables selected", "start_char_idx": 0, "end_char_idx": 3456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7fab722e-f56f-452f-a4b5-58120c90b568": {"__data__": {"id_": "7fab722e-f56f-452f-a4b5-58120c90b568", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b76235e3-4910-4db8-8057-38c074ddc3dd", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "2": {"node_id": "0bcc64a5-3ea5-4b95-b1ae-6815aa0b3b13", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8"}}, "hash": "3aff5f0a410500c576ae1a63ac15f40c3408c357b07286fde8d877c5eb8451b5", "text": "mples of the training data.\nThe union of the subsets of variables selected in the various bootstraps is ta ken as the \ufb01nal \u201cstable\u201d\nsubset. This joint subset may be at least as predictive as the best bootstr ap subset. Analyzing the\nbehavior of the variables across the various bootstraps also provides f urther insight, as described\nin the paper. In particular, an index of relevance of individual variable s can be created considering\nhowfrequentlytheyappear inthebootstraps.\n1174", "start_char_idx": 3382, "end_char_idx": 3864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "480b6e04-3058-4973-869a-cca9762bab88": {"__data__": {"id_": "480b6e04-3058-4973-869a-cca9762bab88", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64cff834-ce6e-4029-8a5d-23c75728c8db", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "3": {"node_id": "ecb700cc-7c50-4360-915d-7f8a4e083b97", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b"}}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nRelated ideas have been described elsewhere in the context of Bayesian variable selection (Je-\nbara and Jaakkola, 2000, Ng and Jordan, 2001, Vehtari and Lampine n, 2002). A distribution over\na population of models using various variable subsets is estimated. Variables a re then ranked ac-\ncording to the marginal distribution, re\ufb02ecting how often they appear in importa nt subsets (i.e.,\nassociatedwiththemostprobablemodels).\n7.2 Variable Rankingin the Contextof Others\nIn Section 2, we limited ourselves to presenting variable ranking methods using a criterion com-\nputed from single variables, ignoring the context of others. In Section 4.2 , we introduced nested\nsubsetmethodsthatprovideausefulrankingofsubsets,notof indiv idualvariables: somevariables\nmayhavealowrankbecausetheyareredundantandyetbehighlyrelev ant. BootstrapandBayesian\nmethods presented in Section 7.1, may be instrumental in producing a good var iable ranking incor-\nporatingthecontext ofothers.\nThe relief algorithm uses another approach based on the nearest-neighb or algorithm (Kira and\nRendell,1992). Foreachexample,theclosestexampleofthesameclass(n earesthit)andtheclosest\nexampleofadifferentclass(nearestmiss)areselected. Thescore S(i)oftheithvariableiscomputed\nas the average over all examples of magnitude of the difference between th e distance to the nearest\nhitandthe distancetothenearestmiss,inprojectiononthe ithvariable.\n7.3 UnsupervisedVariable Selection\nSometimes, no target yis provided, but one still would want to select a set of most signi\ufb01cant\nvariables with respect to a de\ufb01ned criterion. Obviously, there are as many criteria as problems\ncan be stated. Still, a number of variable ranking criteria are useful acros s applications, including\nsaliency,entropy,smoothness ,densityandreliability . A variable is salient if it has a high variance\nor a large range, compared to others. A variable has a high entropy if the d istribution of examples\nis uniform. In a time series, a variable is smooth if on average its local curvatur e is moderate. A\nvariableisinahigh-densityregionifitishighlycorrelatedwithmanyothervaria bles. Avariableis\nreliable if the measurement error bars computed by repeating measurements a re small compared to\nthevariabilityof thevariablevalues(as quanti\ufb01ed,e.g., byanANOVA statistic) .\nSeveral authors have also attempted to perform variable or feature selec tion for clustering ap-\nplications (see,e.g., XingandKarp,2001, Ben-Hur andGuyon, 2003, andreferences therein).\n7.4 Forward vs.BackwardSelection\nItisoftenarguedthatforwardselectioniscomputationallymoreef\ufb01cienttha nbackwardelimination\nto generate nested subsets of variables. However, the defenders of b ackward elimination argue that\nweaker subsets are found by forward selection because the importance of variables is not assessed\nin the context of other variables not included yet. We illustrate this latter argume nt by the example\nofFigure4. Inthatexample,onevariableseparatesthetwoclassesbetter byitselfthaneitherofthe\ntwootheronestakenaloneandwillthereforebeselected\ufb01rstbyforwar dselection. Atthenextstep,\nwhen it is complemented by either of the two other variables, the resulting class s eparation in two\ndimensions will not be as good as the one obtained jointly by the two variables tha t were discarded\nat the \ufb01rst step. A backward selection method may outsmart forward selectio n by eliminating at\nthe \ufb01rst step the variable that by itself provides the best", "start_char_idx": 0, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ecb700cc-7c50-4360-915d-7f8a4e083b97": {"__data__": {"id_": "ecb700cc-7c50-4360-915d-7f8a4e083b97", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64cff834-ce6e-4029-8a5d-23c75728c8db", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "2": {"node_id": "480b6e04-3058-4973-869a-cca9762bab88", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52"}}, "hash": "a9e17444020c2c785e7342a2b21a68cc0f01a4c28b8d114e82bbed71ddbdfd9b", "text": "at\nthe \ufb01rst step the variable that by itself provides the best separation to retain the two variables that\n1175", "start_char_idx": 3432, "end_char_idx": 3543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "53ae76ee-6129-46da-a9cc-b4fabcd382ec": {"__data__": {"id_": "53ae76ee-6129-46da-a9cc-b4fabcd382ec", "embedding": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba05e856-739f-476b-a70a-573a120a6cda", "node_type": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5 \u22125 0 5\u2212505\u2212505\u2212505\nFigure 4: Forward or backward selection? Of the three variables of this example, the third one\nseparatesthetwoclassesbestbyitself(bottomrighthistogram). Itisthere forethebestcandidatein\na forward selection process. Still, the two other variables are better taken to gether than any subset\nof twoincludingit. Abackwardselectionmethod mayperformbetter inthis case.\ntogether perform best. Still, if for some reason we need to get down to a sing le variable, backward\neliminationwillhavegotten ridof thevariablethatworks bestonitsown.\n7.5 TheMulti-class Problem\nSome variable selection methods treat the multi-class case directly rather than de composing it into\nseveral two-class problems: All the methods based on mutual information crite ria extend naturally\nto the multi-class case (see in this issue Bekkerman et al., 2003, Dhillon et al., 20 03, Torkkola,\n2003). Multi-classvariablerankingcriteriaincludeFisher\u2019scriterion(the ratioofthebetweenclass\nvariancetothewithin-classvariance). ItiscloselyrelatedtotheFstatisticuse dintheANOVAtest,\nwhichisonewayofimplementingtheprobemethod(Section6)forthemulti-classca se. Wrappers\nor embedded methods depend upon the capability of the classi\ufb01er used to han dle the multi-class\ncase. Examplesofsuchclassi\ufb01ersincludelineardiscriminantanalysis(LD A),amulti-classversion\nof Fisher\u2019s linear discriminant (Duda et al., 2001), and multi-class SVMs (s ee, e.g., Weston et al.,\n2003).\nOne may wonder whether it is advantageous to use multi-class methods for var iable selection.\nOn one hand, contrary to what is generally admitted for classi\ufb01cation, the multi- class setting is\nin some sense easier for variable selection than the two-class case. This is b ecause the larger the\n1176", "start_char_idx": 0, "end_char_idx": 1771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dbbfe5ca-b997-4453-b8ee-d3d0e0ff0b00": {"__data__": {"id_": "dbbfe5ca-b997-4453-b8ee-d3d0e0ff0b00", "embedding": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c59942b3-8ef2-4a35-a5b8-7a062d18f18f", "node_type": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nnumberofclasses,thelesslikelya\u201crandom\u201dsetoffeaturesprovideago odseparation. Toillustrate\nthis point, consider a simple example where all features are drawn independ ently from the same\ndistribution Pand the \ufb01rst of them is the target y. Assume that all these features correspond to\nrollingadiewith Qfacesntimes(nisthenumberofsamples). Theprobabilitythatone\ufb01xedfeature\n(except the \ufb01rst one) is exactly yis then (1/Q)n. Therefore, \ufb01nding the feature that corresponds to\nthe target ywhen it is embedded in a sea of noisy features is easier when Qis large. On the other\nhand, Forman (2003) points out in this issue that in the case of uneven distr ibutions across classes,\nmulti-classmethodsmayover-representabundantoreasilyseparablecla sses. Apossiblealternative\nis to mix ranked lists of several two-class problems. Weston et al. (2003) pr opose one such mixing\nstrategy.\n7.6 Selectionof Examples\nThedualproblemsoffeatureselection/constructionarethoseofpatterns election/construction. The\nsymmetry of the two problems is made explicit in the paper of Globerson and Tishb y (2003) in\nthis issue. Likewise, both Stoppiglia et al. (2003) and Weston et al. (2003) point out that their\nalgorithm also applies to the selection of examples in kernel methods. Others ha ve already pointed\noutthesimilarityandcomplementarityofthetwoproblems(BlumandLangley,1997 ). Inparticular,\nmislabeledexamplesmayinducethechoiceofwrongvariables. Conversely, ifthelabelingishighly\nreliable,selectingwrongvariablesassociatedwithaconfoundingfactorma ybeavoidedbyfocusing\noninformativepatterns thatareclosetothedecisionboundary(Guyonet al.,2002).\n7.7 InverseProblems\nMostofthespecialissueconcentratesontheproblemof\ufb01ndinga(small)s ubsetofvariablesuseful\ntobuildagoodpredictor. Insomeapplications,particularlyinbioinformatics,th isisnotnecessarily\nthe only goal of variable selection. In diagnosis problems, for instance, it is important to identify\nthe factors that triggered a particular disease or unravel the chain of ev ents from the causes to\nthe symptoms. But reverse engineering the system that produced the data is a more challenging\ntask than building a predictor. The readers interested in these issues can c onsult the literature on\ngene networks in the conference proceedings of the paci\ufb01c symposium o n biocomputing (PSB) or\nintelligent systems for molecular biology conference (ISMB) and the causa lity inference literature\n(see, e.g., Pearl, 2000). At the heart of this problem is the distinction betwe en correlation and\ncausality. Observational data such as the data available to machine learning r esearchers allow us\nonly to observe correlations. For example, observations can be made abou t correlations between\nexpression pro\ufb01les of given genes or between pro\ufb01les and symptoms, b ut a leap of faith is made\nwhendeciding whichgene activatedwhichother one andinturntriggeredth esymptom.\nIn this issue, the paper of Caruana and de Sa (2003) presents interestin g ideas about using\nvariables discarded by variable selection as additional outputs of a neura l network. They show im-\nprovedperformanceonsyntheticandrealdata. Theiranalysissuppor tstheideathatsomevariables\naremoreef\ufb01cientlyusedasoutputsthanasinputs. Thiscouldbeasteptowar ddistinguishingcauses\nfromconsequences.\n1177", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "92d4df26-a2ec-4d9e-9b45-2a02ece8cfc1": {"__data__": {"id_": "92d4df26-a2ec-4d9e-9b45-2a02ece8cfc1", "embedding": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "339eb14b-a403-458d-a90b-4f4f00dcf161", "node_type": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "text": "GUYON AND ELISSEEFF\nDataset Description patterns variables classes References\nLineara,bArti\ufb01ciallinear 10-1200 100-240 reg-2 SWBe\nMulti-clustercArti\ufb01cialnon-linear 1000-1300 100-500 2 PS\nQSARdChemistry 30-300 500-700 reg Bt\nUCIeMLrepository 8-60 500-16000 2-30ReBnToPC\nLVQ-PAKfPhoneme data 1900 20 20 T\nRaetch bench.gUCI/Delve/Statlog 200-7000 8-20 2 Ra\nMicroarrayaCancer classif. 6-100 2000-4000 2 WRa\nMicroarrayaGene classi\ufb01cation 200 80 5 W\nAstonUnivhPipeline transport 1000 12 3 T\nNIPS 2000iUnlabeled data 200-400 5-800 reg Ri\n20 Newsgroupj,oNews postings 20000 300-15000 2-20 GBkD\nText\ufb01lteringkTREC/OSHUMED 200-2500 3000-30000 6-17 F\nIRdatasetslMED/CRAN/CISI 1000 5000 30-225 G\nReuters-21578m,onewswiredocs. 21578 300-15000 114 BkF\nOpenDir. Proj.nWebdirectory 5000 14500 50 D\nTable 1:Publicly available data sets used in the special issue. Approximate numbers or ranges\nof patterns, variables, and classes effectively used are provided. T he \u201cclasses\u201d column indicates\n\u201creg\u201d for regression problems, or the number of queries for Informatio n Retrieval (IR) problems.\nFor arti\ufb01cial data sets, the fraction of variables that are relevant range s from 2 to 10. The initial of\nthe \ufb01rst author are provided as reference: Bk=Bekkerman, Bn=Ben gio, Bt=Bennett, C=Caruana,\nD=Dhillon, F=Forman, G=Globerson, P=Perkins, Re=Reunanen, Ra=R akotomamonjy, Ri=Rivals,\nS=Stoppiglia, T=Torkkola, W=Weston. Please also check the JMLR web site for later additions\nandpreprocesseddata.\na.http://www.kyb.tuebingen.mpg.de/bs/people/weston/l0 ( /lscript0not10)\nb.http://www.clopinet.com/isabelle/Projects/NIPS2001/Arti\ufb01cial.zip\nc.http://nis-www.lanl.gov/ \u223csimes/data/jmlr02/\nd.http://www.rpi.edu/ \u223cbij2/featsele.html\ne.http://www.ics.uci.edu/ \u223cmlearn/MLRepository.html\nf.http://www.cis.hut.\ufb01/research/software.shtml\ng.http://ida.\ufb01rst.gmd.de/ \u223craetsch/data/benchmarks.htm\nh.http://www.nerg.aston.ac.uk/GTM/3PhaseData.html\ni.http://q.cis.uoguelph.ca/skremer/NIPS2000/\nj.http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\nk.http://trec.nist.gov/data.html(FilteringTrackCollection)\nl.http://www.cs.utk.edu/ \u223clsi/\nm.http://www.daviddlewis.com/resources/testcollections/reuters21578/\nn.http://dmoz.org/andhttp://www.cs.utexas.edu/users/manyam/dmoz.txt\no.http://www.cs.technion.ac.il/ \u223cronb/thesis.html\n1178", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "83c1b6d7-9928-453c-8262-dda07c18dc9c": {"__data__": {"id_": "83c1b6d7-9928-453c-8262-dda07c18dc9c", "embedding": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be193238-d2a2-40ca-90f3-8143f602ad96", "node_type": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n8 Conclusion\nThe recent developments in variable and feature selection have address ed the problem from the\npragmatic point of view of improving the performance of predictors. They h ave met the challenge\nof operating on input spaces of several thousand variables. Sophistic ated wrapper or embedded\nmethods improve predictor performance compared to simpler variable ranking methods like corre-\nlation methods, but the improvements are not always signi\ufb01cant: domains with lar ge numbers of\ninput variables suffer from the curse of dimensionality and multivariate metho ds may over\ufb01t the\ndata. For some domains, applying \ufb01rst a method of automatic feature construc tion yields improved\nperformance and a more compact set of features. The methods propose d in this special issue have\nbeentestedonawidevarietyofdatasets(seeTable1),whichlimitsthepossibility ofmakingcom-\nparisonsacrosspapers. Furtherworkincludestheorganizationofab enchmark. Theapproachesare\nvery diverse and motivated by various theoretical arguments, but a unif ying theoretical framework\nislacking. Becauseoftheseshortcomings,itisimportantwhenstartingwithan ewproblemtohave\na few baseline performance values. To that end, we recommend using a line ar predictor of your\nchoice (e.g. a linear SVM) and select variables in two alternate ways: (1) w ith a variable ranking\nmethod using a correlation coef\ufb01cient or mutual information; (2) with a nested subset selection\nmethod performing forward or backward selection or with multiplicative update s. Further down\nthe road, connections need to be made between the problems of variable and feature selection and\nthoseofexperimentaldesignandactivelearning,inanefforttomoveawa yfromobservationaldata\ntowardexperimentaldata,and toaddressproblemsof causalityinference .\nReferences\nE. Amaldi and V. Kann. On the approximation of minimizing non zero variables or unsatis\ufb01ed\nrelations inlinear systems. TheoreticalComputer Science , 209:237\u2013260, 1998.\nR. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. Distributional wor d clusters vs. words for\ntextcategorization. JMLR,3:1183\u20131208 (thisissue),2003.\nA. Ben-Hur and I. Guyon. Detecting stable clusters using principal compo nent analysis. In M.J.\nBrownsteinandA.Kohodursky,editors, MethodsInMolecularBiology ,pages159\u2013182.Humana\nPress,2003.\nY.BengioandN.Chapados. Extensionstometric-basedmodelselection. JMLR,3:1209\u20131227(this\nissue),2003.\nJ. Bi, K. Bennett, M. Embrechts, C. Breneman, and M. Song. Dimensionality r eduction via sparse\nsupportvector machines. JMLR,3:1229\u20131243 (thisissue),2003.\nA.BlumandP.Langley. Selectionofrelevantfeaturesandexamplesinmac hinelearning. Arti\ufb01cial\nIntelligence ,97(1-2):245\u2013271,December 1997.\nB. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin c lassi\ufb01ers. In Fifth\nAnnualWorkshoponComputational LearningTheory ,pages 144\u2013152, Pittsburgh,1992.ACM.\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classi\ufb01cation and Regression Trees .\nWadsworthandBrooks,1984.\n1179", "start_char_idx": 0, "end_char_idx": 3035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "052f6a97-a7a9-4e53-aa2a-e0483aeaeef6": {"__data__": {"id_": "052f6a97-a7a9-4e53-aa2a-e0483aeaeef6", "embedding": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8aa0de19-8c9e-41aa-8fbe-b39f4f750174", "node_type": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "text": "GUYON AND ELISSEEFF\nR. Caruana and V. de Sa. Bene\ufb01tting from the variables that variable sele ction discards. JMLR, 3:\n1245\u20131264 (thisissue),2003.\nI. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic featur e clustering algorithm\nfortextclassi\ufb01cation. JMLR,3:1265\u20131287(this issue),2003.\nT. G. Dietterich. Approximate statistical test for comparing supervised class i\ufb01cation learning algo-\nrithms.NeuralComputation , 10(7):1895\u20131924,1998.\nR.O.Duda,P.E.Hart,andD.G.Stork. PatternClassi\ufb01cation . JohnWiley&amp;Sons,USA,2nd\nedition,2001.\nT. R. Golub et al. Molecular classi\ufb01cation of cancer: Class discovery an d class prediction by gene\nexpressionmonitoring. Science, 286:531\u2013537, 1999.\nG.Forman. Anextensiveempiricalstudyoffeatureselectionmetricsfor tex tclassi\ufb01cation. JMLR,\n3:1289\u20131306 (thisissue),2003.\nT. Furey, N. Cristianini, Duffy, Bednarski N., Schummer D., M., and D. Ha ussler. Support vector\nmachine classi\ufb01cation and validation of cancer tissue samples using microarra y expression data.\nBioinformatics ,16:906\u2013914, 2000.\nA.GlobersonandN.Tishby. Suf\ufb01cientdimensionalityreduction. JMLR,3:1307\u20131331(thisissue),\n2003.\nY.GrandvaletandS.Canu. Adaptive scalingfor featureselectioninSV Ms. InNIPS15,2002.\nI. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for can cer classi\ufb01cation using\nsupportvector machines. MachineLearning ,46(1-3):389\u2013422,2002.\nT. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning . Springer series in\nstatistics.Springer,NewYork,2001.\nT. Jebara and T. Jaakkola. Feature selection and dualities in maximum entrop y discrimination. In\n16thAnnualConference onUncertainty inArti\ufb01cialIntelligence ,2000.\nK. Kira and L. Rendell. A practical approach to feature selection. In D. S leeman and P. Edwards,\neditors,International Conference on Machine Learning , pages 368\u2013377, Aberdeen, July 1992.\nMorganKaufmann.\nR. Kohavi and G. John. Wrappers for feature selection. Arti\ufb01cial Intelligence , 97(1-2):273\u2013324,\nDecember 1997.\nD. Koller and M. Sahami. Toward optimal feature selection. In 13th International Conference on\nMachineLearning ,pages 284\u2013292,July1996.\nY. LeCun, J. Denker, S. Solla, R. E. Howard, and L. D. Jackel. Optimal brain damage. In D. S.\nTouretzky, editor, Advances in Neural Information Processing Systems II , San Mateo, CA, 1990.\nMorganKaufmann.\n1180", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a84e2c8d-9a22-4795-b8a5-f7b792c977a8": {"__data__": {"id_": "a84e2c8d-9a22-4795-b8a5-f7b792c977a8", "embedding": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f11af456-2b3a-46f6-9412-7df72ef636b4", "node_type": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nG. Monari and G. Dreyfus. Withdrawing an example from the training set: a n analytic estimation\nof itseffectonanonlinear parameterized model. Neurocomputing Letters ,35:195\u2013201,2000.\nC. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning (to appear) ,\n2001.\nA. Y. Ng. On feature selection: learning with exponentially many irrelevant f eatures as train-\ning examples. In 15th International Conference on Machine Learning , pages 404\u2013412. Morgan\nKaufmann, SanFrancisco,CA, 1998.\nA. Y. Ng and M. Jordan. Convergence rates of the voting Gibbs classi\ufb01e r, with application to\nBayesian featureselection. In 18thInternationalConference on MachineLearning ,2001.\nJ.Pearl.Causality . Cambridge UniversityPress,2000.\nF. Pereira, N. Tishby, and L. Lee. Distributional clustering of English wo rds. InProc. Meeting of\ntheAssociationfor Computational Linguistics ,pages 183\u2013190,1993.\nS. Perkins, K. Lacker, and J. Theiler. Grafting: Fast incremental fea ture selection by gradient\ndescentinfunctionspace. JMLR,3:1333\u20131356 (thisissue),2003.\nA.Rakotomamonjy. VariableselectionusingSVM-basedcriteria. JMLR,3:1357\u20131370(thisissue),\n2003.\nJ. Reunanen. Over\ufb01tting in making comparisons between variable selection me thods.JMLR, 3:\n1371\u20131382 (thisissue),2003.\nI. Rivals and L. Personnaz. MLPs (mono-layer polynomials and multi-layer perceptrons) for non-\nlinear modeling. JMLR,3:1383\u20131398 (thisissue),2003.\nB. Schoelkopf andA.Smola. Learningwith Kernels . MITPress,Cambridge MA,2002.\nD. Schuurmans. A new metric-based approach to model selection. In 9th Innovative Applications\nofArti\ufb01cialIntelligence Conference , pages 552\u2013558,1997.\nH. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. Ranking a rando m feature for variable and\nfeatureselection. JMLR,3:1399\u20131414(this issue),2003.\nR. Tibshirani. Regression selection and shrinkage via the lasso. Technic al report, Stanford Univer-\nsity,PaloAlto,CA, June1994.\nN. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. InProc. of the 37th\nAnnualAllertonConference onCommunication, ControlandComputing , pages 368\u2013377, 1999.\nK. Torkkola. Feature extraction by non-parametric mutual information maximiza tion.JMLR, 3:\n1415\u20131438 (thisissue),2003.\nV.G.Tusher,R.Tibshirani,andG.Chu. Signi\ufb01canceanalysisofmicroa rraysappliedtotheionizing\nradiationresponse. PNAS,98:5116\u20135121,April2001.\nV. Vapnik. Estimation of dependencies based on empirical data . Springer series in statistics.\nSpringer,1982.\n1181", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c494711-d91a-4721-b6fb-f659de0de062": {"__data__": {"id_": "6c494711-d91a-4721-b6fb-f659de0de062", "embedding": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a7b71192-b6fa-4f0b-a3bb-3cc7ed73be1d", "node_type": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "text": "GUYON AND ELISSEEFF\nV.Vapnik. StatisticalLearningTheory . JohnWiley&amp; Sons,N.Y.,1998.\nA. Vehtari and J. Lampinen. Bayesian input variable selection using poste rior probabilities and\nexpected utilities. ReportB31, 2002.\nJ.Weston,A.Elisseff,B.Schoelkopf,andM.Tipping. Useofthezero normwithlinearmodelsand\nkernelmethods. JMLR,3:1439\u20131461(thisissue),2003.\nJ. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapn ik. Feature selection for\nSVMs. In NIPS13,2000.\nE.P. Xing and R.M. Karp. Cliff: Clustering of high-dimensional microarray d ata via iterative fea-\nture \ufb01ltering using normalized cuts. In 9th International Conference on Intelligence Systems for\nMolecular Biology ,2001.\n1182", "start_char_idx": 0, "end_char_idx": 701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2fc61fe5-9dfc-41ca-a1ac-ba23017d1e12": {"__data__": {"id_": "2fc61fe5-9dfc-41ca-a1ac-ba23017d1e12", "embedding": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ce4202f-87ab-44fd-adb1-f12bbf48f224", "node_type": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "104ac59705109b5284550003d9992372c212dd58b298783696d6849c0953858d"}, "3": {"node_id": "de0c5b40-7b1d-42ab-9788-debfe1e85b78", "node_type": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4a18f93534a475a895f626d968d2298b2e1b3aa7a296ac93f718cef7f8320b48"}}, "hash": "eb60c61f994952055de76f552edc2f9989b77bd49fecc79cf15a96d78d9d8243", "text": "IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007 3\nAvoiding Pitfalls in Neural Network Research\nG. Peter Zhang\nAbstract \u2014Arti\ufb01cial neural networks (ANNs) have gained exten-\nsive popularity in recent years. Research activities are consider-\nable, and the literature is growing. Yet, there is a large amount ofconcern on the appropriate use of neural networks in publishedresearch. The purposes of this paper are to: 1) point out commonpitfalls and misuses in the neural network research; 2) draw at-tention to relevant literature on important issues; and 3) suggestpossible remedies and guidelines for practical applications. Themain message we aim to deliver is that great care must be taken inusing ANNs for research and data analysis.\nIndex Terms \u2014Data, model building, model evaluation, neural\nnetworks, pitfalls, publication bias, software.\nI. INTRODUCTION\nARTIFICIAL neural networks (ANNs) have enjoyed con-\nsiderable popularity in recent years. They have been used\nincreasingly as a promising modeling tool in almost all areas of\nhuman activities where quantitative approaches can be used to\nhelp decision making. Research efforts in ANNs are consider-\nable, and the literature is vast and growing. This trend will con-tinue in the foreseeable future. Indeed, ANNs have already been\ntreated as a standard nonlinear alternative to traditional models\nfor pattern classi\ufb01cation, time series analysis, and regressionproblems. In addition to numerous standalone software devoted\nto neural networks, many in\ufb02uential statistical, machine learn-\ning, and data-mining packages include neural network modelsas add-on modules in their recent editions.\nThe popularity of ANNs is, to a large extent, due to their\npowerful modeling capability for pattern recognition, objectclassi\ufb01cation, and future prediction without many unrealistic\nap r i o r i assumptions about the speci\ufb01c model structure and\ndata-generating process. The modeling process is highly adap-tive, and the model is largely determined by the characteristics\nor patterns the network learned from the data in the learning\nprocess. This data-driven approach is highly applicable for anyreal-world situation where theory on the underlying relation-\nships is scarce or dif\ufb01cult to prescribe but data are plentiful\nor easy to collect. In addition, the mathematical property of\nthe neural network in accurately approximating or representing\nvarious complex relationships has been established and sup-ported by solid theoretical work [9], [15], [23], [34], [48], [59],\n[64]\u2013[67], [125], [126]. This universal approximation capability\nis important because many decision support problems such aspattern recognition, classi\ufb01cation, and forecasting can be treated\nas function mapping or approximation problems.\nManuscript received February 5, 2003; revised August 3, 2005. This paper\nwas recommended by Associate Editor Jun Wang.\nThe author is with the J. Mack Robinson College of Business, Georgia State\nUniversity, Atlanta, GA 30303 USA (e-mail: gpzhang@gsu.edu).\nDigital Object Identi\ufb01er 10.1109/TSMCC.2006.876059Despite the growing success of neural networks, we have\nseen many problems, pitfalls, and misuses frequently emergewith neural network research and applications in the literature.\nANNs\u2019 powerful pattern recognition ability and \ufb02exible mod-\neling approach make them attractive, bringing with them greatopportunities and the strong potential to be useful in solving real-\nworld problems. But at the same time, this tremendous \ufb02exibility\nand wide applicability also exposes them to the real danger ofinappropriate uses. For example, ANNs", "start_char_idx": 0, "end_char_idx": 3662, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de0c5b40-7b1d-42ab-9788-debfe1e85b78": {"__data__": {"id_": "de0c5b40-7b1d-42ab-9788-debfe1e85b78", "embedding": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8ce4202f-87ab-44fd-adb1-f12bbf48f224", "node_type": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "104ac59705109b5284550003d9992372c212dd58b298783696d6849c0953858d"}, "2": {"node_id": "2fc61fe5-9dfc-41ca-a1ac-ba23017d1e12", "node_type": null, "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "eb60c61f994952055de76f552edc2f9989b77bd49fecc79cf15a96d78d9d8243"}}, "hash": "4a18f93534a475a895f626d968d2298b2e1b3aa7a296ac93f718cef7f8320b48", "text": "also exposes them to the real danger ofinappropriate uses. For example, ANNs have recently been pro-\nmoted as a data-mining tool to search for valuable information in\na large database. However, it is often too easy for the techniqueto be used instead for data dredging or data snooping [20], [127].\nThe real danger here is that even if there is no useful information\nin the data, neural networks may still \ufb01nd something \u201csigni\ufb01-cant,\u201d misleading unwary users. As Salzberg [104] points out,\n\u201cwhen one repeatedly searches a large database with powerful\nalgorithms, it is all too easy to \u2018\ufb01nd\u2019 a phenomenon or patternthat looks impressive, even when there is nothing to discover.\u201d\nThis danger of data snooping is also discussed by many re-\nsearchers including [22], [58], and [127].\nPitfalls can arise in the use of many quantitative methods.\nThis can happen when researchers do not have a complete un-derstanding of the technique or a careful design to avoid pos-\nsible abuses, especially when the technique can be \u201ceasily\u201d\nimplemented with an automatic software package. It is wellknown that statistics is often misused [2], [62], [69], [75]. As\nearly as in 1938, Cohen [29] observed various pitfalls in the\nuse of descriptive statistics in practice. King [76] identi\ufb01es aset of serious statistical mistakes appearing in the quantitative\npolitical science literature. Chat\ufb01eld [17] gives many examples\nof common modern-day pitfalls in statistical investigations andcomments that \u201cstatistics is perhaps more open to misuse than\nany other subject, particularly by the nonspecialist.\u201d Misuses of\ndiscriminant analysis are detailed in [39] for business, \ufb01nance,and economics applications, in [147] for medical diagnoses, and\nin [146] for psychology problems. The abuse of variance models\nin regression is discussed in [118]. Issues of statistical pitfalls\nrelated to model uncertainty and data dredging are discussed\nin [20] and [21].\nThe fundamental principle of ANNs for data analysis and\nmodeling is the same as or similar to that of statistics, and\nin many aspects ANNs can be treated as the nonlinear coun-terparts of statistical techniques [24], [28], [58], [101], [102],\n[108], [111], [124]. Thus, pitfalls in statistical analysis are likely\nseen also in neural network research. For example, Schwartzeret al. [140] list six major types of ANN misuses, which are\nsimilar to those observed in [147] with statistical discriminant\nanalysis. While any quantitative method is subject to misuses,methods that are complicated, automatic, and new are generally\nmore likely to be misused than simple, nonautomatic, and estab-\nlished methods. Because of their newness, complexity, and lack\n1094-6977/$25.00 \u00a9 2007 IEEE", "start_char_idx": 3586, "end_char_idx": 6296, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d0222da8-239c-4dad-89e0-9e8aeb695470": {"__data__": {"id_": "d0222da8-239c-4dad-89e0-9e8aeb695470", "embedding": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64fa3a3d-c466-495a-b6bc-dd9053dcda10", "node_type": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "93f3689c88e1ff4f0020eea9ccc5681671393154fd5359543d173661bbdfecf4"}, "3": {"node_id": "5e19d9ae-f851-42d2-80a0-575113f15109", "node_type": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "918bd859ea8f64e487260d64e86d8c17f6a1d47fd510b1115c891271e7c795be"}}, "hash": "f4db57d9ddc8b459af52b0397cf6e6bdf2e216e56de9fb8897cb181beebe888a", "text": "4 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\nof standard modeling procedure, as well as the \u201cblack-box\u201d na-\nture, ANNs are even more susceptible to the danger of misuses\nthan other subjects including statistics. These probably are the\nreasons that we see more controversial or contradictory resultsreported in neural network research.\nPitfalls and abuses in the neural network research are harm-\nful to the \ufb01eld. Indeed, skeptical opinions regarding neural net-\nworks as hype or passing fad are abundant [18], [19], [55], [129].\nSome of this skepticism may be justi\ufb01able given a large numberof problems observed in the neural network community. For ex-\nample, Schwarzer et al. [140] review the literature of neural net-\nwork applications for prognostic and diagnostic classi\ufb01cation inoenological studies between 1991 and 1995 and \ufb01nd the follow-\ning seven misuses: 1) mistakes in estimation of misclassi\ufb01cation\nprobabilities; 2) \ufb01tting of implausible functions; 3) incorrectlydescribing the complexity of a network; 4) no information on\ncomplexity of the network; 5) use of inadequate statistical com-\npetitors; 6) insuf\ufb01cient comparison with statistical method; and7) na \u00a8\u0131ve application to survival data. They conclude that \u201cthere\nis no evidence so far that application of ANNs represents real\nprogress in the \ufb01eld of diagnosis and prognosis in oncology.\u201d\nTherefore, in order for the \ufb01eld to grow in a healthy direction\nand achieve signi\ufb01cant advances in the future, it is important for\nresearchers to be aware of potential pitfalls as well as ways toavoid them. The goals of this paper are to: 1) point out various\ncommon pitfalls and misuses in the neural network research;\n2) draw attention to relevant literature on important issues; and\n3) suggest possible remedies and guidelines for practical appli-\ncations. We will mainly focus on the multilayer feedforward typeof neural networks, although many issues discussed could also\nbe applied and extended to other types of neural networks. The\nfocus on feedforward neural networks is due to their popularityin research and applications as according to Wong et al. [151],\nabout 95% of business applications of neural networks reported\nin the literature use this type of neural model. The main messagewe aim to deliver is that great care must be taken in using ANNs\nfor research and data modeling.\nThe remainder of the paper is organized as follows. Section II\nprovides several major factors that contribute to the common\npitfalls in neural network applications. Sections III\u2013IX discuss\nvarious pitfalls in neural network research as well as recom-mended approaches to avoid them. Finally, Section X provides\nconcluding remarks.\nII. F\nACTORS CAUSING COMMON PITFALLS\nPitfalls in neural network research arise in many different\nforms due to various factors. The most important contributionto the many pitfalls is perhaps the nonlinear nonparametric na-\nture of the neural network model. While this property is de-\nsirable for many real-world applications, it also brings about\nmore opportunities to go wrong in the modeling and application\nprocess. Compared to their linear statistical counterpart, neuralnetworks have fewer assumptions, more parameters to estimate,\nmany more options to select in the modeling process, all of\nwhich open more possibilities for inappropriate uses and prob-lematic applications. As Granger [56] puts it, \u201cbuilding modelsof nonlinear relationships are inherently more dif\ufb01cult than", "start_char_idx": 0, "end_char_idx": 3533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5e19d9ae-f851-42d2-80a0-575113f15109": {"__data__": {"id_": "5e19d9ae-f851-42d2-80a0-575113f15109", "embedding": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "64fa3a3d-c466-495a-b6bc-dd9053dcda10", "node_type": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "93f3689c88e1ff4f0020eea9ccc5681671393154fd5359543d173661bbdfecf4"}, "2": {"node_id": "d0222da8-239c-4dad-89e0-9e8aeb695470", "node_type": null, "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "f4db57d9ddc8b459af52b0397cf6e6bdf2e216e56de9fb8897cb181beebe888a"}}, "hash": "918bd859ea8f64e487260d64e86d8c17f6a1d47fd510b1115c891271e7c795be", "text": "modelsof nonlinear relationships are inherently more dif\ufb01cult than lin-\near ones. There are more possibilities, many more parameters\nand thus more mistakes can be made.\u201d\nThe second major reason is the lack of a uniform standard in\nbuilding neural network models. For example, numerous non-\nlinear algorithms that are alternatives or variations to the basic\nbackpropagation (BP) algorithm exist. These algorithms vary in\nef\ufb01ciency and effectiveness in estimating parameters. In addi-\ntion, different and sometimes con\ufb02icting guidelines are providedon many factors that could affect ANN performance. The prob-\nlem is that ANN models are sensitive to many of these factors.\nPitfalls are more likely to occur to unwary researchers who\nlack the expertise and knowledge of the various forms of abuses.\nThey often have the inappropriate supposition that ANNs can\nbe built with automatic software, and that users do not need toknow much of the model detail.\nAnother reason that many inappropriate uses of ANNs are\npublished is the lack of details on several key aspects of themodel-building process. Authors or researchers often do not\ngive suf\ufb01cient detail, essential features, or adequate description\nof their study methodology, which hinders easy understandingor replications for others. On the other hand, reviewers may not\npay attention to these issues. The lack of transparency, thus,\ncontributes to errors in published research work.\nIII. B\nLACK BOXTREATMENT OF ANN S\nNeural networks are often treated and used as black boxes.\nIn a survey by Vellido et al. [119], the lack of explanatory\ncapability in terms of the \u201cincapacity to identify the relevance\nof independent variables and to generate a set of rules to expressthe operation of the model\u201d is considered by researchers as the\nmain shortcoming in the application of neural networks. While\nit is true that ANNs are not able to give the same level of insight\nand interpretability as many statistical models, it is a pitfall to\ntreat them as complete black boxes with the assumption thatwe know nothing about the nature of the ANN model built for a\nparticular application except for the output estimate or predic-\ntion. Often, \u201cblack box\u201d is used either as an excuse to relieveresearchers from exploring further inquiries and examining the\nestablished model more rigorously or as a justi\ufb01cation for au-\ntomatic modeling so that people with little knowledge of neuralnetworks and subject matter can do the modeling easily [63].\nWith this view, users do not need to understand how the model\nworks and formal statistical tests may not be applied to test thesigni\ufb01cance of the model and the parameters. Faraway and Chat-\n\ufb01eld [43] have pointed out the potential danger of the opinion\nthat ANN models can be built blindly in \u201cblack box\u201d mode.\nAdvances in ANN research have suggested that neural net-\nworks are not totally unexplainable. In fact, there are consider-\nable research interests in offering insights into the \u201cblack box\u201d\noperation of neural networks. One active research area is in the\nunderstanding of the effect of explanatory variables on the de-pendent variable or output of the model. Numerous measures\nhave been proposed to estimate the relative importance or contri-\nbution of input variables. Some of these measures are reviewedin [130]. Intrator and Intrator [71] propose a method based on", "start_char_idx": 3467, "end_char_idx": 6823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ffa40d35-addb-4215-ade5-40d5e99a0794": {"__data__": {"id_": "ffa40d35-addb-4215-ade5-40d5e99a0794", "embedding": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "525b571f-a260-4cb8-8ed5-9062dd74803c", "node_type": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "33bd3d348d17b5aebac514bb5f2707d537e65c0b043bc2736dd1c2d5adb79eb6"}, "3": {"node_id": "cea5f0f6-713c-4a98-bf4a-c10a71482825", "node_type": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "5614f142aba9f125fd98bdfe760bc99123202c50edeb74ac5ba6ca31dd4d34bf"}}, "hash": "61ad54c4a9603417b7f1e08551a136780c972d1946ee16efb9957af14b9853a7", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 5\nthe robusti\ufb01cation technique to interpret neural network results\nin terms of the input effects and interactions among input vari-\nables. Another area of research is in rule or knowledge extraction\nfrom trained networks. Benitez et al. [10] and Castro et al. [16]\nestablish the equality of ANNs to fuzzy-rule-based systems and\npropose the methods to translate the knowledge embedded in\nthe neural networks into more understandable fuzzy rules. Se-\ntiono et al. [135], [136] propose the algorithms to discover\nrules from networks for regression problems. Andrews et al. [4]\nand Tickle et al. [116] survey the techniques used for extract-\ning rules and knowledge embedded in trained ANNs. Tickle\net al. [117] even conclude that after more than 10 years of re-\nsearch in the knowledge discovery in ANNs, we have already\nreached a point where the de\ufb01ciency of the black box nature is\n\u201call but redressed.\u201d\nIt is a well-known fact that, for classi\ufb01cation problems, ANNs\nprovide direct estimates of the posterior probabilities [52], [100],\n[122]. The interpretation of neural network outputs as poste-rior probabilities is of fundamental importance because many\ntraditional Bayesian classi\ufb01cation methods are established on\nthe ability to estimate the posterior probability. As summarizedin [122], \u201cInterpretation of network outputs as Bayesian proba-\nbilities allows outputs from multiple networks to be combined\nfor higher level decision making, simpli\ufb01es creation of rejectionthresholds, makes it possible to compensate for difference be-\ntween pattern class probabilities in training and test data, allows\noutput to be used to minimize alternative risk functions, and\nsuggests alternative measures of network performance.\u201d\nThe links and equivalence between ANNs and various tradi-\ntional statistical models have been well established. For exam-\nple, Raudys [97] shows that decision boundaries of single-layer\nperceptrons are equivalent or close to those of the seven statisti-cal classi\ufb01ers. Gallinari et al. [49] and Schumacher et al. [105]\nestablish the theoretical connection of ANNs to discriminant\nanalysis and logistic regression. Certain ANN models have beensuggested to be equivalent to conventional time series models.\nFor example, autoregressive models can be implemented via\nneural networks [31]. Connor et al. [30] demonstrate that recur-\nrent neural networks are a special case of nonlinear autoregres-\nsive and moving average models, while feedforwared ANNs are\na special case of nonlinear autogressive models. Therefore, thefundamental mechanism of neural networks is the same as or\nvery similar to many statistical methods in classi\ufb01cation and\nforecasting.\nMany people believe that the functional form of the neural\nnetwork model cannot be known precisely. In addition, the ex-act relationship between inputs and outputs is too complex to\nexpress. This is not true. The general functional form of a sin-\ngle hidden layer feedforward neural network can be written asfollows:\ny\nk=\u03b10k+q/summationdisplay\nj=1\u03b1jkf/parenleftBiggp/summationdisplay\ni=1\u03b2ijxi+\u03b20j/parenrightBigg\n+\u03b5,\nk=1,2,...,r (1)\nwhere {xi}and{yk}are the vectors of the input and output\nvariables, respectively; p,q, and rare the numbers of input,hidden, and output nodes; fis a transfer function such as the\npopular logistic f(x)=1/(1 + exp( \u2212x));{\u03b1jk}and{\u03b2ij}are\nthe sets of weights from the hidden", "start_char_idx": 0, "end_char_idx": 3426, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cea5f0f6-713c-4a98-bf4a-c10a71482825": {"__data__": {"id_": "cea5f0f6-713c-4a98-bf4a-c10a71482825", "embedding": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "525b571f-a260-4cb8-8ed5-9062dd74803c", "node_type": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "33bd3d348d17b5aebac514bb5f2707d537e65c0b043bc2736dd1c2d5adb79eb6"}, "2": {"node_id": "ffa40d35-addb-4215-ade5-40d5e99a0794", "node_type": null, "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "61ad54c4a9603417b7f1e08551a136780c972d1946ee16efb9957af14b9853a7"}}, "hash": "5614f142aba9f125fd98bdfe760bc99123202c50edeb74ac5ba6ca31dd4d34bf", "text": "sets of weights from the hidden to output nodes and from the\ninput to hidden nodes, respectively; and \u03b10kand\u03b20jare weights\nof arcs leading from the bias terms, which have values always\nequal to 1. Equation (1) suggests that the neural network model\nis a weighted sum of a basis function of a linear combination of\ninput variables.\nOf course, the model parameters such as p,q, andr,a sw e l la s\n{\u03b1jk}and{\u03b2ij}, will vary from application to application. But\nthe basic relationship speci\ufb01ed in (1) is always held. Thus, it is\nnot dif\ufb01cult to express the trained neural network model withan exact mathematical relationship, although it can be complex.\nThis knowledge along with the estimates of weights is often\nuseful as one may wish to perform further analysis to explorethe property of the relationship and extract the knowledge em-\nbedded in the connecting weights. Therefore, totally ignoring\nthe internal workings of ANNs could result in inappropriatetreatment of neural networks and loss of the opportunity to gain\ninsights from the established model.\nIV . O\nVERFITTING AND UNDERFITTING\nOver\ufb01tting is one of the most cited problems with ANNs. The\ntopic is well discussed and every neural network researcher isperhaps aware of the danger of over\ufb01tting. Over\ufb01tting limits the\ngeneralization ability of predictive models. For neural networks,\nit is easy to get a good or excellent result on the in-sample data,but this by no means suggests that a good model is found. It\nis likely that the model memorizes noises or captures spurious\nstructures, which will cause very poor performance in the out-of-sample data. Over\ufb01tting typically happens when users build\noverly large neural networks and/or the in-sample data used to\ntrain networks are small. Therefore, it is more likely to occur\nwith ANN models than most statistical models due to their\n\ufb02exible modeling approach and the large number of parametersto be estimated from the data.\nThe above fact is well known. The guidelines and techniques\nto avoid over\ufb01tting are plentiful. Unfortunately, the dangers ofover\ufb01tting \u201care not always heeded\u201d [21]. In the literature, we\nsee many applications with inappropriate model sizes relative\nto sample sizes. For example, Fletcher and Goss [44] use 36observations in their study of bankruptcy prediction applica-\ntion. Davis et al. [138] have only 32 observations in training\na neural network with more than 200 input nodes. Adya andCollopy [1] \ufb01nd that, among 27 effectively validated studies in\nforecasting and prediction, only three attempt to control the po-\ntential problem of over\ufb01tting. In a review for auditing and riskassessment applications, Calderon and Cheh [137] report that\nmost studies suffer from the over\ufb01tting or overtraining prob-\nlems. Business applications listed in [79] and [119] also indi-\ncate many over\ufb01tting problems. In a survey on 43 applications\nof neural networks for the forecasting of water resources vari-ables, Maier and Dandy [84] \ufb01nd that, for most applications, the\nrelationship between the network size in terms of the connection\nweights and training sample size is ignored and the number ofhidden nodes used is greater than the theoretical upper bounds.", "start_char_idx": 3395, "end_char_idx": 6580, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e830824-b646-4a31-b24f-24d469085926": {"__data__": {"id_": "8e830824-b646-4a31-b24f-24d469085926", "embedding": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0", "node_type": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "3f897ba4d5dfb7ad5077816d7ff2baaab8c66a160845a5f66e70ea871458ff8d"}, "3": {"node_id": "1c5d8e2d-84d7-4ee1-8b44-b0c67a863f77", "node_type": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "f8ee67091c90178b09696bd9d2808082479942379b294bf28ed28c072d287bf6"}}, "hash": "4515d5a11a7e7677bbe1b76e1caca290c14d3ebbf45e33118ccd29a055ffe7b7", "text": "6 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\nHippert et al. [61] review 40 papers in the applications of ANNs\nfor short-term load forecasting and conclude that \u201cmost of the\npapers proposed ANN architectures that seemed to be too large\nfor the data samples they intended to model .... These ANNs\napparently over\ufb01tted their data.\u201d\nAnother related pitfall is to include as many input variables\nas possible in the model, believing or hoping that the ANN can\nidentify the most important and relevant variables through the\nlinking weights\u2019 adjustment during the model-building process.Including a large number of unnecessary variables not only in-\ncreases the model complexity and the likelihood of over\ufb01tting,\nbut also causes more time and effort wasted in training. More-over, the true pattern may be masked by the irrelevant factors\nand their interactions. In [138], for an application of neural net-\nworks for audit control risk assessment, 210 input variables areused with only 32 observations in the training sample.\nOn the other hand, under\ufb01tting occurs if a neural network\nmodel is under-speci\ufb01ed or not trained well. With under\ufb01tting,the model does not give good \ufb01t even to the training set. While\nunder\ufb01tting is usually not a major concern compared to over-\n\ufb01tting, ignoring the under\ufb01tting can also cause problems inapplications, especially when the training algorithm is not ap-\npropriately used to guarantee a good solution in the estimation\nprocess.\nThe phenomena of under- and over\ufb01tting are well known\nin the statistical literature and are well discussed in the neural\nnetwork community with the concept of bias/variance tradeoff\n[50]. A large number of research publications have appeared\non the bias/variance issues of neural networks learning (see[130] for relevant research activities in the classi\ufb01cation area).\nAlthough most studies in the area are theoretical in nature, it is\nhelpful for ANN users to have a clear understanding of the basicissue of bias versus variance.\nBecause of the over- and under\ufb01tting problems, it is often de-\nsirable to report the training process and both the in-sample andout-of-sample performances. Published research studies, how-\never, do not do well in reporting the in-sample results. Although\nthe main focus and interest of ANN applications are on the out-of-sample performance, it may be dif\ufb01cult to comprehensively\njudge the quality of the model without also looking at the in-\nsample results. Adya and Collopy [1] \ufb01nd that the most commonproblem for effective implementation of the model in published\nstudies is their failure to report in-sample results. They further\nadvise, \u201cif a study does not report in-sample performance on\nthe network, we suggest caution in acceptance of its ex ante\nresults.\u201d\nBuilding a parsimonious model with minimum number of\ninput variables and parameters but at the same time achieving\nhigh predictive accuracy is critical to avoiding under- and over-\ufb01tting problems. To this end, researchers should combine both\nqualitative domain knowledge and quantitative variable selec-\ntion approach to select the most important predictor variables.Additionally, node pruning and weight elimination methods can\nbe used to reduce the complexity of the neural model [99]. For\nrecent reviews on statistics-based and neural-network-based fea-ture variable selection techniques, readers are referred to [72]\nand [130].V. D\nATA-RELATED PROBLEMS\nANNs are data-driven methods. Without data, it would be\nimpossible to build an ANN model in the \ufb01rst place. However,\nwhat data should be", "start_char_idx": 0, "end_char_idx": 3618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c5d8e2d-84d7-4ee1-8b44-b0c67a863f77": {"__data__": {"id_": "1c5d8e2d-84d7-4ee1-8b44-b0c67a863f77", "embedding": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0", "node_type": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "3f897ba4d5dfb7ad5077816d7ff2baaab8c66a160845a5f66e70ea871458ff8d"}, "2": {"node_id": "8e830824-b646-4a31-b24f-24d469085926", "node_type": null, "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4515d5a11a7e7677bbe1b76e1caca290c14d3ebbf45e33118ccd29a055ffe7b7"}}, "hash": "f8ee67091c90178b09696bd9d2808082479942379b294bf28ed28c072d287bf6", "text": "build an ANN model in the \ufb01rst place. However,\nwhat data should be used and what quality characteristics thedata should possess are rarely considered by ANN researchers.\nIn many applications, data are used as if they are free of errors\nand are representative of the true underlying process. This is\ncertainly not necessarily true in many situations. It is well known\nthat neural networks as well as other modeling techniques cannotturn garbage inputs into golden information, or \u201cgarbage in and\ngarbage out.\u201d Consequently, the reliability of neural network\nmodels depends to a great extent on the quality of data.\nData used in neural network research typically come from\ntwo sources: The primary source and the secondary source. The\nprimary data source is the original data collected with a speci\ufb01cresearch question to be investigated. On the other hand, the sec-\nondary data source contains data sets that have previously been\nused for other purposes. Both primary and secondary data havebeen used in neural network research, although the secondary\ndata source is used much more frequently due to its convenience\nand much less effort to obtain. Secondary data are often usedfor the purposes of model evaluation, model comparison, and\nmethodology illustration.\nData stored in organizational databases often contain signif-\nicant errors [80], which can affect the predictive accuracy of\nneural network models. While Bansal and Kauffman [6] \ufb01ndthat ANN models are more robust than linear regression models\nwhen data quality decreases, results reported in [78] suggest\nthat error rate and its magnitude can have substantial impact onneural network performance. Klein and Rossin [78] believe that\nan understanding of errors in a data set should be an important\nconsideration to ANN users and efforts to lower error rates arewell deserved.\nIn addition to errors in the data set, data representativeness\nis another issue that is largely ignored in the literature due tothe convenience of sample selection. If the data are not random\nor not representative of the true population, results obtained\nfrom the neural network analysis may not be useful or gener-alizable. Few studies, however, examine this issue explicitly.\nIn conventional time series forecasting, it is well known that\nnonstationarity can have signi\ufb01cant impact on the analysis and\nforecasting and preprocessing are often necessary to make data\nstationary. In the neural network literature, most studies do notconsider the possible effect of nonstationarity. Although some\nresearchers explicitly address the issues of model uncertainty\nand the shift of underlying data-generating process, others over-look them entirely, even if the data may indicate some potential\nproblems.\nAn interesting exception to the above representativeness is-\nsue is in a two-group classi\ufb01cation with ANNs. Although group\ncomposition of the two classes in the population is rarely equal\n(and in many cases extremely unbalanced), it is often bene\ufb01cialto include equal number of examples from both the classes to\nensure a good representation of the small group [11], [131].\nWilson and Sharda [128] and Jain and Nag [73] study the effectof training sample composition on neural classi\ufb01er performance\nand \ufb01nd that using balanced samples from two groups to build", "start_char_idx": 3552, "end_char_idx": 6843, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "779ac90b-690b-4dea-86d1-8cc1e795440a": {"__data__": {"id_": "779ac90b-690b-4dea-86d1-8cc1e795440a", "embedding": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "201136ce-f795-4b86-b23a-4feba505c627", "node_type": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "32e89ba79cd3d7f65c08ea7e9b4299ca39b6fed0e3104cad5394d697504473a0"}, "3": {"node_id": "31eae50a-01e2-472e-b1a7-ac5c75cf5ab3", "node_type": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "c9faa28545a0494dbcbe9b84539dee41140168a475c4c1fb976731ac4199cf4a"}}, "hash": "4190f466f7189d4d4ddccdffaf47d8d97e02c71304e2e31c611ef0e4260b1a22", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 7\nthe network model can yield the best prediction results on the\nholdout sample, even if the holdout sample is representative of\nunbalanced population. Thus, using unrepresentative in-sample\ndata in this context is warranted. In other words, using a \u201crep-resentative\u201d sample can result in a suboptimal solution in the\npractical application of the neural classi\ufb01er.\nMany studies have used data sets stored in online reposito-\nries such as the University of California, Irvine (UCI) machine\nlearning repository and several well-known forecasting compe-tition databases. Often these public data sets are used to either\ndemonstrate the proposed new methodology or compare differ-\nent methods. It is advantageous to have a common convenientdatabase to serve as a benchmark to test new methods. Fisher\u2019s\niris data for classi\ufb01cation and Wolf\u2019s sunspot series for nonlin-\near time series forecasting are two well-known data sets used bynumerous researchers for many decades. But the problem with\nusing the same data set repeatedly is that signi\ufb01cant results may\nbe \u201cmere accidents of chance\u201d [104]. Individual data snoopingis possible if an individual becomes familiar with the character-\nistics of some data sets and develops speci\ufb01c algorithms tailored\nto these data sets. Denton [36] and Salzberg [104] analyze thedanger of the so-called \u201ccollective data mining\u201d and show that\nusing the same data set by more than one investigator distorts\nthe Type I error rate\u2014the probability of making at least onemistake. The publication bias against nonsigni\ufb01cant results can\nfurther exacerbate the problem [8].\nAnother problem with the use of public data is that the data\nset may not be a random or unbiased sample even though the\ndata repository contains a large number of data sets. For exam-ple, the well-known M1 and M3 forecasting competition data\nrepositories comprise more than 1000 and 3000 business and\neconomic time series, respectively, with different types and pe-riodicities. Yet, few agree that they are random or representative\nsamples of all possible data series [85], [90]. Therefore, it is\na pitfall to try to generalize the results beyond the data setstested. One possible solution to this problem is to increase the\ndata repository over time to improve representativeness and to\nbe careful in interpreting results obtained from using such datasets. Another solution is to use arti\ufb01cial or simulation data to\ncontrol the properties of the data for which new methods or\nalgorithms are targeted. Thus, the use of arti\ufb01cial data can \u201ctestmore precisely the strengths and weaknesses\u201d [104] of a new\nmethod.\nSample size of the data set is another important issue in all\nquantitative modeling endeavors including neural network anal-\nysis. Some techniques have a higher requirement on minimumsize for data modeling and analysis. For example, in the time\nseries forecasting context, Box and Jenkins [12] suggested that\nat least 50, or better 100, observations are necessary to build asuccessful autoregressive integrated moving average (ARIMA)\nmodel. Neural networks are nonlinear nonparametric models\nthat typically require larger sample sizes than conventional sta-tistical procedures for model building and validation. In general,\nthe larger the sample is, the better is the chance for a neural net-\nwork to adequately approximate the underlying complex pat-terns without suffering from the problem of over\ufb01tting. Raudys\nand Jain [98] study small sample size effects and \ufb01nd that smallsample size can make the problem of designing a pattern classi-\n\ufb01er very dif\ufb01cult. On the other hand, we have seen reports that\nlarger sample sizes do not always", "start_char_idx": 0, "end_char_idx": 3698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31eae50a-01e2-472e-b1a7-ac5c75cf5ab3": {"__data__": {"id_": "31eae50a-01e2-472e-b1a7-ac5c75cf5ab3", "embedding": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "201136ce-f795-4b86-b23a-4feba505c627", "node_type": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "32e89ba79cd3d7f65c08ea7e9b4299ca39b6fed0e3104cad5394d697504473a0"}, "2": {"node_id": "779ac90b-690b-4dea-86d1-8cc1e795440a", "node_type": null, "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4190f466f7189d4d4ddccdffaf47d8d97e02c71304e2e31c611ef0e4260b1a22"}}, "hash": "c9faa28545a0494dbcbe9b84539dee41140168a475c4c1fb976731ac4199cf4a", "text": "On the other hand, we have seen reports that\nlarger sample sizes do not always result in better out-of-sample\nperformances [120]. This could also be true for time series fore-casting problems in which data may not be stationary or may\ncontain structural changes over time.\nThe literature certainly does not give speci\ufb01c guidance on the\nsample size requirement for particular applications other than\nthe general recommendation for larger samples. Indeed, thereis no such thing as \u201cone size \ufb01ts all\u201d because the appropriate\nsample size depends on many factors such as the complexity of\nthe problem, the number of input variables, the number of pa-rameters in the model, and the noise in the data. Neural network\nresearchers, therefore, must be cognizant of the sample size is-\nsue in designing their particular models. In particular, smallersample size requires researchers to pay closer attention in select-\ning model parameters. Though sample size is often constrained\nby the availability of data, the practice of simply accepting adata set regardless of its sample size should be avoided. If the\nsample size is too small, remedial actions such as resampling or\ncross-validation techniques may not be very helpful.\nAn important issue related to sample size is data splitting, or\ndividing the data into two portions: an in-sample and an out-of-\nsample. The in-sample is used for model \ufb01tting and estimationwhile the out-of-sample or holdout sample is used to evaluate\nthe predictive ability of the model. Because of the bias/variance\nconcern, it is critical to test the ANN model with an independent\nholdout data set, which is not used in neural network training.\nThat is, we must set a portion of the whole available data asideand never touch it in the model-building process. This practice\nis necessary to ensure that the model \ufb01nally built has a true value\nfor practical uses. As a consequence, the sample size used totrain the network is smaller than the total number of available\ndata points. The true size for model building may be further\nreduced if the in-sample data are further divided into a trainingset for model estimation and a validation set for model selection.\nIn a recent survey of 43 papers in forecasting water resources\nvariables using ANNs, it was found that \u201cdata division wascarried out incorrectly in most papers .... The proportion of\ndata used for training and validation varied greatly. Generally,\nthe division of data was carried out on an arbitrary basis andthe statistical properties of the respective data sets were seldom\nconsidered\u201d [84].\nThere is no consensus on how to divide the data into an in-\nsample for learning and an out-of-sample for testing. Picard\nand Berk [142] suggest that 25%\u201350% data are used for val-idation for linear regression problems and if the emphasis is\non parameter estimation, fewer observations should be reserved\nfor validation. According to [22], forecasting analysts typicallyretain about 10% of the data as holdout sample. Granger [56]\nrecommends that, for nonlinear modeling, at least 20% of any\nsample should be held back for an out-of-sample evaluation.Michie et al. [88] also recommend holding back approximately\n20% of the data for testing and dividing the remaining data into\na training set and a validation set. Hoptroff [63] suggests us-ing 10%\u201325% of data as the testing sample while Church and\nCurram [27] use \u201cmore conservative 30%.\u201d Many other different", "start_char_idx": 3620, "end_char_idx": 7051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5846ae96-8da0-4508-9076-4ea967c70230": {"__data__": {"id_": "5846ae96-8da0-4508-9076-4ea967c70230", "embedding": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29566d85-e498-4767-8362-36e69e2deea3", "node_type": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "f61f3e3c9778509e3846c3a2f90ac4c892461204a1a884cf9de334ce241f6d1d"}, "3": {"node_id": "e58ca118-5200-446c-887a-ef2a97b870f0", "node_type": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "26628e28b99239bc04f7230fb0f496c2de78633d0641a2b66c630cc3b7318d2a"}}, "hash": "f7bb5f7701f2bcd1364941cf6e9921f61e90068cf4a5846ca6a897ad5894817f", "text": "8 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\nsplitting strategies have been used in the literature. When the\nin-sample data need to be further split into a training sample\nand a validation sample, the issue is more complicated. Hastie\net al. [60], however, note a typical division of 50% for training\nand 25% each for validation and testing.\nIt is important to note that the issue of data splitting is not\nabout what proportion of data should be put in each subsample.\nBut rather it is about an adequate sample size in each sam-\nple to ensure suf\ufb01cient learning, validation, and testing. As theavailable data size varies dramatically from application to ap-\nplication, the number of observations used in each sample can\ndiffer greatly with the same proportion of data for testing pur-poses. Although statistical methods such as regression can have\nas much as 50% of the data used for testing [109], most neu-\nral network applications may not be able to afford that largeportion and typically a much smaller percentage such as 10%\nor 20% should be considered. Nevertheless, it is important to\ninclude a suf\ufb01cient number of observations in the test sampleso that the model\u2019s generalization ability can be evaluated ade-\nquately. Hoptroff [63] recommended at least 10 data points in the\ntest sample, while the study by Ashley [5] suggested that muchlarger out-of-sample size is necessary to achieve statistically sig-\nni\ufb01cant improvements for forecasting problems. Although more\ntest data are desirable to ensure that the test sample performanceis not due to chance, the tradeoff has to be made between the\nin-sample size and the test sample size and typically more data\nshould be allocated for ANN model construction.\nThe lack of guidelines does not mean that data splitting may\nbe done arbitrarily. When the size of available data set is large(e.g., more than 1000 observations), different splitting strategies\nmay not have a major impact on adequate learning and evalua-\ntion. But it is quite different when the sample size is small. Inaddition, splitting generally should be done randomly to make\nsure each subsample is representative of the population. There\nis no question of doing this for regression and classi\ufb01cationproblems. On the other hand, time series data are dif\ufb01cult or\nimpossible to be split randomly. For time series problems, data\nsplitting is typically done at researchers\u2019 discretion. However, itis still important to make sure that each portion of the sample is\ncharacteristic of the true data-generating process. LeBaron and\nWeigend [81] evaluate the effect of data splitting on time seriesforecasting and \ufb01nd that data splitting can cause more sample\nvariation, which in turn causes the variability of forecast per-\nformance. They caution the pitfall of ignoring variability across\nthe splits and drawing too strong conclusions from such splits.\nTheir \ufb01nding is in line with that of Faraway [41] who shows thatfor regression modeling, data splitting may increase variability\nin estimates. Furthermore, data splitting may lose ef\ufb01ciency and\neffectiveness in different contexts (see [21], [32], and [42]).\nVI. M\nODEL BUILDING\nBuilding a successful predictive neural network model is not\nan easy task. There are many possible ways to build an ANN\nmodel and a large number of choices to make during the model-\nbuilding process. Numerous parameters and issues need to beconsidered and experimented with before a satisfactory modelmay emerge. Adding to the dif\ufb01culty is the lack of standards\nin the process and there are a great number of controversial\nrules of thumb and guidelines in the literature. It is important to\nnote that most empirical rules work only for special problemsor situations, and", "start_char_idx": 0, "end_char_idx": 3787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e58ca118-5200-446c-887a-ef2a97b870f0": {"__data__": {"id_": "e58ca118-5200-446c-887a-ef2a97b870f0", "embedding": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "29566d85-e498-4767-8362-36e69e2deea3", "node_type": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "f61f3e3c9778509e3846c3a2f90ac4c892461204a1a884cf9de334ce241f6d1d"}, "2": {"node_id": "5846ae96-8da0-4508-9076-4ea967c70230", "node_type": null, "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "f7bb5f7701f2bcd1364941cf6e9921f61e90068cf4a5846ca6a897ad5894817f"}}, "hash": "26628e28b99239bc04f7230fb0f496c2de78633d0641a2b66c630cc3b7318d2a", "text": "to\nnote that most empirical rules work only for special problemsor situations, and therefore, treating these rules as universal and\nusing them blindly is a pitfall that should be avoided.\nThe major decisions a designer or a builder of a neural net-\nwork model must make include data preparation, input variable\nselection, the network architecture parameters such as the num-ber of input, hidden and output nodes, node connection, training\nalgorithm, transfer functions, and many others. Some of these\nissues must be solved before actual model building starts whileothers are determined during the model-building process. Neu-\nral network design should be treated as a more important issue\nthan the subsequent analysis because if there are \ufb02aws in thedesign of ANN model building, then further analyses are worth-\nless no matter how good they may look. Unfortunately, great\nemphasis is often placed on the analysis of the results ratherthan on good design issues in neural network research.\nData preprocessing is often recommended and used to high-\nlight important relationships and create more uniform data tofacilitate network learning, meet algorithm requirements, or\navoid computation problems. However, the necessity and effect\nof data preprocessing on neural network learning and predictionis still undecided as research \ufb01ndings are often contradictory.\nSome researchers conclude that because of the universal ap-\nproximation capability, data preprocessing is not necessary and\nthe model can pick up all the underlying structure from the\nraw data. For example, Gorr [54] believes that neural networksshould be able to simultaneously detect both the nonlinear trend\nand the seasonality in the data. Earlier studies on seasonal time\nseries forecasting found that neural networks are able to di-rectly model the seasonal behavior and preseasonal adjustment\nis not necessary [107]. Recent studies, however, suggest that\npredeseasonalizing data is critical in improving forecasting per-formance [89], [145]. Callen et al. [14] report discouraging\nresults with neural networks for predicting quarterly accounting\nearnings. One of the potential reasons is that they do not considerdata preprocessing such as deseasonalization. Thus, ignoring the\npotential effect of data transformation or using inappropriate\ndata preprocessing techniques can reach quite different resultsor conclusions.\nOne related problem with data transformation is that results\nwith transformed data will have different scale than the original\ndata. To better interpret the results or to compare them with other\nmethods built with raw data, the outputs from the ANN modelsneed to be scaled back to the original data range. From a practical\npoint of view, the accuracy measure obtained by the ANNs\nshould be based on the original data scale. In many studies,however, researchers fail to indicate whether the performance\nmeasures are calculated on the original or transformed scale.\nDetermining appropriate neural network architectures is one\nof the most important tasks and numerous guidelines are avail-\nable. Yet, many pitfalls have been observed in building and\nselecting ANN models.\nFor most regression and classi\ufb01cation problems, the numbers\nof input and output nodes are usually determined based on prior", "start_char_idx": 3705, "end_char_idx": 6983, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6f25503e-9d81-4dcc-8386-09da9b2b1eda": {"__data__": {"id_": "6f25503e-9d81-4dcc-8386-09da9b2b1eda", "embedding": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7be1bbf4-4356-4e23-b140-55c33f115ccc", "node_type": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "49cd8355d9fcf6cad605895d221011124b5a0cdc64e3fea4c255779374cdfb19"}, "3": {"node_id": "05771a4e-97d9-47cc-8f67-ce38a1288bb5", "node_type": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "78ca13cf0c22874a93220907b68a243566f29253a73ce81626f1880fe4dc37e3"}}, "hash": "91bf55c5931d26573b645630dfca0651ce5476ba531ca8ed24d51467422d2d0d", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 9\nor subject matter knowledge. For the time series forecasting\nproblems, however, both numbers need to be determined via\nexperimentations. In particular, the choice of input variables is\ncritical [133]. One of the pitfalls is the use of the linear methodto select model parameters. For example, principle component\nanalysis (PCA) has been used for feature selection in regression\nand classi\ufb01cation problems and the ARIMA model has been\nsuggested and employed for selecting input lag structure in time\nseries forecasting [83], [114]. Park et al. [91] use both linear\nAR model to identify the input lag structure and PCA to deter-\nmine the number of hidden nodes. Balkin and Ord [141] apply\nthe stepwise regression approach to select the inputs to neuralnetworks. The inappropriateness of these methods is that they\ncannot capture nonlinear structures. In addition, linear models\nsuch as PCA are unsupervised learning procedures and do notconsider the correlation between dependent variables and input\nvariables.\nAnother related problem in determining input variables is the\ntendency to throw a large number of variables to the model\nregardless of their relevance or redundancy, hoping ANNs can\npick the most appropriate input variables by adjusting the linkingweights. The potential effects of this practice are the over\ufb01tting,\nmasked patterns, and increased modeling time. On the other\nhand, choosing input variables \u201ctoo carefully\u201d via data snoopingand then reporting the best results as if the input variables are\nchosen in the \ufb01rst place can be even more dangerous.\nAlmost all studies in time series forecasting use one output\nnode for both one-step forecasting and multistep forecasting.\nWhile single output node networks are suitable for one-stepforecasting, they may not be effective for multistep forecasting\nsituations as empirical \ufb01ndings [87] suggest that a forecasting\nmodel best for a short term is not necessarily good for a longterm. For this and other reasons [132], it is recommended that\nmultiple output nodes be used for multistep forecasting situa-\ntions. This is consistent with the suggestion in [21] and [22] touse different models for different lead times.\nNeural networks with single hidden layer have been shown to\nhave universal approximation ability and they are also relativelyeasier to train. This is the reason that two or more hidden-layered\nnetworks are rarely used in applications. However, excluding\nmore hidden layers from considerations may cause inef\ufb01ciencyand poor performance in neural network training and prediction,\nespecially when a one-layer model requires a large number of\nhidden nodes to give desirable performance. Research studies\nin [77], [110], and [134] show that two-hidden-layer networks\ncan provide more bene\ufb01ts for some problems.\nThe number of hidden nodes determines not only the network\ncomplexity to model nonlinear and interactive behavior but also\nthe ability of neural networks to learn and generalize. Too manyor too few will cause the over\ufb01tting or under\ufb01tting problem.\nUnfortunately, there is no unique magic formula that can be used\nto calculate this parameter before training starts and it usuallymust be determined by the trial and error method. Interestingly,\nif the number of hidden nodes could be predetermined, ANNs\nwould not be called a \u201cdata-driven\u201d method because hiddennodes to a large extent determine the neural network model.\nAlthough empirical formulas or rules are plentiful, users shouldbe careful in applying them. In the literature, some studies have\nblindly used previous empirical rules without further exploring\nthe possibility that the number is not optimal for their particular\napplications, while others choose a particular amount withoutreporting how it is obtained.\nTraining a neural network is a complicated issue because of\nthe nonlinear", "start_char_idx": 0, "end_char_idx": 3884, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "05771a4e-97d9-47cc-8f67-ce38a1288bb5": {"__data__": {"id_": "05771a4e-97d9-47cc-8f67-ce38a1288bb5", "embedding": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7be1bbf4-4356-4e23-b140-55c33f115ccc", "node_type": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "49cd8355d9fcf6cad605895d221011124b5a0cdc64e3fea4c255779374cdfb19"}, "2": {"node_id": "6f25503e-9d81-4dcc-8386-09da9b2b1eda", "node_type": null, "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "91bf55c5931d26573b645630dfca0651ce5476ba531ca8ed24d51467422d2d0d"}}, "hash": "78ca13cf0c22874a93220907b68a243566f29253a73ce81626f1880fe4dc37e3", "text": "obtained.\nTraining a neural network is a complicated issue because of\nthe nonlinear optimization involved. A good training algorithm\ncan make a difference in adequate model estimation. Therefore,\nusers should use more ef\ufb01cient algorithms whenever possible.Because of the local minima problem inherent in nonlinear op-\ntimization procedures, \ufb01nding a global optimal or better local\nsolution is the goal in the training process. Because of the sen-sitivity of neural network estimation to the initial conditions,\nusing multiple random starting points to reduce the risk of bad\nlocal minima is often recommended. Nevertheless, many stud-ies still use older less ef\ufb01cient BP algorithms due to easy access\nand availability in software, and do not consider multiple train-\ning methods. Curry and Morgan [33] discussed many problemswith the basic BP training algorithm.\nCommon practice in building a neural network model is to\ndivide the available data into two portions: An in-sample formodel building and a holdout sample or out-of-sample for model\ntesting or assessment. The in-sample data may be further split\ninto a training sample for model \ufb01tting and a validation sam-ple for model selection. It is important to note that except for\nthe training sample, the nomenclature for the other two sam-\nples is not used consistently in the literature. That is, a \u201cvali-\ndation (or test) sample\u201d in one study may become a \u201ctest (or\nvalidation) sample\u201d in another one. It can be further compli-cated if all available data are divided into only two portions, the\nlater part is sometimes called \u201ctest\u201d sample while other times\n\u201cvalidation.\u201d This inconsistency may cause confusion to someresearchers.\nThe major pitfall in ANN model building is the use of the\nwhole data set to do model estimation and model selection. Thiscan happen in two forms. The \ufb01rst is that researchers divide\nthe data into only two portions of a training set and a test (or\nvalidation) set and then choose the model based on the bestperformance of the test set. The second is that researchers do\nhave three portions of training, validation, and test samples. But\nrather than using the last sample as an independent one for modelevaluation, they use it repeatedly to \ufb01ne tune the model estima-\ntion and selection process conducted on the \ufb01rst two parts of the\ndata. Of course, without appropriate evaluation, the model de-\nveloped may not have any value for practical uses as the model\nis tailored too much to the data on hand and the true perfor-mance on unseen data cannot be assessed. However, this lack\nof independent holdout samples for a genuine out-of-sample\nevaluation is fairly common in published research as Duin [38]points out, \u201cthere will be a strong temptation for the researcher\nto do some more tuning when he \ufb01nds out that his neural net-\nwork performs relatively badly. Papers that emphasize that thishas not been done are very rare.\u201d Lisboa [148, p. 29] notes\nthat in most medical applications of ANNs, \u201cthere is no attempt\nto separate a design data set, used for training and parametertuning, or testing, from a validation set used for performance\nestimation.\u201d", "start_char_idx": 3801, "end_char_idx": 6939, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "565a65e4-4f3c-45d5-9edb-40c45f9f8dd6": {"__data__": {"id_": "565a65e4-4f3c-45d5-9edb-40c45f9f8dd6", "embedding": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a09293c-15d6-4168-a143-4ca043b3e3cc", "node_type": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "0e1202c4a357dfd27102ae190e9f3fe898e1efb93807c8e9bcf893997c474aba"}, "3": {"node_id": "c3258fa9-5cd8-40ec-88c5-6397a1295197", "node_type": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4d7a7dceab210ab5b79ab969d8a45f7e0fc3a43f75ca0ee8fe7ff7255237276b"}}, "hash": "8ad4941f434543d0cd1bf39c3a3842bc28376f71de3ab057f7452b62bdfc4bcf", "text": "10 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\nIt should be noted that if the cross-validation approach is\nused to select the best model, then the validation sample result\nshould not be treated as the true performance of the model.\nTo test a model selected this way, an independent validationsample must be used. It is, however, possible to combine the\n\ufb01rst two parts of the data (training and validation) to reestimate\nthe model parameters. Of course, there is no guarantee that\nthis will yield better results out of sample. If this strategy is\nused in the research, it is important to make it clear in thepublication.\nIt is important to note that it is possible to use only two data\nsets for model building and testing. This is typically the casewhen researchers apply some special in-sample model-building\nand selection procedures. Some of the pruning methods such as\nnode and weight pruning [99] as well as constructive methodssuch as the upstart and cascade correlation algorithms [40], [47]\nbelong to these methods. In-sample model selection approaches\nbased on traditional information-based criteria such as Akaike\u2019sinformation criterion (AIC) and Bayesian (BIC) or Schwarz\ninformation criterion (SIC) have been proposed and used in\ntime series studies [21], [43]. However, these in-sample modelselection criteria are developed based on the assumption of\nasymptotic normality of the maximum likelihood estimators.\nAlthough suitable and commonly used for linear parametricmodels, in-sample criteria are not directly applicable to and\ntheoretically justi\ufb01ed for neural networks [3]. Furthermore, the\neffectiveness of these criteria for nonlinear neural network mod-\nels has not been supported by empirical studies as Swanson\nand White [112], [113] and Qi and Zhang [95] \ufb01nd that thein-sample criteria such as AIC and BIC cannot provide a reli-\nable guide to out-of-sample prediction performance. The use of\nthese information-based selection criteria in practice, therefore,should be with caution.\nAnother major pitfall in the published research is the lack of\ndetail of the model-building process. Some studies simply listthe architectures used without giving any indication on how a\nparticular architecture is selected. Others give some vague justi-\n\ufb01cation without giving further detail. In fact, statements similarto \u201cour networks used ten nodes in the hidden layer, which\nwas found to be suf\ufb01cient for all the models\u201d [27] and \u201cwe\ntried and tested a number of different architectures of the neuralnetwork .... The results reported here are based upon the best\nANN forecast\u201d [14] are common. This lack of clear documenta-\ntion of the model-building process is a serious problem reported\nin several recent surveys of ANN applications [61], [79], [84].\nWithout suf\ufb01cient detail of the modeling process, it is dif\ufb01cult\nor impossible to judge if the research design is conducted appro-\npriately as well as how much tuning is done. Of course, tuning\nthe parameters in the training set is acceptable. But if the test setis also involved, then it is problematic. Furthermore, as repeated\ntuning of modeling parameters could be done during the process,\nit is impossible for others to replicate the study if the authorsdo not report this tuning process as results obtained from neural\nnetworks can vary dramatically depending on numerous factors\nincluding weight initialization, learning rate, momentum, train-ing length, stopping condition, and whether or not using special\ntechniques such as weight decay and node pruning are used. Asreplicability is a critical principle of scienti\ufb01c research, lack of\ndetail can be detrimental to the neural network", "start_char_idx": 0, "end_char_idx": 3721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c3258fa9-5cd8-40ec-88c5-6397a1295197": {"__data__": {"id_": "c3258fa9-5cd8-40ec-88c5-6397a1295197", "embedding": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0a09293c-15d6-4168-a143-4ca043b3e3cc", "node_type": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "0e1202c4a357dfd27102ae190e9f3fe898e1efb93807c8e9bcf893997c474aba"}, "2": {"node_id": "565a65e4-4f3c-45d5-9edb-40c45f9f8dd6", "node_type": null, "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "8ad4941f434543d0cd1bf39c3a3842bc28376f71de3ab057f7452b62bdfc4bcf"}}, "hash": "4d7a7dceab210ab5b79ab969d8a45f7e0fc3a43f75ca0ee8fe7ff7255237276b", "text": "research, lack of\ndetail can be detrimental to the neural network \ufb01eld.\nOne recent example of the importance of replication is given\nby Racine [96] who tries to repeat a previous study by Qi [94].Although the complete design was not detailed in the original\narticle and recalled from the author, Racine was able to conduct\nan approximate replication study by using the same data, soft-\nware, and modeling approach as in Qi [94] as well as several\ndifferent scenarios when exact detail was not available. His re-sults suggest that \u201cboth replicability and the claimed superiority\nof the ANN are elusive\u201d [96, p. 380]. Another example is the\nattempt by Zhao et al. [143] to replicate a previous study by Hill\net al. [144] for a time series forecasting. They \ufb01nd that while the\ngeneral conclusion of good neural network performance is still\nvalid, they are not able to achieve the same magnitude of theimprovement reported in [144] due to the lack of information on\nseveral key factors in the original study, which does not permit\nreproducing them.\nThus, it is imperative to report the detail of the ANN model de-\nsign and model-building process. The minimum detail should in-\nclude the architectures experimented, data splitting and prepro-cessing, training settings such as weight initialization method,\nlearning rate, momentum, training length, stopping condition,\nalgorithm used, and model selection criterion. If special proce-dures such as regularization, weight decay, or node pruning are\nemployed, it is necessary to give the detail or references.\nVII. S\nOFTWARE USES\nThere are many software packages available in ANNs, rang-\ning from stand-alone freeware or shareware to expensive com-mercial packages. These packages vary greatly in features,\noptions, training algorithm, programming capability, and user\ninterface. While the availability of powerful and easy-to-use\nANN software greatly enhances the research capability and re-\nsearch activities, it also increases the risk of misuse and er-ror [33]. The real danger is the tendency for ANN users \u201cto\nthrow a problem blindly at a neural network in the hope that it\nwill formulate an acceptable solution\u201d [46].\nIt is unwise to believe each software package has the same\ncapability to perform modeling and predicting tasks and can be\nrelied upon to give satisfactory results. It is especially dangerousto believe that the software can be used in a purely automatic\nmode and that users without much knowledge of ANNs can\nbuild ANN models easily and successfully. If a user does nothave knowledge of the many important issues in ANN model\nbuilding mentioned earlier and are not aware of the choices,\nassumptions, default settings, training algorithm, and limitationsof the package, it is not unlikely that dubious results will be\ngenerated. Even with a good solid software package, users still\nface a large number of important choices and decisions to make,\nand using the default settings is not always the best strategy.\nMost ANN software packages serve only as a convenient\nmeans to do nonlinear optimization inherent in any neural net-\nwork training. They do not address adequately the issue of data\nsplitting, model selection, and model evaluation. The methodused for choosing the number of parameters is often not well", "start_char_idx": 3656, "end_char_idx": 6932, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3c789972-a947-411a-b5da-c0d09680da8a": {"__data__": {"id_": "3c789972-a947-411a-b5da-c0d09680da8a", "embedding": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f23a4b70-f536-4585-8e92-286c459c5e2d", "node_type": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "887d5edcbb82d61768118464ac6fa82b9250b46f5afd5ef3eecfca24fa38708c"}, "3": {"node_id": "8c22712d-644d-4209-bc65-6b52fa516820", "node_type": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "6fc62fcb533523c5b7b1a46e94dfb38676c6114caabebe47e46a8ea7b15a2bd7"}}, "hash": "3723a20064ca76a3e869fbea2e861d8399e9c3e68dcd98d611eb282b14d0899e", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 11\nimplemented. In addition, many vendors of ANN software do\nnot address the over\ufb01tting problem and use the result from the\ntraining data as a selling point for their algorithms [35].\nMATLAB Neural Network Toolbox is one of the most pop-\nular commercial packages in the market. Yet, many problems\nexperienced with the software such as the reliability and numer-\nical accuracy have been reported by Gencay and Selcuk [51].\nCurry and Morgan [33] raise the concern of inappropriate use\nof training algorithms, especially the widely used BP algorithmin many ANN software packages. Because of its popularity, the\nBP algorithm may be the default of many packages even though\nit has many weaknesses. Because of this limitation and becauseof unwary users who may not be aware of the internal workings\nof the algorithm, \u201ccommercially available software should be\nused with great care\u201d [33, p. 131].\nTherefore, it is critical to fully understand the capabilities as\nwell as the limitations of the software. Users should be famil-\niar with the many default values on key parameters and defaultsettings and, if possible, how to make changes on these val-\nues. Small toy problems should be tried to gain con\ufb01dence on\nthe capability of the software before serious applications areimplemented.\nVIII. M\nODEL EV ALUATION AND COMPARISON\nOnce the modeling process is completed, model performance\nmust be tested or validated using the data not used in the model-building stage. In addition, as ANNs are often used as a nonlinear\nalternative to traditional statistical models, the performance of\nANNs needs to be compared to that of conventional methodsto show the value of ANN models. As noted in [1], \u201cif such\na comparison is not conducted, it is dif\ufb01cult to argue that the\nstudy has taught us much about the value of ANNs.\u201d\nThere are many problems in neural network research with\nregard to an appropriate evaluation and comparison of neuralnetwork models. According to Adya and Collopy [1], \u201ca sig-\nni\ufb01cant portion of the ANN research in forecasting and predic-\ntion lacks validity\u201d in terms of: 1) comparison with establishedmethods; 2) use of true out-of-sample for testing; and 3) use\nof a reasonable test sample size. Flexer [45] criticizes the lack\nof statistical evaluations in published studies and proposes theminimum requirements for such evaluations.\nAs pointed out earlier, one of the major pitfalls in the lit-\nerature is the failure to use independent holdout samples forout-of-sample evaluations. Sometimes, the holdout sample is\nclearly not used and other times, it is not clear if the holdout\nsample is used to \ufb01nd the best model or \ufb01ne tune the networkparameters. It is worthwhile to reemphasize that the holdout (or\ntest) sample should not be used in the model estimation and\nselection process. If it is, then it is not a genuine out-of-sample.\nUnfortunately, many ANN researchers overlook this point and\nregard the accuracy measures obtained in this way as being atrue out-of-sample testing result. As the real prediction accuracy\nwill be generally worse than that found for the holdout sample\nthat has already been used in the model-building process, it isalmost certain that the reported ANN performance is overstated.Prechelt [92] examines nearly 200 papers on ANN learning\nalgorithms published in four leading neural network journals and\n\ufb01nds that many of them are not evaluated thoroughly enough to\nbe \u201cacceptable.\u201d He de\ufb01nes the criteria for acceptable evaluationas: 1) use of at least two real problems; and 2) comparison with at\nleast one alternative algorithm. Unfortunately, 78% of published\nstudies do not meet this minimum standard. In a", "start_char_idx": 0, "end_char_idx": 3703, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8c22712d-644d-4209-bc65-6b52fa516820": {"__data__": {"id_": "8c22712d-644d-4209-bc65-6b52fa516820", "embedding": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f23a4b70-f536-4585-8e92-286c459c5e2d", "node_type": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "887d5edcbb82d61768118464ac6fa82b9250b46f5afd5ef3eecfca24fa38708c"}, "2": {"node_id": "3c789972-a947-411a-b5da-c0d09680da8a", "node_type": null, "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "3723a20064ca76a3e869fbea2e861d8399e9c3e68dcd98d611eb282b14d0899e"}}, "hash": "6fc62fcb533523c5b7b1a46e94dfb38676c6114caabebe47e46a8ea7b15a2bd7", "text": "Unfortunately, 78% of published\nstudies do not meet this minimum standard. In a survey [45] of\ntwo leading journals in neural networks, only three out of 43\npapers clearly use the third independent data set. Adya andCollopy [1] \ufb01nd that 21 out of 48 (44%) studies in business\napplications are not effectively evaluated or validated. Vellido\net al. [119] give similar \ufb01ndings. Maier and Dandy [84] report\nthat among 43 ANN applications to water resource variable\nforecasting, \u201conly two papers used an independent test set in\naddition to the training and validation sets.\u201d In a review ofneural networks used for short-term load forecasting in the last\ndecade, Hippert et al. [61] conclude that most ANN models are\n\u201cnot systematically tested.\u201d The same problem is also reportedby a number of other survey studies such as [137] and [140].\nTo adequately evaluate the true performance of a model,\nenough sample size should be allocated to the holdout test sam-ple. With too few observations in the testing set, it is possible\nthat the results obtained are due to chance. One of the problems\nreported in [1] is the insuf\ufb01cient sample size for validation.For evaluation purposes, they \ufb01nd that 40 or more cases for\nclassi\ufb01cation and 75 or more observations for forecasting are\nreasonable. Ashley [5] has recently studied the issue of how\nmuch out-of-sample data are necessary in order for the forecast-\ning improvement to be statistically signi\ufb01cant. His simulationresults suggest that at least 100 observations are necessary in\norder for a 20% forecasting error reduction to be statistically\nsigni\ufb01cant at the 5% level.\nA major problem in evaluating the ANN classi\ufb01ers is the\nfailure to consider the relative cost of misclassi\ufb01cation. Many\nstudies only use the overall classi\ufb01cation rate as the sole per-formance measure of the capability of the model, taking no\naccount of different misclassi\ufb01cation errors, which are typically\nmore critical for various decision-making situations. Berardiand Zhang [139] discuss the effect of unequal misclassi\ufb01ca-\ntion costs on classi\ufb01cation as well as decision making and \ufb01nd\nthat misclassi\ufb01cation cost could have signi\ufb01cant impact on theclassi\ufb01cation results and ignoring the cost information could\nadversely affect the decision making. In a survey of neural net-\nwork applications in auditing and risk assessment, Calderon and\nCheh [137] \ufb01nd that almost all the studies do not take different\ncosts into consideration.\nMany pitfalls lie in the inappropriate comparison of ANN\nmodels with other models in statistics, machine learning, and\ndata mining. Either there is no comparison at all or the com-parison is not done in an entirely satisfactory manner. Often\nthe comparison is made based on simple measures of accuracy,\nand statistical signi\ufb01cance is mostly overlooked. In addition, thecomparison to important benchmark models is not often con-\nducted. For example, in the \ufb01nancial time series literature, the\nrandom walk model often emerges as the dominant one amongmany linear and nonlinear methods. In classi\ufb01cation problems,\nit is well known that by chance alone, one can achieve high-hit", "start_char_idx": 3624, "end_char_idx": 6749, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d1a2adc-5071-43eb-bf4a-410bd174bed1": {"__data__": {"id_": "2d1a2adc-5071-43eb-bf4a-410bd174bed1", "embedding": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c126f3d3-56ed-416e-8c77-7a89015a94a6", "node_type": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "3bc7bb504c3058fd6c196a6827450d71b85104bf11c10e74d7e1d86b84940b3d"}, "3": {"node_id": "d4de6415-2189-485d-b0d2-f6a67a303dac", "node_type": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "0852ca76e6a8cd6836ec76925592ebd53fcb5c3f5d732e186e9c0074e55c1dcc"}}, "hash": "6aaaab140ee93dfdfadc24c9b19be17f1a88f3f122faf5eda4fec2b52a87fb12", "text": "12 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\nrate if the classes are very unbalanced. Therefore, it is important\nto compare the performance of neural networks to that of the\nbenchmarks. It is possible that ANNs may outperform another\nstatistical model but both fail to show signi\ufb01cant improvementover the benchmark models.\nOther pitfalls on comparing classi\ufb01ers have been pointed out\nby several authors [38], [92], [104]. Duin\u2019s [38] major con-\ncern is the user-dependent nature of neural network results, and\nneural network investigators may have \u201ca strong temptation\u201dto do more tuning if they \ufb01nd their models performing rela-\ntively poorly. Therefore, the performance reported can be bi-\nased. Prechelt [92] reports that 33% of 190 studies he examinedhave no comparison with other algorithms. Salzberg [104] \ufb01nds\nthat the literature largely ignores the experimentwise overall er-\nror in comparing multiple classi\ufb01ers and discusses the majorproblem of the practice of using simple t-tests to compare mul-\ntiple algorithms on multiple data sets even if the test sets are not\nindependent.\nAdya and Collopy [1] propose the following three valida-\ntion criteria to objectively evaluate the performance of ANNs:\n1) comparing to well-accepted models; 2) using true out-of-samples; and 3) ensuring enough sample size in the out-of-\nsample (40 for classi\ufb01cation and 75 for time series forecasting).\nStatistical testing should be considered in most of the com-\nparisons. As many comparisons are based on the same hold-\nout sample, special matched sample statistical procedures can\nbe used. To compare accuracies of several classi\ufb01ers, the F\n+\nstatistic based on repeated measures analysis and described by\nLooney [82] is recommended. If only two classi\ufb01ers are in-volved, then the binomial test [104] for dependent samples or\nthe Goldstein test [53] for independent samples can be used.\nIn addition, the Kappa statistic has been increasingly recom-mended as an appropriate measure for agreement between clas-\nsi\ufb01ers when the data are categorical [149], [150]. For time series\nforecasting, researchers should consider the Diebold\u2013Marianotest [37].\nMany studies use only one training sample and one vali-\ndation or testing sample to compare different models. Resultsbased on one comparison may suffer from sample biases [106]\nor random in\ufb02uences [45]. This is largely due to the unsta-\nble nature of the neural network model in model building andestimation [13], [122]. Therefore, it is often desirable to use\nmultiple runs or samples. Bootstrap and other resampling tech-\nniques are useful in this regard to evaluate ANN performance\nstatistically [25]. Major multiple sample approaches to classi\ufb01er\nevolutions can be found in [72]. Although bootstrapping maybe dif\ufb01cult for time series forecasting problems, using multiple\ncross-validation techniques [68], [74] and multiple test peri-\nods [115] can be helpful.\nIt is also important to note that while summary measures\nsuch as the overall error rate or average absolute error are\nuseful to give overall performance of the model, they do notprovide useful information for decision makers regarding indi-\nvidual case decisions. In addition, they do not provide evidence\non the quality of each point estimate or prediction. For classi-\ufb01cation problems, the receiver-operating characteristics (ROC)\ncurve [57] is better used as it is a more comprehensive perfor-mance measure than the single classi\ufb01cation error measure. One\nimportant", "start_char_idx": 0, "end_char_idx": 3544, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4de6415-2189-485d-b0d2-f6a67a303dac": {"__data__": {"id_": "d4de6415-2189-485d-b0d2-f6a67a303dac", "embedding": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c126f3d3-56ed-416e-8c77-7a89015a94a6", "node_type": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "3bc7bb504c3058fd6c196a6827450d71b85104bf11c10e74d7e1d86b84940b3d"}, "2": {"node_id": "2d1a2adc-5071-43eb-bf4a-410bd174bed1", "node_type": null, "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "6aaaab140ee93dfdfadc24c9b19be17f1a88f3f122faf5eda4fec2b52a87fb12"}}, "hash": "0852ca76e6a8cd6836ec76925592ebd53fcb5c3f5d732e186e9c0074e55c1dcc", "text": "measure than the single classi\ufb01cation error measure. One\nimportant feature of the ROC curve is its ready incorporation of\nprevalence and misclassi\ufb01cation cost factors, which are critical\nfor many decision-making problems where different misclas-si\ufb01cation errors carry signi\ufb01cantly uneven consequences. Lis-\nboa [148] points out that although ROC is the de facto standard in\nmedical diagnosis, it is unfortunate that \u201cscant attention is given\nto the usually skewed nature of the data.\u201d On the other hand, con-\n\ufb01dence or prediction intervals should also be used in conjunctionwith the point estimates or predictions for forecasting especially\ntime series forecasting problems [26], [35], [70], [103]. The use\nof con\ufb01dence interval provides a means to assess the reliabilityof the model and its estimates.\nIX. P\nUBLICATION BIAS\nPublication bias against nonsigni\ufb01cant results can promote\nsome pitfalls in neural network research. It encourages data\nsnooping by repeatedly tuning the model architectures and otherparameters if initial results on the holdout sample are not satis-\nfactory. Although mixed \ufb01ndings do appear in published studies,\nthe general tendency in neural network research discouragesthe negative results reported with ANNs. Overall, it is much\neasier to publish positive \ufb01ndings than to publish negative re-\nsults [8], [19].\nMany large quantitative competitions in forecasting [86] and\nclassi\ufb01cation [88] show that no single method including neural\nnetworks is dominantly the best for every problem in everysituation. Thus, to prove that neural networks are universally\nthe best can be futile. However, in ANN research, studies are\nplentiful aiming to show and claim the best performance ofANNs for all problems. Often times, the conclusion is drawn\non the limited empirical evidence based on a limited number of\ndata sets. Therefore, readers of these \ufb01ndings must be aware of\nthe fact that the obtained results may not be able to generalize\nbeyond the data sets used in the particular study. It is quitepossible that a particular method or algorithm for ANNs works\nwell for a fairly large number of problems, but there are possibly\nmany other problems or data sets, for which the method mayperform badly.\nThe problem can be this. If a researcher tried a number of\ndata sets and chose to report only the good results with his/hermethod, then the study is easy to get through the review process\nfor publication. However, if he or she honestly reports mixed\nresults with the method, it is dif\ufb01cult or impossible for the studyto be published. This is an awkward situation. On one hand, we\naccept the reality that there is no such thing as the best method\nor algorithm for any problem. On the other hand, we are notready to publish mixed results in one study. Of course, with\nmixed \ufb01ndings, researchers must seek to address under what\nconditions or for what types of problems the proposed method\nworks best. Addressing such issues, however, is not always an\neasy task.\nAnother major problem in the published articles is that many\napplications of ANNs do not reveal the detail of many aspects\nof the modeling process including data, data processing, exper-imental design, model selection, parameter, and other tunings", "start_char_idx": 3478, "end_char_idx": 6705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7dac650b-20b9-44d0-a421-b4163e55cdc5": {"__data__": {"id_": "7dac650b-20b9-44d0-a421-b4163e55cdc5", "embedding": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "b92aaffce55f627c27c4a8113c82f6525c2b14a72bd43d1561221de33cbc949c"}, "3": {"node_id": "6a7e5d01-af71-4c39-9478-d85c81c048d9", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "644a94a186a6435ae4e087272288546423ce3e76cb3d38fe109e83d85f377d83"}}, "hash": "4b546abd43627433ebca10d8e88d23b41d0d679726cbf4f57e77a9e9176c52be", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 13\nmade during the process. This can hide pitfalls and misuses\nof the technique employed in studies. It will also affect the\nobjective and rigorous evaluation of the methods used. More\nimportantly, it prevents the possibility of replication, a \u201ckeyrequirement of genuine scienti\ufb01c progress\u201d [19].\nReviewers of academic journals should set high standards\nin reviewing articles. They should demand more detailed de-\nscriptions on several key modeling parameters as well as the\nmodel-building experiment. Analysis of the results is certainlyimportant but only after the modeling exercise is done \ufb02awlessly.\nOn the other hand, we should be more open-minded to accept\nmixed or negative \ufb01ndings toward ANNs for well-planned andexecuted studies.\nX. C\nONCLUSION\nIn this paper, we present many pitfalls in neural network\nresearch and applications. We believe that the key to avoiding\npitfalls in neural network research is the awareness of the poten-tial pitfalls and their harms to the research study. It is important\nto realize that there are numerous ways that ANN techniques\ncan be misapplied and misused. Unwary investigators are morelikely to incur pitfalls. Furthermore, an awareness of the prob-\nlems can lead to healthy skepticism and higher standards in the\ninterpretation of reported \ufb01ndings in the literature. We also offermany insights and good practices that can help neural network\nresearchers and practitioners in their endeavor to improve the\nquality of research and application.\nThere is no doubt that ANNs can be one of the most useful\ntools in one\u2019s toolkit for quantitative modeling. ANNs are good\nalternative candidates to traditional modeling techniques fortasks of pattern recognition, pattern classi\ufb01cation, system con-\ntrol, and forecasting. The promises and opportunities of neural\nnetwork research are evident, judging by the growing literature\nand numerous exciting business, industrial, and medical appli-\ncations. Despite the skepticism, the \ufb01eld of neural network iswell past the stage of a \u201cpassing fad.\u201d\nHowever, ANNs are not a panacea for all problems under all\nenvironments. They cannot replace all other data analysis meth-ods from statistics, machine learning, and data mining. They are\nnot without problems and dif\ufb01culties. It is incorrect to believe\nthat like statistics, ANNs are already an established \ufb01eld andthe application of ANNs can be as easy as running automated\nsoftware. It is important to note that the uncritical use of the\nvery \ufb02exibility inherent in the nonlinear capabilities of neuralnetworks can easily generate implausible solutions, leading to\nexaggerated claims of their potentials. It is equally important\nto note that there are still many practical and theoretical prob-lems that hinder the development and practical use of neural\nnetworks.\nAs pointed out earlier, ANNs can be treated as statistical\nmethods. Consequently, general guidelines and good practices\nfrom statistics can and should be considered and followed inANN research [17], [93]. Neural network community may gain\nmuch by listening to Kingman\u2019s address to statisticians: \u201ceven\nif statistics can never be a closed profession, it would be bothfoolish and irresponsible to deny a collective responsibility onthe part of the statistical community, a responsibility to en-\nsure the highest possible standards of competence, integrity and\njudgment ....\u201d If not, \u201cthe credibility of statistics as a whole is\nthreatened\u201d [77].\nR\nEFERENCES\n[1] M. Adya and F. Collopy, \u201cHow effective are neural networks at forecasting\nand prediction? A review and evaluation,\u201d J. Forecast. , vol. 17, pp. 481\u2013\n495,", "start_char_idx": 0, "end_char_idx": 3661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6a7e5d01-af71-4c39-9478-d85c81c048d9": {"__data__": {"id_": "6a7e5d01-af71-4c39-9478-d85c81c048d9", "embedding": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "b92aaffce55f627c27c4a8113c82f6525c2b14a72bd43d1561221de33cbc949c"}, "2": {"node_id": "7dac650b-20b9-44d0-a421-b4163e55cdc5", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4b546abd43627433ebca10d8e88d23b41d0d679726cbf4f57e77a9e9176c52be"}, "3": {"node_id": "c8943b4b-2f25-4092-8631-2b31b2e8be9f", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "cc1394e56ffe3a12753d910d28559e7dab38cf8b5dae736379583f655b5bb96c"}}, "hash": "644a94a186a6435ae4e087272288546423ce3e76cb3d38fe109e83d85f377d83", "text": "J. Forecast. , vol. 17, pp. 481\u2013\n495, 1998.\n[ 2 ] E .C .A l m e r , Statistical Tricks and Traps: An Illustrated Guide to the\nMisuses of Statistics . Los Angeles, CA: Pyrczak, 2000.\n[3] U. Anders and O. Korn, \u201cModel selection in neural networks,\u201d Neural\nNetw. , vol. 12, pp. 309\u2013323, 1999.\n[4] R. Andrews, J. Diederich, and A. B. Tickle, \u201cSurvey and critique of\ntechniques for extracting rules from trained arti\ufb01cial neural networks,\u201dKnowl.-Based Syst. , vol. 8, no. 6, pp. 373\u2013389, 1995.\n[5] R. Ashley, \u201cStatistically signi\ufb01cant forecasting improvements: How much\nout-of sample data is likely necessary?\u201d Int. J. Forecast. , vol. 19, pp. 229\u2013\n239, 2003.\n[6] A. R. Bansal, J. Kauffman, and R. R. Weitz, \u201cComparing the modeling\nperformance of regression and neural networks as data quality varies: A\nbusiness value approach,\u201d J. Manage. Inf. Syst. , vol. 10, pp. 11\u201332, 1993.\n[7] A. R. Barron, \u201cA comment on \u2018Neural networks: A review from a statistical\nperspective\u2019,\u201d Stat. Sci. , vol. 9, pp. 33\u201335, 1994.\n[8] C. B. Begg and J. A. Berlin, \u201cPublication bias: A problem in interpreting\nmedical data (with discussion),\u201d J. R. Stat. Soc. Ser. A , vol. 151, pp. 419\u2013\n463, 1988.\n[9] M. R. Belli, M. Conti, P. Crippa, and C. Turchetti, \u201cArti\ufb01cial neural\nnetworks as approximators of stochastic processes,\u201d Neural Netw. , vol. 12,\nno. 4\u20135, pp. 647\u2013658, 1999.\n[10] J. M. Benitez, J. L. Castro, and I. Requena, \u201cAre arti\ufb01cial neural networks\nblack boxes?\u201d IEEE Trans. Neural Netw. , vol. 8, no. 5, pp. 1156\u20131164,\nSep. 1997.\n[11] M. Bishop, Neural Networks for Pattern Recognition . Oxford: Oxford\nUniv. Press, 1995.\n[12] G. E. P. Box and G. M. Jenkins, Time Series Analysis: Forecasting and\nControl . San Francisco, CA: Holden-Day, 1976.\n[13] P. Burrascano, M. Battisti, and D. Pirollo, \u201cFinite sample size and neural\nmodel uncertainty,\u201d Neurocomputing , vol. 19, pp. 121\u2013131, 1998.\n[14] J. L. Callen, C. C. Y . Kwan, P. C. Y . Yip, and Y . Yuan, \u201cNeural network\nforecasting of quarterly accounting earnings,\u201d Int. J. Forecast. , vol. 12,\npp. 475\u2013482, 1996.\n[15] J. L. Castro, C. J. Mantas, and J. M. Ben \u00b4\u0131tez, \u201cNeural networks with a\ncontinuous squashing function in the output are universal approximators,\u201d\nNeural Netw. , vol. 13, no. 6, pp. 561\u2013563, 2000.\n[16] J. L. Castro, I. Requena, and J. M. Benitez, \u201cInterpretation of arti\ufb01cial\nneural networks by means of fuzzy rules,\u201d IEEE Trans. Neural Netw. ,\nvol. 13, no. 1, pp. 101\u2013116, Jan.", "start_char_idx": 3631, "end_char_idx": 6062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8943b4b-2f25-4092-8631-2b31b2e8be9f": {"__data__": {"id_": "c8943b4b-2f25-4092-8631-2b31b2e8be9f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "336a0ff8-9216-4f1c-85b3-20a8b1c529cf", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "b92aaffce55f627c27c4a8113c82f6525c2b14a72bd43d1561221de33cbc949c"}, "2": {"node_id": "6a7e5d01-af71-4c39-9478-d85c81c048d9", "node_type": null, "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "644a94a186a6435ae4e087272288546423ce3e76cb3d38fe109e83d85f377d83"}}, "hash": "cc1394e56ffe3a12753d910d28559e7dab38cf8b5dae736379583f655b5bb96c", "text": ",\nvol. 13, no. 1, pp. 101\u2013116, Jan. 2002.\n[17] C. Chat\ufb01eld, \u201cAvoiding statistical pitfalls,\u201d Stat. Sci. , vol. 6, no. 3, pp. 240\u2013\n268, 1991.\n[18]\n, \u201cNeural networks: Forecasting breakthrough or just a passing fad?\u201d\nInt. J. Forecast. , vol. 9, pp. 1\u20133, 1993.\n[19] , \u201cPositive or negative?\u201d Int. J. Forecast. , vol. 11, pp. 501\u2013502, 1995.\n[20] , \u201cModel uncertainty, data mining and statistical inference,\u201d J. R.\nStat. Soc. Ser. A , vol. 158, pp. 419\u2013466, 1995.\n[21] , \u201cModel uncertainty and forecast accuracy,\u201d J. Forecast. , vol. 15,\npp. 495\u2013508, 1996.\n[22] Time-Series Forecasting . Boca Raton, FL: Chapman & Hall/CRC,\n2001.\n[23] T. Chen and H. Chen, \u201cUniversal approximation to nonlinear operators by\nneural networks with arbitrary activation functions and its application todynamical systems,\u201d Neural Netw. , vol. 6, pp. 911\u2013917, 1995.\n[24] B. Cheng and D. Titterington, \u201cNeural networks: A review from a statis-\ntical perspective,\u201d Stat. Sci. , vol. 9, no. 1, pp. 2\u201354, 1994.\n[25] M. R. Chernick, V . K. Murthy, and C. D. Nealy, \u201cApplication of bootstrap\nand other resampling techniques: Evaluation of classi\ufb01er performance,\u201d\nPattern Recognit. Lett. , vol. 3, pp. 167\u2013178, 1985.\n[26] G. Chryssolouris, M. Lee, and A. Ramsey, \u201cCon\ufb01dence interval prediction\nfor neural network models,\u201d IEEE Trans. Neural Netw. , vol. 7, no. 1,\npp. 229\u2013232, Jan. 1996.\n[27] K. B. Church and S. P. Curram, \u201cForecasting comsumers\u2019 expenditure:\nA comparison between econometric and neural network models,\u201d Int. J.\nForecast. , vol. 12, pp. 255\u2013267, 1996.", "start_char_idx": 6058, "end_char_idx": 7592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ddedcbde-18dc-40db-a01d-4bd767f7bb23": {"__data__": {"id_": "ddedcbde-18dc-40db-a01d-4bd767f7bb23", "embedding": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67c6fcd3-9922-45dd-823f-9deac67ac662", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "8f422dff8972d4662b23bcb9ae92836b69b198606e1c8d82077f51300e539538"}, "3": {"node_id": "06da36df-7d8e-46c0-9ca0-d9715428a94d", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "1e9b3a428beffae5ebf27545c979331d245570ef19594149f6058268208682ba"}}, "hash": "ec4927860db5901b82b2d4f911b9719282775477102caeca890067dd4e47e089", "text": "14 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\n[28] A. Ciampi and Y . Lechevallier, \u201cStatistical models as building blocks\nof neural networks,\u201d Commun. Stat.\u2014Theory Methods , vol. 26, no. 4,\npp. 991\u20131009, 1997.\n[29] J. B. Cohen, \u201cThe misuse of statistics,\u201d J. Amer. Stat. Assoc. , vol. 33,\npp. 657\u2013674, 1938.\n[30] J. T. Connor, R. D. Martin, and L. E. Atlas, \u201cRecurrent neural networks\nand robust time series prediction,\u201d IEEE Trans. Neural Netw. , vol. 51,\nno. 2, pp. 240\u2013254, Mar. 1994.\n[31] M. Cottrell, B. Girard, Y . Girard, M. Mangeas, and C. Muller, \u201cNeural\nmodeling for time series: A statistical stepwise method for weight elim-\nination,\u201d IEEE Trans. Neural Netw. , vol. 6, no. 6, pp. 1355\u20131364, Nov.\n1995.\n[32] D. R. Cox, \u201cA note on data-splitting for the evaluation of signi\ufb01cance\nlevels,\u201d Biometrika , vol. 62, no. 2, pp. 441\u2013444, 1975.\n[33] B. Curry and P. Morgan, \u201cNeural networks: A need for caution,\u201d Omega ,\nvol. 25, no. 1, pp. 123\u2013133, 1997.\n[34] G. Cybenko, \u201cApproximation by superpositions of a sigmoidal function,\u201d\nMath. Control Signals Syst. , vol. 2, pp. 303\u2013314, 1989.\n[35] R. D. D. Veaux, J. Schumi, J. Schweinsberg, and L. H. Ungar, \u201cPrediction\nintervals for neural networks via nonlinear regression,\u201d Technometrics ,\nvol. 40, no. 4, pp. 273\u2013282, 1998.\n[36] F. Denton, \u201cData mining as an industry,\u201d Rev. Econ. Stat. , vol. 67, pp. 124\u2013\n127, 1985.\n[37] F. X. Diebold and R. S. Mariano, \u201cComparing predictive accuracy,\u201d J.\nBus. Econ. Stat. , vol. 13, pp. 253\u2013263, 1995.\n[38] R. P. W. Duin, \u201cA note on comparing classi\ufb01ers,\u201d Pattern Recognit. Lett. ,\nvol. 17, pp. 529\u2013536, 1996.\n[39] R. A. Eisenbeis, \u201cPitfalls in the application of discriminant analysis in\nbusiness, \ufb01nance, and economics,\u201d J. Finance , vol. 32, no. 3, pp. 875\u2013\n900, 1977.\n[40] D. Touretzky, Ed. Denver, CO:Morgan Kau\ufb01nann,S. Fahlman and\nC. Lebiere, \u201cThe cascade-correlation learning architecture,\u201d in Advances\nin Neural Information Processing Systems , vol. 2, D. Touretzky, Ed. Den-\nver, CO: Morgan Kau\ufb01mann, pp. 524\u2013532.\n[41] J. J. Faraway, \u201cOn the cost of data analysis,\u201d J. Comput. Graph. Stat. ,\nvol. 1, pp. 213\u2013229, 1992.\n[42] , \u201cData splitting strategies for reducing the effect of model selection\non inference,\u201d Comput. Sci. Stat. , vol. 30, pp. 332\u2013341, 1998.\n[43] J. J. Faraway and C. Chat\ufb01eld, \u201cTime", "start_char_idx": 0, "end_char_idx": 2372, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "06da36df-7d8e-46c0-9ca0-d9715428a94d": {"__data__": {"id_": "06da36df-7d8e-46c0-9ca0-d9715428a94d", "embedding": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67c6fcd3-9922-45dd-823f-9deac67ac662", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "8f422dff8972d4662b23bcb9ae92836b69b198606e1c8d82077f51300e539538"}, "2": {"node_id": "ddedcbde-18dc-40db-a01d-4bd767f7bb23", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "ec4927860db5901b82b2d4f911b9719282775477102caeca890067dd4e47e089"}, "3": {"node_id": "a4f8d700-2d0c-493b-a8bc-22bd4e713274", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "ee8c27ed0d3a2997cad2d2f66ca080e3578a552df8d6528a812a85a6119f2015"}}, "hash": "1e9b3a428beffae5ebf27545c979331d245570ef19594149f6058268208682ba", "text": "J. J. Faraway and C. Chat\ufb01eld, \u201cTime series forecasting with neural net-\nworks: A comparative study using the airline data,\u201d Appl. Stat. , vol. 47,\npp. 231\u2013250, 1998.\n[44] D. Fletcher and E. Goss, \u201cForecasting with neural networks\u2014An appli-\ncation using bankruptcy data,\u201d Inf. Manage. , vol. 24, pp. 159\u2013167, 1993.\n[45] A. Flexer, \u201cStatistical evaluation of neural network experiments: Minimum\nrequirements and current practice,\u201d in Proc. 13th Eur. Meeting Cybern.\nSyst. Res. , 1996, R. Trappl, Ed., pp. 1005\u20131008.\n[46] I. Flood and N. Kartam, \u201cNeural network in civil engineering\u2014I: Princi-\nples and understanding,\u201d J. Comput. Civil Eng. , vol. 8, no. 2, pp. 131\u2013148,\n1994.\n[47] M. Frean, \u201cThe Upstart algorithm: A method for constructing and train-\ning feed-forward networks,\u201d Neural Comput. , vol. 2, pp. 198\u2013209,\n1990.\n[48] K. Funahashi, \u201cOn the approximate realization of continuous mappings\nby neural networks,\u201d Neural Netw. , vol. 2, pp. 183\u2013192, 1989.\n[49] P. Gallinari, S. Thiria, R. Badran, and F. Fogelman-Soulie, \u201cOn the re-\nlationships between discriminant analysis and multilayer perceptrons,\u201d\nNeural Netw. , vol. 4, pp. 349\u2013360, 1991.\n[50] S. Geman, E. Bienenstock, and T. Doursat, \u201cNeural networks and the\nbias/variance dilemma,\u201d Neural Comput. , vol. 5, pp. 1\u201358, 1992.\n[51] R. Gencay and F. Selcuk, \u201cSoftware reviews: Neural network toolbox 3.0\nfor use with MATLAB,\u201d Int. J. Forecast. , vol. 17, pp. 305\u2013322, 2001.\n[52] H. Gish, \u201cA probabilistic approach to the understanding and training of\nneural network classi\ufb01ers,\u201d in Proc. IEEE Int. Conf. Acoustic, Speech\nSignal Process. 3\u20136, 1990, vol. 3, pp. 1361\u20131364.\n[53] M. Goldstein, \u201cAn approximate test for comparative discriminatory\npower,\u201d Multivariate Behav. Res. , vol. 11, pp. 157\u2013163, 1976.\n[54] W. L. Gorr, \u201cResearch prospective on neural network forecasting,\u201d Int. J.\nForecast. , vol. 10, pp. 1\u20134, 1994.\n[55] R. S. Govindaraju and A. R. Rao, \u201cArti\ufb01cial neural networks in hydrology:\nA passing fad?\u201d J. Hydrologic Eng., ASCE , vol. 5, no. 3, pp. 225\u2013226,\n2000.\n[56] C. W. J. Granger, \u201cStrategies for modeling nonlinear time-series relation-\nships,\u201d Econ. Rec. , vol. 69, pp. 233\u2013238, 1993.\n[57] D. J. Hand, Construction and Assessment of Classi\ufb01cation Rules . Chich-\nester, U.K.: Wiley, 1997.[58] , \u201cData mining: Statistics and more?,\u201d Amer. Stat. , vol. 52, no. 2,\npp. 112\u2013118, 1998.\n[59] E. J. Hartman, J. D. Keeler, and J. M. Kowalski, \u201cLayered neural net-\nworks with Gaussian hidden units as universal approximations,\u201d Neural\nComput. ,", "start_char_idx": 2343, "end_char_idx": 4856, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4f8d700-2d0c-493b-a8bc-22bd4e713274": {"__data__": {"id_": "a4f8d700-2d0c-493b-a8bc-22bd4e713274", "embedding": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67c6fcd3-9922-45dd-823f-9deac67ac662", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "8f422dff8972d4662b23bcb9ae92836b69b198606e1c8d82077f51300e539538"}, "2": {"node_id": "06da36df-7d8e-46c0-9ca0-d9715428a94d", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "1e9b3a428beffae5ebf27545c979331d245570ef19594149f6058268208682ba"}, "3": {"node_id": "08e42bea-5768-4b78-b076-2bb042521d63", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "9fe95398da98bab5dfddbb57287eddab94b82bff3597413ef60bb9e6c73bd9b0"}}, "hash": "ee8c27ed0d3a2997cad2d2f66ca080e3578a552df8d6528a812a85a6119f2015", "text": "Gaussian hidden units as universal approximations,\u201d Neural\nComput. , vol. 2, pp. 210\u2013215, 1990.\n[60] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical\nLearning: Data Mining, Inference, and Prediction . New York: Springer-\nVerlag, 2001.\n[61] H. S. Hippert, C. E. Pedreira, and R. C. Souza, \u201cNeural networks for\nshort-term forecasting: A review and evaluation,\u201d IEEE Trans. Power\nSyst., vol. 16, no. 1, pp. 44\u201355, Feb. 2001.\n[62] R. Hooke, How to Tell the Liars from the Statisticians .N e w Y o r k :\nMarcel Dekker, 1983.\n[63] R. G. Hoptroff, \u201cThe principles and practice of time series forecasting and\nbusiness modeling using neural networks,\u201d Neural Comput. Appl. ,v o l .1 ,\npp. 59\u201366, 1993.\n[64] K. Hornik, \u201cApproximation capabilities of multilayer feed-forward net-\nworks,\u201d Neural Netw. , vol. 4, pp. 251\u2013257, 1991.\n[65] , \u201cSome new results on neural network approximation,\u201d Neural\nNetw. , vol. 6, pp. 1069\u20131072, 1993.\n[66] K. Hornik, M. Stinchcombe, and H. White, \u201cMultilayer feedforward net-\nworks are universal approximators,\u201d Neural Netw. , vol. 2, pp. 359\u2013366,\n1989.\n[67] , \u201cUniversal approximation of an unknown mapping and its deriva-\ntives using multilayer feedforward networks,\u201d Neural Netw. ,v o l .3 ,\npp. 551\u2013560, 1990.\n[68] M. Y . Hu, G. P. Zhang, C. X. Jiang, and B. E. Patuwo, \u201cA cross-validation\nanalysis of neural network out-of-sample performance in exchange rateforecasting,\u201d Decision Sci. , vol. 30, pp. 197\u2013216, 1999.\n[69] D. Huff, How to Lie with Statistics . New York: Norton, 1954.\n[70] J. T. G. Hwang and A. A. Ding, \u201cPrediction intervals for arti\ufb01cial neural\nnetworks,\u201d J. Amer. Stat. Assoc. , vol. 92, pp. 748\u2013757, 1997.\n[71] O. Intrator and N. Intrator, \u201cInterpreting neural-network results: A simu-\nlation study,\u201d Comput. Stat. Data Anal. , vol. 37, pp. 373\u2013393, 2001.\n[72] A. K. Jain, R. P. W. Duin, and J. Mao, \u201cStatistical pattern recognition: A\nreview,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1, pp. 4\u201337,\nJan. 2000.\n[73] B. A. Jain and B. N. Nag, \u201cPerformance evaluation of neural network\ndecision models,\u201d J. Manage. Inf. Syst. , vol. 14, no. 2, pp. 201\u2013216,\n1997.\n[74] I. Kaastra and M. Boyd, \u201cDesigning a neural network for forecasting\n\ufb01nancial and economic time series,\u201d Neurocomputing , vol. 10, pp. 215\u2013\n236, 1996.\n[75] G. A. Kimble, How to Use (and Misuse) Statistics . Englewood Cliffs,\nNJ: Prentice-Hall, 1978.\n[76] G. King, \u201cHow not to lie with statistics: Avoiding common mistakes in\nquantitative political science,\u201d Amer. J. Pol. Sci. , vol. 30, pp.", "start_char_idx": 4826, "end_char_idx": 7362, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "08e42bea-5768-4b78-b076-2bb042521d63": {"__data__": {"id_": "08e42bea-5768-4b78-b076-2bb042521d63", "embedding": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67c6fcd3-9922-45dd-823f-9deac67ac662", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "8f422dff8972d4662b23bcb9ae92836b69b198606e1c8d82077f51300e539538"}, "2": {"node_id": "a4f8d700-2d0c-493b-a8bc-22bd4e713274", "node_type": null, "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "ee8c27ed0d3a2997cad2d2f66ca080e3578a552df8d6528a812a85a6119f2015"}}, "hash": "9fe95398da98bab5dfddbb57287eddab94b82bff3597413ef60bb9e6c73bd9b0", "text": "Amer. J. Pol. Sci. , vol. 30, pp. 666\u2013687,\n1986.\n[77] J. Kingman, \u201cStatistical responsibility,\u201d J. R. Stat. Soc. Ser. A , vol. 152,\npt. 3, pp. 277\u2013285, 1989.\n[78] B. D. Klein and D. F. Rossin, \u201cData quality in neural network models: Ef-\nfect of error rate and magnitude of error on predictive accuracy,\u201d Omega ,\nvol. 27, pp. 569\u2013582, 1999.\n[79] K. A. Kracha and U. Wagner, \u201cApplications of arti\ufb01cial neural networks\nin management science: A survey,\u201d J. Retailing Consum. Services ,v o l .6 ,\npp. 185\u2013203, 1999.\n[80] K. Laudon, \u201cData quality and due process in large interorganizational\nrecord systems,\u201d Commun. ACM , vol. 29, no. 1, pp. 4\u201311, Jan. 1986.\n[81] B. LeBaron and A. S. Weigend, \u201cA bootstrap evaluation of the effect of\ndata splitting on \ufb01nancial time series,\u201d IEEE Trans. Neural Netw. ,v o l .9 ,\nno. 1, pp. 213\u2013220, Jan. 1998.\n[82] S. W. Looney, \u201cA statistical technique for comparing the accuracies of\nseveral classi\ufb01ers,\u201d Pattern Recognit. Lett. , vol. 8, pp. 5\u20139, 1988.\n[83] C. N. Lu, H. T. Wu, and S. Vemuri, \u201cNeural network based short term load\nforecasting,\u201d IEEE Trans. Power Syst. , vol. 8, no. 1, pp. 336\u2013342, Feb.\n1993.\n[84] H. R. Maier and G. C. Dandy, \u201cNeural networks for the prediction and\nforecasting of water resources variables: A review of modeling issues and\napplications,\u201d Environ. Model. Softw. , vol. 15, pp. 101\u2013124, 2000.\n[85] S. Makridakis, A. Anderson, R. Carbone, R. Fildes, M. Hibon,\nR. Lewandowski, J. Newton, P. Parzen, and R. Winkler, \u201cThe accuracy of\nextrapolation (time series) methods: Results of a forecasting competition,\u201dJ. Forecast. , vol. 1, pp. 111\u2013153, 1982.\n[86] S. Makridakis and M. Hibon, \u201cThe M3-Competition: Results, conclusions\nand implications,\u201d Int. J. Forecast. , vol. 16, pp. 451\u2013476, 2000.", "start_char_idx": 7390, "end_char_idx": 9143, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "45565f4d-a61c-4371-a344-12fa4d0e7568": {"__data__": {"id_": "45565f4d-a61c-4371-a344-12fa4d0e7568", "embedding": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b549672b-d944-422c-850a-74fc852af027", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "12fb114b0340ce9dfe734c30a7165da6be9bae3be920d17df8e84afd8899e9ec"}, "3": {"node_id": "57d8b109-4bf7-44f9-836c-53eced112f4b", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "5c4c39c593f630fa7fdc71fc3e353aa2c78c6a7eadd125b08a6ff8209c466762"}}, "hash": "4e9b448e4cf73eebf4ddfda587bfd7399d9fbf7231f56b565ffa40aa674314d4", "text": "ZHANG: A VOIDING PITFALLS IN NEURAL NETWORK RESEARCH 15\n[87] R. Meese and J. Geweke, \u201cA comparison of autoregressive univariate\nforecasting procedures for macroeconomic time series,\u201d J. Bus. Econ.\nStat., vol. 2, pp. 191\u2013200, 1984.\n[88] D. Michie, D. J. Spiegelhalter, and C. C. Taylor, Machine Learning, Neural\nand Statistical Classi\ufb01cation . New York: Ellis Horwood, 1994.\n[89] M. Nelson, T. Hill, T. Remus, and M. O\u2019Connor, \u201cTime series forecasting\nusing NNs: Should the data be deseasonalized \ufb01rst?\u201d J. Forecast. , vol. 18,\npp. 359\u2013367, 1999.\n[90] K. Ord, \u201cCommentaries on the M3-competition: An introduction, some\ncomments and a scorecard,\u201d Int. J. Forecast. , vol. 17, pp. 537\u2013584, 2001.\n[91] Y . R. Park, T. J. Murray, and C. Chen, \u201cPredicting sun spots using a\nlayered perceptron neural network,\u201d IEEE Trans. Neural Netw. ,v o l .7 ,\nno. 2, pp. 501\u2013505, Mar. 1996.\n[92] L. Prechelt, \u201cA quantitative study of experimental evaluations of neural\nnetwork learning algorithms: Current research practice,\u201d Neural Netw. ,\nvol. 9, no. 3, pp. 457\u2013462, 1996.\n[93] D. A. Preece, \u201cGood statistical practice,\u201d Statistician , vol. 36, pp. 397\u2013\n408, 1987.\n[94] M. Qi, \u201cNonlinear predictability of stock returns using \ufb01nancial and eco-\nnomic variables,\u201d J. Bus. Econ. Stat. , vol. 17, pp. 419\u2013429, 1999.\n[95] M. Qi and G. P. Zhang, \u201cAn investigation of model selection criteria for\nneural network time series forecasting,\u201d Eur. J. Oper. Res. , vol. 132,\npp. 666\u2013680, 2001.\n[96] J. Racine, \u201cOn the nonlinear predictability of stock returns using \ufb01nancial\nand economic variables,\u201d J. Bus. Econ. Stat. , vol. 19, pp. 380\u2013382, 2001.\n[97] S. Raudys, \u201cEvolution and generalization of a single neuron\u2014I: Single-\nlayer perceptron as seven statistical classi\ufb01ers,\u201d Neural Netw. , vol. 11,\npp. 283\u2013296, 1998.\n[98] S. J. Raudys and A. K. Jain, \u201cSmall sample size effects in statistical\npattern recognition: Recommendations for practitioners,\u201d IEEE Trans.\nPattern Anal. Mach. Intell. , vol. 13, no. 3, pp. 252\u2013264, Mar. 1991.\n[99] R. Reed, \u201cPruning algorithms\u2014A survey,\u201d IEEE Trans. Neural Netw. ,\nvol. 4, no. 5, pp. 740\u2013747, Sep. 1993.\n[100] M. D. Richard and R. Lippmann, \u201cNeural network classi\ufb01ers estimate\nBayesian a posteriori probabilities,\u201d Neural Comput. , vol. 3, pp. 461\u2013483,\n1991.\n[101] B. D. Ripley, \u201cStatistical aspects of neural networks,\u201d in Networks and\nChaos\u2014Statistical and Probabilistic Aspects , O. E. Barndorff-Nielsen,\nJ. L. Jensen, and W. S. Kendall, Eds. London, U.K.: Chapman & Hall,1993, pp. 40\u2013123.\n[102]\n, \u201cNeural networks and", "start_char_idx": 0, "end_char_idx": 2533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57d8b109-4bf7-44f9-836c-53eced112f4b": {"__data__": {"id_": "57d8b109-4bf7-44f9-836c-53eced112f4b", "embedding": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b549672b-d944-422c-850a-74fc852af027", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "12fb114b0340ce9dfe734c30a7165da6be9bae3be920d17df8e84afd8899e9ec"}, "2": {"node_id": "45565f4d-a61c-4371-a344-12fa4d0e7568", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "4e9b448e4cf73eebf4ddfda587bfd7399d9fbf7231f56b565ffa40aa674314d4"}, "3": {"node_id": "8997ec64-5b20-4ee6-83d8-ef8837e5f5da", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "a9d70090c32b438db8298d39a0d5b805646e6db371e5b7e1483206322fce0290"}}, "hash": "5c4c39c593f630fa7fdc71fc3e353aa2c78c6a7eadd125b08a6ff8209c466762", "text": "pp. 40\u2013123.\n[102]\n, \u201cNeural networks and related methods for classi\ufb01cation,\u201d J. R.\nStat. Soc. Ser. B , vol. 56, no. 3, pp. 409\u2013456, 1994.\n[103] I. Rivals and L. Personnaz, \u201cConstruction of con\ufb01dence intervals for\nneural networks based on least squares estimation,\u201d Neural Netw. , vol. 13,\npp. 463\u2013484, 2000.\n[104] S. L. Salzberg, \u201cOn comparing classi\ufb01ers: Pitfalls to avoid and a rec-\nommended approach,\u201d Data Min. Knowl. Discov. , vol. 1, pp. 317\u2013328,\n1997.\n[105] M. Schumacher, R. Robner, and W. Vach, \u201cNeural networks and logistic\nregression\u2014Part I,\u201d Comput. Stat. Data Anal. , vol. 21, pp. 661\u2013682,\n1996.\n[106] R. Sharda, \u201cNeural networks for the MS/OR analyst: An application\nbibliography,\u201d Interfaces , vol. 24, no. 2, pp. 116\u2013130, 1994.\n[107] R. Sharda and R. B. Patil, \u201cConnectionist approach to time series predic-\ntion: An empirical test,\u201d J. Intell. Manuf. , vol. 3, pp. 317\u2013323, 1992.\n[108] M. Smith, Neural Networks for Statistical Modeling . New York: Rein-\nhold, 1993.\n[109] R. D. Snee, \u201cValidation of regression models: Methods and examples,\u201d\nTechnometrics , vol. 19, pp. 415\u2013428, 1977.\n[110] D. Srinivasan, A. C. Liew, and C. S. Chang, \u201cA neural network short-term\nload forecaster,\u201d Elec. Power Syst. Res. , vol. 28, pp. 227\u2013234, 1994.\n[111] S. Stern, \u201cNeural networks in applied statistics,\u201d Technometrics , vol. 38,\nno. 3, pp. 205\u2013214, 1996.\n[112] N. R. Swanson and H. White, \u201cA model-selection approach to assessing\nthe information in the term structure using linear models and arti\ufb01cialneural networks,\u201d J. Bus. Econ. Stat. , vol. 13, pp. 265\u201375, 1995.\n[113]\n, \u201cA model selection approach to real-time macroeconomic fore-\ncasting using linear models and arti\ufb01cial neural networks,\u201d Rev. Econ.\nStat., vol. 79, pp. 540\u2013550, 1997.\n[114] Z. Tang and P. A. Fishwick, \u201cFeedforward neural nets as models for time\nseries forecasting,\u201d ORSA J. Comput. , vol. 5, no. 4, pp. 374\u2013385, 1993.\n[115] L. J. Tashman, \u201cOut-of-sample tests of forecasting accuracy: An analysis\nand review,\u201d Int. J. Forecast. , vol. 16, pp. 437\u2013450, 2000.\n[116] A. B. Tickle, R. Andrews, M. Golea, and J. Diederich, \u201cThe truth will\ncome to light: Directions and challenges in extracting the knowledgeembedded within trained arti\ufb01cial neural networks,\u201d IEEE Trans. Neural\nNetw. , vol. 9, no. 6, pp. 1057\u20131068, Nov. 1998.\n[117] A. B. Tickle, F. Maire, G. Bologna, R. Andrews, and J. Diederich,\n\u201cLessons from past, current issues, and future research directions in ex-tracting the knowledge embedded in arti\ufb01cial neural networks,\u201d in Hy-\nbrid Neural Systems , S. Wermter and R.", "start_char_idx": 2498, "end_char_idx": 5056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8997ec64-5b20-4ee6-83d8-ef8837e5f5da": {"__data__": {"id_": "8997ec64-5b20-4ee6-83d8-ef8837e5f5da", "embedding": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b549672b-d944-422c-850a-74fc852af027", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "12fb114b0340ce9dfe734c30a7165da6be9bae3be920d17df8e84afd8899e9ec"}, "2": {"node_id": "57d8b109-4bf7-44f9-836c-53eced112f4b", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "5c4c39c593f630fa7fdc71fc3e353aa2c78c6a7eadd125b08a6ff8209c466762"}, "3": {"node_id": "573c2a95-bf1c-4af3-8361-f37c326fab84", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "7c755efd47218dc11f6649a7d0a4f4750f5cad7a45ffbcbf755e89e9f44508c8"}}, "hash": "a9d70090c32b438db8298d39a0d5b805646e6db371e5b7e1483206322fce0290", "text": "in Hy-\nbrid Neural Systems , S. Wermter and R. Sun, Eds. Berlin, Germany:\nSpringer-Verlag, 2000, pp. 226\u2013239.\n[118] J. C. van Houwelingen, \u201cUse and abuse of variance models in regression,\u201d\nBiometrics , vol. 44, no. 4, pp. 1073\u20131081, 1988.\n[119] A. Vellido, P. J. G. Lisboa, and J. Vaughan, \u201cNeural networks in business:\nA survey of applications (1992\u20131998),\u201d Expert Syst. Appl. , vol. 17, pp. 51\u2013\n70, 1999.\n[120] S. Walczak, \u201cAn empirical analysis of data requirements for \ufb01nancial\nforecasting with neural networks,\u201d J. Manage. Inf. Syst. , vol. 17, no. 4,\npp. 203\u2013222, 2001.\n[121] E. A. Wan, \u201cNeural network classi\ufb01cation: A Bayesian interpretation,\u201d\nIEEE Trans. Neural Netw. , vol. 1, no. 4, pp. 303\u2013305, Dec. 1990.\n[122] S. Wang, \u201cThe unpredictability of standard back propagation neural net-\nworks in classi\ufb01cation applications,\u201d Manage. Sci. , vol. 41, no. 3, pp. 555\u2013\n559, 1995.\n[123] B. Warner and M. Misra, \u201cUnderstanding neural networks as statistical\ntools,\u201d Amer. Stat. , vol. 50, no. 4, pp. 284\u2013293, 1996.\n[124] H. White, \u201cLearning in arti\ufb01cial neural networks: A statistical perspec-\ntive,\u201d Neural Comput. , vol. 1, pp. 425\u2013464, 1989.\n[125]\n, \u201cSome asymptotic results for learning in single hidden layer feed-\nforward network models,\u201d J. Amer. Stat. Assoc. , vol. 84, pp. 1003\u20131013,\n1989.\n[126] , \u201cConnectionist nonparametric regression: Multilayer feedforward\nnetworks can learn arbitrary mappings,\u201d Neural Netw. , vol. 3, pp. 535\u2013\n549, 1990.\n[127] , \u201cA reality check for data snooping,\u201d Econometrica , vol. 68, no. 5,\npp. 1097\u20131126, 2000.\n[128] R. L. Wilson and R. Sharda, \u201cBankruptcy prediction using neural net-\nworks,\u201d Decis. Support Syst. , vol. 11, pp. 545\u2013557, 1994.\n[129] J. Wyatt, \u201cNervous about arti\ufb01cial neural networks?\u201d Lancet , vol. 346,\npp. 1175\u20131177, 1995.\n[130] G. P. Zhang, \u201cNeural networks for classi\ufb01cation: A survey,\u201d IEEE Trans.\nSyst., Man, Cybern. C , vol. 30, no. 4, pp. 451\u2013462, Nov. 2000.\n[131] G. P. Zhang, M. Y . Hu, B. E. Patuwo, and D. C. Indro, \u201cArti\ufb01cial neu-\nral networks in bankruptcy prediction: General framework and cross-\nvalidation analysis,\u201d Eur. J. Oper. Res. , vol. 116, pp. 16\u201332, 1999.\n[132] G. Zhang, B. E. Patuwo, and M. Y . Hu, \u201cForecasting with arti\ufb01cial\nneural networks: The state of the art,\u201d Int. J. Forecast. , vol. 14, pp. 35\u201362,\n1998.\n[133] , \u201cA simulation study of arti\ufb01cial neural networks for nonlinear\ntime-series forecasting,\u201d Comput. Oper. Res. , vol.", "start_char_idx": 5055, "end_char_idx": 7480, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "573c2a95-bf1c-4af3-8361-f37c326fab84": {"__data__": {"id_": "573c2a95-bf1c-4af3-8361-f37c326fab84", "embedding": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b549672b-d944-422c-850a-74fc852af027", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "12fb114b0340ce9dfe734c30a7165da6be9bae3be920d17df8e84afd8899e9ec"}, "2": {"node_id": "8997ec64-5b20-4ee6-83d8-ef8837e5f5da", "node_type": null, "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "a9d70090c32b438db8298d39a0d5b805646e6db371e5b7e1483206322fce0290"}}, "hash": "7c755efd47218dc11f6649a7d0a4f4750f5cad7a45ffbcbf755e89e9f44508c8", "text": "forecasting,\u201d Comput. Oper. Res. , vol. 28, pp. 381\u2013396, 2001.\n[134] X. Zhang, \u201cTime series analysis and prediction by neural networks,\u201d\nOptim. Methods Softw. , vol. 4, pp. 151\u2013170, 1994.\n[135] R. Setiono, W. K. Leow, and J. Zurada, \u201cExtraction of rules from arti\ufb01cial\nneural networks for nonlinear regression,\u201d IEEE Trans. Neural Netw. ,\nvol. 13, no. 3, pp. 564\u2013577, May 2002.\n[136] R. Setiono and J. Y . L. Thong, \u201cAn approach to generate rules from\nneural networks for regression problems,\u201d Eur. J. Oper. Res. , vol. 155,\npp. 239\u2013250, 2004.\n[137] T. G. Calderon and J. J. Cheh, \u201cA roadmap for future neural networks\nresearch in auditing and risk assessment,\u201d Int. J. Account. Inf. Syst. ,v o l .3 ,\npp. 203\u2013236, 2002.\n[138] J. T. Davis, A. P. Massey, and P. E. R. Lovell, \u201cSupporting a complex\naudit judgment task: An expert network approach,\u201d Eur. J. Oper. Res. ,\nvol. 103, pp. 305\u2013372, 1997.\n[139] V . L. Berardi and G. P. Zhang, \u201cThe effect of misclassi\ufb01cation costs on\nneural network classi\ufb01ers,\u201d Decis. Sci. , vol. 30, pp. 659\u2013682, 1999.\n[140] G. Schwarzer, W. Vach, and M. Schumacher, \u201cOn the misuses of arti\ufb01cial\nneural networks for prognostic and diagnostic classi\ufb01cation in oncology,\u201d\nStat. Med. , vol. 19, pp. 541\u2013561, 2000.\n[141] S. D. Balkin and J. K. Ord, \u201cAutomatic neural network modeling for\nunivariate time series,\u201d Int. J. Forecast. , vol. 16, pp. 509\u2013515, 2000.\n[142] R. R. Picard and K. N. Berk, \u201cData splitting,\u201d Amer. Stat. , vol. 44,\npp. 140\u2013147, 1990.\n[143] L. Zhao, F. Collopy, and M. Kennedy, \u201cThe problem of neural networks\nin business forecasting: An attempt to reproduce the Hill, O\u2019Connor and\nRemus study,\u201d Sprouts: Working Papers on Information Environments,\nSystems, and Organizations , vol. 3, 2004.\n[144] T. Hill, M. O\u2019Connor, and W. Remus, \u201cNeural network models for time\nseries forecasting,\u201d Manage. Sci. , vol. 42, pp. 1082\u20131092, 1996.", "start_char_idx": 7479, "end_char_idx": 9354, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af396777-2c81-4bb9-a0bb-ad71404d76ce": {"__data__": {"id_": "af396777-2c81-4bb9-a0bb-ad71404d76ce", "embedding": null, "metadata": {"page_label": "14", "file_name": "Pitfalls on Neural Networks.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c30060bd-1f1a-4b7e-945e-632aa226a2a4", "node_type": null, "metadata": {"page_label": "14", "file_name": "Pitfalls on Neural Networks.pdf"}, "hash": "c7264c918ba517da0707c240271da2a4121956b01590c40699252c5080862af3"}}, "hash": "c7264c918ba517da0707c240271da2a4121956b01590c40699252c5080862af3", "text": "16 IEEE TRANSACTIONS ON SYSTEMS, MAN, AND CYBERNETICS\u2014PART C: APPLICATIONS AND REVIEWS, VOL. 37, NO. 1, JANUARY 2007\n[145] G. P. Zhang and M. Qi, \u201cNeural network forecasting of seasonal and\ntrend time series,\u201d Eur. J. Oper. Res. , vol. 160, pp. 501\u2013514, 2005.\n[146] C. J. Huberty, \u201cIssues in the use and interpretation of discriminant anal-\nysis,\u201d Psychol. Bull. , vol. 95, no. 1, pp. 156\u2013171, 1984.\n[147] P. A. Lachebruch, \u201cSome misuses of discriminant analysis,\u201d Methods\nInf. Med. , vol. 16, pp. 255\u2013258, 1977.\n[148] P. J. G. Lisboa, \u201cA review of evidence of health bene\ufb01t from arti\ufb01cial\nneural networks in medical intervention,\u201d Neural Netw. , vol. 15, pp. 11\u2013\n39, 2002.\n[149] R. G. Congalton and K. Green, Assessing the Accuracy of Remotely\nSensed Data: Principles and Practices . New York: Lewis, 1999.\n[150] L. G. Protney and M. P. Watkins, Foundations of Clinical Research:\nApplications to Practice . Princeton, NJ: Prentice-Hall, 2000.\n[151] B. K. Wong, T. A. Bodnovich, and Y . Selvi, \u201cNeural network applications\nin business: A review and analysis of the literature (1988\u20131995),\u201d Decis.\nSupport Syst. , vol. 19, pp. 301\u2013320, 1997.\nG. Peter Zhang received the B.Sc. and M.Sc. de-\ngrees from East China Normal University, Shang-\nhai, China in 1985 and 1987, respectively, and the\nPh.D. degree from Kent State University, Kent, OH, in1998.\nHe is an Associate Professor of Managerial Sci-\nences at Georgia State University, Atlanta. His re-\nsearch interests include neural networks, forecasting,time-series analysis, and supply chain management.He is the Editor of the book Neural Networks in Busi-\nness Forecasting (IRM Press, 2004), and the author\nof more than 50 publications in journals, conferences, and book chapters. Hecurrently serves as an Associate Editor of Neurocomputing and is on the edito-\nrial review board of Production and Operations Management journal.\nDr. Zhang received the Best Paper Award from the IEEE T\nRANSACTIONS ON\nENGINEERING MANAGEMENT in 2004.", "start_char_idx": 0, "end_char_idx": 1980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b7c68259-cd9f-47da-bdae-eff58aff0592": {"__data__": {"id_": "b7c68259-cd9f-47da-bdae-eff58aff0592", "embedding": null, "metadata": {"page_label": "443", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "68d0bcc1-7d30-4c35-95a9-9fd96e4f52b0", "node_type": null, "metadata": {"page_label": "443", "file_name": "Time Series.pdf"}, "hash": "5ef095a76abc85902bec026f149d7b8a45e11091478fd3b963aaf0ed9a77b1c3"}}, "hash": "5ef095a76abc85902bec026f149d7b8a45e11091478fd3b963aaf0ed9a77b1c3", "text": "25 years of time series forecasting\nJan G. De Gooijera,1, Rob J. Hyndmanb,*\naDepartment of Quantitative Economics, University of Amsterdam, Roetersstraat 11, 1018 WB Amsterdam, The Netherlands\nbDepartment of Econometrics and Business Statistics, Monash University, VIC 3800, Australia\nAbstract\nWe review the past 25 years of research into time series forecasting. In this silver jubilee issue, we naturally highlight results\npublished in journals managed by the International Institute of Forecasters ( Journal of Forecasting 1982\u20131985 and\nInternational Journal of Forecasting 1985\u20132005). During this period, over one third of all papers published in these journals\nconcerned time series forecasting. We also review highly influential works on time series forecasting that have been published\nelsewhere during this period. Enormous progress has been made in many areas, but we find that there are a large number oftopics in need of further development. We conclude with comments on possible future research directions in this field.D2006 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.\nKeywords: Accuracy measures; ARCH; ARIMA; Combining; Count data; Densities; Exponential smoothing; Kalman filter; Long memory;\nMultivariate; Neural nets; Nonlinearity; Prediction intervals; Regime-switching; Robustness; Seasonality; State space; Structural models;\nTransfer function; Univariate; VAR\n1. Introduction\nThe International Institute of Forecasters (IIF) was\nestablished 25 years ago and its silver jubilee provides\nan opportunity to review progress on time seriesforecasting. We highlight research published injournalsspons oredbytheInstitute ,althoughwealso\ncover key publications in other journals. In 1982, theIIF set up the Journal of Forecasting (JoF), publishedwith John Wiley and Sons. After a break with Wileyin 1985,\n2the IIF decided to start the International\nJournal of Forecasting (IJF), published with Elsevier\nsince 1985. This paper provides a selective guide tothe literature on time series forecasting, covering theperiod 1982\u20132005 and summarizing over 940 papersincluding about 340 papers published under the bIIF-\nflag Q. The proportion of papers that concern time\nseries forecasting has been fairly stable over time. Wealso review key papers and books published else-\nwhere that have been highly influential to various\ndevelopments in the field. The works referenced\n0169-2070/$ - see front matter D2006 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.\ndoi:10.1016/j. ijforecast .2006.01.001* Corresponding author. Tel.: +61 3 9905 2358; fax: +61 3 9905\n5474.\nE-mail  addresses:  j.g.degooijer @uva.n l(J.G.DeGooijer),\nRob.Hyndm an@buseco.m onash.edu. au(R.J.Hyndman).\n1Tel.: +31 20 525 4244; fax: +31 20 525 4349.2The IIF was involved with JoF issue 44:1 (1985).International Journal of Forecasting 22 (2006) 443\u2013473\nwww.elsevier.com/locate/ijforecast", "start_char_idx": 0, "end_char_idx": 2947, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "95c99584-a3c9-4719-90db-1d50ff789de4": {"__data__": {"id_": "95c99584-a3c9-4719-90db-1d50ff789de4", "embedding": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9763d30-6fcc-428f-83c9-3001953fb17b", "node_type": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "hash": "b977706d20a9f43d1acae6cb85d356fafc2ff8da9c7f6be00d7e702072998e08"}, "3": {"node_id": "3adbe498-a854-4baf-8ecd-7807b4efab46", "node_type": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "hash": "ac1f94e1770dbba7ad8ce036164a0a098645de368d4ba807ffde10d0dc20adf2"}}, "hash": "08d409f42b1209910ad3bcef5d6709cdc0f07769b49dbdb47fd6e0bce8c0db8b", "text": "comprise 380 journal papers and 20 books and\nmonographs.\nIt was felt to be convenient to first classify the\npapers according to the models (e.g., exponentialsmoothing, ARIMA) introduced in the time seriesliterature, rather than putting papers under a headingassociated with a particular method. For instance,Bayesian methods in general can be applied to all\nmodels. Papers not concerning a particular model\nwere then classified according to the various problems(e.g., accuracy measures, combining) they address. Inonly a few cases was a subjective decision needed onour part to classify a paper under a particular sectionheading. To facilitate a quick overview in a particularfield, the papers are listed in alphabetical order undereach of the section headings.\nDetermining what to include and what not to\ninclude in the list of references has been a problem.There may be papers that we have missed and papersthat are also referenced by other authors in this SilverAnniversary issue. As such the review is somewhatbselective Q, although this does not imply that a\nparticular paper is unimportant if it is not reviewed.\nThe review is not intended to be critical, but rather\na (brief) historical and personal tour of the main\ndevelopments. Still, a cautious reader may detectcertain areas where the fruits of 25 years of intensiveresearch interest has been limited. Conversely, clearexplanations for many previously anomalous timeseries forecasting results have been provided by theend of 2005. Section 13 discusses some currentresearch directions that hold promise for the future,\nbut of course the list is far from exhaustive.\n2. Exponential smoothing\n2.1. Preamble\nTwenty-five years ago, exponential smoothing\nmethods were often considered a collection of ad\nhoc techniques for extrapolating various types ofunivariate time series. Although exponential smooth-ing methods were widely used in business andindustry, they had received little attention fromstatisticians and did not have a well-developedstatistical foundation. These methods originated inthe 1950s and 1960s with the work of Brown (1959,1963) ,Holt (1957, reprinted 2004) ,a n d Winters\n(1960) .Pegels (1969) provided a simple but useful\nclassification of the trend and the seasonal patternsdepending on whether they are additive (linear) ormultiplicative (nonlinear).\nMuth (1960) was the first to suggest a statistical\nfoundation for simple exponential smoothing (SES)by demonstrating that it provided the optimal fore-\ncasts for a random walk plus noise. Further steps\ntowards putting exponential smoothing within astatistical framework were provided by Box and\nJenkins (1970) ,Roberts (1982) , and Abraham and\nLedolter (1983, 1986) , who showed that some linear\nexponential smoothing forecasts arise as special casesof ARIMA models. However, these results did notextend to any nonlinear exponential smoothing\nmethods.\nExponential smoothing methods received a boost\nfrom two papers published in 1985, which laid thefoundation for much of the subsequent work in thisarea. First, Gardner (1985) provided a thorough\nreview and synthesis of work in exponential smooth-ing to that date and extended Pegels\u2019 classification toinclude damped trend. This paper brought together a\nlot of existing work which stimulated the use of these\nmethods and prompted a substantial amount ofadditional research. Later in the same year, Snyder\n(1985) showed that SES could be considered as\narising from an innovation state space model (i.e., amodel with a single source of error). Although thisinsight went largely unnoticed at the time, in recentyears it has provided the basis for a large amount of\nwork on state space models underlying exponential\nsmoothing methods.\nMost of the work since 1980 has involved studying\nthe empirical properties of the methods (e.g., Barto-\nlomei & Sweet, 1989; Makridakis & Hibon, 1991 ),\nproposals for new methods", "start_char_idx": 0, "end_char_idx": 3892, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3adbe498-a854-4baf-8ecd-7807b4efab46": {"__data__": {"id_": "3adbe498-a854-4baf-8ecd-7807b4efab46", "embedding": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9763d30-6fcc-428f-83c9-3001953fb17b", "node_type": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "hash": "b977706d20a9f43d1acae6cb85d356fafc2ff8da9c7f6be00d7e702072998e08"}, "2": {"node_id": "95c99584-a3c9-4719-90db-1d50ff789de4", "node_type": null, "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}, "hash": "08d409f42b1209910ad3bcef5d6709cdc0f07769b49dbdb47fd6e0bce8c0db8b"}}, "hash": "ac1f94e1770dbba7ad8ce036164a0a098645de368d4ba807ffde10d0dc20adf2", "text": "Makridakis & Hibon, 1991 ),\nproposals for new methods of estimation or initiali-zation ( Ledolter & Abraham, 1984 ), evaluation of the\nforecasts ( McClain, 1988; Sweet & Wilson, 1988 ), or\nhas concerned statistical models that can be consid-\nered to underly the methods (e.g., McKenzie, 1984 ).\nThe damped multiplicative methods of Taylor (2003)\nprovide the only genuinely new exponential smooth-ing methods over this period. There have, of course,been numerous studies applying exponential smooth-ing methods in various contexts including computercomponents ( Gardner, 1993 ), air passengers ( Grubb &J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 444", "start_char_idx": 3839, "end_char_idx": 4531, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ceb758fc-cb7c-48b4-8d0e-fd8ab86ba137": {"__data__": {"id_": "ceb758fc-cb7c-48b4-8d0e-fd8ab86ba137", "embedding": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "211ec4ae-796d-4880-a198-411bd743e0ea", "node_type": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "hash": "7ef55b24a10dea5000b4b463730db1ab18b95cf2cc81d15a5ea5a5f4e4454ce6"}, "3": {"node_id": "cecff25a-4f94-4ffd-baa9-028b9f38ed43", "node_type": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "hash": "8741f382de9716dbc4c93bb7d0790cb9b5ea90b5980dcca4de763780defc0141"}}, "hash": "c080a880b443b22bcd339c4975fc1a6ae31df5b0a4de8fa11c285ab86111d3c3", "text": "Masa, 2001 ), and production planning ( Miller &\nLiberatore, 1993 ).\nTheHyndman, Koehler, Snyder, and Grose (2002)\ntaxonomy (extended by Taylor, 2003 ) provides a\nhelpful categorization for describing the variousmethods. Each method consists of one of five typesof trend (none, additive, damped additive, multiplica-tive, and damped multiplicative) and one of three\ntypes of seasonality (none, additive, and multiplica-\ntive). Thus, there are 15 different methods, the bestknown of which are SES (no trend, no seasonality),Holt\u2019s linear method (additive trend, no seasonality),Holt\u2013Winters\u2019 additive method (additive trend, addi-tive seasonality), and Holt\u2013Winters\u2019 multiplicativemethod (additive trend, multiplicative seasonality).\n2.2. Variations\nNumerous variations on the original methods have\nbeen proposed. For example, Carreno and Madina-\nveitia (1990) and Williams and Miller (1999) pro-\nposed modifications to deal with discontinuities, andRosas and Guerrero (1994) looked at exponential\nsmoothing forecasts subject to one or more con-\nstraints. There are also variations in how and when\nseasonal components should be normalized. Lawton\n(1998) argued for renormalization of the seasonal\nindices at each time period, as it removes bias inestimates of level and seasonal components. Slightlydifferent normalization schemes were given byRoberts (1982) and McKenzie (1986) .Archibald\nand Koehler (2003) developed new renormalization\nequations that are simpler to use and give the same\npoint forecasts as the original methods.\nOne useful variation, part way between SES and\nHolt\u2019s method, is SES with drift. This is equivalent toHolt\u2019s method with the trend parameter set to zero.Hyndman and Billah (2003) showed that this method\nwas also equivalent to Assimakopoulos and Nikolo-\npoulos (2000) bTheta method Qwhen the drift param-\neter is set to half the slope of a linear trend fitted to the\ndata. The Theta method performed extremely well inthe M3-competition, although why this particularchoice of model and parameters is good has not yetbeen determined.\nThere has been remarkably little work in developing\nmultivariate versions of the exponential smoothingmethods for forecasting. One notable exception isPfeffermann and Allon (1989) who looked at Israeli\ntourism data. Multivariate SES is used for processcontrol charts (e.g., Pan, 2005 ), where it is called\nbmultivariate exponentially weighted moving averages Q,\nbut here the focus is not on forecasting.\n2.3. State space models\nOrd, Koehler, and Snyder (1997) built on the work\nofSnyder (1985) by proposing a class of innovation\nstate space models which can be considered asunderlying some of the exponential smoothing meth-ods. Hyndman et al. (2002) and Taylor (2003)\nextended this to include all of the 15 exponentialsmoothing methods. In fact, Hyndman et al. (2002)\nproposed two state space models for each method,\ncorresponding to the additive error and the multipli-\ncative error cases. These models are not unique andother related state space models for exponentialsmoothing methods are presented in Koehler, Snyder,\nand Ord (2001) and Chatfield, Koehler, Ord, and\nSnyder (2001) . It has long been known that some\nARIMA models give equivalent forecasts to the linearexponential smoothing methods. The significance of\nthe recent work on innovation state space models is\nthat the nonlinear exponential smoothing methods canalso be derived from statistical models.\n2.4. Method selection\nGardner and McKenzie (1988) provided some\nsimple rules based on the variances of differenced\ntime series for choosing", "start_char_idx": 0, "end_char_idx": 3569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cecff25a-4f94-4ffd-baa9-028b9f38ed43": {"__data__": {"id_": "cecff25a-4f94-4ffd-baa9-028b9f38ed43", "embedding": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "211ec4ae-796d-4880-a198-411bd743e0ea", "node_type": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "hash": "7ef55b24a10dea5000b4b463730db1ab18b95cf2cc81d15a5ea5a5f4e4454ce6"}, "2": {"node_id": "ceb758fc-cb7c-48b4-8d0e-fd8ab86ba137", "node_type": null, "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}, "hash": "c080a880b443b22bcd339c4975fc1a6ae31df5b0a4de8fa11c285ab86111d3c3"}}, "hash": "8741f382de9716dbc4c93bb7d0790cb9b5ea90b5980dcca4de763780defc0141", "text": "some\nsimple rules based on the variances of differenced\ntime series for choosing an appropriate exponential\nsmoothing method. Tashman and Kruk (1996) com-\npared these rules with others proposed by Collopy and\nArmstrong (1992) and an approach based on the BIC.\nHyndman et al. (2002) also proposed an information\ncriterion approach, but using the underlying state\nspace models.\n2.5. Robustness\nThe remarkably good forecasting performance of\nexponential smoothing methods has been addressedby several authors. Satchell and Timmermann (1995)\nandChatfield et al. (2001) showed that SES is optimal\nfor a wide range of data generating processes. In asmall simulation study, Hyndman (2001) showed thatJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 445", "start_char_idx": 3489, "end_char_idx": 4272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87788c3b-80f9-42f4-9731-ee272e8871c7": {"__data__": {"id_": "87788c3b-80f9-42f4-9731-ee272e8871c7", "embedding": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb9dddcc-38f5-4d24-9501-559b09bbf1f5", "node_type": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "hash": "942a6d1a99816add767a89c59268a8ef3808fc25f35374d3b0c406d6c03742d5"}, "3": {"node_id": "3f162376-dd96-4779-85bf-334cbaa22f61", "node_type": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "hash": "d2df550573be6a21536b3f67c28eb664bf9bbe513af067168d8eceed26163829"}}, "hash": "09c8cb8e2af36b5ba29eb2b9b9a0eca0976010d7a961e13a55ad0c632aa15370", "text": "simple exponential smoothing performed better than\nfirst order ARIMA models because it is not so subjectto model selection problems, particularly when dataare non-normal.\n2.6. Prediction intervals\nOne of the criticisms of exponential smoothing\nmethods 25 years ago was that there was no way to\nproduce prediction intervals for the forecasts. The firstanalytical approach to this problem was to assume thatthe series were generated by deterministic functions oftime plus white noise ( Brown, 1963; Gardner, 1985;\nMcKenzie, 1986; Sweet, 1985 ). If this was so, a\nregression model should be used rather than expo-nential smoothing methods; thus, Newbold and Bos\n(1989) strongly criticized all approaches based on this\nassumption.\nOther authors sought to obtain prediction intervals\nvia the equivalence between exponential smoothingmethods and statistical models. Johnston and Harrison\n(1986) found forecast variances for the simple and\nHolt exponential smoothing methods for state spacemodels with multiple sources of errors. Yar and\nChatfield (1990) obtained prediction intervals for the\nadditive Holt\u2013Winters\u2019 method by deriving theunderlying equivalent ARIMA model. Approximateprediction intervals for the multiplicative Holt\u2013Win-ters\u2019 method were discussed by Chatfield and Yar\n(1991) , making the assumption that the one-step-\nahead forecast errors are independent. Koehler et al.\n(2001) also derived an approximate formula for the\nforecast variance for the multiplicative Holt\u2013Winters\u2019\nmethod, differing from Chatfield and Yar (1991) only\nin how the standard deviation of the one-step-aheadforecast error is estimated.\nOrd et al. (1997) andHyndman et al. (2002) used\nthe underlying innovation state space model tosimulate future sample paths, and thereby obtainedprediction intervals for all the exponential smoothing\nmethods. Hyndman, Koehler, Ord, and Snyder\n(2005) used state space models to derive analytical\nprediction intervals for 15 of the 30 methods,including all the commonly used methods. Theyprovide the most comprehensive algebraic approachto date for handling the prediction distributionproblem for the majority of exponential smoothingmethods.2.7. Parameter space and model properties\nIt is common practice to restrict the smoothing\nparameters to the range 0 to 1. However, now thatunderlying statistical models are available, the natural(invertible) parameter space for the models can beused instead. Archibald (1990) showed that it is\npossible for smoothing parameters within the usual\nintervals to produce non-invertible models. Conse-\nquently, when forecasting, the impact of change in thepast values of the series is non-negligible. Intuitively,such parameters produce poor forecasts and theforecast performance deteriorates. Lawton (1998) also\ndiscussed this problem.\n3. ARIMA models\n3.1. Preamble\nEarly attempts to study time series, particularly in\nthe 19th century, were generally characterized by theidea of a deterministic world. It was the majorcontribution of Yule (1927) which launched the notion\nof stochasticity in time series by postulating that every\ntime series can be regarded as the realization of astochastic process. Based on this simple idea, anumber of time series methods have been developedsince then. Workers such as Slutsky, Walker, Yaglom,and Yule first formulated the concept of autoregres-sive (AR) and moving average (MA) models. Wold\u2019sdecomposition theorem led to the formulation and\nsolution of the linear forecasting problem of Kolmo-\ngorov (1941) . Since then, a considerable body of\nliterature has appeared in the area of time series,dealing with parameter estimation, identification,model checking, and forecasting; see, e.g., Newbold\n(1983) for an", "start_char_idx": 0, "end_char_idx": 3705, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3f162376-dd96-4779-85bf-334cbaa22f61": {"__data__": {"id_": "3f162376-dd96-4779-85bf-334cbaa22f61", "embedding": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fb9dddcc-38f5-4d24-9501-559b09bbf1f5", "node_type": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "hash": "942a6d1a99816add767a89c59268a8ef3808fc25f35374d3b0c406d6c03742d5"}, "2": {"node_id": "87788c3b-80f9-42f4-9731-ee272e8871c7", "node_type": null, "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}, "hash": "09c8cb8e2af36b5ba29eb2b9b9a0eca0976010d7a961e13a55ad0c632aa15370"}}, "hash": "d2df550573be6a21536b3f67c28eb664bf9bbe513af067168d8eceed26163829", "text": "checking, and forecasting; see, e.g., Newbold\n(1983) for an early survey.\nThe publication Time Series Analysis: Forecasting\nand Control byBox and Jenkins (1970)\n3integrated\nthe existing knowledge. Moreover, these authors\ndeveloped a coherent, versatile three-stage iterative\n3The book by Box, Jenkins, and Reinsel (1994) with Gregory\nReinsel as a new co-author is an updated version of the bclassic Q\nBox and Jenkins (1970) text. It includes new material on\nintervention analysis, outlier detection, testing for unit roots, andprocess control.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 446", "start_char_idx": 3646, "end_char_idx": 4279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a985d85b-a468-4956-8b55-f9cdb91c2421": {"__data__": {"id_": "a985d85b-a468-4956-8b55-f9cdb91c2421", "embedding": null, "metadata": {"page_label": "447", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e31273be-cbcb-42fe-9eb1-eeccd5389112", "node_type": null, "metadata": {"page_label": "447", "file_name": "Time Series.pdf"}, "hash": "dbcc8405b1cb0f8ce4aea946e687df4e2ec60951c2ba5d6efed6dd1d155632f3"}}, "hash": "dbcc8405b1cb0f8ce4aea946e687df4e2ec60951c2ba5d6efed6dd1d155632f3", "text": "cycle for time series identification, estimation, and\nverification (rightly known as the Box\u2013Jenkinsapproach). The book has had an enormous impacton the theory and practice of modern time seriesanalysis and forecasting. With the advent of thecomputer, it popularized the use of autoregressiveintegrated moving average (ARIMA) models and theirextensions in many areas of science. Indeed, forecast-\ning discrete time series processes through univariate\nARIMA models, transfer function (dynamic regres-sion) models, and multivariate (vector) ARIMAmodels has generated quite a few IJFpapers. Often\nthese studies were of an empirical nature, using one ormore benchmark methods/models as a comparison.Without pretending to be complete, Table 1 gives a list\nof these studies. Naturally, some of these studies aremore successful than others. In all cases, the\nforecasting experiences reported are valuable. Theyhave also been the key to new developments, whichmay be summarized as follows.\n3.2. Univariate\nThe success of the Box\u2013Jenkins methodology is\nfounded on the fact that the various models can,\nbetween them, mimic the behaviour of diverse typesof series\u2014and do so adequately without usuallyrequiring very many parameters to be estimated inthe final choice of the model. However, in the mid-sixties, the selection of a model was very much amatter of the researcher\u2019s judgment; there was noalgorithm to specify a model uniquely. Since then,\nTable 1\nA list of examples of real applications\nDataset Forecast horizon Benchmark Reference\nUnivariate ARIMA\nElectricity load (min) 1\u201330 min Wiener filter Di Caprio, Genesio, Pozzi, and Vicino\n(1983)\nQuarterly automobile insurance\npaid claim costs8 quarters Log-linear regression Cummins and Griepentrog (1985)\nDaily federal funds rate 1 day Random walk Hein and Spudeck (1988)\nQuarterly macroeconomic data 1\u20138 quarters Wharton model Dhrymes and Peristiani (1988)\nMonthly department store sales 1 month Simple exponential smoothing Geurts and Kelly (1986 ,1990) ,\nPack (1990)\nMonthly demand for telephone services 3 years Univariate state space Grambsch and Stahel (1990)\nYearly population totals 20\u201330 years Demographic models Pflaumer (1992)\nMonthly tourism demand 1\u201324 months Univariate state space,\nmultivariate state spacedu Preez and Witt (2003)\nDynamic regression/transfer function\nMonthly telecommunications traffic 1 month Univariate ARIMA Layton, Defris, and Zehnwirth (1986)\nWeekly sales data 2 years n.a. Leone (1987)\nDaily call volumes 1 week Holt\u2013Winters Bianchi, Jarrett, and Hanumara (1998)\nMonthly employment levels 1\u201312 months Univariate ARIMA Weller (1989)\nMonthly and quarterly consumption\nof natural gas1 month/1 quarter Univariate ARIMA Liu and Lin (1991)\nMonthly electricity consumption 1\u20133 years Univariate ARIMA Harris and Liu (1993)\nVARIMA\nYearly municipal budget data Yearly (in-sample) Univariate ARIMA Downs and Rocke (1983)\nMonthly accounting data 1 month Regression, univariate, ARIMA,\ntransfer functionHillmer, Larcker, and Schroeder (1983)\nQuarterly macroeconomic data 1\u201310 quarters Judgmental methods, univariate\nARIMAO\u00a8ller (1985)\nMonthly truck sales 1\u201313 months Univariate ARIMA, Holt\u2013Winters Heuts and Bronckers (1988)\nMonthly hospital patient movements 2 years Univariate ARIMA, Holt\u2013Winters Lin (1989)\nQuarterly unemployment rate 1\u20138 quarters Transfer function Edlund and Karlsson (1993)J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 447", "start_char_idx": 0, "end_char_idx": 3455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc0bdd5d-4b83-466e-8069-d848f1b0e8ba": {"__data__": {"id_": "bc0bdd5d-4b83-466e-8069-d848f1b0e8ba", "embedding": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78e33315-f640-4781-948c-2d3ad5f2baf9", "node_type": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "hash": "f1c19eb41faa14bb22b940a30d7042674839fc09b3df4c23aa629d42238c8089"}, "3": {"node_id": "5a510654-e2cf-4a24-a874-20e89774268e", "node_type": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "hash": "429f2ccb9ed48f0f8ad1c5b433e57992a485357d57d142b063c160cb93ed8b88"}}, "hash": "c867ec4767af50193f98bc29c04545922ebc1cc5c257600226a0642328c7bbf4", "text": "many techniques and methods have been suggested to\nadd mathematical rigour to the search process of anARMA model, including Akaike\u2019s information crite-rion (AIC), Akaike\u2019s final prediction error (FPE), andthe Bayes information criterion (BIC). Often thesecriteria come down to minimizing (in-sample) one-step-ahead forecast errors, with a penalty term foroverfitting. FPE has also been generalized for multi-\nstep-ahead forecasting (see, e.g., Bhansali, 1996,\n1999 ), but this generalization has not been utilized\nby applied workers. This also seems to be the casewith criteria based on cross-validation and split-sample validation (see, e.g., West, 1996 ) principles,\nmaking use of genuine out-of-sample forecast errors;seePen\u02dca and Sa \u00b4nchez (2005) for a related approach\nworth considering.\nThere are a number of methods (cf. Box et al.,\n1994 ) for estimating the parameters of an ARMA\nmodel. Although these methods are equivalentasymptotically, in the sense that estimates tend tothe same normal distribution, there are large differ-ences in finite sample properties. In a comparativestudy of software packages, Newbold, Agiakloglou,\nand Miller (1994) showed that this difference can be\nquite substantial and, as a consequence, may influ-\nence forecasts. They recommended the use of fullmaximum likelihood. The effect of parameter esti-mation errors on the probability limits of the forecastswas also noticed by Zellner (1971) . He used a\nBayesian analysis and derived the predictive distri-bution of future observations by treating the param-eters in the ARMA model as random variables. More\nrecently, Kim (2003) considered parameter estimation\nand forecasting of AR models in small samples. Hefound that (bootstrap) bias-corrected parameter esti-mators produce more accurate forecasts than the leastsquares estimator. Landsman and Damodaran (1989)\npresented evidence that the James-Stein ARIMAparameter estimator improves forecast accuracyrelative to other methods, under an MSE loss\ncriterion.\nIf a time series is known to follow a univariate\nARIMA model, forecasts using disaggregated obser-vations are, in terms of MSE, at least as good asforecasts using aggregated observations. However, inpractical applications, there are other factors to beconsidered, such as missing values in disaggregatedseries. Both Ledolter (1989) and Hotta (1993)analyzed the effect of an additive outlier on the\nforecast intervals when the ARIMA model parametersare estimated. When the model is stationary, Hotta and\nCardoso Neto (1993) showed that the loss of\nefficiency using aggregated data is not large, even ifthe model is not known. Thus, prediction could bedone by either disaggregated or aggregated models.\nThe problem of incorporating external (prior)\ninformation in the univariate ARIMA forecasts has\nbeen considered by Cholette (1982) ,Guerrero (1991) ,\nandde Alba (1993) .\nAs an alternative to the univariate ARIMA\nmethodology, Parzen (1982) proposed the ARARMA\nmethodology. The key idea is that a time series istransformed from a long-memory AR filter to a short-memory filter, thus avoiding the bharsher Qdifferenc-\ning operator. In addition, a different approach to the\ndconventional TBox\u2013Jenkins identification step is\nused. In the M-competition ( Makridakis et al.,\n1982 ), the ARARMA models achieved the lowest\nMAPE for longer forecast horizons. Hence, it issurprising to find that, apart from the paper by Meade\nand Smith (1985) , the ARARMA methodology has\nnot really taken off in applied work. Its ultimate value\nmay perhaps be better judged", "start_char_idx": 0, "end_char_idx": 3548, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5a510654-e2cf-4a24-a874-20e89774268e": {"__data__": {"id_": "5a510654-e2cf-4a24-a874-20e89774268e", "embedding": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "78e33315-f640-4781-948c-2d3ad5f2baf9", "node_type": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "hash": "f1c19eb41faa14bb22b940a30d7042674839fc09b3df4c23aa629d42238c8089"}, "2": {"node_id": "bc0bdd5d-4b83-466e-8069-d848f1b0e8ba", "node_type": null, "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}, "hash": "c867ec4767af50193f98bc29c04545922ebc1cc5c257600226a0642328c7bbf4"}}, "hash": "429f2ccb9ed48f0f8ad1c5b433e57992a485357d57d142b063c160cb93ed8b88", "text": "really taken off in applied work. Its ultimate value\nmay perhaps be better judged by assessing the study\nbyMeade (2000) who compared the forecasting\nperformance of an automated and non-automatedARARMA method.\nAutomatic univariate ARIMA modelling has been\nshown to produce one-step-ahead forecasts as accu-rate as those produced by competent modellers ( Hill\n& Fildes, 1984; Libert, 1984; Poulos, Kvanli, &\nPavur, 1987; Texter & Ord, 1989 ). Several software\nvendors have implemented automated time seriesforecasting methods (including multivariate methods);see, e.g., Geriner and Ord (1991) ,Tashman and Leach\n(1991) , and Tashman (2000) . Often these methods act\nas black boxes. The technology of expert systems(Me\u00b4lard & Pasteels, 2000\n) can be used to avoid this\nproblem. Some guidelines on the choice of an\nautomatic forecasting method are provided by Chat-\nfield (1988) .\nRather than adopting a single AR model for all\nforecast horizons, Kang (2003) empirically investi-\ngated the case of using a multi-step-ahead forecastingAR model selected separately for each horizon. Theforecasting performance of the multi-step-ahead pro-cedure appears to depend on, among other things,J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 448", "start_char_idx": 3467, "end_char_idx": 4737, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a1d4b0f-fde6-48fe-9011-2023f53f1bbd": {"__data__": {"id_": "9a1d4b0f-fde6-48fe-9011-2023f53f1bbd", "embedding": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1532b3a-bea4-4655-a624-677381c7e0c3", "node_type": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "hash": "73933f3d9f02b7e422658df15dc8e35767ad100614497f8afde458dc77afea0a"}, "3": {"node_id": "04813eca-6f5e-432c-b7b5-ebc95bfadd76", "node_type": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "hash": "e2c2b0401692f6899254d9472fcebfafd2bed67812bc7bad94c4ded62c9b8674"}}, "hash": "424a61d9e23be4c091212fde729a2fd6a6502756e60b796fd5fd0222089e45f8", "text": "optimal order selection criteria, forecast periods,\nforecast horizons, and the time series to be forecast.\n3.3. Transfer function\nThe identification of transfer function models can\nbe difficult when there is more than one inputvariable. Edlund (1984) presented a two-step method\nfor identification of the impulse response function\nwhen a number of different input variables arecorrelated. Koreisha (1983) established various rela-\ntionships between transfer functions, causal implica-tions, and econometric model specification. Gupta\n(1987) identified the major pitfalls in causality testing.\nUsing principal component analysis, a parsimoniousrepresentation of a transfer function model was\nsuggested by del Moral and Valderrama (1997) .\nKrishnamurthi, Narayan, and Raj (1989) showed\nhow more accurate estimates of the impact ofinterventions in transfer function models can beobtained by using a control variable.\n3.4. Multivariate\nThe vector ARIMA (VARIMA) model is a\nmultivariate generalization of the univariate ARIMAmodel. The population characteristics of VARMAprocesses appear to have been first derived byQuenouille (1957) , although software to implement\nthem only became available in the 1980s and 1990s.Since VARIMA models can accommodate assump-tions on exogeneity and on contemporaneous relation-\nships, they offered new challenges to forecasters and\npolicymakers. Riise and Tj\u00f8stheim (1984) addressed\nthe effect of parameter estimation on VARMAforecasts. Cholette and Lamy (1986) showed how\nsmoothing filters can be built into VARMA models.The smoothing prevents irregular fluctuations inexplanatory time series from migrating to the forecastsof the dependent series. To determine the maximum\nforecast horizon of VARMA processes, De Gooijer\nand Klein (1991) established the theoretical properties\nof cumulated multi-step-ahead forecasts and cumulat-ed multi-step-ahead forecast errors. Lu\u00a8tkepohl (1986)\nstudied the effects of temporal aggregation andsystematic sampling on forecasting, assuming thatthe disaggregated (stationary) variable follows aVARMA process with unknown order. Later, Bidar-kota (1998) considered the same problem but with the\nobserved variables integrated rather than stationary.\nVector autoregressions (VARs) constitute a special\ncase of the more general class of VARMA models. Inessence, a VAR model is a fairly unrestricted(flexible) approximation to the reduced form of awide variety of dynamic econometric models. VARmodels can be specified in a number of ways. Funke\n(1990) presented five different VAR specifications\nand compared their forecasting performance usingmonthly industrial production series. Dhrymes and\nThomakos (1998) discussed issues regarding the\nidentification of structural VARs. Hafer and Sheehan\n(1989) showed the effect on VAR forecasts of changes\nin the model structure. Explicit expressions for VARforecasts in levels are provided by Arin\u02dco and Franses\n(2000) ;s e ea l s o Wieringa and Horva \u00b4th (2005) .\nHansson, Jansson, and Lo \u00a8f (2005) used a dynamic\nfactor model as a starting point to obtain forecastsfrom parsimoniously parametrized VARs.\nIn general, V AR models tend to suffer from\ndoverfitting Twith too many free insignificant param-\neters. As a result, these models can provide poor out-of-sample forecasts, even though within-sample fit-\nting is good; see, e.g., Liu, Gerlow, and Irwin (1994)\nandSimkins (1995) . Instead of restricting some of the\nparameters in the usual way, Litterman (1986) and\nothers imposed a prior distribution on the parameters,expressing the belief that many economic variablesbehave like a random walk. BVAR models have beenchiefly", "start_char_idx": 0, "end_char_idx": 3635, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "04813eca-6f5e-432c-b7b5-ebc95bfadd76": {"__data__": {"id_": "04813eca-6f5e-432c-b7b5-ebc95bfadd76", "embedding": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b1532b3a-bea4-4655-a624-677381c7e0c3", "node_type": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "hash": "73933f3d9f02b7e422658df15dc8e35767ad100614497f8afde458dc77afea0a"}, "2": {"node_id": "9a1d4b0f-fde6-48fe-9011-2023f53f1bbd", "node_type": null, "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}, "hash": "424a61d9e23be4c091212fde729a2fd6a6502756e60b796fd5fd0222089e45f8"}}, "hash": "e2c2b0401692f6899254d9472fcebfafd2bed67812bc7bad94c4ded62c9b8674", "text": "that many economic variablesbehave like a random walk. BVAR models have beenchiefly used for macroeconomic forecasting ( Artis &\nZhang, 1990; Ashley, 1988; Holden & Broomhead,\n1990; Kunst & Neusser, 1986 ), for forecasting market\nshares ( Ribeiro Ramos, 2003 ), for labor market\nforecasting ( LeSage & Magura, 1991 ), for business\nforecasting ( Spencer, 1993 ), or for local economic\nforecasting (\nLeSage, 1989 ).Kling and Bessler (1985)\ncompared out-of-sample forecasts of several then-known multivariate time series methods, includingLitterman\u2019s BVAR model.\nTheEngle and Granger (1987) concept of cointe-\ngration has raised various interesting questions re-garding the forecasting ability of error correctionmodels (ECMs) over unrestricted VARs and BVARs.Shoesmith (1992) ,Shoesmith (1995) ,Tegene and\nKuchler (1994) ,a n d Wang and Bessler (2004)\nprovided empirical evidence to suggest that ECMsoutperform VARs in levels, particularly over longerJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 449", "start_char_idx": 3552, "end_char_idx": 4591, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "00ad34ef-4b64-4f07-b1c4-b642f5749761": {"__data__": {"id_": "00ad34ef-4b64-4f07-b1c4-b642f5749761", "embedding": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c3939ad-4272-44d6-b398-73f8530a3f91", "node_type": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "hash": "900b56fa5e036826a21f0cbc68a709c59ac3f9ae92204dca5a83bca324b464ae"}, "3": {"node_id": "8a0237f9-81f4-4cef-9500-e3615211a218", "node_type": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "hash": "9d2675886923b047e29de82d0ea29b097d19a4f3802a4e2ac88c999cec6bd6c4"}}, "hash": "ad7d9cc4cb87cb620207237fa15732c8603274aeebcff7764922123cbd326c90", "text": "forecast horizons. Shoesmith (1995) , and later Villani\n(2001) , also showed how Litterman\u2019s (1986) Bayesian\napproach can improve forecasting with cointegratedVARs. Reimers (1997) studied the forecasting perfor-\nmance of seasonally cointegrated vector time seriesprocesses using an ECM in fourth differences. Poskitt\n(2003) discussed the specification of cointegrated\nVA RM A s ys t ems . Chevillon and Hendry (2005)\nanalyzed the relationship between direct multi-step\nestimation of stationary and nonstationary VARs andforecast accuracy.\n4. Seasonality\nThe oldest approach to handling seasonality in time\nseries is to extract it using a seasonal decomposition\nprocedure such as the X-11 method. Over the past 25years, the X-11 method and its variants (including themost recent version, X-12-ARIMA, Findley, Monsell,\nBell, Otto, & Chen, 1998 ) have been studied\nextensively.\nOne line of research has considered the effect of\nusing forecasting as part of the seasonal decomposi-\ntion method. For example, Dagum (1982) andHuot,\nChiu, and Higginson (1986) looked at the use of\nforecasting in X-11-ARIMA to reduce the size ofrevisions in the seasonal adjustment of data, andPfeffermann, Morry, and Wong (1995) explored the\neffect of the forecasts on the variance of the trend andseasonally adjusted values.\nQuenneville, Ladiray, and Lefranc \u00b8ois (2003) took a\ndifferent perspective and looked at forecasts implied\nby the asymmetric moving average filters in the X-11method and its variants.\nA third approach has been to look at the\neffectiveness of forecasting using seasonally adjusteddata obtained from a seasonal decomposition method.Miller and Williams (2003, 2004) showed that greater\nforecasting accuracy is obtained by shrinking the\nseasonal component towards zero. The commentaries\non the latter paper ( Findley, Wills, & Monsell, 2004;\nHyndman, 2004; Koehler, 2004; Ladiray & Quenne-ville, 2004; Ord, 2004 ) gave several suggestions\nregarding the implementation of this idea.\nIn addition to work on the X-11 method and its\nvariants, there have also been several new methods forseasonal adjustment developed, the most importantbeing the model based approach of TRAMO-SEATS\n(Go\u00b4mez & Maravall, 2001; Kaiser & Maravall, 2005 )\nand the nonparametric method STL ( Cleveland,\nCleveland, McRae, & Terpenning, 1990 ). Another\nproposal has been to use sinusoidal models ( Simmons,\n1990 ).\nWhen forecasting several similar series, With-\nycombe (1989) showed that it can be more efficient\nto estimate a combined seasonal component from the\ngroup of series, rather than individual seasonalpatterns. Bunn and Vassilopoulos (1993) demonstrat-\ned how to use clustering to form appropriate groupsfor this situation, and Bunn and Vassilopoulos (1999)\nintroduced some improved estimators for the groupseasonal indices.\nTwenty-five years ago, unit root tests had only\nrecently been invented and seasonal unit root tests\nwere yet to appear. Subsequently, there has beenconsiderable work done on the use and implementa-tion of seasonal unit root tests including Hylleberg\nand Pagan (1997) ,Taylor (1997) , and Franses and\nKoehler (1998) .Paap, Franses, and Hoek (1997) and\nClements and Hendry (1997) studied the forecast\nperformance of models with unit roots, especially in\nthe context of level shifts.\nSome authors have cautioned against the wide-\nspread use of standard seasonal unit root models foreconomic time series. Osborn (1990) argued that\ndeterministic seasonal", "start_char_idx": 0, "end_char_idx": 3455, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a0237f9-81f4-4cef-9500-e3615211a218": {"__data__": {"id_": "8a0237f9-81f4-4cef-9500-e3615211a218", "embedding": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7c3939ad-4272-44d6-b398-73f8530a3f91", "node_type": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "hash": "900b56fa5e036826a21f0cbc68a709c59ac3f9ae92204dca5a83bca324b464ae"}, "2": {"node_id": "00ad34ef-4b64-4f07-b1c4-b642f5749761", "node_type": null, "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}, "hash": "ad7d9cc4cb87cb620207237fa15732c8603274aeebcff7764922123cbd326c90"}}, "hash": "9d2675886923b047e29de82d0ea29b097d19a4f3802a4e2ac88c999cec6bd6c4", "text": "foreconomic time series. Osborn (1990) argued that\ndeterministic seasonal components are more common\nin economic series than stochastic seasonality. Franses\nand Romijn (1993) suggested that seasonal roots in\nperiodic models result in better forecasts. Periodic\ntime series models were also explored by Wells\n(1997) ,Herwartz (1997) , and Novales and de Fruto\n(1997) , all of whom found that periodic models can\nlead to improved forecast performance compared tonon-periodic models under some conditions. Fore-casting of multivariate periodic ARMA processes isconsidered by Ullah (1993) .\nSeveral papers have compared various seasonal\nmodels empirically. Chen (1997) explored the robust-\nness properties of a structural model, a regressionmodel with seasonal dummies, an ARIMA model, andHolt\u2013Winters\u2019 method, and found that the latter twoyield forecasts that are relatively robust to modelmisspecification. Noakes, McLeod, and Hipel (1985) ,\nAlbertson and Aylen (1996) ,Kulendran and King\n(1997) ,a n d Franses and van Dijk (2005) eachJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 450", "start_char_idx": 3382, "end_char_idx": 4505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ccab159a-ce4f-49fd-91b1-aaa9b8991dac": {"__data__": {"id_": "ccab159a-ce4f-49fd-91b1-aaa9b8991dac", "embedding": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926", "node_type": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "hash": "b5d019d1625b2cdcb7026ba3372aac3acac6bc60da0b5626d22c2150a6fec624"}, "3": {"node_id": "0e36bc01-5c79-4d36-8c38-b04430963953", "node_type": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "hash": "4e20e05ad6259ebbb64109edcb968ebb29e0a36fe82e5539af0e2a0d28ce0694"}}, "hash": "5b7632af600bcd64a7dc811d2f8bb287c001e923bd778c0c7e594449d86c1129", "text": "compared the forecast performance of several season-\nal models applied to real data. The best performingmodel varies across the studies, depending on whichmodels were tried and the nature of the data. Thereappears to be no consensus yet as to the conditionsunder which each model is preferred.\n5. State space and structural models and the\nKalman filter\nAt the start of the 1980s, state space models were\nonly beginning to be used by statisticians forforecasting time series, although the ideas had beenpresent in the engineering literature since Kalman\u2019s\n(1960) ground-breaking work. State space models\nprovide a unifying framework in which any linear\ntime series model can be written. The key forecastingcontribution of Kalman (1960) was to give a\nrecursive algorithm (known as the Kalman filter)for computing forecasts. Statisticians became inter-ested in state space models when Schweppe (1965)\nshowed that the Kalman filter provides an efficientalgorithm for computing the one-step-ahead predic-\ntion errors and associated variances needed to\nproduce the likelihood function. Shumway and\nStoffer (1982) combined the EM algorithm with the\nKalman filter to give a general approach to forecast-ing time series using state space models, includingallowing for missing observations.\nA particular class of state space models, known\nasbdynamic linear models Q(DLM), was introduced\nbyHarrison and Stevens (1976) , who also proposed\na Bayesian approach to estimation. Fildes (1983)\ncompared the forecasts obtained using Harrison andStevens method with those from simpler methodssuch as exponential smoothing, and concluded thatthe additional complexity did not lead to improvedforecasting performance. The modelling and esti-mation approach of Harrison and Stevens was\nfurther developed by West, Harrison, and Migon\n(1985) and West and Harrison (1989) .Harvey\n(1984, 1989) extended the class of models and\nfollowed a non-Bayesian approach to estimation. Healso renamed the models bstructural models Q, al-\nthough in later papers he uses the term bunobserved\ncomponent models Q.Harvey (2006) provides a com-\nprehensive review and introduction to this class ofmodels including continuous-time and non-Gaussian\nvariations.\nThese models bear many similarities with expo-\nnential smoothing methods, but have multiple sourcesof random error. In particular, the bbasic structural\nmodel Q(BSM) is similar to Holt\u2013Winters\u2019 method for\nseasonal data and includes level, trend and seasonalcomponents.\nRay (1989) discussed convergence rates for the\nlinear growth structural model and showed that theinitial states (usually chosen subjectively) have a non-negligible impact on forecasts. Harvey and Snyder\n(1990) proposed some continuous-time structural\nmodels for use in forecasting lead time demand forinventory control. Proietti (2000) discussed several\nvariations on the BSM, compared their properties and\nevaluated the resulting forecasts.\nNon-Gaussian structural models have been the\nsubject of a large number of papers, beginning withthe power steady model of Smith (1979) with further\ndevelopment by West et al. (1985) . For example, these\nmodels were applied to forecasting time series ofproportions by Grunwald, Raftery, and Guttorp (1993)\nand to counts by Harvey and Fernandes (1989) .\nHowever, Grunwald, Hamza, and Hyndman (1997)\nshowed that most of the commonly used models havethe substantial flaw of all sample paths converging toa constant when the sample space is less than thewhole real line, making them unsuitable for anythingother than point forecasting.\nAnother class of state space models, known as\nbbalanced state space models Q, has been used\nprimarily for forecasting macroeconomic time series.\nMittnik (1990) provided a survey of this class of\nmodels, and Vinod and Basu (1995) obtained\nforecasts of consumption, income, and interest", "start_char_idx": 0, "end_char_idx": 3840, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e36bc01-5c79-4d36-8c38-b04430963953": {"__data__": {"id_": "0e36bc01-5c79-4d36-8c38-b04430963953", "embedding": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926", "node_type": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "hash": "b5d019d1625b2cdcb7026ba3372aac3acac6bc60da0b5626d22c2150a6fec624"}, "2": {"node_id": "ccab159a-ce4f-49fd-91b1-aaa9b8991dac", "node_type": null, "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}, "hash": "5b7632af600bcd64a7dc811d2f8bb287c001e923bd778c0c7e594449d86c1129"}}, "hash": "4e20e05ad6259ebbb64109edcb968ebb29e0a36fe82e5539af0e2a0d28ce0694", "text": "and Basu (1995) obtained\nforecasts of consumption, income, and interest ratesusing balanced state space models. These modelshave only one source of random error and subsumevarious other time series models including ARMAXmodels, ARMA models, and rational distributed lag\nmodels. A related class of state space models are the\nbsingle source of error Qmodels that underly expo-\nnential smoothing methods; these were discussed inSection 2.\nAs well as these methodological developments,\nthere have been several papers proposing innovativestate space models to solve practical forecastingproblems. These include Coomes (1992) who used aJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 451", "start_char_idx": 3769, "end_char_idx": 4489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7d7781fd-9233-459d-93e4-e8570b097fcf": {"__data__": {"id_": "7d7781fd-9233-459d-93e4-e8570b097fcf", "embedding": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83ed9163-6ee0-44c4-a92d-5978408267f9", "node_type": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "hash": "cd614389f4f2c79f69253d4e27ad6935474c6df6b88bb7ba40899b707a167684"}, "3": {"node_id": "139f4968-5b88-4fdc-9ff4-11b476b435a6", "node_type": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "hash": "0ecde3394f5eb1886d94ac9971ee0862cbccb7431e6f8a39a1da75f01dd1cff0"}}, "hash": "6e12d78f4ade9d7bde45c3b6ebbba4e464a036e7dc69caafd8e66a8ee3066021", "text": "state space model to forecast jobs by industry for local\nregions and Patterson (1995) who used a state space\napproach for forecasting real personal disposableincome.\nAmongst this research on state space models,\nKalman filtering, and discrete/continuous-time struc-tural models, the books by Harvey (1989) ,West and\nHarrison (1989) , and Durbin and Koopman (2001)\nhave had a substantial impact on the time series\nliterature. However, forecasting applications of thestate space framework using the Kalman filter havebeen rather limited in the IJF. In that sense, it is\nperhaps not too surprising that even today, sometextbook authors do not seem to realize that theKalman filter can, for example, track a nonstationaryprocess stably.\n6. Nonlinear models\n6.1. Preamble\nCompared to the study of linear time series, the\ndevelopment of nonlinear time series analysis and\nforecasting is still in its infancy. The beginning of\nnonlinear time series analysis has been attributed toV olterra (1930) . He showed that any continuous\nnonlinear function in tcould be approximated by a\nfinite Volterra series. Wiener (1958) became interested\nin the ideas of functional series representation andfurther developed the existing material. Although theprobabilistic properties of these models have been\nstudied extensively, the problems of parameter esti-\nmation, model fitting, and forecasting have beenneglected for a long time. This neglect can largelybe attributed to the complexity of the proposedWiener model and its simplified forms like thebilinear model ( Poskitt & Tremayne, 1986 ). At the\ntime, fitting these models led to what were insur-mountable computational difficulties.\nAlthough linearity is a useful assumption and a\npowerful tool in many areas, it became increasinglyclear in the late 1970s and early 1980s that linearmodels are insufficient in many real applications. Forexample, sustained animal population size cycles (thewell-known Canadian lynx data), sustained solarcycles (annual sunspot numbers), energy flow, andamplitude\u2013frequency relations were found not to besuitable for linear models. Accelerated by practical\ndemands, several useful nonlinear time series modelswere proposed in this same period. De Gooijer and\nKumar (1992) provided an overview of the develop-\nments in this area to the beginning of the 1990s. Theseauthors argued that the evidence for the superiorforecasting performance of nonlinear models is patchy.\nOne factor that has probably retarded the wide-\nspread reporting of nonlinear forecasts is that up to\nthat time it was not possible to obtain closed-formanalytical expressions for multi-step-ahead forecasts.However, by using the so-called Chapman\u2013Kolmo-gorov relationship, exact least squares multi-step-ahead forecasts for general nonlinear AR models can,in principle, be obtained through complex numericalintegration. Early examples of this approach are\nreported by Pemberton (1987) and Al-Qassem and\nLane (1989) . Nowadays, nonlinear forecasts are\nobtained by either Monte Carlo simulation or bybootstrapping. The latter approach is preferred sinceno assumptions are made about the distribution of theerror process.\nThe monograph by Granger and Tera \u00a8svirta (1993)\nhas boosted new developments in estimating, evaluat-\ning, and selecting among nonlinear forecasting models\nfor economic and financial time series. A goodoverview of the current state-of-the-art is IJFSpecial\nIssue 20:2 (2004). In their introductory paper, Clem-\nents, Franses, and Swanson (2004) outlined a variety\nof topics for future research. They concluded thatb...the day is still long off when simple, reliable, and\neasy to use nonlinear model specification, estimation,\nand forecasting procedures will be readily available Q.\n6.2. Regime-switching models\nThe class of (self-exciting) threshold AR", "start_char_idx": 0, "end_char_idx": 3806, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "139f4968-5b88-4fdc-9ff4-11b476b435a6": {"__data__": {"id_": "139f4968-5b88-4fdc-9ff4-11b476b435a6", "embedding": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83ed9163-6ee0-44c4-a92d-5978408267f9", "node_type": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "hash": "cd614389f4f2c79f69253d4e27ad6935474c6df6b88bb7ba40899b707a167684"}, "2": {"node_id": "7d7781fd-9233-459d-93e4-e8570b097fcf", "node_type": null, "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}, "hash": "6e12d78f4ade9d7bde45c3b6ebbba4e464a036e7dc69caafd8e66a8ee3066021"}}, "hash": "0ecde3394f5eb1886d94ac9971ee0862cbccb7431e6f8a39a1da75f01dd1cff0", "text": "Regime-switching models\nThe class of (self-exciting) threshold AR (SETAR)\nmodels has been prominently promoted through thebooks by Tong (1983, 1990) . These models, which are\npiecewise linear models in their most basic form, have\nattracted some attention in the IJF.Clements and\nSmith (1997) compared a number of methods for\nobtaining multi-step-ahead forecasts for univariatediscrete-time SETAR models. They concluded thatforecasts made using Monte Carlo simulation aresatisfactory in cases where it is known that thedisturbances in the SETAR model come from asymmetric distribution. Otherwise, the bootstrapJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 452", "start_char_idx": 3741, "end_char_idx": 4440, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "690ecd27-48fe-4c63-bf45-6651769faf28": {"__data__": {"id_": "690ecd27-48fe-4c63-bf45-6651769faf28", "embedding": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d", "node_type": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "hash": "5f6ae6ecea237871ff51ded158b24da188204472a83ce7ff62e4d47900e4bcb5"}, "3": {"node_id": "750202ef-9145-482f-8c72-567e9380d5c7", "node_type": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "hash": "88da2f48cfec338586a62819013616d82a44d06380bd7b8aa2a15503963cfce5"}}, "hash": "48ca70b8d48d6293e940f8714d4662fc2f78ceef67e2e7d9cc9521f9c1106e6d", "text": "method is to be preferred. Similar results were reported\nbyDe Gooijer and Vidiella-i-Anguera (2004) for\nthreshold VAR models. Brockwell and Hyndman\n(1992) obtained one-step-ahead forecasts for univari-\nate continuous-time threshold AR models (CTAR).Since the calculation of multi-step-ahead forecastsfrom CTAR models involves complicated higherdimensional integration, the practical use of CTARs\nis limited. The out-of-sample forecast performance of\nvarious variants of SETAR models relative to linearmodels has been the subject of several IJF papers,including Astatkie, Watts, and Watt (1997) ,Boero and\nMarrocu (2004) , and Enders and Falk (1998) .\nOne drawback of the SETAR model is that the\ndynamics change discontinuously from one regime tothe other. In contrast, a smooth transition AR (STAR)\nmodel allows for a more gradual transition between\nthe different regimes. Sarantis (2001) found evidence\nthat STAR-type models can improve upon linear ARand random walk models in forecasting stock prices atboth short-term and medium-term horizons. Interest-ingly, the recent study by Bradley and Jansen (2004)\nseems to refute Sarantis\u2019 conclusion.\nCan forecasts for macroeconomic aggregates like\ntotal output or total unemployment be improved by\nusing a multi-level panel smooth STAR model fordisaggregated series? This is the key issue examinedbyFok, van Dijk, and Franses (2005) . The proposed\nSTAR model seems to be worth investigating in moredetail since it allows the parameters that govern theregime-switching to differ across states. Based onsimulation experiments and empirical findings, the\nauthors claim that improvements in one-step-ahead\nforecasts can indeed be achieved.\nFranses, Paap, and Vroomen (2004) proposed a\nthreshold AR(1) model that allows for plausibleinference about the specific values of the parameters.The key idea is that the values of the AR parameterdepend on a leading indicator variable. The resultingmodel outperforms other time-varying nonlinear\nmodels, including the Mar kov regime-switching\nmodel, in terms of forecasting.\n6.3. Functional-coefficient model\nA functional coefficient AR (FCAR or FAR) model\nis an AR model in which the AR coefficients areallowed to vary as a measurable smooth function ofanother variable, such as a lagged value of the time\nseries itself or an exogenous variable. The FCARmodel includes TAR and STAR models as specialcases, and is analogous to the generalized additivemodel of Hastie and Tibshirani (1991) .Chen and Tsay\n(1993) proposed a modeling procedure using ideas\nfrom both parametric and nonparametric statistics.The approach assumes little prior information on\nmodel structure without suffering from the bcurse of\ndimensionality Q; see also Cai, Fan, and Yao (2000) .\nHarvill and Ray (2005) presented multi-step-ahead\nforecasting results using univariate and multivariatefunctional coefficient (V)FCAR models. Theseauthors restricted their comparison to three forecastingmethods: the na\u0131 \u00a8ve plug-in predictor, the bootstrap\npredictor, and the multi-stage predictor. Both simula-\ntion and empirical results indicate that the bootstrap\nmethod appears to give slightly more accurate forecastresults. A potentially useful area of future research iswhether the forecasting power of VFCAR models canbe enhanced by using exogenous variables.\n6.4. Neural nets\nAn artificial neural network (ANN) can be useful\nfor nonlinear processes that have an unknownfunctional relationship and as a result are difficult tofit (Darbellay & Slama, 2000 ). The main idea with\nANNs is that inputs, or dependent variables, getfiltered through one or more hidden layers each ofwhich consist of hidden units, or nodes, before theyreach the output variable. The", "start_char_idx": 0, "end_char_idx": 3708, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "750202ef-9145-482f-8c72-567e9380d5c7": {"__data__": {"id_": "750202ef-9145-482f-8c72-567e9380d5c7", "embedding": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d", "node_type": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "hash": "5f6ae6ecea237871ff51ded158b24da188204472a83ce7ff62e4d47900e4bcb5"}, "2": {"node_id": "690ecd27-48fe-4c63-bf45-6651769faf28", "node_type": null, "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}, "hash": "48ca70b8d48d6293e940f8714d4662fc2f78ceef67e2e7d9cc9521f9c1106e6d"}}, "hash": "88da2f48cfec338586a62819013616d82a44d06380bd7b8aa2a15503963cfce5", "text": "ofwhich consist of hidden units, or nodes, before theyreach the output variable. The intermediate output is\nrelated to the final output. Various other nonlinear\nmodels are specific versions of ANNs, where morestructure is imposed; see JoF Special Issue 17:5/6\n(1998) for some recent studies.\nOne major application area of ANNs is forecasting;\nseeZhang, Patuwo, and Hu (1998) and Hippert,\nPedreira, and Souza (2001) for good surveys of the\nliterature. Numerous studies outside the IJF have\ndocumented the successes of ANNs in forecasting\nfinancial data. However, in two editorials in thisJournal ,Chatfield (1993, 1995) questioned whether\nANNs had been oversold as a miracle forecastingtechnique. This was followed by several papersdocumenting that na\u0131 \u00a8ve models such as the random\nwalk can outperform ANNs (see, e.g., Callen, Kwan,\nYip, & Yuan, 1996; Church & Curram, 1996; Conejo,J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 453", "start_char_idx": 3624, "end_char_idx": 4596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b200fcca-f4f3-4644-a420-d53fb30f7cc4": {"__data__": {"id_": "b200fcca-f4f3-4644-a420-d53fb30f7cc4", "embedding": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4223da55-06cf-450c-9d00-4c56f099f5cf", "node_type": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "hash": "7103a20d7613ab8679031fc7f77f0f99fca493d9d672091214be4666e9f0d0a9"}, "3": {"node_id": "c96c8444-4bcd-4b2c-8fff-740c2edf2f5e", "node_type": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "hash": "1f0601cbed8d56328ee3f5b9fd519ac70b8e89c69413468bc5ddacf6646faf88"}}, "hash": "081c88494cd0bc9580510446fc87e85f882438036c0f4319d61f96608d136032", "text": "Contreras, Esp\u0131 \u00b4nola, & Plazas, 2005; Gorr, Nagin, &\nSzczypula, 1994; Tkacz, 2001 ). These observations\nare consistent with the results of Adya and Collopy\n(1998) evaluating the effectiveness of ANN-based\nforecasting in 48 studies done between 1988 and1994.\nGorr (1994) and Hill, Marquez, OConnor, and\nRemus (1994) suggested that future research should\ninvestigate and better define the border between\nwhere ANNs and btraditional Qtechniques outperform\none other. That theme is explored by several authors.Hill et al. (1994) noticed that ANNs are likely to work\nbest for high frequency financial data and Balkin and\nOrd (2000) also stressed the importance of a long time\nseries to ensure optimal results from training ANNs.Qi (2001) pointed out that ANNs are more likely to\noutperform other methods when the input data is kept\nas current as possible, using recursive modelling (seealso Olson & Mossman, 2003 ).\nA general problem with nonlinear models is the\nbcurse of model complexity and model over-para-\nmetrization Q. If parsimony is considered to be really\nimportant, then it is interesting to compare the out-of-sample forecasting performance of linear versus\nnonlinear models, using a wide variety of different\nmodel selection criteria. This issue was considered inquite some depth by Swanson and White (1997) .\nTheir results suggested that a single hidden layerdfeed-forward TANN model, which has been by far the\nmost popular in time series econometrics, offers auseful and flexible alternative to fixed specificationlinear models, particularly at forecast horizons greater\nthan one-step-ahead. However, in contrast to Swanson\nand White, Heravi, Osborn, and Birchenhall (2004)\nfound that linear models produce more accurateforecasts of monthly seasonally unadjusted Europeanindustrial production series than ANN models.Ghiassi, Saidane, and Zimbra (2005) presented a\ndynamic ANN and compared its forecasting perfor-mance against the traditional ANN and ARIMA\nmodels.\nTimes change, and it is fair to say that the risk of\nover-parametrization and overfitting is now recog-nized by many authors; see, e.g., Hippert, Bunn, and\nSouza (2005) who use a large ANN (50 inputs, 15\nhidden neurons, 24 outputs) to forecast daily electric-ity load profiles. Nevertheless, the question ofwhether or not an ANN is over-parametrized stillremains unanswered. Some potentially valuable ideas\nfor building parsimoniously parametrized ANNs,using statistical inference, are suggested by Tera\u00a8svirta,\nvan Dijk, and Medeiros (2005) .\n6.5. Deterministic versus stochastic dynamics\nThe possibility that nonlinearities in high-frequen-\ncy financial data (e.g., hourly returns) are produced by\na low-dimensional deterministic chaotic process hasbeen the subject of a few studies published in the IJF.\nCecen and Erkal (1996) showed that it is not possible\nto exploit deterministic nonlinear dependence in dailyspot rates in order to improve short-term forecasting.Lisi and Medio (1997) reconstructed the state space\nfor a number of monthly exchange rates and, using a\nlocal linear method, approximated the dynamics of the\nsystem on that space. One-step-ahead out-of-sampleforecasting showed that their method outperforms arandom walk model. A similar study was performedbyCao and Soofi (1999) .\n6.6. Miscellaneous\nA host of other, often less well known, nonlinear\nmodels have been used for forecasting purposes. Forinstance, Ludlow and Enders (2000) adopted Fourier\ncoefficients to approximate the various types ofnonlinearities present in time series data. Herwartz\n(2001) extended the linear vector ECM to allow for\nasymmetries. Dahl and", "start_char_idx": 0, "end_char_idx": 3619, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c96c8444-4bcd-4b2c-8fff-740c2edf2f5e": {"__data__": {"id_": "c96c8444-4bcd-4b2c-8fff-740c2edf2f5e", "embedding": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4223da55-06cf-450c-9d00-4c56f099f5cf", "node_type": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "hash": "7103a20d7613ab8679031fc7f77f0f99fca493d9d672091214be4666e9f0d0a9"}, "2": {"node_id": "b200fcca-f4f3-4644-a420-d53fb30f7cc4", "node_type": null, "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}, "hash": "081c88494cd0bc9580510446fc87e85f882438036c0f4319d61f96608d136032"}}, "hash": "1f0601cbed8d56328ee3f5b9fd519ac70b8e89c69413468bc5ddacf6646faf88", "text": "extended the linear vector ECM to allow for\nasymmetries. Dahl and Hylleberg (2004) compared\nHamilton\u2019s (2001) flexible nonlinear regression mod-\nel, ANNs, and two versions of the projection pursuit\nregression model. Time-varying AR models areincluded in a comparative study by Marcellino\n(2004) . The nonparametric, nearest-neighbour method\nwas applied by Ferna\u00b4ndez-Rodr\u0131 \u00b4guez, Sosvilla-Rivero,\nand Andrada-Fe \u00b4lix (1999) .\n7. Long memory models\nWhen the integration parameter din an ARIMA\nprocess is fractional and greater than zero, the processexhibits long memory in the sense that observations along time-span apart have non-negligible dependence.Stationary long-memory models (0 bdb0.5), also\ntermed fractionally differenced ARMA (FARMA) orJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 454", "start_char_idx": 3554, "end_char_idx": 4391, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ba598f3f-7c82-4bb3-bc5c-eb48f816ad2a": {"__data__": {"id_": "ba598f3f-7c82-4bb3-bc5c-eb48f816ad2a", "embedding": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd963a5c-8382-4ea1-8afa-b0eb842a4568", "node_type": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "hash": "8a7d525a286224bcd3711f18bc090578fe7517303ab9ee39c857ff92f67c245a"}, "3": {"node_id": "e4eaa70b-3adf-4556-9770-cc57fc59472a", "node_type": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "hash": "e0c12056879bbf0792f6e7ad57aeea7562fff3a1773dff62585b93bf48ad2cf7"}}, "hash": "c8b49bc5400fce5b5d25b4dbc045cf7e255fb62ee7778f491164fb3485ec8f69", "text": "fractionally integrated ARMA (ARFIMA) models,\nhave been considered by workers in many fields; seeGranger and Joyeux (1980) for an introduction. One\nmotivation for these studies is that many empiricaltime series have a sample autocorrelation functionwhich declines at a slower rate than for an ARIMAmodel with finite orders and integer d.\nThe forecasting potential of fitted FARMA/\nARFIMA models, as opposed to forecast results\nobtained from other time series models, has been atopic of various IJFpapers and a special issue (2002,\n18:2). Ray (1993a, 1993b) undertook such a compar-\nison between seasonal FARMA/ARFIMA models andstandard (non-fractional) seasonal ARIMA models.The results show that higher order AR models arecapable of forecasting the longer term well when\ncompared with ARFIMA models. Following Ray\n(1993a, 1993b) ,Smith and Yadav (1994) investigated\nthe cost of assuming a unit difference when a series isonly fractionally integrated with dp1. Over-differenc-\ning a series will produce a loss in forecastingperformance one-step-ahead, with only a limited lossthereafter. By contrast, under-differencing a series ismore costly with larger potential losses from fitting a\nmis-specified AR model at all forecast horizons. This\nissue is further explored by Andersson (2000) who\nshowed that misspecification strongly affects theestimated memory of the ARFIMA model, using arule which is similar to the test of O\u00a8ller (1985) .Man\n(2003) argued that a suitably adapted ARMA(2,2)\nmodel can produce short-term forecasts that arecompetitive with estimated ARFIMA models. Multi-\nstep-ahead forecasts of long-memory models have\nbeen developed by Hurvich (2002) and compared by\nBhansali and Kokoszka (2002) .\nMany extensions of ARFIMA models and compar-\nisons of their relative forecasting performance havebeen explored. For instance, Franses and Ooms (1997)\nproposed the so-called periodic ARFIMA(0, d,0) mod-\nel where dcan vary with the seasonality parameter.\nRavishanker and Ray (2002) considered the estimation\nand forecasting of multivariate ARFIMA models.Baillie and Chung (2002) discussed the use of linear\ntrend-stationary ARFIMA models, while the paper byBeran, Feng, Ghosh and Sibbertsen (2002) extended\nthis model to allow for nonlinear trends. Souza and\nSmith (2002) investigated the effect of different\nsampling rates, such as monthly versus quarterly data,on estimates of the long-memory parameter d.I na\nsimilar vein, Souza and Smith (2004) looked at the\neffects of temporal aggregation on estimates andforecasts of ARFIMA processes. Within the contextof statistical quality control, Ramjee, Crato, and Ray\n(2002) introduced a hyperbolically weighted moving\naverage forecast-based control chart, designed specif-ically for nonstationary ARFIMA models.\n8. ARCH/GARCH models\nA key feature of financial time series is that large\n(small) absolute returns tend to be followed by large(small) absolute returns, that is, there are periodswhich display high (low) volatility. This phenomenon\nis referred to as volatility clustering in econometrics\nand finance. The class of autoregressive conditionalheteroscedastic (ARCH) models, introduced by Engle\n(1982) , describe the dynamic changes in conditional\nvariance as a deterministic (typically quadratic)function of past returns. Because the variance isknown at time t/C01, one-step-ahead forecasts are\nreadily available. Next, multi-step-ahead forecasts can\nbe computed recursively. A more parsimonious model\nthan ARCH is the so-called generalized ARCH(GARCH)", "start_char_idx": 0, "end_char_idx": 3520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4eaa70b-3adf-4556-9770-cc57fc59472a": {"__data__": {"id_": "e4eaa70b-3adf-4556-9770-cc57fc59472a", "embedding": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fd963a5c-8382-4ea1-8afa-b0eb842a4568", "node_type": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "hash": "8a7d525a286224bcd3711f18bc090578fe7517303ab9ee39c857ff92f67c245a"}, "2": {"node_id": "ba598f3f-7c82-4bb3-bc5c-eb48f816ad2a", "node_type": null, "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}, "hash": "c8b49bc5400fce5b5d25b4dbc045cf7e255fb62ee7778f491164fb3485ec8f69"}}, "hash": "e0c12056879bbf0792f6e7ad57aeea7562fff3a1773dff62585b93bf48ad2cf7", "text": "parsimonious model\nthan ARCH is the so-called generalized ARCH(GARCH) model ( Bollerslev, Engle, & Nelson,\n1994; Taylor, 1987 ) where additional dependencies\nare permitted on lags of the conditional variance. AGARCH model has an ARMA-type representation, sothat the models share many properties.\nThe GARCH family, and many of its extensions,\nare extensively surveyed in, e.g., Bollerslev, Chou,\nand Kroner (1992) ,Bera and Higgins (1993)\n, and\nDiebold and Lopez (1995). Not surprisingly many ofthe theoretical works have appeared in the economet-rics literature. On the other hand, it is interesting tonote that neither the IJF nor the JoF became an\nimportant forum for public ations on the relative\nforecasting performance of GARCH-type models or\nthe forecasting performance of various other volatility\nmodels in general. As can be seen below, very fewIJF/JoF papers have dealt with this topic.\nSabbatini and Linton (1998) showed that the\nsimple (linear) GARCH(1,1) model provides a goodparametrization for the daily returns on the Swissmarket index. However, the quality of the out-of-sample forecasts suggests that this result should beJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 455", "start_char_idx": 3451, "end_char_idx": 4680, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "143cf4f0-c7eb-49be-a710-be91f81a1e1d": {"__data__": {"id_": "143cf4f0-c7eb-49be-a710-be91f81a1e1d", "embedding": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d68f85a-81a3-402f-8583-ca18dc3c0682", "node_type": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "hash": "57d68a2398c3436191050aba730f5b91c9ab6ece58446a8ab1d530da98a9a165"}, "3": {"node_id": "39e93058-b7f5-4e53-a1eb-3e6597d660e7", "node_type": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "hash": "74ee2b7485a66f521cf45e847ebaad58ead970dd3154f4e7ea9ce922c7d2331c"}}, "hash": "642e85ad3eb2952f4da56f5318f8729e50388a95ffedeba9f9f822cc54bb4e8c", "text": "taken with caution. Franses and Ghijsels (1999)\nstressed that this feature can be due to neglectedadditive outliers (AO). They noted that GARCHmodels for AO-corrected returns result in improvedforecasts of stock market volatility. Brooks (1998)\nfinds no clear-cut winner when comparing one-step-ahead forecasts from standard (symmetric) GARCH-type models with those of various linear models and\nANNs. At the estimation level, Brooks, Burke, and\nPersand (2001) argued that standard econometric\nsoftware packages can produce widely varying results.Clearly, this may have some impact on the forecastingaccuracy of GARCH models. This observation is verymuch in the spirit of Newbold et al. (1994) , referenced\nin Section 3.2, for univariate ARMA models. OutsidetheIJF, multi-step-ahead prediction in ARMA models\nwith GARCH in mean effects was considered by\nKaranasos (2001) . His method can be employed in the\nderivation of multi-step predictions from more com-plicated models, including multivariate GARCH.\nUsing two daily exchange rates series, Galbraith\nand Kisinbay (2005) compared the forecast content\nfunctions both from the standard GARCH model andfrom a fractionally integrated GARCH (FIGARCH)\nmodel ( Baillie, Bollerslev, & Mikkelsen, 1996 ).\nForecasts of conditional variances appear to haveinformation content of approximately 30 trading days.Another conclusion is that forecasts by autoregressiveprojection on past realized volatilities provide betterresults than forecasts based on GARCH, estimated byquasi-maximum likelihood, and FIGARCH models.This seems to confirm the earlier results of Bollerslev\nand Wright (2001) , for example. One often heard\ncriticism of these models (FIGARCH and its general-izations) is that there is no economic rationale forfinancial forecast volatility having long memory. For amore fundamental point of criticism of the use oflong-memory models, we refer to Granger (2002) .\nEmpirically, returns and conditional variance of the\nnext period\u2019s returns are negatively correlated. That is,\nnegative (positive) returns are generally associated\nwith upward (downward) revisions of the conditionalvolatility. This phenomenon is often referred to asasymmetric volatility in the literature; see, e.g., Engle\nand Ng (1993) . It motivated researchers to develop\nvarious asymmetric GARCH-type models (includingregime-switching GARCH); see, e.g., Hentschel\n(1995) and Pagan (1996) for overviews. Awartaniand Corradi (2005) investigated the impact of\nasymmetries on the out-of-sample forecast ability ofdifferent GARCH models, at various horizons.\nBesides GARCH, many other models have been\nproposed for volatility-forecasting. Poon and Granger\n(2003) , in a landmark paper, provide an excellent and\ncarefully conducted survey of the research in this areain the last 20 years. They compared the volatility\nforecast findings in 93 published and working papers.\nImportant insights are provided on issues like forecastevaluation, the effect of data frequency on volatilityforecast accuracy, measurement of bactual volatility Q,\nthe confounding effect of extreme values, and manymore. The survey found that option-implied volatilityprovides more accurate forecasts than time seriesmodels. Among the time series models (44 studies),\nthere was no clear winner between the historical\nvolatility models (including random walk, historicalaverages, ARFIMA, and various forms of exponentialsmoothing) and GARCH-type models (includingARCH and its various extensions), but both classesof models outperform the stochastic volatility model;see also Poon and Granger (2005) for an update on\nthese findings.\nThe Poon and Granger survey paper contains many\nissues for further study. For example, asymmetricGARCH models came out relatively well in", "start_char_idx": 0, "end_char_idx": 3755, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "39e93058-b7f5-4e53-a1eb-3e6597d660e7": {"__data__": {"id_": "39e93058-b7f5-4e53-a1eb-3e6597d660e7", "embedding": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2d68f85a-81a3-402f-8583-ca18dc3c0682", "node_type": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "hash": "57d68a2398c3436191050aba730f5b91c9ab6ece58446a8ab1d530da98a9a165"}, "2": {"node_id": "143cf4f0-c7eb-49be-a710-be91f81a1e1d", "node_type": null, "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}, "hash": "642e85ad3eb2952f4da56f5318f8729e50388a95ffedeba9f9f822cc54bb4e8c"}}, "hash": "74ee2b7485a66f521cf45e847ebaad58ead970dd3154f4e7ea9ce922c7d2331c", "text": "further study. For example, asymmetricGARCH models came out relatively well in theforecast contest. However, it is unclear to what extentthis is due to asymmetries in the conditional mean,asymmetries in the conditional variance, and/or asym-metries in high order conditional moments. Anotherissue for future research concerns the combination of\nforecasts. The results in two studies ( Doidge & Wei,\n1998; Kroner, Kneafsey, & Claessens, 1995 ) find\ncombining to be helpful, but another study ( Vasilellis\n& Meade, 1996 ) does not. It would also be useful to\nexamine the volatility-forecasting performance ofmultivariate GARCH-type models and multivariatenonlinear models, incorporating both temporal andcontemporaneous dependencies; see also Engle (2002)\nfor some further possible areas of new research.\n9. Count data forecasting\nCount data occur frequently in business and\nindustry, especially in inventory data where they areoften called bintermittent demand data Q. Consequent-J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 456", "start_char_idx": 3677, "end_char_idx": 4746, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c7eab4f-0ec0-4119-bb74-5b89e864f29a": {"__data__": {"id_": "4c7eab4f-0ec0-4119-bb74-5b89e864f29a", "embedding": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "043f98c2-687a-451a-a86b-e0bd2441a38b", "node_type": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "hash": "f4c1215515138da8413f5723a7fe87a6ad171663266006ab5cfe32a88051d1b2"}, "3": {"node_id": "887581e0-a6dd-40b2-83d2-925e6a6dbec5", "node_type": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "hash": "fa31adaf6a9a77331f4c7f73a71aeda9c5fb1e90bc8655cc768528633a4340fe"}}, "hash": "1287f4cdffecedadd77cc7439976439c965aaa01b0687405419e220f11dab40a", "text": "ly, it is surprising that so little work has been done on\nforecasting count data. Some work has been done onad hoc methods for forecasting count data, but fewpapers have appeared on forecasting count time seriesusing stochastic models.\nMost work on count forecasting is based on Croston\n(1972) who proposed using SES to independently\nforecast the non-zero values of a series and the time\nbetween non-zero values. Willemain, Smart, Shockor,\nand DeSautels (1994) compared Croston\u2019s method to\nSES and found that Croston\u2019s method was morerobust, although these results were based on MAPEswhich are often undefined for count data. Theconditions under which Croston\u2019s method does betterthan SES were discussed in Johnston and Boylan\n(1996) .Willemain, Smart, and Schwarz (2004) pro-\nposed a bootstrap procedure for intermittent demand\ndata which was found to be more accurate than eitherSES or Croston\u2019s method on the nine series evaluated.\nEvaluating count forecasts raises difficulties due to\nthe presence of zeros in the observed data. Syntetos\nand Boylan (2005) proposed using the relative mean\nabsolute error (see Section 10), while Willemain et al.\n(2004) recommended using the probability integral\ntransform method of Diebold, Gunther, and Tay\n(1998) .\nGrunwald, Hyndman, Tedesco, and Tweedie\n(2000) surveyed many of the stochastic models for\ncount time series, using simple first-order autoregres-sion as a unifying framework for the variousapproaches. One possible model, explored by Bra\u00a8nna\u00a8s\n(1995) , assumes the series follows a Poisson distri-\nbution with a mean that depends on an unobserved\nand autocorrelated process. An alternative integer-valued MA model was used by Bra\u00a8nna\u00a8s, Hellstro \u00a8m,\nand Nordstro \u00a8m (2002) to forecast occupancy levels in\nSwedish hotels.\nThe forecast distribution can be obtained by\nsimulation using any of these stochastic models, buthow to summarize the distribution is not obvious.\nFreeland and McCabe (2004) proposed using the\nmedian of the forecast distribution, and gave a methodfor computing confidence intervals for the entireforecast distribution in the case of integer-valuedautoregressive (INAR) models of order 1. McCabe\nand Martin (2005) further extended these ideas by\npresenting a Bayesian methodology for forecastingfrom the INAR class of models.A great deal of research on count time series has\nalso been done in the biostatistical area (see, forexample, Diggle, Heagerty, Liang, & Zeger, 2002 ).\nHowever, this usually concentrates on the analysis ofhistorical data with adjustment for autocorrelatederrors, rather than using the models for forecasting.Nevertheless, anyone working in count forecastingought to be abreast of research developments in the\nbiostatistical area also.\n10. Forecast evaluation and accuracy measures\nA bewildering array of accuracy measures have\nbeen used to evaluate the performance of forecastingmethods. Some of them are listed in the early survey\npaper of Mahmoud (1984) . We first define the most\ncommon measures.\nLetY\ntdenote the observation at time tand Ft\ndenote the forecast of Yt. Then define the forecast\nerror as et=Yt/C0Ftand the percentage error as\npt=100 et/Yt. An alternative way of scaling is to\ndivide each error, by the error obtained with anotherstandard method of forecasting. Let r\nt=et/et* denote\nthe relative error, where et* is the forecast error\nobtained from the base method. Usually, the basemethod is the bna\u0131\u00a8ve method Qwhere F\ntis equal to the\nlast observation. We use the notation mean( xt)t o\ndenote the sample mean of { xt} over the period of\ninterest (or over the series of", "start_char_idx": 0, "end_char_idx": 3587, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "887581e0-a6dd-40b2-83d2-925e6a6dbec5": {"__data__": {"id_": "887581e0-a6dd-40b2-83d2-925e6a6dbec5", "embedding": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "043f98c2-687a-451a-a86b-e0bd2441a38b", "node_type": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "hash": "f4c1215515138da8413f5723a7fe87a6ad171663266006ab5cfe32a88051d1b2"}, "2": {"node_id": "4c7eab4f-0ec0-4119-bb74-5b89e864f29a", "node_type": null, "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}, "hash": "1287f4cdffecedadd77cc7439976439c965aaa01b0687405419e220f11dab40a"}}, "hash": "fa31adaf6a9a77331f4c7f73a71aeda9c5fb1e90bc8655cc768528633a4340fe", "text": "the sample mean of { xt} over the period of\ninterest (or over the series of interest). Analogously,we use median( x\nt) for the sample median and\ngmean( xt) for the geometric mean. The most com-\nmonly used methods are defined in Table 2 on the\nfollowing page, where the subscript b refers tomeasures obtained from the base method.\nNote that Armstrong and Collopy (1992) referred\nto RelMAE as CumRAE and that RelRMSE is alsoknown as Theil\u2019s Ustatistic ( Theil, 1966 , Chapter 2),\nand is sometimes called U2. In addition to these, the\naverage ranking (AR) of a method relative to all other\nmethods considered has sometimes been used.\nThe evolution of measures of forecast accuracy and\nevaluation can be seen through the measures used toevaluate methods in the major comparative studies thathave been undertaken. In the original M-competition(Makridakis et al., 1982 ), measures used included the\nMAPE, MSE, AR, MdAPE, and PB. However, asChatfield (1988) andArmstrong and Collopy (1992)J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 457", "start_char_idx": 3512, "end_char_idx": 4584, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6574937f-9c44-4d1f-b74e-4e3868986dcc": {"__data__": {"id_": "6574937f-9c44-4d1f-b74e-4e3868986dcc", "embedding": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87c3a2-9cf9-4a78-9273-db6fabe1644a", "node_type": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "hash": "af5ef4556ef45b9de64ad7e7de69ff76ff7f7e251f4294f491d3705401525c3b"}, "3": {"node_id": "23744b20-264f-497a-a3ba-8e5a742c7575", "node_type": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "hash": "9f6cc9b59df00af51fc261eff22067bcd326deaa8c2ea4a048abce82162af7aa"}}, "hash": "a5955c4f537a8969a6f4ad2b7528af707f75b30896512581e2e5694e07d487a0", "text": "pointed out, the MSE is not appropriate for compar-\nisons between series as it is scale dependent. Fildes and\nMakridakis (1988) contained further discussion on this\npoint. The MAPE also has problems when the serieshas values close to (or equal to) zero, as noted by\nMakridakis, Wheelwright, and Hyndman (1998, p.45) .\nExcessively large (or infinite) MAPEs were avoided inthe M-competitions by only including data that werepositive. However, this is an artificial solution that isimpossible to apply in all situations.\nIn 1992, one issue of IJFcarried two articles and\nseveral commentaries on forecast evaluation meas-ures. Armstrong and Collopy (1992) recommended\nthe use of relative absolute errors, especially the\nGMRAE and MdRAE, despite the fact that relativeerrors have infinite variance and undefined mean.They recommended bwinsorizing Qto trim extreme\nvalues which partially overcomes these problems, butwhich adds some complexity to the calculation and alevel of arbitrariness as the amount of trimming mustbe specified. Fildes (1992) also preferred the GMRAE\nalthough he expressed it in an equivalent form as the\nsquare root of the geometric mean of squared relativeerrors. This equivalence does not seem to have beennoticed by any of the discussants in the commentariesofAhlburg et al. (1992) .\nThe study of Fildes, Hibon, Makridakis, and\nMeade (1998) , which looked at forecasting tele-\ncommunications data, used MAPE, MdAPE, PB,AR, GMRAE, and MdRAE, taking into account some\nof the criticism of the methods used for the M-competition.\nThe M3-competition ( Makridakis & Hibon, 2000 )\nused three different measures of accuracy: MdRAE,\nsMAPE, and sMdAPE. The bsymmetric Qmeasures\nwere proposed by Makridakis (1993) in response to\nthe observation that the MAPE and MdAPE have thedisadvantage that they put a heavier penalty onpositive errors than on negative errors. However,these measures are not as bsymmetric Qas their name\nsuggests. For the same value of Y\nt, the value of\n2|Yt/C0Ft|/(Yt+Ft) has a heavier penalty when fore-\ncasts are high compared to when forecasts are low.\nSeeGoodwin and Lawton (1999) andKoehler (2001)\nfor further discussion on this point.\nNotably, none of the major comparative studies\nhave used relative measures (as distinct from meas-ures using relative errors) such as RelMAE or LMR.The latter was proposed by Thompson (1990) who\nargued for its use based on its good statistical\nproperties. It was applied to the M-competition data\ninThompson (1991) .\nApart from Thompson (1990) , there has been very\nlittle theoretical work on the statistical properties ofthese measures. One exception is Wun and Pearn\n(1991) who looked at the statistical properties of MAE.\nA novel alternative measure of accuracy is btime\ndistance Q, which was considered by Granger and JeonTable 2\nCommonly used forecast accuracy measures\nMSE Mean squared error =mean( et2)\nRMSE Root mean squared error =\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\ufb03\nMSEp\nMAE Mean Absolute error =mean(| et|)\nMdAE Median absolute error =median(| et|)\nMAPE Mean absolute percentage error =mean(| pt|)\nMdAPE Median absolute percentage error =median(| pt|)\nsMAPE Symmetric mean absolute percentage error =mean(2| Yt/C0Ft|/(Yt+Ft))\nsMdAPE Symmetric median absolute percentage error =median(2|", "start_char_idx": 0, "end_char_idx": 3245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "23744b20-264f-497a-a3ba-8e5a742c7575": {"__data__": {"id_": "23744b20-264f-497a-a3ba-8e5a742c7575", "embedding": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf87c3a2-9cf9-4a78-9273-db6fabe1644a", "node_type": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "hash": "af5ef4556ef45b9de64ad7e7de69ff76ff7f7e251f4294f491d3705401525c3b"}, "2": {"node_id": "6574937f-9c44-4d1f-b74e-4e3868986dcc", "node_type": null, "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}, "hash": "a5955c4f537a8969a6f4ad2b7528af707f75b30896512581e2e5694e07d487a0"}}, "hash": "9f6cc9b59df00af51fc261eff22067bcd326deaa8c2ea4a048abce82162af7aa", "text": "Symmetric median absolute percentage error =median(2| Yt/C0Ft|/(Yt+Ft))\nMRAE Mean relative absolute error =mean(| rt|)\nMdRAE Median relative absolute error =median(| rt|)\nGMRAE Geometric mean relative absolute error =gmean(| rt|)\nRelMAE Relative mean absolute error =MAE/MAE b\nRelRMSE Relative root mean squared error =RMSE/RMSE b\nLMR Log mean squared error ratio =log(RelMSE)\nPB Percentage better =100 mean( I{|rt|b1})\nPB(MAE) Percentage better (MAE) =100 mean( I{MAE bMAE b})\nPB(MSE) Percentage better (MSE) =100 mean( I{MSE bMSE b})\nHere I{u}=1 if u is true and 0 otherwise.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 458", "start_char_idx": 3192, "end_char_idx": 3859, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4b69d50-a8b9-4b68-8d7d-86c3cf36c00f": {"__data__": {"id_": "a4b69d50-a8b9-4b68-8d7d-86c3cf36c00f", "embedding": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db728600-99fc-4a82-ad52-11fd6b0d956d", "node_type": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "hash": "05c5a701153f616e16bb93fc2b4a1177f4f43c0d4d91fc6e86d85feea420f6d8"}, "3": {"node_id": "31fdc1af-033d-4afd-9780-4c99bfc13301", "node_type": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "hash": "f4aa29309edf656bed2daebb461b864533ef18525b31104b7b5bc842681c4de4"}}, "hash": "73457974f44f278e53e3d5c01cd2975d95dfcd956751e30c4a701152f4e810c3", "text": "(2003a, 2003b) . In this measure, the leading and\nlagging properties of a forecast are also captured.Again, this measure has not been used in any majorcomparative study.\nA parallel line of research has looked at statistical\ntests to compare foreca sting methods. An early\ncontribution was Flores (1989) . The best known\napproach to testing differences between the accuracy\nof forecast methods is the Diebold and Mariano\n(1995) test. A size-corrected modification of this test\nwas proposed by Harvey, Leybourne, and Newbold\n(1997) .McCracken (2004) looked at the effect of\nparameter estimation on such tests and provided a newmethod for adjusting for parameter estimation error.\nAnother problem in forecast evaluation, and more\nserious than parameter estimation error, is bdata\nsharing Q\u2014the use of the same data for many different\nforecasting methods. Sullivan, Timmermann, and\nWhite (2003) proposed a bootstrap procedure\ndesigned to overcome the resulting distortion ofstatistical inference.\nAn independent line of research has looked at the\ntheoretical forecasting properties of time series mod-els. An important contribution along these lines was\nClements and Hendry (1993) who showed that the\ntheoretical MSE of a forecasting model was notinvariant to scale-preserving linear transformationssuch as differencing of the data. Instead, theyproposed the bgeneralized forecast error second\nmoment Q(GFESM) criterion, which does not have\nthis undesirable property. However, such measures aredifficult to apply empirically and the idea does not\nappear to be widely used.\n11. Combining\nCombining forecasts, mixing, or pooling quan-\ntitative\n4forecasts obtained from very different time\nseries methods and different sources of informa-\ntion has been studied for the past three decades.\nImportant early contributions in this area weremade by Bates and Granger (1969) ,Newbold and\nGranger (1974) ,a n d Winkler and Makridakis(1983) . Compelling evidence on the relative effi-\nciency of combined forecasts, usually defined interms of forecast error variances, was summarizedbyClemen (1989) in a comprehensive bibliography\nreview.\nNumerous methods for selecting the combining\nweights have been proposed. The simple average isthe most widely used combining method (see Clem-\nen\u2019s review and Bunn, 1985 ), but the method does not\nutilize past information regarding the precision of theforecasts or the dependence among the forecasts.Another simple method is a linear mixture of theindividual forecasts with combining weights deter-mined by OLS (assuming unbiasedness) from thematrix of past forecasts and the vector of pastobservations ( Granger & Ramanathan, 1984 ). How-\never, the OLS estimates of the weights are inefficient\ndue to the possible presence of serial correlation in thecombined forecast errors. Aksu and Gunter (1992)\nandGunter (1992) investigated this problem in some\ndetail. They recommended the use of OLS combina-tion forecasts with the weights restricted to sum tounity. Granger (1989) provided several extensions of\nthe original idea of Bates and Granger (1969) ,\nincluding combining forecasts with horizons longer\nthan one period.\nRather than using fixed weights, Deutsch, Granger,\nand Tera \u00a8svirta (1994) allowed them to change through\ntime using regime-switching models and STARmodels. Another time-dependent weighting schemewas proposed by Fiordaliso (1998) , who used a fuzzy\nsystem to combine a set of individual forecasts in a\nnonlinear way. Diebold and Pauly (1990) used\nBayesian shrinkage techniques to allow the incorpo-ration of prior information into the estimation ofcombining weights. Combining forecasts from verysimilar models, with weights sequentially updated,was considered by Zou and Yang (2004) .\nCombining weights determined from", "start_char_idx": 0, "end_char_idx": 3763, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "31fdc1af-033d-4afd-9780-4c99bfc13301": {"__data__": {"id_": "31fdc1af-033d-4afd-9780-4c99bfc13301", "embedding": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db728600-99fc-4a82-ad52-11fd6b0d956d", "node_type": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "hash": "05c5a701153f616e16bb93fc2b4a1177f4f43c0d4d91fc6e86d85feea420f6d8"}, "2": {"node_id": "a4b69d50-a8b9-4b68-8d7d-86c3cf36c00f", "node_type": null, "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}, "hash": "73457974f44f278e53e3d5c01cd2975d95dfcd956751e30c4a701152f4e810c3"}}, "hash": "f4aa29309edf656bed2daebb461b864533ef18525b31104b7b5bc842681c4de4", "text": "considered by Zou and Yang (2004) .\nCombining weights determined from time-invari-\nant methods can lead to relatively poor forecasts if\nnonstationarity occurs among component forecasts.\nMiller, Clemen, and Winkler (1992) examined the\neffect of dlocation-shift Tnonstationarity on a range of\nforecast combination methods. Tentatively, they con-cluded that the simple average beats more complexcombination devices; see also Hendry and Clements\n(2002) for more recent results. The related topic of\ncombining forecasts from linear and some nonlinear\n4See Kamstra and Kennedy (1998) for a computationally\nconvenient method of combining qualitative forecasts.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 459", "start_char_idx": 3694, "end_char_idx": 4437, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "453e0062-d979-4ce4-a620-4e5941f9cacc": {"__data__": {"id_": "453e0062-d979-4ce4-a620-4e5941f9cacc", "embedding": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb", "node_type": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "hash": "09af099d41af035c05ff3c05a210c4d1dca69b2e7eedd9f8728eb1d35645de1c"}, "3": {"node_id": "33d67853-e6c9-4f14-a6ba-c4b59ffcf35c", "node_type": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "hash": "e3baaf949a7b6668573cb8b00a0756f4a9e4b84c175b732844059cd278b30716"}}, "hash": "dd438440fbcae60476d2ef1b1fbb128ad62af23851873048ef92c4a7781a0dcf", "text": "time series models, with OLS weights as well as\nweights determined by a time-varying method, wasaddressed by Terui and van Dijk (2002) .\nThe shape of the combined forecast error distribu-\ntion and the corresponding stochastic behaviour wasstudied by de Menezes and Bunn (1998) andTaylor\nand Bunn (1999) . For non-normal forecast error\ndistributions skewness emerges as a relevant criterion\nfor specifying the method of combination. Some\ninsights into why competing forecasts may befruitfully combined to produce a forecast superior toindividual forecasts were provided by Fang (2003) ,\nusing forecast encompassing tests. Hibon and Evge-\nniou (2005) proposed a criterion to select among\nforecasts and their combinations.\n12. Prediction intervals and densities\nThe use of prediction intervals, and more recently\nprediction densities, has become much more commonover the past 25 years as practitioners have come tounderstand the limitations of point forecasts. Animportant and thorough review of interval forecasts\nis given by Chatfield (1993) , summarizing the\nliterature to that time.\nUnfortunately, there is still some confusion in\nterminology with many authors using bconfidence\ninterval Qinstead of bprediction interval Q. A confidence\ninterval is for a model parameter, whereas a predictioninterval is for a random variable. Almost always,forecasters will want prediction intervals\u2014intervals\nwhich contain the true values of future observations\nwith specified probability.\nMost prediction intervals are based on an underlying\nstochastic model. Consequently, there has been a largeamount of work done on formulating appropriatestochastic models underlying some common forecast-ing procedures (see, e.g., Section 2 on exponentialsmoothing).\nThe link between prediction interval formulae and\nthe model from which they are derived has not alwaysbeen correctly observed. For example, the predictioninterval appropriate for a random walk model wasapplied by Makridakis and Hibon (1987) andLefran-\nc\u00b8ois (1989) to forecasts obtained from many other\nmethods. This problem was noted by Koehler (1990)\nandChatfield and Koehler (1991) .With most model-based prediction intervals for\ntime series, the uncertainty associated with modelselection and parameter estimation is not accountedfor. Consequently, the intervals are too narrow. Therehas been considerable research on how to makemodel-based prediction intervals have more realisticcoverage. A series of papers on using the bootstrap tocompute prediction intervals for an AR model has\nappeared beginning with Masarotto (1990) ,a n d\nincluding McCullough (1994, 1996) ,Grigoletto\n(1998) ,Clements and Taylor (2001) ,a n d Kim\n(2004b) . Similar procedures for other models have\nalso been considered including ARIMA models(Pascual, Romo, & Ruiz, 2001, 2004, 2005; Wall &\nStoffer, 2002 ), VAR ( Kim, 1999, 2004a ), ARCH\n(Reeves, 2005 ), and regression ( Lam & Veall, 2002 ).\nIt seems likely that such bootstrap methods will\nbecome more widely used as computing speedsincrease due to their better coverage properties.\nWhen the forecast error distribution is non-\nnormal, finding the entire forecast density is usefulas a single interval may no longer provide anadequate summary of the expected future. A reviewof density forecasting is provided by Tay and Wallis\n(2000) , along with several other articles in the same\nspecial issue of the JoF. Summarizing, a density\nforecast has been the subject of some interestingproposals including bfan charts Q(Wallis, 1999 ) and\nbhighest density regions Q(Hyndman, 1995 ). The use\nof these graphical summaries has grown rapidly inrecent years as density forecasts have becomerelatively widely used.\nAs prediction intervals and forecast densities have\nbecome more commonly used,", "start_char_idx": 0, "end_char_idx": 3757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "33d67853-e6c9-4f14-a6ba-c4b59ffcf35c": {"__data__": {"id_": "33d67853-e6c9-4f14-a6ba-c4b59ffcf35c", "embedding": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb", "node_type": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "hash": "09af099d41af035c05ff3c05a210c4d1dca69b2e7eedd9f8728eb1d35645de1c"}, "2": {"node_id": "453e0062-d979-4ce4-a620-4e5941f9cacc", "node_type": null, "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}, "hash": "dd438440fbcae60476d2ef1b1fbb128ad62af23851873048ef92c4a7781a0dcf"}}, "hash": "e3baaf949a7b6668573cb8b00a0756f4a9e4b84c175b732844059cd278b30716", "text": "prediction intervals and forecast densities have\nbecome more commonly used, attention has turned totheir evaluation and testing. Diebold, Gunther, and\nTay (1998) introduced the remarkably simple\nbprobability integral transform Qmethod, which can\nbe used to evaluate a univariate density. This approachhas become widely used in a very short period of timeand has been a key research advance in this area. The\nidea is extended to multivariate forecast densities in\nDiebold, Hahn, and Tay (1999) .\nOther approaches to interval and density evaluation\nare given by Wallis (2003) who proposed chi-squared\ntests for both intervals and densities, and Clements\nand Smith (2002) who discussed some simple but\npowerful tests when evaluating multivariate forecastdensities.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 460", "start_char_idx": 3682, "end_char_idx": 4533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "458cc8fe-f9fd-4eda-84ae-0f82e544263c": {"__data__": {"id_": "458cc8fe-f9fd-4eda-84ae-0f82e544263c", "embedding": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1f246f7-342b-4855-be98-1780f90f25b3", "node_type": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "hash": "ba4a1b2974f64e993c14713306179fd5d5aa04d4cb9210c0c5d1c9682fb12e8e"}, "3": {"node_id": "4db2653f-f003-4856-a2a6-2d5da7d7841b", "node_type": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "hash": "dc2d50943d8d39377bd14e97010e07cfa8cd2a806ae8ff90b1539eb2adce542f"}}, "hash": "fd7ab3e17f841fc9806ca5a7e5aac0ee9896cb13f417cba82685dbf1dd7df4dd", "text": "13. A look to the future\nIn the preceding sections, we have looked back at\nthe time series forecasting history of the IJF, in the\nhope that the past may shed light on the present. Buta silver anniversary is also a good time to lookahead. In doing so, it is interesting to reflect on theproposals for research in time series forecasting\nidentified in a set of related papers by Ord, Cogger,\nand Chatfield published in this Journal more than 15years ago.\n5\nChatfield (1988) stressed the need for future\nresearch on developing multivariate methods with anemphasis on making them more of a practicalproposition. Ord (1988) also noted that not much\nwork had been done on multiple time series models,\nincluding multivariate exponential smoothing. Eigh-\nteen years later, multivariate time series forecasting isstill not widely applied despite considerable theoret-ical advances in this area. We suspect that two reasonsfor this are: a lack of empirical research on robustforecasting algorithms for multivariate models, and alack of software that is easy to use. Some of themethods that have been suggested (e.g., VARIMA\nmodels) are difficult to estimate because of the large\nnumbers of parameters involved. Others, such asmultivariate exponential smoothing, have not receivedsufficient theoretical attention to be ready for routineapplication. One approach to multivariate time seriesforecasting is to use dynamic factor models. Thesehave recently shown promise in theory ( Forni, Hallin,\nLippi, & Reichlin, 2005; Stock & Watson, 2002 ) and\napplication (e.g., Pen\u02dca & Poncela, 2004 ), and we\nsuspect they will become much more widely used inthe years ahead.\nOrd (1988) also indicated the need for deeper\nresearch in forecasting methods based on nonlinearmodels. While many aspects of nonlinear models havebeen investigated in the IJF, they merit continued\nresearch. For instance, there is still no clear consensus\nthat forecasts from nonlinear models substantivelyoutperform those from linear models (see, e.g., Stock\n& Watson, 1999 ).\nOther topics suggested by Ord (1988) include the\nneed to develop model selection procedures that makeeffective use of both data and prior knowledge, andthe need to specify objectives for forecasts anddevelop forecasting systems that address those objec-tives. These areas are still in need of attention and we\nbelieve that future research will contribute tools to\nsolve these problems.\nGiven the frequent misuse of methods based on\nlinear models with Gaussian i.i.d. distributed errors,Cogger (1988) argued that new developments in the\narea of drobust Tstatistical methods should receive\nmore attention within the time series forecastingcommunity. A robust procedure is expected to work\nwell when there are outliers or location shifts in the\ndata that are hard to detect. Robust statistics can bebased on both parametric and nonparametric methods.An example of the latter is the Koenker and Bassett\n(1978) concept of regression quantiles investigated by\nCogger. In forecasting, these can be applied asunivariate and multivariate conditional quantiles.One important area of application is in estimating\nrisk management tools such as value-at-risk. Recently,\nEngle and Manganelli (2004) made a start in this\ndirection, proposing a conditional value at risk model.We expect to see much future research in this area.\nA related topic in which there has been a great deal\nof recent research activity is density forecasting (seeSection 12), where the focus is on the probabilitydensity of future observations rather than the mean or\nvariance. For instance, Yao and Tong (1995) proposed\nthe concept of the conditional percentile predictioninterval. Its width is no longer a constant, as in thecase of linear models, but may vary with respect to theposition in the state space from which forecasts arebeing made; see also De Gooijer and Gannoun (2000)\nandPolonik and Yao (2000) .\nClearly,", "start_char_idx": 0, "end_char_idx": 3909, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4db2653f-f003-4856-a2a6-2d5da7d7841b": {"__data__": {"id_": "4db2653f-f003-4856-a2a6-2d5da7d7841b", "embedding": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e1f246f7-342b-4855-be98-1780f90f25b3", "node_type": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "hash": "ba4a1b2974f64e993c14713306179fd5d5aa04d4cb9210c0c5d1c9682fb12e8e"}, "2": {"node_id": "458cc8fe-f9fd-4eda-84ae-0f82e544263c", "node_type": null, "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}, "hash": "fd7ab3e17f841fc9806ca5a7e5aac0ee9896cb13f417cba82685dbf1dd7df4dd"}}, "hash": "dc2d50943d8d39377bd14e97010e07cfa8cd2a806ae8ff90b1539eb2adce542f", "text": "(2000)\nandPolonik and Yao (2000) .\nClearly, the area of improved forecast intervals\nrequires further research. This is in agreement with\nArmstrong (2001) who listed 23 principles in great\nneed of research including item 14:13: bFor prediction\nintervals, incorporate the uncertainty associated withthe prediction of the explanatory variables Q.\nIn recent years, non-Gaussian time series have\nbegun to receive considerable attention and forecast-ing methods are slowly being developed. One\n5Outside the IJF, good reviews on the past and future of time\nseries methods are given by Dekimpe and Hanssens (2000) in\nmarketing and by Tsay (2000) in statistics. Casella et al. (2000)\ndiscussed a large number of potential research topics in the theoryand methods of statistics. We daresay that some of these topics willattract the interest of time series forecasters.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 461", "start_char_idx": 3866, "end_char_idx": 4814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ff8bb785-dd23-40b2-8148-ff05dc6b5d21": {"__data__": {"id_": "ff8bb785-dd23-40b2-8148-ff05dc6b5d21", "embedding": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61be5a83-3321-486a-97d3-5d1c8c693a2d", "node_type": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "hash": "cf8a133a01e114dee45d0f8735cc091fd55bccc08a51bc92552a95459c264f11"}, "3": {"node_id": "6576f2fb-0b95-4d45-8b56-ead8019e2a92", "node_type": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "hash": "5fa4b3eb4b5bf247cfde6886212c068b5a7fbd020a53ab6bc9644a6eef7020fc"}}, "hash": "9973e4633901639b826bbdf6d1f449ff30f214400330d4b1967df7a48c20659f", "text": "particular area of non-Gaussian time series that has\nimportant applications is time series taking positivevalues only. Two important areas in finance in whichthese arise are realized volatility and the durationbetween transactions. Important contributions to datehave been Engle and Russell\u2019s (1998) bautoregressive\nconditional duration Qmodel and Andersen, Bollerslev,\nDiebold, and Labys (2003) . Because of the impor-\ntance of these applications, we expect much more\nwork in this area in the next few years.\nWhile forecasting non-Gaussian time series with a\ncontinuous sample space has begun to receiveresearch attention, especially in the context offinance, forecasting time series with a discretesample space (such as time series of counts) is stillin its infancy (see Section 9). Such data are very\nprevalent in business and industry, and there are many\nunresolved theoretical and practical problems associ-ated with count forecasting; therefore, we also expectmuch productive research in this area in the nearfuture.\nIn the past 15 years, some IJFauthors have tried\nto identify new important research topics. Both De\nGooijer (1990) and Clements (2003) in two\neditorials, and Ord as a part of a discussion paper\nbyDawes, Fildes, Lawrence, and Ord (1994) ,\nsuggested more work on combining forecasts.Although the topic has received a fair amount ofattention (see Section 11), there are still several openquestions. For instance, what is the bbest Qcombining\nmethod for linear and nonlinear models and whatprediction interval can be put around the combined\nforecast? A good starting point for further research in\nthis area is Tera\u00a8svirta (2006) ; see also Armstrong\n(2001, items 12.5\u201312.7) . Recently, Stock and Watson\n(2004) discussed the dforecast combination puzzle T,\nnamely the repeated empirical finding that simplecombinations such as averages outperform moresophisticated combinations which theory suggestsshould do better. This is an important practical issue\nthat will no doubt receive further research attention in\nthe future.\nChanges in data collection and storage will also\nlead to new research directions. For example, in thepast, panel data (called longitudinal data in biostatis-tics) have usually been available where the time seriesdimension thas been small whilst the cross-section\ndimension nis large. However, nowadays in manyapplied areas such as marketing, large datasets can be\neasily collected with nand tboth being large.\nExtracting features from megapanels of panel data isthe subject of bfunctional data analysis Q; see, e.g.,\nRamsay and Silverman (1997) . Yet, the problem of\nmaking multi-step-ahead forecasts based on functionaldata is still open for both theoretical and appliedresearch. Because of the increasing prevalence of this\nkind of data, we expect this to be a fruitful future\nresearch area.\nLarge datasets also lend themselves to highly\ncomputationally intensive methods. While neuralnetworks have been used in forecasting for more thana decade now, there are many outstanding issuesassociated with their use and implementation, includ-ing when they are likely to outperform other methods.\nOther methods involving heavy computation (e.g.,\nbagging and boosting) are even less understood in theforecasting context. With the availability of very largedatasets and high powered computers, we expect thisto be an important area of research in the comingyears.\nLooking back, the field of time series forecasting is\nvastly different from what it was 25 years ago when\nthe IIF was formed. It has grown up with the advent of\ngreater computing power, better statistical models,and more mature approaches to forecast calculationand evaluation. But there is much to be done, withmany problems still unsolved and many new prob-lems arising.\nWhen the IIF celebrates its Golden Anniversary\nin 25 years Ttime, we hope there will be another\nreview paper summarizing the main developments", "start_char_idx": 0, "end_char_idx": 3914, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6576f2fb-0b95-4d45-8b56-ead8019e2a92": {"__data__": {"id_": "6576f2fb-0b95-4d45-8b56-ead8019e2a92", "embedding": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "61be5a83-3321-486a-97d3-5d1c8c693a2d", "node_type": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "hash": "cf8a133a01e114dee45d0f8735cc091fd55bccc08a51bc92552a95459c264f11"}, "2": {"node_id": "ff8bb785-dd23-40b2-8148-ff05dc6b5d21", "node_type": null, "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}, "hash": "9973e4633901639b826bbdf6d1f449ff30f214400330d4b1967df7a48c20659f"}}, "hash": "5fa4b3eb4b5bf247cfde6886212c068b5a7fbd020a53ab6bc9644a6eef7020fc", "text": "Ttime, we hope there will be another\nreview paper summarizing the main developments in\ntime series forecasting. Besides the topics mentionedabove, we also predict that such a review will shedmore light on Armstrong\u2019s 23 open research prob-lems for forecasters. In this sense, it is interesting tomention David Hilbert who, in his 1900 address tothe Paris International Congress of Mathematicians,listed 23 challenging problems for mathematicians of\nthe 20th century to work on. Many of Hilbert\u2019s\nproblems have resulted in an explosion of researchstemming from the confluence of several areas ofmathematics and physics. We hope that the ideas,problems, and observations presented in this reviewprovide a similar research impetus for those workingin different areas of time series analysis andforecasting.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 462", "start_char_idx": 3831, "end_char_idx": 4724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "454c1031-4ca1-4019-8e83-195c85ad9edf": {"__data__": {"id_": "454c1031-4ca1-4019-8e83-195c85ad9edf", "embedding": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24232b86-b4df-4a0c-ae88-6db8fd096838", "node_type": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "hash": "6f21f504ff4ee3af67ebcfa3667de0f08fc5cc48b8c5df101bbe9d9c8f34634e"}, "3": {"node_id": "ebf5b508-8483-455d-a697-c2f7a81eed04", "node_type": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "hash": "d60a0485458b25b5e38464392c1972443f7a45d9949c80ecdec144e23204b086"}}, "hash": "572f747816d299798676e4ce745f6bf339b70117146b7800ea12cfa8cccd948c", "text": "Acknowledgments\nWe are grateful to Robert Fildes and Andrey\nKostenko for valuable comments. We also thank twoanonymous referees and the editor for many helpfulcomments and suggestions that resulted in a substan-tial improvement of this manuscript.\nReferences\nSection 2. Exponential smoothing\nAbraham, B., & Ledolter, J. (1983). Statistical methods for\nforecasting . New York 7John Wiley and Sons.\nAbraham, B., & Ledolter, J. (1986). Forecast functions implied by\nautoregressive integrated moving average models and other\nrelated forecast procedures. International Statistical Review ,54,\n51\u201366.\nArchibald, B. C. (1990). Parameter space of the Holt\u2013Winters\nmodel. International Journal of Forecasting ,6, 199\u2013209.\nArchibald, B. C., & Koehler, A. B. (2003). Normalization of\nseasonal factors in Winters methods. International Journal of\nForecasting ,19, 143\u2013148.\nAssimakopoulos, V., & Nikolopoulos, K. (2000). The theta model:\nA decomposition approach to forecasting. International Journal\nof Forecasting ,16, 521\u2013530.\nBartolomei, S. M., & Sweet, A. L. (1989). A note on a comparison\nof exponential smoothing methods for forecasting seasonalseries. International Journal of Forecasting ,5, 111 \u2013 116.\nBox, G. E. P., & Jenkins, G. M. (1970). Time series analysis:\nForecasting and control . San Francisco 7Holden Day (revised\ned. 1976).\nBrown, R. G. (1959). Statistical forecasting for inventory control .\nNew York 7McGraw-Hill.\nBrown, R. G. (1963). Smoothing, forecasting and prediction of\ndiscrete time series . Englewood Cliffs, NJ 7Prentice-Hall.\nCarreno, J., & Madinaveitia, J. (1990). A modification of time series\nforecasting methods for handling announced price increases.\nInternational Journal of Forecasting ,6, 479\u2013484.\nChatfield, C., & Yar, M. (1991). Prediction intervals for multipli-\ncative Holt\u2013Winters. International Journal of Forecasting ,7,\n31\u201337.\nChatfield, C., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2001). A\nnew look at models for exponential smoothing. The Statistician ,\n50, 147\u2013159.\nCollopy, F., & Armstrong, J. S. (1992). Rule-based forecasting:\nDevelopment and validation of an expert systems approach tocombining time series extrapolations. Management Science ,38,\n1394\u20131414.\nGardner Jr., E. S. (1985). Exponential smoothing: The state of the\nart.Journal of Forecasting ,4, 1\u201338.\nGardner Jr., E. S. (1993). Forecasting the failure of component parts\nin computer systems: A case study. International Journal of\nForecasting ,9, 245\u2013253.Gardner Jr., E. S., & McKenzie, E. (1988). Model identification in\nexponential smoothing.\nJournal of the Operational Research\nSociety ,39, 863\u2013867.\nGrubb, H., & Masa, A. (2001). Long lead-time forecasting of UK\nair passengers by Holt\u2013Winters methods with damped trend.\nInternational Journal of Forecasting ,17, 71\u201382.\nHolt, C. C. (1957). Forecasting seasonals and trends by exponen-\ntially weighted averages. O.N.R. Memorandum 52/1957,\nCarnegie Institute of Technology. Reprinted with discussion in2004. International Journal of Forecasting ,20, 5\u201313.\nHyndman, R. J. (2001). It Ts time to move from what to why.\nInternational Journal of Forecasting ,17, 567\u2013570.\nHyndman, R. J., & Billah, B. (2003).", "start_char_idx": 0, "end_char_idx": 3164, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ebf5b508-8483-455d-a697-c2f7a81eed04": {"__data__": {"id_": "ebf5b508-8483-455d-a697-c2f7a81eed04", "embedding": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24232b86-b4df-4a0c-ae88-6db8fd096838", "node_type": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "hash": "6f21f504ff4ee3af67ebcfa3667de0f08fc5cc48b8c5df101bbe9d9c8f34634e"}, "2": {"node_id": "454c1031-4ca1-4019-8e83-195c85ad9edf", "node_type": null, "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}, "hash": "572f747816d299798676e4ce745f6bf339b70117146b7800ea12cfa8cccd948c"}}, "hash": "d60a0485458b25b5e38464392c1972443f7a45d9949c80ecdec144e23204b086", "text": "R. J., & Billah, B. (2003). Unmasking the Theta method.\nInternational Journal of Forecasting ,19, 287\u2013290.\nHyndman, R. J., Koehler, A. B., Snyder, R. D., & Grose, S. (2002).\nA state space framework for automatic forecasting using\nexponential smoothing methods. International Journal of\nForecasting ,18, 439\u2013454.\nHyndman, R. J., Koehler, A. B., Ord, J. K., & Snyder, R. D. (2005).\nPrediction intervals for exponential smoothing state spacemodels. Journal of Forecasting ,24, 17\u201337.\nJohnston, F. R., & Harrison, P. J. (1986). The variance of lead-\ntime demand. Journal of Operational Research Society ,37,\n303\u2013308.\nKoehler, A. B., Snyder, R. D., & Ord, J. K. (2001). Forecasting\nmodels and prediction intervals for the multiplicative Holt\u2013\nWinters method. International Journal of Forecasting ,17,\n269\u2013286.\nLawton, R. (1998). How should additive Holt\u2013Winters esti-\nmates be corrected? International Journal of Forecasting ,\n14, 393\u2013 403.\nLedolter, J., & Abraham, B. (1984). Some comments on the\ninitialization of exponential smoothing. Journal of Forecasting ,\n3, 79\u201384.\nMakridakis, S., & Hibon, M. (1991). Exponential smoothing: The\neffect of initial values and loss functions on post-sampleforecasting accuracy. International Journal of Forecasting ,7,\n317\u2013330.\nMcClain, J. G. (1988). Dominant tracking signals. International\nJournal of Forecasting ,4, 563\u2013572.\nMcKenzie, E. (1984). General exponential smoothing and the\nequivalent ARMA process. Journal of Forecasting ,3, 333\u2013344.\nMcKenzie, E. (1986). Error analysis for Winters additive seasonal\nforecasting system. International Journal of Forecasting ,2,\n373\u2013382.\nMiller, T., & Liberatore, M. (1993). Seasonal exponential smooth-\ning with damped trends. An application for production planning.International Journal of Forecasting ,9\n, 509\u2013515.\nMuth, J. F. (1960). Optimal properties of exponentially weighted\nforecasts. Journal of the American Statistical Association ,55,\n299\u2013306.\nNewbold, P., & Bos, T. (1989). On exponential smoothing and the\nassumption of deterministic trend plus white noise data-\ngenerating models. International Journal of Forecasting ,5,\n523\u2013527.\nOrd, J. K., Koehler, A. B., & Snyder, R. D. (1997). Estimation\nand prediction for a class of dynamic nonlinear statisticalJ.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 463", "start_char_idx": 3137, "end_char_idx": 5475, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "822511ba-a3a0-4575-a7f3-6921fda4967d": {"__data__": {"id_": "822511ba-a3a0-4575-a7f3-6921fda4967d", "embedding": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba8740db-1722-4584-b585-a469555c0ef5", "node_type": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "hash": "e176fc43d0bf0b89db5ceca9890d74bff97f74cb8375467804ec408c2ff3c1df"}, "3": {"node_id": "79f29ca5-b46f-4175-b8aa-a7f513b5dbe5", "node_type": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "hash": "6f54dfcc15116475dcc5afbcc0cf712a4e3b92b433c18cfa17c6382c00772980"}}, "hash": "4f5827b5c3e1112d0034350b188c3850cbb4eb871b1a6f25574b2a645bc5ade0", "text": "models. Journal of the American Statistical Association ,92,\n1621\u20131629.\nPan, X. (2005). An alternative approach to multivariate EWMA\ncontrol chart. Journal of Applied Statistics ,32, 695\u2013705.\nPegels, C. C. (1969). Exponential smoothing: Some new variations.\nManagement Science ,12, 311\u2013315.\nPfeffermann, D., & Allon, J. (1989). Multivariate exponential\nsmoothing: Methods and practice. International Journal of\nForecasting ,5, 83\u201398.\nRoberts, S. A. (1982). A general class of Holt\u2013Winters type\nforecasting models. Management Science ,28, 808\u2013820.\nRosas, A. L., & Guerrero, V. M. (1994). Restricted forecasts using\nexponential smoothing techniques. International Journal of\nForecasting ,10, 515\u2013527.\nSatchell, S., & Timmermann, A. (1995). On the optimality of\nadaptive expectations: Muth revisited. International Journal of\nForecasting ,11, 407\u2013416.\nSnyder, R. D. (1985). Recursive estimation of dynamic linear\nstatistical models. Journal of the Royal Statistical Society (B) ,\n47, 272\u2013276.\nSweet, A. L. (1985). Computing the variance of the forecast error\nfor the Holt\u2013Winters seasonal models. Journal of Forecasting ,\n4, 235\u2013243.\nSweet, A. L., & Wilson, J. R. (1988). Pitfalls in simulation-based\nevaluation of forecast monitoring schemes. International Jour-\nnal of Forecasting ,4, 573\u2013579.\nTashman, L., & Kruk, J. M. (1996). The use of protocols to select\nexponential smoothing procedures: A reconsideration of fore-casting competitions. International Journal of Forecasting ,12,\n235\u2013253.\nTaylor, J. W. (2003). Exponential smoothing with a damped\nmultiplicative trend. International Journal of Forecasting ,19,\n273\u2013289.\nWilliams, D. W., & Miller, D. (1999). Level-adjusted exponential\nsmoothing for modeling planned discontinuities. International\nJournal of Forecasting ,15, 273\u2013289.\nWinters, P. R. (1960). Forecasting sales by exponentially weighted\nmoving averages. Management Science ,6, 324\u2013342.\nYar, M., & Chatfield, C. (1990). Prediction intervals for the Holt\u2013\nWinters forecasting procedure. International Journal of Fore-\ncasting ,6, 127\u2013137.\nSection 3. ARIMA\nde Alba, E. (1993). Constrained forecasting in autoregressive time\nseries models: A Bayesian analysis. International Journal of\nForecasting ,9, 95\u2013108.\nArin\u02dco, M. A., & Franses, P. H. (2000). Forecasting the levels of\nvector autoregressive log-transformed time series. International\nJournal of Forecasting ,16, 111 \u2013 116.\nArtis, M. J., & Zhang, W. (1990). BV AR forecasts for the G-7.\nInternational Journal of Forecasting ,6, 349\u2013362.\nAshley, R. (1988). On the relative worth of recent macroeconomic\nforecasts. International Journal of Forecasting ,4, 363\u2013376.\nBhansali, R. J. (1996). Asymptotically efficient autoregressive\nmodel selection for multistep prediction. Annals of the Institute\nof Statistical Mathematics ,48, 577\u2013602.Bhansali, R. J. (1999). Autoregressive model selection for multistep\nprediction. Journal of Statistical Planning and Inference ,78,\n295\u2013305.\nBianchi, L., Jarrett, J., & Hanumara, T. C. (1998). Improving\nforecasting for telemarketing centers by ARIMA modeling\nwith interventions. International Journal of Forecasting ,14,\n497\u2013504.\nBidarkota, P. V. (1998). The comparative forecast performance of\nunivariate", "start_char_idx": 0, "end_char_idx": 3207, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "79f29ca5-b46f-4175-b8aa-a7f513b5dbe5": {"__data__": {"id_": "79f29ca5-b46f-4175-b8aa-a7f513b5dbe5", "embedding": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ba8740db-1722-4584-b585-a469555c0ef5", "node_type": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "hash": "e176fc43d0bf0b89db5ceca9890d74bff97f74cb8375467804ec408c2ff3c1df"}, "2": {"node_id": "822511ba-a3a0-4575-a7f3-6921fda4967d", "node_type": null, "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}, "hash": "4f5827b5c3e1112d0034350b188c3850cbb4eb871b1a6f25574b2a645bc5ade0"}}, "hash": "6f54dfcc15116475dcc5afbcc0cf712a4e3b92b433c18cfa17c6382c00772980", "text": "P. V. (1998). The comparative forecast performance of\nunivariate and multivariate models: An application to realinterest rate forecasting. International Journal of Forecasting ,\n14, 457\u2013468.\nBox, G. E. P., & Jenkins, G. M. (1970). Time series analysis:\nForecasting and control . San Francisco 7Holden Day (revised\ned. 1976).\nBox, G. E. P., Jenkins, G. M., & Reinsel, G. C. (1994). Time series\nanalysis: Forecasting and control (3rd ed.). Englewood Cliffs,\nNJ7Prentice Hall.\nChatfield, C. (1988). What is the dbest Tmethod of forecasting?\nJournal of Applied Statistics ,15, 19\u201338.\nChevillon, G., & Hendry, D. F. (2005). Non-parametric direct multi-\nstep estimation for forecasting economic processes. Internation-\nal Journal of Forecasting ,21, 201\u2013218.\nCholette, P. A. (1982). Prior information and ARIMA forecasting.\nJournal of Forecasting ,1, 375\u2013383.\nCholette, P. A., & Lamy, R. (1 986). Multivariate ARIMA\nforecasting of irregular time series. International Journal of\nForecasting ,2, 201\u2013216.\nCummins, J. D., & Griepentrog, G. L. (1985). Forecasting\nautomobile insurance paid claims using econometric andARIMA models. International Journal of Forecasting ,1,\n203\u2013215.\nDe Gooijer, J. G., & Klein, A. (1991). On the cumulated multi-step-\nahead predictions of vector autoregressive moving average\nprocesses. International Journal of Forecasting ,7, 501\u2013513.\ndel Moral, M. J., & Valderrama, M. J. (1997). A principal\ncomponent approach to dynamic regression models. Interna-\ntional Journal of Forecasting ,13, 237\u2013244.\nDhrymes, P. J., & Peristiani, S. C. (1988). A comparison of the\nforecasting performance of WEFA and ARIMA time seriesmethods. International Journal of Forecasting ,4, 81\u2013101.\nDhrymes, P. J., & Thomakos, D. (1998). Structural VAR, MARMA\nand open economy models. International Journal of Forecast-\ning,14, 187\u2013198.\nDi Caprio, U., Genesio, R., Pozzi, S., & Vicino, A. (1983). Short\nterm load forecasting in electric power systems: A comparison\nof ARMA models and extended Wiener filtering. Journal of\nForecasting ,2, 59\u201376.\nDowns, G. W., & Rocke, D. M. (1983). Municipal budget\nforecasting with multiv ariate ARMA models. Journal of\nForecasting ,2, 377\u2013387.\ndu Preez, J., & Witt, S. F. (2003). Univariate versus multivariate\ntime series forecasting: An application to international\ntourism demand. International Journal of Forecasting ,19,\n435\u2013451.\nEdlund, P. -O. (1984). Identification of the multi-input Box\u2013\nJenkins transfer function model. Journal of Forecasting ,3,\n297\u2013308.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 464", "start_char_idx": 3143, "end_char_idx": 5729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab0311a8-f633-4052-8abc-a4877db7fb17": {"__data__": {"id_": "ab0311a8-f633-4052-8abc-a4877db7fb17", "embedding": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a84ad94-6d87-4774-884a-487f13436df8", "node_type": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "hash": "96cdebdd0396bfeb5853dd15f9e519723df8fa157fd8c816a5adef617aa9da9a"}, "3": {"node_id": "01b73d52-a1f6-46db-b1e0-dfb4c40e4e34", "node_type": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "hash": "880bc71b94500de38c6774761dd52beb5f588febcccb94e0b76d068a1de785af"}}, "hash": "dada1891bd2b6b3d9c84ab7520656eb95b0040f0ad2c884e55c0fc742fc355ba", "text": "Edlund, P. -O., & Karlsson, S. (1993). Forecasting the Swedish\nunemployment rate. VAR vs. transfer function modelling.International Journal of Forecasting ,9, 61\u201376.\nEngle, R. F., & Granger, C. W. J. (1987). Co-integration and error\ncorrection: Representation, estimation, and testing. Econometr-\nica,55, 1057\u20131072.\nFunke, M. (1990). Assessing the forecasting accuracy of monthly\nvector autoregressive models: The case of five OECD countries.\nInternational Journal of Forecasting ,6, 363\u2013378.\nGeriner, P. T., & Ord, J. K. (1991). Automatic forecasting using\nexplanatory variables: A comparative study. International\nJournal of Forecasting ,7, 127\u2013140.\nGeurts, M. D., & Kelly, J. P. (1986). Forecasting retail sales using\nalternative models. International Journal of Forecasting ,2,\n261\u2013272.\nGeurts, M. D., & Kelly, J. P. (1990). Comments on: In defense of\nARIMA modeling by D.J. Pack. International Journal of\nForecasting ,6, 497\u2013499.\nGrambsch, P., & Stahel, W. A. (1990). Forecasting demand for\nspecial telephone services: A case study. International Journal\nof Forecasting ,6, 53\u201364.\nGuerrero, V. M. (1991). ARIMA forecasts with restrictions derived\nfrom a structural change. International Journal of Forecasting ,\n7, 339\u2013347.\nGupta, S. (1987). Testing causality: Some caveats and a suggestion.\nInternational Journal of Forecasting ,3, 195\u2013209.\nHafer, R. W., & Sheehan, R. G. (1989). The sensitivity of VAR\nforecasts to alternative lag structures. International Journal of\nForecasting ,5, 399\u2013408.\nHansson, J., Jansson, P., & Lo \u00a8f, M. (2005). Business survey data:\nDo they help in forecasting GDP growth? International Journal\nof Forecasting ,21, 377\u2013389.\nHarris, J. L., & Liu, L. -M. (1993). Dynamic structural analysis and\nforecasting of residential electricity consumption. International\nJournal of Forecasting ,9, 437\u2013455.\nHein, S., & Spudeck, R. E. (1988). Forecasting the daily federal\nfunds rate. International Journal of Forecasting ,4, 581\u2013591.\nHeuts, R. M. J., & Bronckers, J. H. J. M. (1988). Forecasting the\nDutch heavy truck market: A multivariate approach. Interna-\ntional Journal of Forecasting ,4, 57\u201359.\nHill, G., & Fildes, R. (1984). The accuracy of extrapolation\nmethods: An automatic Box\u2013Jenkins package SIFT. Journal of\nForecasting ,3, 319\u2013323.\nHillmer, S. C., Larcker, D. F., & Schroeder, D. A. (1983).\nForecasting accounting data: A multiple time-series analysis.\nJournal of Forecasting ,2, 389\u2013404.\nHolden, K., & Broomhead, A. (1990). An examination of vector\nautoregressive forecasts for the U.K. economy. International\nJournal of Forecasting ,6, 11\u201323.\nHotta, L. K. (1993). The effect of additive outliers on the estimates\nfrom aggregated and disaggregated ARIMA models. Interna-\ntional Journal of Forecasting ,9, 85\u201393.\nHotta, L. K., & Cardoso Neto, J. (1993). The effect of aggregation\non prediction in ARIMA models. Journal of Time Series\nAnalysis ,14, 261\u2013269.\nKang, I. -B. (2003). Multi-period forecasting using different mo-\ndels for different horizons: An application to U.S. economictime series data. International Journal of Forecasting ,19,\n387\u2013400.\nKim, J. H.", "start_char_idx": 0, "end_char_idx": 3099, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "01b73d52-a1f6-46db-b1e0-dfb4c40e4e34": {"__data__": {"id_": "01b73d52-a1f6-46db-b1e0-dfb4c40e4e34", "embedding": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a84ad94-6d87-4774-884a-487f13436df8", "node_type": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "hash": "96cdebdd0396bfeb5853dd15f9e519723df8fa157fd8c816a5adef617aa9da9a"}, "2": {"node_id": "ab0311a8-f633-4052-8abc-a4877db7fb17", "node_type": null, "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}, "hash": "dada1891bd2b6b3d9c84ab7520656eb95b0040f0ad2c884e55c0fc742fc355ba"}}, "hash": "880bc71b94500de38c6774761dd52beb5f588febcccb94e0b76d068a1de785af", "text": "International Journal of Forecasting ,19,\n387\u2013400.\nKim, J. H. (2003). Forecasting autoregressive time series with bias-\ncorrected parameter estimators. International Journal of Fore-\ncasting ,19, 493\u2013502.\nKling, J. L., & Bessler, D. A. (1985). A comparison of multivariate\nforecasting procedures for economic time series. International\nJournal of Forecasting ,1, 5\u201324.\nKolmogorov, A. N. (1941). Stationary sequences in Hilbert space\n(in Russian). Bull. Math. Univ. Moscow ,2(6), 1\u201340.\nKoreisha, S. G. (1983). Causal implications: The linkage between\ntime series and econometric modelling. Journal of Forecasting ,\n2, 151\u2013168.\nKrishnamurthi, L., Narayan, J., & Raj, S. P. (1989). Intervention\nanalysis using control series and exogenous variables in a\ntransfer function model: A case study. International Journal of\nForecasting ,5, 21\u201327.\nKunst, R., & Neusser, K. (1986). A forecasting comparison of\nsome VAR techniques. International Journal of Forecasting ,2,\n447\u2013456.\nLandsman, W. R., & Damodaran, A. (1989). A comparison of\nquarterly earnings per share forecast using James-Stein and\nunconditional least squares parameter estimators. International\nJournal of Forecasting ,5, 491\u2013500.\nLayton, A., Defris, L. V., & Zehnwirth, B. (1986). An inter-\nnational comparison of economic leading indicators of tele-\ncommunication traffic. International Journal of Forecasting ,2,\n413\u2013425.\nLedolter, J. (1989). The effect of additive outliers on the forecasts\nfrom ARIMA models. International Journal of Forecasting ,5,\n231\u2013240.\nLeone, R. P. (1987). Forecasting the effect of an environmental\nchange on market performance: An intervention time-series.\nInternational Journal of Forecasting ,3, 463\u2013478.\nLeSage, J. P. (1989). Incorporating regional wage relations in local\nforecasting models with a Bayesian prior. International Journal\nof Forecasting ,5, 37\u201347.\nLeSage, J. P., & Magura, M. (1991). Using interindustry input\u2013\noutput relations as a Bayesian prior in employment forecastingmodels. International Journal of Forecasting ,7, 231\u2013238.\nLibert, G. (1984). The M-competition with a fully automatic Box\u2013\nJenkins procedure. Journal of Forecasting ,3, 325\u2013328.\nLin, W. T. (1989). Modeling and forecasting hospital patient\nmovements: Univariate and multiple time series approaches.\nInternational Journal of Forecasting ,5, 195\u2013208.\nLitterman, R. B. (1986). Forecasting with Bayesian vector\nautoregressions\u2014Five years of experience. Journal of Business\nand Economic Statistics ,4, 25\u201338.\nLiu, L. -M., & Lin, M. -W. (1991). Forecasting residential\nconsumption of natural gas using monthly and quarterly timeseries. International Journal of Forecasting ,7, 3\u201316.\nLiu, T. -R., Gerlow, M. E., & Irwin, S. H. (1994). The performance\nof alternative VAR models in forecasting exchange rates.\nInternational Journal of Forecasting ,10, 419\u2013433.\nLu\u00a8tkepohl, H. (1986). Comparison of predictors for temporally and\ncontemporaneously aggregated time series. International Jour-\nnal of Forecasting ,2, 461\u2013475.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 465", "start_char_idx": 3038, "end_char_idx": 6116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "83721b6a-913f-4d5b-b4d7-37836f1de808": {"__data__": {"id_": "83721b6a-913f-4d5b-b4d7-37836f1de808", "embedding": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f07b21-b7de-41dc-86f4-3cb847800e0f", "node_type": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "hash": "ee02b9977d6f5a6692d837094d1666b5cdee89a9447e6a660602bab8f42936f6"}, "3": {"node_id": "605623b7-9e41-468f-a110-3c815fcd67b6", "node_type": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "hash": "e2a357c85d83975ceb3adc8fbdf308db80948caa5dfe8111d016febf94261ba2"}}, "hash": "3b448506e907e9c7b970f8f3cc6909c04b6b27ebc82c60088721bc36c233481a", "text": "Makridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M.,\nLewandowski, R., et al. (1982). The accuracy of extrapolation(time series) methods: Results of a forecasting competition.Journal of Forecasting ,1, 111\u2013153.\nMeade, N. (2000). A note on the robust trend and ARARMA\nmethodologies used in the M3 competition. International\nJournal of Forecasting ,16, 517\u2013519.\nMeade, N., & Smith, I. (1985). ARARMA vs ARIMA\u2014a study of\nthe benefits of a new approach to forecasting. Omega ,13,\n519\u2013534.\nMe\u00b4lard, G., & Pasteels, J. -M. (2000). Automatic ARIMA modeling\nincluding interventions, using time series expert software.\nInternational Journal of Forecasting ,16, 497\u2013508.\nNewbold, P. (1983). ARIMA model building and the time series analysis\napproach to forecasting. Journal of Forecasting ,2, 23\u201335.\nNewbold, P., Agiakloglou, C., & Miller, J. (1994). Adventures with\nARIMA software. International Journal of Forecasting ,10,\n573\u2013581.\nO\u00a8ller, L. -E. (1985). Macroeconomic forecasting with a vector ARIMA\nmodel. International Journal of Forecasting ,1, 143\u2013150.\nPack, D. J. (1990). Rejoinder to: Comments on: In defense of\nARIMA modeling by M.D. Geurts and J.P. Kelly. International\nJournal of Forecasting ,6, 501\u2013502.\nParzen, E. (1982). ARARMA models for time series analysis and\nforecasting. Journal of Forecasting ,1, 67\u201382.\nPen\u02dca, D., & Sa \u00b4nchez, I. (2005). Multifold predictive validation in\nARMAX time series models. Journal of the American Statistical\nAssociation ,100, 135\u2013146.\nPflaumer, P. (1992). Forecasting US population totals with the Box\u2013\nJenkins approach. International Journal of Forecasting ,8,\n329\u2013338.\nPoskitt, D. S. (2003). On the specification of cointegrated\nautoregressive moving-average forecasting systems. Interna-\ntional Journal of Forecasting ,19, 503\u2013519.\nPoulos, L., Kvanli, A., & Pavur, R. (1987). A comparison of the\naccuracy of the Box\u2013Jenkins method with that of automatedforecasting methods. International Journal of Forecasting ,3,\n261\u2013267.\nQuenouille, M. H. (1957). The analysis of multiple time-series (2nd\ned. 1968). London 7Griffin.\nReimers, H. -E. (1997). Forecasting of seasonal cointegrated\nprocesses. International Journal of Forecasting ,13\n, 369\u2013380.\nRibeiro Ramos, F. F. (2003). Forecasts of market shares from VAR\nand BV AR models: A comparison of their accuracy. Interna-\ntional Journal of Forecasting ,19, 95\u2013110.\nRiise, T., & Tj\u00f8stheim, D. (1984). Theory and practice of\nmultivariate ARMA forecasting. Journal of Forecasting ,3,\n309\u2013317.\nShoesmith, G. L. (1992). Non-cointegration and causality: Impli-\ncations for VAR modeling. International Journal of Forecast-\ning,8, 187\u2013199.\nShoesmith, G. L. (1995). Multiple cointegrating vectors, error\ncorrection, and forecasting with Littermans model. International\nJournal of Forecasting ,11, 557\u2013567.\nSimkins, S. (1995). Forecasting with vector autoregressive (V AR)\nmodels subject to business cycle restrictions. International\nJournal of Forecasting ,11, 569\u2013583.Spencer, D. E. (1993). Developing a Bayesian vector autoregressive\nforecasting model. International Journal of Forecasting", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "605623b7-9e41-468f-a110-3c815fcd67b6": {"__data__": {"id_": "605623b7-9e41-468f-a110-3c815fcd67b6", "embedding": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "70f07b21-b7de-41dc-86f4-3cb847800e0f", "node_type": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "hash": "ee02b9977d6f5a6692d837094d1666b5cdee89a9447e6a660602bab8f42936f6"}, "2": {"node_id": "83721b6a-913f-4d5b-b4d7-37836f1de808", "node_type": null, "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}, "hash": "3b448506e907e9c7b970f8f3cc6909c04b6b27ebc82c60088721bc36c233481a"}}, "hash": "e2a357c85d83975ceb3adc8fbdf308db80948caa5dfe8111d016febf94261ba2", "text": "Developing a Bayesian vector autoregressive\nforecasting model. International Journal of Forecasting ,9,\n407\u2013421.\nTashman, L. J. (2000). Out-of sample tests of forecasting accuracy:\nA tutorial and review. International Journal of Forecasting ,16,\n437\u2013450.\nTashman, L. J., & Leach, M. L. (1991). Automatic forecasting\nsoftware: A survey and evaluation. International Journal of\nForecasting ,7, 209\u2013230.\nTegene, A., & Kuchler, F. (1994). Evaluating forecasting models\nof farmland prices. International Journal of Forecasting ,10,\n65\u201380.\nTexter, P. A., & Ord, J. K. (1989). Forecasting using automatic\nidentification procedures: A comparative analysis. International\nJournal of Forecasting ,5, 209\u2013215.\nVillani, M. (2001). Bayesian prediction with cointegrated vector\nautoregression. International Journal of Forecasting ,17,\n585\u2013605.\nWang, Z., & Bessler, D. A. (2004). Forecasting performance of\nmultivariate time series models with a full and reduced rank: Anempirical examination. International Journal of Forecasting ,\n20, 683\u2013695.\nWeller, B. R. (1989). National indicator series as quantitative\npredictors of small region monthly employment levels. Inter-\nnational Journal of Forecasting ,5, 241\u2013247.\nWest, K. D. (1996). Asymptotic inference about predictive ability.\nEconometrica ,68, 1084\u20131097.\nWieringa, J. E., & Horva \u00b4th, C. (2005). Computing level-impulse\nresponses of log-specified VAR systems. International Journal\nof Forecasting\n,21, 279\u2013289.\nYule, G. U. (1927). On the method of investigating periodicities in\ndisturbed series, with special reference to Wo \u00a8lfer Ts sunspot\nnumbers. Philosophical Transactions of the Royal Society\nLondon, Series A ,226, 267\u2013298.\nZellner, A. (1971). An introduction to Bayesian inference in\neconometrics . New York 7Wiley.\nSection 4. Seasonality\nAlbertson, K., & Aylen, J. (1996). Modelling the Great Lake freeze:\nForecasting and seasonality in the market for ferrous scrap.\nInternational Journal of Forecasting ,12, 345\u2013359.\nBunn, D. W., & Vassilopoulos, A. I. (1993). Using group seasonal\nindices in multi-item short-term forecasting. International\nJournal of Forecasting ,9, 517\u2013526.\nBunn, D. W., & Vassilopoulos, A. I. (1999). Comparison of\nseasonal estimation methods in multi-item short-term forecast-ing.International Journal of Forecasting ,15, 431\u2013443.\nChen, C. (1997). Robustness properties of some forecasting\nmethods for seasonal time series: A Monte Carlo study.International Journal of Forecasting ,13, 269\u2013280.\nClements, M. P., & Hendry, D. F. (1997). An empirical study of\nseasonal unit roots in forecasting. International Journal of\nForecasting ,13, 341\u2013355.\nCleveland, R. B., Cleveland, W. S., McRae, J. E., & Terpenning, I.\n(1990). STL: A seasonal-trend decomposition procedure based on\nLoess (with discussion). Journal of Official Statistics ,6, 3\u201373.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 466", "start_char_idx": 2984, "end_char_idx": 5890, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97ca86dd-1809-41b5-9595-39d097001f1e": {"__data__": {"id_": "97ca86dd-1809-41b5-9595-39d097001f1e", "embedding": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f298ea32-3817-4de2-958f-5f09c5674ab0", "node_type": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "hash": "84332997d8ef8036d417ce3e1ff731aff4ffa7966dc3c154d10f2727153dd84d"}, "3": {"node_id": "b153b385-a273-49f0-9fd0-8ea1df4829bd", "node_type": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "hash": "c63ef34ad5a07bd2e08c61bcce3e51d299b10fc9bd02aee6434fc7dcc095f2d5"}}, "hash": "8f02a2742b9bcebd6f871f6e137324c89de15b9370a9e44f3e5765144aaf2c2c", "text": "Dagum, E. B. (1982). Revisions of time varying seasonal filters.\nJournal of Forecasting ,1, 173\u2013187.\nFindley, D. F., Monsell, B. C., Bell, W. R., Otto, M. C., & Chen, B.-\nC. (1998). New capabilities and methods of the X-12-ARIMA\nseasonal adjustment program. Journal of Business and Eco-\nnomic Statistics ,16, 127\u2013152.\nFindley, D. F., Wills, K. C., & Monsell, B. C. (2004). Seasonal\nadjustment perspectives on damping seasonal factors: Shrinkage\nestimators for the X-12-ARIMA program. International Journal\nof Forecasting ,20, 551\u2013556.\nFranses, P. H., & Koehler, A. B. (1998). A model selection strategy\nfor time series with increasing seasonal variation. International\nJournal of Forecasting ,14, 405\u2013414.\nFranses, P. H., & Romijn, G. (1993). Periodic integration in\nquarterly UK macroeconomic variables. International Journal\nof Forecasting ,9, 467\u2013476.\nFranses, P. H., & van Dijk, D. (2005). The forecasting performance\nof various models for seasonality and nonlinearity for quarterly\nindustrial production. International Journal of Forecasting ,21,\n87\u2013102.\nGo\u00b4mez, V., & Maravall, A. (2001). Seasonal adjustment and signal\nextraction in economic time series. In D. Pen \u02dca, G. C. Tiao, & R.\nS. Tsay (Eds.), Chapter 8 in a course in time series analysis .\nNew York 7John Wiley and Sons.\nHerwartz, H. (1997). Performance of periodic error correction\nmodels in forecasting consumption data. International Journal\nof Forecasting ,13, 421\u2013431.\nHuot, G., Chiu, K., & Higginson, J. (1986). Analysis of revisions\nin the seasonal adjustment of data using X-11-ARIMAmodel-based filters. International Journal of Forecasting ,2,\n217\u2013229.\nHylleberg, S., & Pagan, A. R. (1997). Seasonal integration and the\nevolving seasonals model. International Journal of Forecasting ,\n13, 329\u2013340.\nHyndman, R. J. (2004). The interaction between trend and\nseasonality. International Journal of Forecasting ,20, 561\u2013563.\nKaiser, R., & Maravall, A. (2005). Combining filter design with\nmodel-based filtering (with an application to business-cycle\nestimation). International Journal of Forecasting ,21, 691\u2013710.\nKoehler, A. B. (2004). Comments on damped seasonal factors and\ndecisions by potential users. International Journal of Forecast-\ning,20, 565\u2013566.\nKulendran, N., & King, M. L. (1997). Forecasting interna-\ntional quarterly tourist flows using error-correction and\ntime-series models. International Journal of Forecasting ,13,\n319\u2013327.\nLadiray, D., & Quenneville, B. (2004). Implementation issues on\nshrinkage estimators for seasonal factors within the X-11\nseasonal adjustment method. International Journal of Forecast-\ning,20, 557\u2013560.\nMiller, D. M., & Williams, D. (2003). Shrinkage estimators of time\nseries seasonal factors and their effect on forecasting accuracy.\nInternational Journal of Forecasting ,19, 669\u2013684.\nMiller, D. M., & Williams, D. (2004). Damping seasonal factors:\nShrinkage estimators for seasonal factors within the X-11\nseasonal adjustment method (with commentary). International\nJournal of Forecasting ,20, 529\u2013550.Noakes, D. J., McLeod, A. I., & Hipel, K. W. (1985). Forecasting\nmonthly riverflow time series. International Journal of Fore-\ncasting ,1,", "start_char_idx": 0, "end_char_idx": 3158, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b153b385-a273-49f0-9fd0-8ea1df4829bd": {"__data__": {"id_": "b153b385-a273-49f0-9fd0-8ea1df4829bd", "embedding": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f298ea32-3817-4de2-958f-5f09c5674ab0", "node_type": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "hash": "84332997d8ef8036d417ce3e1ff731aff4ffa7966dc3c154d10f2727153dd84d"}, "2": {"node_id": "97ca86dd-1809-41b5-9595-39d097001f1e", "node_type": null, "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}, "hash": "8f02a2742b9bcebd6f871f6e137324c89de15b9370a9e44f3e5765144aaf2c2c"}}, "hash": "c63ef34ad5a07bd2e08c61bcce3e51d299b10fc9bd02aee6434fc7dcc095f2d5", "text": "Forecasting\nmonthly riverflow time series. International Journal of Fore-\ncasting ,1, 179\u2013190.\nNovales, A., & de Fruto, R. F. (1997). Forecasting with time\nperiodic models: A comparison with time invariant coefficient\nmodels. International Journal of Forecasting ,13, 393\u2013405.\nOrd, J. K. (2004). Shrinking: When and how? International Journal\nof Forecasting ,20, 567\u2013568.\nOsborn, D. (1990). A survey of seasonality in UK macroeconomic\nvariables. International Journal of Forecasting ,6, 327\u2013336.\nPaap, R., Franses, P. H., & Hoek, H. (1997). Mean shifts, unit roots\nand forecasting seasonal time series. International Journal of\nForecasting ,13, 357\u2013368.\nPfeffermann, D., Morry, M., & Wong, P. (1995). Estimation of the\nvariances of X-11 ARIMA seasonally adjusted estimators for a\nmultiplicative decomposition and heteroscedastic variances.\nInternational Journal of Forecasting ,11, 271\u2013283.\nQuenneville, B., Ladiray, D., & Lefranc \u00b8ois, B. (2003). A note on\nMusgrave asymmetrical trend-cycle filters. International Jour-\nnal of Forecasting ,19, 727\u2013734.\nSimmons, L. F. (1990). Time-series decomposition using the\nsinusoidal model. International Journal of Forecasting ,6,\n485\u2013495.\nTaylor, A. M. R. (1997). On the practical problems of computing\nseasonal unit root tests. International Journal of Forecasting ,\n13, 307\u2013318.\nUllah, T. A. (1993). Forecasting of multivariate periodic autore-\ngressive moving-average process. Journal of Time Series\nAnalysis ,14, 645\u2013657.\nWells, J. M. (1997). Modelling seasonal patterns and long-run\ntrends in U.S. time series. International Journal of Forecasting ,\n13, 407\u2013420.\nWithycombe, R. (1989). Forecasting with combined seasonal\nindices. International Journal of Forecasting ,5, 547\u2013552.\nSection 5. State space and structural models and the Kalman filter\nCoomes, P. A. (1992). A Kalman filter formulation for noisy regional\njob data. International Journal of Forecasting ,7, 473\u2013481.\nDurbin, J., & Koopman, S. J. (2001). Time series analysis by state\nspace methods . Oxford 7Oxford University Press.\nFildes, R. (1983). An evaluation of Bayesian forecasting. Journal of\nForecasting ,2, 137\u2013150.\nGrunwald, G. K., Raftery, A. E., & Guttorp, P. (1993). Time series\nof continuous proportions. Journal of the Royal Statistical\nSociety (B) ,55, 103\u2013116.\nGrunwald, G. K., Hamza, K., & Hyndman, R. J. (1997). Some\nproperties and generalizations of nonnegative Bayesian time\nseries models. Journal of the Royal Statistical Society (B) ,59,\n615\u2013626.\nHarrison, P. J., & Stevens, C. F. (1976). Bayesian forecasting.\nJournal of the Royal Statistical Society (B) ,38, 205\u2013247.\nHarvey, A. C. (1984). A unified view of statistical forecast-\ning procedures (with discussion). Journal of Forecasting ,3,\n245\u2013283.\nHarvey, A. C. (1989). Forecasting, structural time series models\nand the Kalman filter . Cambridge 7Cambridge University Press.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 467", "start_char_idx": 3073, "end_char_idx": 6023, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94cd3887-69f9-4cd6-b672-162d18cf085c": {"__data__": {"id_": "94cd3887-69f9-4cd6-b672-162d18cf085c", "embedding": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc", "node_type": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "hash": "8bc14240e686c45ddb404313f741c00f6e75e05613474d3dddee9ce90e7f0f51"}, "3": {"node_id": "d53c3265-651b-4a0f-969e-d555e7950fc2", "node_type": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "hash": "b3736bc1d83e6f7161b6a283297b253ee6d1eb6f133099bf874aa35cc3ef95db"}}, "hash": "a5e2120d3f95602a652e258f7c32c7b3fc79e3155aef61885960f2ee9635a8fc", "text": "Harvey, A. C. (2006). Forecasting with unobserved component time\nseries models. In G. Elliot, C. W. J. Granger, & A. Timmermann(Eds.), Handbook of economic forecasting . Amsterdam 7Elsevier\nScience.\nHarvey, A. C., & Fernandes, C. (1989). Time series models for\ncount or qualitative observations. Journal of Business and\nEconomic Statistics ,7, 407\u2013422.\nHarvey, A. C., & Snyder, R. D. (1990). Structural time series\nmodels in inventory control. International Journal of Forecast-\ning,6, 187\u2013198.\nKalman, R. E. (1960). A new approach to linear filtering and\nprediction problems. Transactions of the ASME\u2014Journal of\nBasic Engineering ,82D, 35\u201345.\nMittnik, S. (1990). Macroeconomic forecasting experience with\nbalanced state space models. International Journal of Forecast-\ning,6, 337\u2013345.\nPatterson, K. D. (1995). Forecasting the final vintage of real\npersonal disposable income: A state space approach. Interna-\ntional Journal of Forecasting ,11, 395\u2013405.\nProietti, T. (2000). Comparing seasonal components for structural\ntime series models. International Journal of Forecasting ,16,\n247\u2013260.\nRay, W. D. (1989). Rates of convergence to steady state for the\nlinear growth version of a dynamic linear model (DLM).International Journal of Forecasting ,5, 537\u2013545.\nSchweppe, F. (1965). Evaluation of likelihood functions for\nGaussian signals. IEEE Transactions on Information Theory ,\n11(1), 61\u201370.\nShumway, R. H., & Stoffer, D. S. (1982). An approach to time\nseries smoothing and forecasting using the EM algorithm.\nJournal of Time Series Analysis ,3, 253\u2013264.\nSmith, J. Q. (1979). A generalization of the Bayesian steady\nforecasting model. Journal of the Royal Statistical Society,\nSeries B ,41, 375\u2013387.\nVinod, H. D., & Basu, P. (1995). Forecasting consumption, income\nand real interest rates from alternative state space models.International Journal of Forecasting ,11, 217\u2013231.\nWest, M., & Harrison, P. J. (1989). Bayesian forecasting and\ndynamic models (2nd ed., 1997). New York 7Springer-Verlag.\nWest, M., Harrison, P. J., & Migon, H. S. (1985). Dynamic\ngeneralized linear models and Bayesian forecasting (with\ndiscussion). Journal of the American Statistical Association ,\n80, 73\u201383.\nSection 6. Nonlinear\nAdya, M., & Collopy, F. (1998). How effective are neural networks\nat forecasting and prediction? A review and evaluation. Journal\nof Forecasting ,17, 481\u2013495.\nAl-Qassem, M. S., & Lane, J. A. (1989). Forecasting exponential\nautoregressive models of order 1.\nJournal of Time Series\nAnalysis ,10, 95\u2013113.\nAstatkie, T., Watts, D. G., & Watt, W. E. (1997). Nested threshold\nautoregressive (NeTAR) models. International Journal of\nForecasting ,13, 105\u2013116.\nBalkin, S. D., & Ord, J. K. (2000). Automatic neural network\nmodeling for univariate time series. International Journal of\nForecasting ,16, 509\u2013515.Boero, G., & Marrocu, E. (2004). The performance of SETAR\nmodels: A regime conditional evaluation of point, interval anddensity forecasts. International Journal of Forecasting ,20,\n305\u2013320.\nBradley, M. D., & Jansen, D. W. (2004). Forecasting with\na nonlinear dynamic model of stock returns andindustrial production. International Journal of Forecasting ,\n20, 321\u2013342.\nBrockwell, P. J., & Hyndman, R. J. (1992). On", "start_char_idx": 0, "end_char_idx": 3219, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d53c3265-651b-4a0f-969e-d555e7950fc2": {"__data__": {"id_": "d53c3265-651b-4a0f-969e-d555e7950fc2", "embedding": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc", "node_type": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "hash": "8bc14240e686c45ddb404313f741c00f6e75e05613474d3dddee9ce90e7f0f51"}, "2": {"node_id": "94cd3887-69f9-4cd6-b672-162d18cf085c", "node_type": null, "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}, "hash": "a5e2120d3f95602a652e258f7c32c7b3fc79e3155aef61885960f2ee9635a8fc"}}, "hash": "b3736bc1d83e6f7161b6a283297b253ee6d1eb6f133099bf874aa35cc3ef95db", "text": "P. J., & Hyndman, R. J. (1992). On continuous-time\nthreshold autoregression. International Journal of Forecasting ,\n8, 157\u2013173.\nCai, Z., Fan, J., & Yao, Q. (2000). Functional-coefficient regression\nmodels for nonlinear time series. Journal of the American\nStatistical Association ,95, 941\u2013956.\nCallen, J. F., Kwan, C. C. Y., Yip, P. C. Y., & Yuan, Y. (1996).\nNeural network forecasting of quarterly accounting earnings.\nInternational Journal of Forecasting ,12, 475\u2013482.\nCao, L., & Soofi, A. S. (1999). Nonlinear deterministic forecasting\nof daily dollar exchange rates. International Journal of\nForecasting ,15, 421\u2013430.\nCecen, A. A., & Erkal, C. (1996). Distinguishing between stochastic\nand deterministic behavior in high frequency foreign rate\nreturns: Can non-linear dynamics help forecasting? Internation-\nal Journal of Forecasting ,12, 465\u2013473.\nChatfield, C. (1993). Neural network: Forecasting breakthrough or\npassing fad? International Journal of Forecasting ,9,1 \u2013 3 .\nChatfield, C. (1995). Positive or negative. International Journal of\nForecasting ,11, 501\u2013502.\nChen, R., & Tsay, R. S. (1993). Functional-coefficient autoregres-\nsive models. Journal of the American Statistical Association ,\n88, 298\u2013308.\nChurch, K. B., & Curram, S. P. (1996). Forecasting consumers\nexpenditure: A comparison between econometric and neural\nnetwork models. International Journal of Forecasting ,12,\n255\u2013267.\nClements, M. P., & Smith, J. (1997). The performance of alternative\nmethods for SETAR models. International Journal of Fore-\ncasting ,13, 463\u2013475.\nClements, M. P., Franses, P. H., & Swanson, N. R. (2004).\nForecasting economic and financial time-series with non-linear\nmodels. International Journal of Forecasting ,20, 169\u2013183.\nConejo, A. J., Contreras, J., Esp\u0131 \u00b4nola, R., & Plazas, M. A. (2005).\nForecasting electricity pric es for a day-ahead pool-based\nelectricity market. International Journal of Forecasting ,21,\n435\u2013462.\nDahl, C. M., & Hylleberg, S. (2004). Flexible regression models\nand relative forecast performance. International Journal of\nForecasting ,20, 201\u2013217.\nDarbellay, G. A., & Slama, M. (2000). Forecasting the short-term\ndemand for electricity: Do neural networks stand a betterchance? International Journal of Forecasting ,16, 71\u201383.\nDe Gooijer, J. G., & Kumar, V. (1992). Some recent developments\nin non-linear time series modelling, testing and forecasting.\nInternational Journal of Forecasting ,8, 135\u2013156.\nDe Gooijer, J. G., & Vidiella-i-Anguera, A. (2004). Forecasting\nthreshold cointegrated systems. International Journal of Fore-\ncasting ,20, 237\u2013253.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 468", "start_char_idx": 3185, "end_char_idx": 5857, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4a86b7b9-c47a-4308-b480-866e59b21c7b": {"__data__": {"id_": "4a86b7b9-c47a-4308-b480-866e59b21c7b", "embedding": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e22a1468-6f1a-4e87-a5dc-3cbb33449b07", "node_type": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "hash": "e228c73aefbcdbfb967a28d4cd98e27418c1cbaddf4ae861e178704e70c1aedf"}, "3": {"node_id": "7cfd25f8-69ac-4f6b-bdde-3f6e77142b44", "node_type": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "hash": "3bb54a23cb1e10212dba0a6f422820a60882ce6fb47f213e4d79a7a7898f3763"}}, "hash": "38259b5841cc37a4197e1c81877985872fd2877b324f0622ee10a7f887fce4d3", "text": "Enders, W., & Falk, B. (1998). Threshold-autoregressive, median-\nunbiased, and cointegration tests of purchasing power parity.International Journal of Forecasting ,14, 171\u2013186.\nFerna\u00b4ndez-Rodr\u0131 \u00b4guez, F., Sosvilla-Rivero, S., & Andrada-Fe \u00b4lix, J.\n(1999). Exchange-rate forecasts with simultaneous nearest-\nneighbour methods; evidence from the EMS. International\nJournal of Forecasting ,15, 383\u2013392.\nFok, D. F., van Dijk, D., & Franses, P. H. (2005). Forecasting\naggregates using panels of nonlinear time series. International\nJournal of Forecasting ,21, 785\u2013794.\nFranses, P. H., Paap, R., & Vroomen, B. (2004). Forecasting\nunemployment using an autoregression with censored latent\neffects parameters. International Journal of Forecasting ,20,\n255\u2013271.\nGhiassi, M., Saidane, H., & Zimbra, D. K. (2005). A dynamic\nartificial neural network model for forecasting series events.\nInternational Journal of Forecasting ,21, 341\u2013362.\nGorr, W. (1994). Research prospective on neural network forecast-\ning.International Journal of Forecasting ,10,1 \u2013 4 .\nGorr, W., Nagin, D., & Szczypula, J. (1994). Comparative study of\nartificial neural network and statistical models for predictingstudent grade point averages. International Journal of Fore-\ncasting ,10, 17\u201334.\nGranger, C. W. J., & Tera \u00a8svirta, T. (1993). Modelling nonlinear\neconomic relationships . Oxford 7Oxford University Press.\nHamilton, J. D. (2001). A parametric approach to flexible nonlinear\ninference. Econometrica ,69, 537\u2013573.\nHarvill, J. L., & Ray, B. K. (2005). A note on multi-step forecasting\nwith functional coefficient autoregressive models. International\nJournal of Forecasting ,21, 717\u2013727.\nHastie, T. J., & Tibshirani, R. J. (1991). Generalized additive\nmodels . London 7Chapman and Hall.\nHeravi, S., Osborn, D. R., & Birchenhall, C. R. (2004). Linear versus\nneural network forecasting for European industrial production\nseries. International Journal of Forecasting ,20, 435\u2013446.\nHerwartz, H. (2001). Investigating the JPY/DEM-rate: Arbitrage\nopportunities and a case for asymmetry. International Journal of\nForecasting ,17, 231\u2013245.\nHill, T., Marquez, L., OConnor, M., & Remus, W. (1994). Artificial\nneural network models for forecasting and decision making.International Journal of Forecasting ,10, 5\u201315.\nHippert, H. S., Pedreira, C. E., & Souza, R. C. (2001). Neural\nnetworks for short-term load forecasting: A review andevaluation. IEEE Transactions on Power Systems ,16, 44\u201355.\nHippert, H. S., Bunn, D. W., & Souza, R. C. (2005). Large neural\nnetworks for electricity load forecasting: Are they overfitted?International Journal of Forecasting ,21, 425\u2013434.\nLisi, F., & Medio, A. (1997). Is a random walk the best exchange rate\npredictor? International Journal of Forecasting ,13, 255\u2013267.\nLudlow, J., & Enders, W. (2000). Estimating non-linear ARMA\nmodels using Fourier coefficients. International Journal of\nForecasting ,16, 333\u2013347.\nMarcellino, M. (2004). Forecasting EMU macroeconomic variables.\nInternational Journal of Forecasting ,20, 359\u2013372.\nOlson, D., & Mossman, C. (2003). Neural network forecasts of\nCanadian stock returns using accounting ratios.", "start_char_idx": 0, "end_char_idx": 3131, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7cfd25f8-69ac-4f6b-bdde-3f6e77142b44": {"__data__": {"id_": "7cfd25f8-69ac-4f6b-bdde-3f6e77142b44", "embedding": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e22a1468-6f1a-4e87-a5dc-3cbb33449b07", "node_type": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "hash": "e228c73aefbcdbfb967a28d4cd98e27418c1cbaddf4ae861e178704e70c1aedf"}, "2": {"node_id": "4a86b7b9-c47a-4308-b480-866e59b21c7b", "node_type": null, "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}, "hash": "38259b5841cc37a4197e1c81877985872fd2877b324f0622ee10a7f887fce4d3"}}, "hash": "3bb54a23cb1e10212dba0a6f422820a60882ce6fb47f213e4d79a7a7898f3763", "text": "(2003). Neural network forecasts of\nCanadian stock returns using accounting ratios. International\nJournal of Forecasting ,19, 453\u2013465.Pemberton, J. (1987). Exact least squares multi-step prediction from\nnonlinear autoregressive models. Journal of Time Series\nAnalysis ,8, 443\u2013448.\nPoskitt, D. S., & Tremayne, A. R. (1986). The selection and use of\nlinear and bilinear time series models. International Journal of\nForecasting ,2, 101\u2013114.\nQi, M. (2001). Predicting US recessions with leading indicators via\nneural network models. International Journal of Forecasting ,\n17, 383\u2013401.\nSarantis, N. (2001). Nonlinearities, cyclical behaviour and predict-\nability in stock markets: International evidence. International\nJournal of Forecasting ,17, 459\u2013482.\nSwanson, N. R., & White, H. (1997). Forecasting economic time\nseries using flexible versus fixed specification and linear versusnonlinear econometric models. International Journal of Fore-\ncasting ,13, 439\u2013461.\nTera\u00a8svirta, T. (2006). Forecasting economic variables with nonlinear\nmodels. In G. Elliot, C. W. J. Granger, & A. Timmermann\n(Eds.), Handbook of economic forecasting . Amsterdam 7Elsevier\nScience.\nTkacz, G. (2001). Neural network forecasting of Canadian GDP\ngrowth. International Journal of Forecasting ,17, 57\u201369.\nTong, H. (1983). Threshold models in non-linear time series\nanalysis . New York 7Springer-Verlag.\nTong, H. (1990). Non-linear time series: A dynamical system\napproach . Oxford 7Clarendon Press.\nVolterra, V. (1930). Theory of functionals and of integro-differential\nequations . New York 7Dover.\nWiener, N. (1958). Non-linear problems in random theory . London 7\nWiley.\nZhang, G., Patuwo, B. E., & Hu, M. Y. (1998). Forecasting with\nartificial networks: The state of the art. International Journal of\nForecasting ,14, 35\u201362.\nSection 7. Long memory\nAndersson, M. K. (2000). Do long-memory models have long\nmemory? International Journal of Forecasting ,16, 121\u2013124.\nBaillie, R. T., & Chung, S. -K. (2002). Modeling and forecas-\nting from trend-stationary long memory models with applica-tions to climatology. International Journal of Forecasting ,18,\n215\u2013226.\nBeran, J., Feng, Y., Ghosh, S., & Sibbertsen, P. (2002). On robust\nlocal polynomial estimation with long-memory errors. Interna-\ntional Journal of Forecasting ,18, 227\u2013241.\nBhansali, R. J., & Kokoszka, P. S. (2002). Computation of the fore-\ncast coefficients for multistep prediction of long-range dependenttime series. International Journal of Forecasting ,18, 181\u2013206.\nFranses, P. H., & Ooms, M. (1997). A periodic long-memory model\nfor quarterly UK inflation. International Journal of Forecasting ,\n13, 117\u2013126.\nGranger, C. W. J., & Joyeux, R. (1980). An introduction to long\nmemory time series models and fractional differencing. Journal\nof Time Series Analysis ,1, 15\u201329.\nHurvich, C. M. (2002). Multistep forecasting of long memory series\nusing fractional exponential models. International Journal of\nForecasting ,18, 167\u2013179.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 469", "start_char_idx": 3048, "end_char_idx": 6103, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8332f834-b7d6-40fd-be14-452612491e6d": {"__data__": {"id_": "8332f834-b7d6-40fd-be14-452612491e6d", "embedding": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec1565e1-aa8f-41e8-a78c-1be832aaff8f", "node_type": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "hash": "d470ee82c8586d5c2e4b273c7b82633773bf045edb92de128d8a944a9ed40234"}, "3": {"node_id": "4ac8c7c3-bfd8-4754-978c-d37287648cc3", "node_type": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "hash": "60c69d80a5ce567b3169813a49b4a3fb416ec1335ab61f52453bfcce53d1cf9f"}}, "hash": "1b846e2e70456c92a8b7a3ebdbe9ae905d76450d369ee747512627a6c716a3a5", "text": "Man, K. S. (2003). Long memory time series and short term\nforecasts. International Journal of Forecasting ,19, 477\u2013491.\nO\u00a8ller, L. -E. (1985). How far can changes in general business\nactivity be forecasted? International Journal of Forecasting ,1,\n135\u2013141.\nRamjee, R., Crato, N., & Ray, B. K. (2002). A note on moving\naverage forecasts of long memory processes with an application\nto quality control. International Journal of Forecasting ,18,\n291\u2013297.\nRavishanker, N., & Ray, B. K. (2002). Bayesian prediction for\nvector ARFIMA processes. International Journal of Forecast-\ning,18, 207\u2013214.\nRay, B. K. (1993a). Long-range forecasting of IBM product\nrevenues using a seasonal fractionally differenced ARMA\nmodel. International Journal of Forecasting ,9, 255\u2013269.\nRay, B. K. (1993b). Modeling long-memory processes for optimal\nlong-range prediction. Journal of Time Series Analysis ,14,\n511\u2013525.\nSmith, J., & Yadav, S. (1994). Forecasting costs incurred from unit\ndifferencing fractionally integrated processes. International\nJournal of Forecasting ,10, 507\u2013514.\nSouza, L. R., & Smith, J. (2002). Bias in the memory for\ndifferent sampling rates. International Journal of Forecasting ,\n18, 299\u2013313.\nSouza, L. R., & Smith, J. (2004). Effects of temporal aggregation on\nestimates and forecasts of fractionally integrated processes: A\nMonte-Carlo study. International Journal of Forecasting ,20,\n487\u2013502.\nSection 8. ARCH/GARCH\nAwartani, B. M. A., & Corradi, V. (2005). Predicting the\nvolatility of the S&P-500 stock index via GARCH models:\nThe role of asymmetries. International Journal of Forecasting ,\n21, 167\u2013183.\nBaillie, R. T., Bollerslev, T., & Mikkelsen, H. O. (1996).\nFractionally integrated generalized autoregressive conditionalheteroskedasticity. Journal of Econometrics ,74, 3\u201330.\nBera, A., & Higgins, M. (1993). ARCH models: Properties, esti-\nmation and testing. Journal of Economic Surveys ,7, 305\u2013365.\nBollerslev, T., & Wright, J. H. (2001). High-frequency data,\nfrequency domain inference, and volatility forecasting. Review\nof Economics and Statistics ,83, 596\u2013602.\nBollerslev, T., Chou, R. Y., & Kroner, K. F. (1992). ARCH\nmodeling in finance: A review of the theory and empirical\nevidence. Journal of Econometrics ,52, 5\u201359.\nBollerslev, T., Engle, R. F., & Nelson, D. B. (1994). ARCH models.\nIn R. F. Engle, & D. L. McFadden (Eds.), Handbook of\neconometrics, vol. IV (pp. 2959\u20133038). Amsterdam 7North-\nHolland.\nBrooks, C. (1998). Predicting stock index volatility: Can market\nvolume help? Journal of Forecasting ,17, 59\u201380.\nBrooks, C., Burke, S. P., & Persand, G. (2001). Benchmarks and the\naccuracy of GARCH model estimation. International Journal of\nForecasting ,17, 45\u201356.\nDiebold, F. X., & Lopez, J. (1995). Modeling volatility dynamics. In\nKevin Hoover (Ed.), Macroeconometrics: developments, ten-sions and prospects (pp. 427\u2013472). Boston 7Kluwer Academic\nPress.\nDoidge, C., & Wei, J. Z. (1998). Volatility forecasting and the\nefficiency of the Toronto 35 index options market. Canadian\nJournal of Administrative Sciences ,15,", "start_char_idx": 0, "end_char_idx": 3044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4ac8c7c3-bfd8-4754-978c-d37287648cc3": {"__data__": {"id_": "4ac8c7c3-bfd8-4754-978c-d37287648cc3", "embedding": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec1565e1-aa8f-41e8-a78c-1be832aaff8f", "node_type": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "hash": "d470ee82c8586d5c2e4b273c7b82633773bf045edb92de128d8a944a9ed40234"}, "2": {"node_id": "8332f834-b7d6-40fd-be14-452612491e6d", "node_type": null, "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}, "hash": "1b846e2e70456c92a8b7a3ebdbe9ae905d76450d369ee747512627a6c716a3a5"}}, "hash": "60c69d80a5ce567b3169813a49b4a3fb416ec1335ab61f52453bfcce53d1cf9f", "text": "of the Toronto 35 index options market. Canadian\nJournal of Administrative Sciences ,15, 28\u201338.\nEngle, R. F. (1982). Autoregressive conditional heteroscedasticity\nwith estimates of the variance of the United Kingdom inflation.\nEconometrica ,50, 987\u20131008.\nEngle, R. F. (2002). New frontiers for ARCH models. Manuscript\nprepared for the conference bModeling and Forecasting Finan-\ncial Volatility (Perth, Australia, 2001). Available at http://pages.stern.nyu.edu/~rengle\nEngle, R. F., & Ng, V. (1993). Measuring and testing the impact of\nnews on volatility. Journal of Finance ,48, 1749\u20131778.\nFranses, P. H., & Ghijsels, H. (1999). Additive outliers, GARCH\nand forecasting volatility. International Journal of Forecasting ,\n15, 1\u20139.\nGalbraith, J. W., & Kisinbay, T. (2005). Content horizons for\nconditional variance forecasts. International Journal of Fore-\ncasting ,21, 249\u2013260.\nGranger, C. W. J. (2002). Long memory, volatility, risk and\ndistribution. Manuscript . San Diego 7University of California\nAvailableathttp://www.cass.city.ac.uk/conferences/esrc2002/\nGranger.pdf\nHentschel, L. (1995). All in the family: Nesting symmetric and\nasymmetric GARCH models. Journal of Financial Economics ,\n39, 71\u2013104.\nKaranasos, M. (2001). Prediction in ARMA models with GARCH\nin mean effects. Journal of Time Series Analysis ,22, 555\u2013576.\nKroner, K. F., Kneafsey, K. P., & Claessens, S. (1995). Forecasting\nvolatility in commodity markets. Journal of Forecasting ,14,\n77\u201395.\nPagan, A. (1996). The econometrics of financial markets. Journal of\nEmpirical Finance ,3, 15\u2013102.\nPoon, S. -H., & Granger, C. W. J. (2003). Forecasting volatility in\nfinancial markets: A review. Journal of Economic Literature ,\n41, 478\u2013539.\nPoon, S. -H., & Granger, C. W. J. (2005). Practical issues\nin forecasting volatility. Financial Analysts Journal ,61,\n45\u201356.\nSabbatini, M., & Linton, O. (1998). A GARCH model of the\nimplied volatility of the Swiss market index from option prices.\nInternational Journal of Forecasting ,14, 199\u2013213.\nTaylor, S. J. (1987). Forecasting the volatility of currency exchange\nrates. International Journal of Forecasting ,3, 159\u2013170.\nVasilellis, G. A., & Meade, N. (1996). Forecasting volatility for\nportfolio selection. Journal of Business Finance and Account-\ning,23, 125\u2013143.\nSection 9. Count data forecasting\nBra \u00a8nna \u00a8s, K. (1995). Prediction and control for a time-series\ncount data model. International Journal of Forecasting ,11,\n263\u2013270.\nBra \u00a8nna \u00a8s, K., Hellstro \u00a8m, J., & Nordstro \u00a8m, J. (2002). A new approach\nto modelling and forecasting monthly guest nights in hotels.\nInternational Journal of Forecasting ,18, 19\u201330.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 470", "start_char_idx": 2956, "end_char_idx": 5673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd7e9bd3-9ede-4022-9cff-14dcd16bca56": {"__data__": {"id_": "bd7e9bd3-9ede-4022-9cff-14dcd16bca56", "embedding": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dc415f5-1f27-4d16-a0c1-c03f15ad0889", "node_type": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "hash": "22a1cd4278dc272aea5f97540b84d41d20d05ea398e6e1c1a3bf0f6de8504218"}, "3": {"node_id": "db02def1-dc5f-4a97-82f5-e20f25c36ca3", "node_type": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "hash": "0195b4c60a343186ac87053a9e3da9e1117afeab66183c3466c323bb241a5e9a"}}, "hash": "ef4eff346915ced17488f43f4794885fcc8d49a481bbf4d1dc453b9e85306075", "text": "Croston, J. D. (1972). Forecasting and stock control for intermittent\ndemands. Operational Research Quarterly ,23, 289\u2013303.\nDiebold, F. X., Gunther, T. A., & Tay, A. S. (1998). Evaluating\ndensity forecasts, with applications to financial risk manage-\nment. International Economic Review ,39, 863\u2013883.\nDiggle, P. J., Heagerty, P., Liang, K. -Y., & Zeger, S. (2002).\nAnalysis of longitudinal data (2nd ed.). Oxford 7Oxford\nUniversity Press.\nFreeland, R. K., & McCabe, B. P. M. (2004). Forecasting discrete\nvalued low count time series. International Journal of Fore-\ncasting ,20, 427\u2013434.\nGrunwald, G. K., Hyndman, R. J., Tedesco, L. M., & Tweedie, R. L.\n(2000). Non-Gaussian conditional linear AR(1) models. Aus-\ntralian and New Zealand Journal of Statistics ,42, 479\u2013495.\nJohnston, F. R., & Boylan, J. E. (1996). Forecasting intermittent\ndemand: A comparative evaluation of Croston Tmethod.\nInternational Journal of Forecasting ,12, 297\u2013298.\nMcCabe, B. P. M., & Martin, G. M. (2005). Bayesian predictions of\nlow count time series. International Journal of Forecasting ,21,\n315\u2013330.\nSyntetos, A. A., & Boylan, J. E. (2005). The accuracy of\nintermittent demand estimates. International Journal of Fore-\ncasting ,21, 303\u2013314.\nWillemain, T. R., Smart, C. N., Shockor, J. H., & DeSautels, P. A.\n(1994). Forecasting intermittent demand in manufacturing: Acomparative evaluation of Croston Ts method. International\nJournal of Forecasting ,10, 529\u2013538.\nWillemain, T. R., Smart, C. N., & Schwarz, H. F. (2004). A new\napproach to forecasting intermittent demand for service partsinventories. International Journal of Forecasting ,20, 375\u2013387.\nSection 10. Forecast evaluation and accuracy measures\nAhlburg, D. A., Chatfield, C., Taylor, S. J., Thompson, P. A.,\nWinkler, R. L., Murphy A. H., et al. (1992). A commentary on\nerror measures. International Journal of Forecasting ,8, 99 \u2013 111.\nArmstrong, J. S., & Collopy, F. (1992). Error measures for\ngeneralizing about forecasting methods: Empirical comparisons.\nInternational Journal of Forecasting ,8, 69\u201380.\nChatfield, C. (1988). Editorial: Apples, oranges and mean square\nerror. International Journal of Forecasting ,4, 515\u2013518.\nClements, M. P., & Hendry, D. F. (1993). On the limitations of\ncomparing mean square forecast errors. Journal of Forecasting ,\n12, 617\u2013637.\nDiebold, F. X., & Mariano, R. S. (1995). Comparing predictive\naccuracy. Journal of Business and Economic Statistics ,13\n,\n253\u2013263.\nFildes, R. (1992). The evaluation of extrapolative forecasting\nmethods. International Journal of Forecasting ,8, 81\u201398.\nFildes, R., & Makridakis, S. (1988). Forecasting and loss functions.\nInternational Journal of Forecasting ,4, 545\u2013550.\nFildes, R., Hibon, M., Makridakis, S., & Meade, N. (1998). General-\nising about univariate forecasting methods: Further empirical\nevidence. International Journal of Forecasting ,14, 339\u2013358.\nFlores, B. (1989). The utilization of the Wilcoxon test to compare\nforecasting methods: A note. International Journal of Fore-\ncasting ,5, 529\u2013535.Goodwin, P., & Lawton, R. (1999).", "start_char_idx": 0, "end_char_idx": 3051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db02def1-dc5f-4a97-82f5-e20f25c36ca3": {"__data__": {"id_": "db02def1-dc5f-4a97-82f5-e20f25c36ca3", "embedding": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2dc415f5-1f27-4d16-a0c1-c03f15ad0889", "node_type": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "hash": "22a1cd4278dc272aea5f97540b84d41d20d05ea398e6e1c1a3bf0f6de8504218"}, "2": {"node_id": "bd7e9bd3-9ede-4022-9cff-14dcd16bca56", "node_type": null, "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}, "hash": "ef4eff346915ced17488f43f4794885fcc8d49a481bbf4d1dc453b9e85306075"}}, "hash": "0195b4c60a343186ac87053a9e3da9e1117afeab66183c3466c323bb241a5e9a", "text": "529\u2013535.Goodwin, P., & Lawton, R. (1999). On the asymmetry of the\nsymmetric MAPE. International Journal of Forecasting ,15,\n405\u2013408.\nGranger, C. W. J., & Jeon, Y. (2003a). A time\u2013distance criterion for\nevaluating forecasting models. International Journal of Fore-\ncasting ,19, 199\u2013215.\nGranger, C. W. J., & Jeon, Y. (2003b). Comparing forecasts of\ninflation using time distance. International Journal of Fore-\ncasting ,19, 339\u2013349.\nHarvey, D., Leybourne, S., & Newbold, P. (1997). Testing the\nequality of prediction mean squared errors. International\nJournal of Forecasting ,13, 281\u2013291.\nKoehler, A. B. (2001). The asymmetry of the sAPE measure and\nother comments on the M3-competition. International Journal\nof Forecasting ,17, 570\u2013574.\nMahmoud, E. (1984). Accuracy in forecasting: A survey. Journal of\nForecasting ,3, 139\u2013159.\nMakridakis, S. (1993). Accuracy measures: Theoretical and\npractical concerns. International Journal of Forecasting ,9,\n527\u2013529.\nMakridakis, S., & Hibon, M. (2000). The M3-competition: Results,\nconclusions and implications. International Journal of Fore-\ncasting ,16, 451\u2013476.\nMakridakis, S., Andersen, A., Carbone, R., Fildes, R., Hibon, M.,\nLewandowski, R., et al. (1982). The accuracy of extrapolation(time series) methods: Results of a forecasting competition.\nJournal of Forecasting ,1, 111\u2013153.\nMakridakis, S., Wheelwright, S. C., & Hyndman, R. J. (1998).\nForecasting: Methods and applications (3rd ed.). New York 7\nJohn Wiley and Sons.\nMcCracken, M. W. (2004). Parameter estimation and tests of equal\nforecast accuracy between non-nested models. International\nJournal of Forecasting ,20, 503\u2013514.\nSullivan, R., Timmermann, A., & White, H. (2003). Forecast\nevaluation with shared data sets. International Journal of\nForecasting ,19, 217\u2013227.\nTheil, H. (1966). Applied economic forecasting . Amsterdam 7North-\nHolland.\nThompson, P. A. (1990). An MSE statistic for comparing forecast\naccuracy across series. International Journal of Forecasting ,6,\n219\u2013227.\nThompson, P. A. (1991). Evaluation of the M-competition forecasts\nvia log mean squared error ratio. International Journal of\nForecasting ,7, 331\u2013334.\nWun, L. -M., & Pearn, W. L. (1991). Assessing the statistical\ncharacteristics of the mean absolute error of forecasting.International Journal of Forecasting ,7, 335\u2013337.\nSection 11. Combining\nAksu, C., & Gunter, S. (1992). An empirical analysis of the\naccuracy of SA, OLS, ERLS and NRLS combination forecasts.\nInternational Journal of Forecasting ,8, 27\u201343.\nBates, J. M., & Granger, C. W. J. (1969). Combination of forecasts.\nOperations Research Quarterly ,20, 451\u2013468.\nBunn, D. W. (1985). Statistical efficiency in the linear combination\nof forecasts. International Journal of Forecasting ,1, 151\u2013163.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 471", "start_char_idx": 3010, "end_char_idx": 5842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cae98f70-b3b3-4d9c-9dfb-67e5ca000312": {"__data__": {"id_": "cae98f70-b3b3-4d9c-9dfb-67e5ca000312", "embedding": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f77fd31-795f-4704-9908-667c24140881", "node_type": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "hash": "4f3e015b18ad2102faee39ac423a108bdc964f94965be1b9e912c76ec148ae4e"}, "3": {"node_id": "a17b4fab-1952-4e52-a7ec-9f34198adfcb", "node_type": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "hash": "1156ffe8218e868ae618dfe78f46efb3f4d45b884e465357d1c116a09eb01bab"}}, "hash": "c6ae125d4a9f6525cc733182339dfe2cb53d99429330cde03b452f98862a2a5a", "text": "Clemen, R. T. (1989). Combining forecasts: A review and annotated\nbiography (with discussion). International Journal of Forecast-\ning,5, 559\u2013583.\nde Menezes, L. M., & Bunn, D. W. (1998). The persistence of\nspecification problems in the distribution of combined forecast\nerrors. International Journal of Forecasting ,14, 415\u2013426.\nDeutsch, M., Granger, C. W. J., & Tera \u00a8svirta, T. (1994). The\ncombination of forecasts using changing weights. International\nJournal of Forecasting ,10, 47\u201357.\nDiebold, F. X., & Pauly, P. (1990). The use of prior information in\nforecast combination. International Journal of Forecasting ,6,\n503\u2013508.\nFang, Y. (2003). Forecasting combination and encompassing tests.\nInternational Journal of Forecasting ,19, 87\u201394.\nFiordaliso, A. (1998). A nonlinear forecast combination method\nbased on Takagi-Sugeno fuzzy systems. International Journal\nof Forecasting ,14, 367\u2013379.\nGranger, C. W. J. (1989). Combining forecasts\u2014twenty years later.\nJournal of Forecasting ,8, 167\u2013173.\nGranger, C. W. J., & Ramanathan, R. (1984). Improved methods of\ncombining forecasts. Journal of Forecasting ,3, 197\u2013204.\nGunter, S. I. (1992). Nonnegativity restricted least squares\ncombinations. International Journal of Forecasting ,8, 45\u201359.\nHendry, D. F., & Clements, M. P. (2002). Pooling of forecasts.\nEconometrics Journal ,5, 1\u201331.\nHibon, M., & Evgeniou, T. (2005). To combine or not to combine:\nSelecting among forecasts and their combinations. International\nJournal of Forecasting ,21, 15\u201324.\nKamstra, M., & Kennedy, P. (1 998). Combining qualitative\nforecasts using logit. International Journal of Forecasting ,14,\n83\u201393.\nMiller, S. M., Clemen, R. T., & Winkler, R. L. (1992). The effect of\nnonstationarity on combined forecasts. International Journal of\nForecasting ,7, 515\u2013529.\nTaylor, J. W., & Bunn, D. W. (1999). Investigating improvements in\nthe accuracy of prediction intervals for combinations offorecasts: A simulation study. International Journal of Fore-\ncasting ,15, 325\u2013339.\nTerui, N., & van Dijk, H. K. (2002). Combined forecasts from linear\nand nonlinear time series models. International Journal of\nForecasting ,18, 421\u2013438.\nWinkler, R. L., & Makridakis, S. (1983). The combination\nof forecasts.\nJournal of the Royal Statistical Society (A) ,146,\n150\u2013157.\nZou, H., & Yang, Y. (2004). Combining time series models for\nforecasting. International Journal of Forecasting ,20, 69\u201384.\nSection 12. Prediction intervals and densities\nChatfield, C. (1993). Calculating interval forecasts. Journal of\nBusiness and Economic Statistics ,11, 121\u2013135.\nChatfield, C., & Koehler, A. B. (1991). On confusing lead time\ndemand with h-period-ahead forecasts. International Journal of\nForecasting ,7, 239\u2013240.\nClements, M. P., & Smith, J. (2002). Evaluating multivariate\nforecast densities: A comparison of two approaches. Interna-\ntional Journal of Forecasting ,18, 397\u2013407.Clements, M. P., & Taylor, N. (2001). Bootstrapping prediction\nintervals for autoregressive models. International Journal of\nForecasting ,17, 247\u2013267.\nDiebold, F. X., Gunther, T. A., & Tay, A. S. (1998). Evaluating\ndensity forecasts with applications to financial risk", "start_char_idx": 0, "end_char_idx": 3144, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a17b4fab-1952-4e52-a7ec-9f34198adfcb": {"__data__": {"id_": "a17b4fab-1952-4e52-a7ec-9f34198adfcb", "embedding": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7f77fd31-795f-4704-9908-667c24140881", "node_type": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "hash": "4f3e015b18ad2102faee39ac423a108bdc964f94965be1b9e912c76ec148ae4e"}, "2": {"node_id": "cae98f70-b3b3-4d9c-9dfb-67e5ca000312", "node_type": null, "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}, "hash": "c6ae125d4a9f6525cc733182339dfe2cb53d99429330cde03b452f98862a2a5a"}}, "hash": "1156ffe8218e868ae618dfe78f46efb3f4d45b884e465357d1c116a09eb01bab", "text": "S. (1998). Evaluating\ndensity forecasts with applications to financial risk management.\nInternational Economic Review ,39, 863\u2013883.\nDiebold, F. X., Hahn, J. Y., & Tay, A. S. (1999). Multivariate\ndensity forecast evaluation and calibration in financial risk\nmanagement: High-frequency returns in foreign exchange.Review of Economics and Statistics ,81, 661\u2013673.\nGrigoletto, M. (1998). Bootstrap prediction intervals for autore-\ngressions: Some alternatives. International Journal of Forecast-\ning,14, 447\u2013456.\nHyndman, R. J. (1995). Highest density forecast regions for non-\nlinear and non-normal time series models. Journal of Forecast-\ning,14, 431\u2013441.\nKim, J. A. (1999). Asymptotic and bootstrap prediction regions for\nvector autoregression. International Journal of Forecasting ,15,\n393\u2013403.\nKim, J. A. (2004a). Bias-corrected bootstrap prediction regions for\nvector autoregression. Journal of Forecasting ,23, 141\u2013154.\nKim, J. A. (2004b). Bootstrap prediction intervals for autoregression\nusing asymptotically mean-unbiased estimators. International\nJournal of Forecasting ,20, 85\u201397.\nKoehler, A. B. (1990). An inappropriate prediction interval.\nInternational Journal of Forecasting ,6, 557\u2013558.\nLam, J. -P., & Veall, M. R. (2002). Bootstrap prediction intervals for\nsingle period regression forecasts. International Journal of\nForecasting ,18, 125\u2013130.\nLefranc \u00b8ois, P. (1989). Confidence intervals for non-stationary\nforecast errors: Some empirical results for the series in\nthe M-competition.\nInternational Journal of Forecasting ,5,\n553\u2013557.\nMakridakis, S., & Hibon, M. (1987). Confidence intervals: An\nempirical investigation of the series in the M-competition.International Journal of Forecasting ,3, 489\u2013508.\nMasarotto, G. (1990). Bootstrap prediction intervals for autore-\ngressions. International Journal of Forecasting ,6, 229\u2013239.\nMcCullough, B. D. (1994). Bootstrapping forecast intervals:\nAn application to AR(p) models. Journal of Forecasting ,13,\n51\u201366.\nMcCullough, B. D. (1996). Consistent forecast intervals when the\nforecast-period exogenous variables are stochastic. Journal of\nForecasting ,15, 293\u2013304.\nPascual, L., Romo, J., & Ruiz, E. (2001). Effects of parameter\nestimation on prediction densities: A bootstrap approach.International Journal of Forecasting ,17, 83\u2013103.\nPascual, L., Romo, J., & Ruiz, E. (2004). Bootstrap predictive\ninference for ARIMA processes. Journal of Time Series\nAnalysis ,25, 449\u2013465.\nPascual, L., Romo, J., & Ruiz, E. (2005). Bootstrap prediction\nintervals for power-transformed time series. International\nJournal of Forecasting ,21, 219\u2013236.\nReeves, J. J. (2005). Bootstrap prediction intervals for ARCH\nmodels. International Journal of Forecasting ,21, 237\u2013248.\nTay, A. S., & Wallis, K. F. (2000). Density forecasting: A survey.\nJournal of Forecasting ,19, 235\u2013254.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 472", "start_char_idx": 3069, "end_char_idx": 5979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d008b1ee-788d-467f-88bf-d9da6886bc63": {"__data__": {"id_": "d008b1ee-788d-467f-88bf-d9da6886bc63", "embedding": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a303b42-cd78-408e-b164-0c66108f5b16", "node_type": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "hash": "82d7672721ca4198f24409c0e3a83c1ed73f2305d05e2c82849bb6e678d65aca"}, "3": {"node_id": "9b1f12ca-379e-4036-b639-1bacbba77bd2", "node_type": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "hash": "8e122227738f3032b2f3027d2a3116f11c8cd5f8c3c1cd648a96f11f0810cc51"}}, "hash": "7ec983aeba2adcf26826f3164684f8bfc84fbe883c5ec39dd3be5344090a4a56", "text": "Wall, K. D., & Stoffer, D. S. (2002). A state space approach to\nbootstrapping conditional forecasts in ARMA models. Journal\nof Time Series Analysis ,23, 733\u2013751.\nWallis, K. F. (1999). Asymmetric density forecasts of inflation and\nthe Bank of England\u2019s fan chart. National Institute Economic\nReview ,167, 106\u2013112.\nWallis, K. F. (2003). Chi-squared tests of interval and density\nforecasts, and the Bank of England fan charts. International\nJournal of Forecasting ,19, 165\u2013175.\nSection 13. A look to the future\nAndersen, T. G., Bollerslev, T., Diebold, F. X., & Labys, P. (2003).\nModeling and forecasting realized volatility. Econometrica ,71,\n579\u2013625.\nArmstrong, J. S. (2001). Suggestions for further research .\nwww.forecastingprinciples.com/researchers.html\nCasella, G., et al., (Eds.). (2000). Vignettes for the year 2000. Journal\nof the American Statistical Association ,95,1269\u20131368.\nChatfield, C. (1988). The future of time-series forecasting.\nInternational Journal of Forecasting ,4, 411\u2013419.\nChatfield, C. (1997). Forecasting in the 1990s. The Statistician ,46,\n461\u2013473.\nClements, M. P. (2003). Editorial: Some possible directions for\nfuture research. International Journal of Forecasting ,19, 1\u20133.\nCogger, K. C. (1988). Proposals for research in time series\nforecasting. International Journal of Forecasting ,4, 403\u2013410.\nDawes, R., Fildes, R., Lawrence, M., & Ord, J. K. (1994). The past\nand the future of forecasting research. International Journal of\nForecasting ,10, 151\u2013159.\nDe Gooijer, J. G. (1990). Editorial: The role of time series analysis\nin forecasting: A personal view. International Journal of\nForecasting ,6, 449\u2013451.\nDe Gooijer, J. G., & Gannoun, A. (2000). Nonparametric\nconditional predictive regions for time series. Computational\nStatistics and Data Analysis ,33, 259\u2013275.\nDekimpe, M. G., & Hanssens, D. M. (2000). Time-series models in\nmarketing: Past, present and future. International Journal of\nResearch in Marketing ,17, 183\u2013193.\nEngle, R. F., & Manganelli, S. (2004). CAViaR: Conditional\nautoregressive value at risk by regression quantiles. Journal of\nBusiness and Economic Statistics ,22, 367\u2013381.Engle, R. F., & Russell, J. R. (1998). Autoregressive conditional\nduration: A new model for irregularly spaced transactions data.Econometrica ,66\n, 1127\u20131162.\nForni, M., Hallin, M., Lippi, M., & Reichlin, L. (2005). The\ngeneralized dynamic factor model: One-sided estimation and\nforecasting. Journal of the American Statistical Association ,\n100, 830\u2013840.\nKoenker, R. W., & Bassett, G. W. (1978). Regression quantiles.\nEconometrica ,46, 33\u201350.\nOrd, J. K. (1988). Future developments in forecasting: The\ntime series connexion. International Journal of Forecasting ,4,\n389\u2013401.\nPen\u02dca, D., & Poncela, P. (2004). Forecasting with nonstation-\nary dynamic factor models. Journal of Econometrics ,119,\n291\u2013321.\nPolonik, W., & Yao, Q. (2000). Conditional minimum volume\npredictive regions for stochastic processes. Journal of the\nAmerican Statistical Association ,95, 509\u2013519.\nRamsay, J. O., & Silverman, B. W. (1997). Functional data analysis\n(2nd ed. 2005). New York", "start_char_idx": 0, "end_char_idx": 3090, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9b1f12ca-379e-4036-b639-1bacbba77bd2": {"__data__": {"id_": "9b1f12ca-379e-4036-b639-1bacbba77bd2", "embedding": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5a303b42-cd78-408e-b164-0c66108f5b16", "node_type": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "hash": "82d7672721ca4198f24409c0e3a83c1ed73f2305d05e2c82849bb6e678d65aca"}, "2": {"node_id": "d008b1ee-788d-467f-88bf-d9da6886bc63", "node_type": null, "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}, "hash": "7ec983aeba2adcf26826f3164684f8bfc84fbe883c5ec39dd3be5344090a4a56"}}, "hash": "8e122227738f3032b2f3027d2a3116f11c8cd5f8c3c1cd648a96f11f0810cc51", "text": "W. (1997). Functional data analysis\n(2nd ed. 2005). New York 7Springer-Verlag.\nStock, J. H., & Watson, M. W. (1999). A comparison of linear and\nnonlinear models for forecasting macroeconomic time series. In\nR. F. Engle, & H. White (Eds.), Cointegration, causality and\nforecasting (pp. 1\u201344). Oxford 7Oxford University Press.\nStock, J. H., & Watson, M. W. (2002). Forecasting using principal\ncomponents from a large number of predictors. Journal of the\nAmerican Statistical Association ,97, 1167\u20131179.\nStock, J. H., & Watson, M. W. (2004). Combination forecasts of\noutput growth in a seven-country data set. Journal of\nForecasting ,23, 405\u2013430.\nTera\u00a8svirta, T. (2006). Forecasting economic variables with nonlinear\nmodels. In G. Elliot, C. W. J. Granger, & A. Timmermann(Eds.), Handbook of economic forecasting. Amsterdam 7Elsevier\nScience.\nTsay, R. S. (2000). Time series and forecasting: Brief history and\nfuture research. Journal of the American Statistical Association ,\n95, 638\u2013643.\nYao, Q., & Tong, H. (1995). On initial-condition and prediction in\nnonlinear stochastic systems. Bulletin International Statistical\nInstitute ,IP10.3 , 395\u2013412.J.G. De Gooijer, R.J. Hyndman / International Journal of Forecasting 22 (2006) 443\u2013473 473", "start_char_idx": 3030, "end_char_idx": 4267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"f5b723fc-2a2f-4948-a3b3-1f653cff943c": {"node_ids": ["1fcb2e57-dd0b-432a-8d0b-4a85370a6d34", "58f3250a-3c15-49d8-a6b1-4ce7d7ec8d7d"], "metadata": {"page_label": "358", "file_name": "Clasification.pdf"}}, "08c34df3-3bc1-45fe-9bbb-9f588c0d1807": {"node_ids": ["26469a1c-1267-440e-9c92-4d5c000808ce", "15d0316f-b6c7-4c79-808f-68aff8e2b7f8"], "metadata": {"page_label": "359", "file_name": "Clasification.pdf"}}, "0c0b507a-870b-4dbe-8711-79f546a86280": {"node_ids": ["463e26d5-5bf8-4df1-8310-da982fbafee3", "a85b3331-8a20-44d7-b6c1-9e9c901fe5dd", "6930ab06-3b63-4130-a205-a8f9663830b7"], "metadata": {"page_label": "360", "file_name": "Clasification.pdf"}}, "5ac57d1d-b8f7-49b6-a56d-e65f186921e0": {"node_ids": ["727b5520-6845-40b1-b734-0a38b196bf67", "0c4d7c05-41d8-415d-9814-2916239e0a89", "8bddc558-f028-4026-814c-7f84ef496937"], "metadata": {"page_label": "361", "file_name": "Clasification.pdf"}}, "eae96a5e-8f42-4a52-bfae-1a9e0905f458": {"node_ids": ["63c546c7-41a0-49fc-9c62-23f2a98a1ec6", "829c1fcf-275d-4e8f-9680-c71989401abe", "91ec94eb-0a66-42fc-bb91-773947f3a7b1"], "metadata": {"page_label": "362", "file_name": "Clasification.pdf"}}, "8379f36a-f221-43f2-b1fd-ceb2784fdca2": {"node_ids": ["25c627e2-0801-4294-a18b-157a0edf0b48", "2946aa2b-e791-47ea-b86e-9cb9ce6eea43", "c4b7764a-daea-409f-ae9d-701d43e8cbf9"], "metadata": {"page_label": "363", "file_name": "Clasification.pdf"}}, "9cf803f2-ac0e-4462-8207-dff6f8aec554": {"node_ids": ["0fec8e8d-56cf-4489-8de0-b9b7a6028d34", "424857bc-8bdc-4854-983b-0d9c8c3b7da6"], "metadata": {"page_label": "364", "file_name": "Clasification.pdf"}}, "904052d6-e23e-4a7a-895d-934074a819dc": {"node_ids": ["2b6644d8-c129-402d-a4f4-f3e127145b3d", "a9429596-81c8-4c43-8b73-a125392a8be6"], "metadata": {"page_label": "365", "file_name": "Clasification.pdf"}}, "cfc341b8-7c7c-42bd-a397-6656a6719120": {"node_ids": ["5abf8005-f0f5-4f14-aa1f-f1e79bd64672", "a7ee1c29-b53b-4c52-b530-b2dbaa18ff2d", "cbca2dff-8b71-46ca-8612-3a76543a55f9"], "metadata": {"page_label": "366", "file_name": "Clasification.pdf"}}, "05100dcf-aae1-4774-a8fa-1153b914e00b": {"node_ids": ["c8c42a8c-dd74-4138-af03-344135c7165a", "fd983614-a094-4a87-abbd-4dfe91740d3d"], "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}}, "0285f646-8be1-4699-a3f0-38b1829c02bc": {"node_ids": ["962f9d3f-47d2-4b75-80d2-c6e383155097", "cb9c835c-b4cb-46f9-9190-781b786f60c6"], "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}}, "18393476-d62c-410e-b17f-f795cc1ec376": {"node_ids": ["85703d6a-0930-4931-8f74-2ee7d93f5bd5", "0dee2263-8357-4a25-b57d-59371e5a7f68"], "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}}, "c8f4ef6c-2bd8-408e-8924-a330d3e0c356": {"node_ids": ["5555f92c-7851-49e7-8f37-6e1e27ed8519"], "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}}, "9b071042-368b-4574-916c-56171c9bcede": {"node_ids": ["353d37da-bb3c-447d-b3d5-5274abeb6317"], "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}}, "919ef11e-1fa6-4961-8a5e-3f2ba143981a": {"node_ids": ["b2203988-5ab2-4c03-8306-0deea9280a12", "cfd6fe0d-bc37-4e17-be7b-c55d3fff9cde"], "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}}, "77519e60-e254-4414-a4d8-dd26e47c0923": {"node_ids": ["946381a4-0c98-48f8-95af-2a5a5d4bcafe", "6aa5575b-33d5-4c9f-a765-f8a9516cbed0"], "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}}, "701e5445-9e31-4562-aa98-1f374562f74f": {"node_ids": ["25996ad9-fc2c-4de5-88b5-e76be56d489e", "00ae5058-315c-4129-89c9-019e4da2e62f"], "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}}, "89236ff8-14bb-4e0d-b565-9320faa60783": {"node_ids": ["6e0bed58-196c-48cf-9fd7-2cea388f8377", "63936964-2624-4e35-9380-a5b697e69a19"], "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}}, "6a776b20-f761-45dc-8e59-47d70437a52a": {"node_ids": ["afcc21c7-378b-4a88-9173-0ba1e0f53b3c", "83c51dca-eaa2-46c6-91dd-40b7f5e000b8"], "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}}, "4fce819b-421d-49b9-b995-40ee1ba0d387": {"node_ids": ["506d1584-d7bb-41fd-9985-e2bc26359c10", "ef0c1f7c-31c4-478d-8778-1ea866949bf8"], "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}}, "6b0ac33d-3b6a-4845-8f1c-8475b84ee395": {"node_ids": ["8a3dc5b0-d106-49a9-b6e4-1b195fde88a5", "3f2f4ede-3176-49ec-b114-961912cc699c"], "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}}, "2c4ed6bb-179c-49bf-95fe-8d948b9527d2": {"node_ids": ["b7d98762-2f56-4370-b6be-ce980d1ec451", "35f0cb4a-44da-4a09-a945-5903a16263e6"], "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}}, "c35f800b-c539-4a91-ac9d-1e568091bcfc": {"node_ids": ["060d849e-db0f-46ab-af16-6cbf6f335f10", "d0103b5f-a473-4495-a086-1256179309af"], "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}}, "faf94140-9051-486b-bd76-9b19bbc6de76": {"node_ids": ["e45e714b-bb58-428c-8583-971665ec4fc5", "d0a1f3ca-4ae2-471f-a970-47b832c5f4f2"], "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}}, "a08f6232-65ec-47b4-8564-c6274517669d": {"node_ids": ["c38c6520-a97c-4a99-bdb9-5548c7fc6665", "b4ebd8f0-dbbc-4820-9b83-a51aff565db9"], "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}}, "1223bf80-dba8-4575-939c-432249e037ac": {"node_ids": ["15525363-3d90-43cd-a223-601934df3bcf", "f96e73ad-9c95-49fa-8f50-e3e7b617fa03"], "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}}, "3c0aa3d7-f0f3-49e5-a19c-0ba2eee86500": {"node_ids": ["532cb39b-74f8-401f-ad08-4af5fe2da04e", "39ea1c51-14b2-4f40-8115-117c27a1f724"], "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}}, "7906c566-aa7c-4424-90f1-0c02cb5faa53": {"node_ids": ["01813f96-3e63-47ed-8779-f0ca8a50b3db", "99e41b42-5242-4e72-8c10-73986a9a27f1"], "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}}, "551278d9-b81e-4bb4-ad7e-9a814ccd95a8": {"node_ids": ["a71a76c7-1e7b-4e8c-b48c-c39635de9755", "dd17a6f4-7ea6-4a76-a6c6-7fa3e8a6629f"], "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}}, "37f2e2f5-dcd1-431c-a9fb-d4b7baf3803d": {"node_ids": ["c7c1381a-a188-4635-ac50-fc52a6c4541b", "58dfe800-c747-4d60-97bd-beddd57d59f1"], "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}}, "86deeb8e-300b-4588-b9c4-3a595f07a249": {"node_ids": ["dd3a61aa-aa78-4316-9ef8-3bf28352765b", "18a240bd-af7f-43c0-a008-f23c6eae1915"], "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}}, "b3c4f649-1a73-4279-ae68-a261bb02aa64": {"node_ids": ["335b5d31-c8fa-4764-a2fc-2c7235adf685"], "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}}, "088ffcb3-8104-413c-9cd0-af1d1c3a7445": {"node_ids": ["fb4ec6a4-a3ae-418b-84b5-4e6f9d38d8f3", "ae0974cb-1360-421d-aa06-4fa9fd539828"], "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}}, "bab26705-5314-421d-8a00-aa0ceb0d94ea": {"node_ids": ["2743dcc2-71bb-4d6d-af3f-0f9e878efdb3", "40085889-33eb-4c53-9be0-99b3346eb316"], "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}}, "55016afa-ef68-433b-ab36-85af350cde22": {"node_ids": ["15e4f284-42a6-43ad-9dab-4c7e823b1ad9"], "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}}, "702049fa-a8c4-4f11-98b8-f3cdabe21869": {"node_ids": ["c1af0d32-7d3c-4e55-a822-953be8f57a9e", "b13b3430-9d01-4bab-bbe1-075bee6e9263"], "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}}, "f3249eb0-b356-4531-9bf2-ade92df3a8d6": {"node_ids": ["67defbe9-d008-41d5-ba59-33112621edca", "f54b8e69-215b-4141-a228-78566b779997"], "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}}, "1a512369-3428-41ba-881b-94bacaafa392": {"node_ids": ["cc770eee-2b46-49c3-b4f9-cfd34c9e0bb5", "4517a38b-d3fb-47cd-a1e9-42bf07b04f51", "d6b5f83e-0bb6-41ee-9baa-f844ec9d99c4", "810113cd-21c4-4998-991c-be7e184bb99d"], "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}}, "24a3dd37-d46d-40cf-a715-1dca69ce6102": {"node_ids": ["ae71050f-527b-4443-a07a-b5f231c8e5d7", "8446f964-cbd4-4078-8677-8cf38f63f83e", "97664fa0-9070-4279-9f21-065320de12c7", "d630ecdd-40eb-4969-b37e-c977e1cd9d17"], "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}}, "d4432947-93c5-44a6-9506-440f898f3273": {"node_ids": ["47bd0dde-315d-489d-ab4d-fac58b5b7527", "41b4e924-ed5e-42db-a942-13397fba5eef", "2066e1f2-0af8-47e7-8dd9-0fe23c989ace", "014cd91d-0ed6-4168-aab3-68a1d818a97f"], "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}}, "41166736-536f-4f15-919b-2bde324e2364": {"node_ids": ["33599ae9-74bc-484d-8322-7cc5901e47fd", "040d2a55-9557-4afb-b016-49b0c056b485", "7183d9ab-bd9e-498a-b905-9c5dfc9aa1d8", "398aac6d-f3ec-4a24-955b-3efea64e7cb9"], "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}}, "f20d19db-543f-4d8e-b038-4715499fb0a0": {"node_ids": ["968838ad-f5fc-4a41-99f3-062c6abed929", "87f6e9dc-a389-4d72-857e-ba0221e36cdb", "5dcc2ded-551c-4808-8bba-58debc0e37c8", "7a7c5737-1d34-46b6-87dc-29d12d88bcb4"], "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}}, "b2dccad2-a69d-41e5-8d00-adc812359e09": {"node_ids": ["fa30bdbf-604b-4ce6-ae49-75bfa61db9c4", "11b43a7e-b2cb-4edb-854f-234a6e38fb90"], "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}}, "f96d6ac3-6f73-4e3e-ace1-a77a1d5e771e": {"node_ids": ["0f6bafeb-afa6-488b-b63f-fd4971a1f0bb"], "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}}, "cb1d3063-d16e-403c-8b4f-1d3ba61e60bc": {"node_ids": ["8a96cdbe-16ea-487c-aa14-7d502b1f171e", "8ac67088-1023-4df3-9a7d-baf022cf4b90"], "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}}, "9d6bcaba-3b0b-4c08-b13a-7a0f0608a212": {"node_ids": ["35ab5ff0-9e87-4d1b-9502-358109c33b21"], "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}}, "a75edbeb-8b17-4b85-aa05-bf0d6cc84390": {"node_ids": ["95340ff0-7c82-453e-bcf4-dea30b5beb85"], "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}}, "9ce75a46-ef3b-4b59-9f97-770e1270b528": {"node_ids": ["715b60c0-0a1f-4154-ae38-7ed6b53d77fb", "5d061b8c-a9a9-4c63-9046-778e3d59773d"], "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}}, "de8c06fd-f6b5-4d1c-82fe-63ff05835138": {"node_ids": ["6062226d-a498-404a-ad40-71d80ab1339c"], "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}}, "f8d615b7-a3ff-4a66-9db1-90b424c1f073": {"node_ids": ["a700928c-a33b-44d5-a855-4bf3702f94f1"], "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}}, "98401388-468a-4828-a610-37ab956db0bb": {"node_ids": ["1501de03-1484-4e57-b4af-bbf848b1012c"], "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}}, "469995a2-27b0-416f-9e96-02aa527d4810": {"node_ids": ["6c5d0b69-48fb-4116-a3a7-180340fc4f6a"], "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}}, "56c156b3-8227-4439-8ae8-9562c6d3cad9": {"node_ids": ["deb80a10-df07-48b3-905c-313c4bb4a014"], "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}}, "a4974486-f0b2-4bd0-bc37-2bf4d107babd": {"node_ids": ["e47ab53c-3b04-43e5-af6b-0ba1a3fce25f", "4d33b75c-78ed-4dbd-abb7-88a2110a240a"], "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}}, "750fd658-a6f1-4028-8d02-ad5d9ba341b1": {"node_ids": ["8259bb22-fab5-404b-80f1-36527104ad33", "7e4bddcf-7e51-49e1-bf41-df22b57639b0"], "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}}, "9c5779f8-061a-4543-8c4d-379530914867": {"node_ids": ["9b28bfdc-5df9-474a-96cd-f349ffc9c5fd", "1b02b20c-795f-4a43-8df6-cad472d2f30a"], "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}}, "09bccbc7-34e7-49f5-8649-6ff13b981a15": {"node_ids": ["1cb99a2b-7b48-48b8-8439-eb2fb79c0d2f", "746912eb-1d84-407a-9e3c-ad81a75bd7c6"], "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}}, "dead74a5-9ffd-4904-a649-10d6e6408d9e": {"node_ids": ["48688efd-4bc2-4ece-b55b-1f4481669419", "c42d5026-9e0a-406c-847e-d1a8a1355344"], "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}}, "23c228c6-04d0-4fc9-8a8f-f6c3099ee4fd": {"node_ids": ["abb82a42-5205-4f94-bc08-c4cbcc1b2749"], "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}}, "a5d2c01a-e43d-4c1f-91de-b2e5430014d5": {"node_ids": ["229f52d5-1f75-4cda-8162-598284efcdce", "c6f518ae-f8e9-49ea-b4bc-d1d9d8e66726"], "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}}, "b76235e3-4910-4db8-8057-38c074ddc3dd": {"node_ids": ["0bcc64a5-3ea5-4b95-b1ae-6815aa0b3b13", "7fab722e-f56f-452f-a4b5-58120c90b568"], "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}}, "64cff834-ce6e-4029-8a5d-23c75728c8db": {"node_ids": ["480b6e04-3058-4973-869a-cca9762bab88", "ecb700cc-7c50-4360-915d-7f8a4e083b97"], "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}}, "ba05e856-739f-476b-a70a-573a120a6cda": {"node_ids": ["53ae76ee-6129-46da-a9cc-b4fabcd382ec"], "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}}, "c59942b3-8ef2-4a35-a5b8-7a062d18f18f": {"node_ids": ["dbbfe5ca-b997-4453-b8ee-d3d0e0ff0b00"], "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}}, "339eb14b-a403-458d-a90b-4f4f00dcf161": {"node_ids": ["92d4df26-a2ec-4d9e-9b45-2a02ece8cfc1"], "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}}, "be193238-d2a2-40ca-90f3-8143f602ad96": {"node_ids": ["83c1b6d7-9928-453c-8262-dda07c18dc9c"], "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}}, "8aa0de19-8c9e-41aa-8fbe-b39f4f750174": {"node_ids": ["052f6a97-a7a9-4e53-aa2a-e0483aeaeef6"], "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}}, "f11af456-2b3a-46f6-9412-7df72ef636b4": {"node_ids": ["a84e2c8d-9a22-4795-b8a5-f7b792c977a8"], "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}}, "a7b71192-b6fa-4f0b-a3bb-3cc7ed73be1d": {"node_ids": ["6c494711-d91a-4721-b6fb-f659de0de062"], "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}}, "8ce4202f-87ab-44fd-adb1-f12bbf48f224": {"node_ids": ["2fc61fe5-9dfc-41ca-a1ac-ba23017d1e12", "de0c5b40-7b1d-42ab-9788-debfe1e85b78"], "metadata": {"page_label": "1", "file_name": "Pitfalls on Neural Networks.pdf"}}, "64fa3a3d-c466-495a-b6bc-dd9053dcda10": {"node_ids": ["d0222da8-239c-4dad-89e0-9e8aeb695470", "5e19d9ae-f851-42d2-80a0-575113f15109"], "metadata": {"page_label": "2", "file_name": "Pitfalls on Neural Networks.pdf"}}, "525b571f-a260-4cb8-8ed5-9062dd74803c": {"node_ids": ["ffa40d35-addb-4215-ade5-40d5e99a0794", "cea5f0f6-713c-4a98-bf4a-c10a71482825"], "metadata": {"page_label": "3", "file_name": "Pitfalls on Neural Networks.pdf"}}, "af0d63c0-8f8d-4aca-82c7-8953aa1d86b0": {"node_ids": ["8e830824-b646-4a31-b24f-24d469085926", "1c5d8e2d-84d7-4ee1-8b44-b0c67a863f77"], "metadata": {"page_label": "4", "file_name": "Pitfalls on Neural Networks.pdf"}}, "201136ce-f795-4b86-b23a-4feba505c627": {"node_ids": ["779ac90b-690b-4dea-86d1-8cc1e795440a", "31eae50a-01e2-472e-b1a7-ac5c75cf5ab3"], "metadata": {"page_label": "5", "file_name": "Pitfalls on Neural Networks.pdf"}}, "29566d85-e498-4767-8362-36e69e2deea3": {"node_ids": ["5846ae96-8da0-4508-9076-4ea967c70230", "e58ca118-5200-446c-887a-ef2a97b870f0"], "metadata": {"page_label": "6", "file_name": "Pitfalls on Neural Networks.pdf"}}, "7be1bbf4-4356-4e23-b140-55c33f115ccc": {"node_ids": ["6f25503e-9d81-4dcc-8386-09da9b2b1eda", "05771a4e-97d9-47cc-8f67-ce38a1288bb5"], "metadata": {"page_label": "7", "file_name": "Pitfalls on Neural Networks.pdf"}}, "0a09293c-15d6-4168-a143-4ca043b3e3cc": {"node_ids": ["565a65e4-4f3c-45d5-9edb-40c45f9f8dd6", "c3258fa9-5cd8-40ec-88c5-6397a1295197"], "metadata": {"page_label": "8", "file_name": "Pitfalls on Neural Networks.pdf"}}, "f23a4b70-f536-4585-8e92-286c459c5e2d": {"node_ids": ["3c789972-a947-411a-b5da-c0d09680da8a", "8c22712d-644d-4209-bc65-6b52fa516820"], "metadata": {"page_label": "9", "file_name": "Pitfalls on Neural Networks.pdf"}}, "c126f3d3-56ed-416e-8c77-7a89015a94a6": {"node_ids": ["2d1a2adc-5071-43eb-bf4a-410bd174bed1", "d4de6415-2189-485d-b0d2-f6a67a303dac"], "metadata": {"page_label": "10", "file_name": "Pitfalls on Neural Networks.pdf"}}, "336a0ff8-9216-4f1c-85b3-20a8b1c529cf": {"node_ids": ["7dac650b-20b9-44d0-a421-b4163e55cdc5", "6a7e5d01-af71-4c39-9478-d85c81c048d9", "c8943b4b-2f25-4092-8631-2b31b2e8be9f"], "metadata": {"page_label": "11", "file_name": "Pitfalls on Neural Networks.pdf"}}, "67c6fcd3-9922-45dd-823f-9deac67ac662": {"node_ids": ["ddedcbde-18dc-40db-a01d-4bd767f7bb23", "06da36df-7d8e-46c0-9ca0-d9715428a94d", "a4f8d700-2d0c-493b-a8bc-22bd4e713274", "08e42bea-5768-4b78-b076-2bb042521d63"], "metadata": {"page_label": "12", "file_name": "Pitfalls on Neural Networks.pdf"}}, "b549672b-d944-422c-850a-74fc852af027": {"node_ids": ["45565f4d-a61c-4371-a344-12fa4d0e7568", "57d8b109-4bf7-44f9-836c-53eced112f4b", "8997ec64-5b20-4ee6-83d8-ef8837e5f5da", "573c2a95-bf1c-4af3-8361-f37c326fab84"], "metadata": {"page_label": "13", "file_name": "Pitfalls on Neural Networks.pdf"}}, "c30060bd-1f1a-4b7e-945e-632aa226a2a4": {"node_ids": ["af396777-2c81-4bb9-a0bb-ad71404d76ce"], "metadata": {"page_label": "14", "file_name": "Pitfalls on Neural Networks.pdf"}}, "68d0bcc1-7d30-4c35-95a9-9fd96e4f52b0": {"node_ids": ["b7c68259-cd9f-47da-bdae-eff58aff0592"], "metadata": {"page_label": "443", "file_name": "Time Series.pdf"}}, "c9763d30-6fcc-428f-83c9-3001953fb17b": {"node_ids": ["95c99584-a3c9-4719-90db-1d50ff789de4", "3adbe498-a854-4baf-8ecd-7807b4efab46"], "metadata": {"page_label": "444", "file_name": "Time Series.pdf"}}, "211ec4ae-796d-4880-a198-411bd743e0ea": {"node_ids": ["ceb758fc-cb7c-48b4-8d0e-fd8ab86ba137", "cecff25a-4f94-4ffd-baa9-028b9f38ed43"], "metadata": {"page_label": "445", "file_name": "Time Series.pdf"}}, "fb9dddcc-38f5-4d24-9501-559b09bbf1f5": {"node_ids": ["87788c3b-80f9-42f4-9731-ee272e8871c7", "3f162376-dd96-4779-85bf-334cbaa22f61"], "metadata": {"page_label": "446", "file_name": "Time Series.pdf"}}, "e31273be-cbcb-42fe-9eb1-eeccd5389112": {"node_ids": ["a985d85b-a468-4956-8b55-f9cdb91c2421"], "metadata": {"page_label": "447", "file_name": "Time Series.pdf"}}, "78e33315-f640-4781-948c-2d3ad5f2baf9": {"node_ids": ["bc0bdd5d-4b83-466e-8069-d848f1b0e8ba", "5a510654-e2cf-4a24-a874-20e89774268e"], "metadata": {"page_label": "448", "file_name": "Time Series.pdf"}}, "b1532b3a-bea4-4655-a624-677381c7e0c3": {"node_ids": ["9a1d4b0f-fde6-48fe-9011-2023f53f1bbd", "04813eca-6f5e-432c-b7b5-ebc95bfadd76"], "metadata": {"page_label": "449", "file_name": "Time Series.pdf"}}, "7c3939ad-4272-44d6-b398-73f8530a3f91": {"node_ids": ["00ad34ef-4b64-4f07-b1c4-b642f5749761", "8a0237f9-81f4-4cef-9500-e3615211a218"], "metadata": {"page_label": "450", "file_name": "Time Series.pdf"}}, "ee5707c3-a0aa-445f-a3ab-a26a8e9fe926": {"node_ids": ["ccab159a-ce4f-49fd-91b1-aaa9b8991dac", "0e36bc01-5c79-4d36-8c38-b04430963953"], "metadata": {"page_label": "451", "file_name": "Time Series.pdf"}}, "83ed9163-6ee0-44c4-a92d-5978408267f9": {"node_ids": ["7d7781fd-9233-459d-93e4-e8570b097fcf", "139f4968-5b88-4fdc-9ff4-11b476b435a6"], "metadata": {"page_label": "452", "file_name": "Time Series.pdf"}}, "ae2b5da2-5fd7-4a42-9680-df76c3e7f10d": {"node_ids": ["690ecd27-48fe-4c63-bf45-6651769faf28", "750202ef-9145-482f-8c72-567e9380d5c7"], "metadata": {"page_label": "453", "file_name": "Time Series.pdf"}}, "4223da55-06cf-450c-9d00-4c56f099f5cf": {"node_ids": ["b200fcca-f4f3-4644-a420-d53fb30f7cc4", "c96c8444-4bcd-4b2c-8fff-740c2edf2f5e"], "metadata": {"page_label": "454", "file_name": "Time Series.pdf"}}, "fd963a5c-8382-4ea1-8afa-b0eb842a4568": {"node_ids": ["ba598f3f-7c82-4bb3-bc5c-eb48f816ad2a", "e4eaa70b-3adf-4556-9770-cc57fc59472a"], "metadata": {"page_label": "455", "file_name": "Time Series.pdf"}}, "2d68f85a-81a3-402f-8583-ca18dc3c0682": {"node_ids": ["143cf4f0-c7eb-49be-a710-be91f81a1e1d", "39e93058-b7f5-4e53-a1eb-3e6597d660e7"], "metadata": {"page_label": "456", "file_name": "Time Series.pdf"}}, "043f98c2-687a-451a-a86b-e0bd2441a38b": {"node_ids": ["4c7eab4f-0ec0-4119-bb74-5b89e864f29a", "887581e0-a6dd-40b2-83d2-925e6a6dbec5"], "metadata": {"page_label": "457", "file_name": "Time Series.pdf"}}, "cf87c3a2-9cf9-4a78-9273-db6fabe1644a": {"node_ids": ["6574937f-9c44-4d1f-b74e-4e3868986dcc", "23744b20-264f-497a-a3ba-8e5a742c7575"], "metadata": {"page_label": "458", "file_name": "Time Series.pdf"}}, "db728600-99fc-4a82-ad52-11fd6b0d956d": {"node_ids": ["a4b69d50-a8b9-4b68-8d7d-86c3cf36c00f", "31fdc1af-033d-4afd-9780-4c99bfc13301"], "metadata": {"page_label": "459", "file_name": "Time Series.pdf"}}, "701b0abf-ee3e-4ac5-81c5-da4f6ad31acb": {"node_ids": ["453e0062-d979-4ce4-a620-4e5941f9cacc", "33d67853-e6c9-4f14-a6ba-c4b59ffcf35c"], "metadata": {"page_label": "460", "file_name": "Time Series.pdf"}}, "e1f246f7-342b-4855-be98-1780f90f25b3": {"node_ids": ["458cc8fe-f9fd-4eda-84ae-0f82e544263c", "4db2653f-f003-4856-a2a6-2d5da7d7841b"], "metadata": {"page_label": "461", "file_name": "Time Series.pdf"}}, "61be5a83-3321-486a-97d3-5d1c8c693a2d": {"node_ids": ["ff8bb785-dd23-40b2-8148-ff05dc6b5d21", "6576f2fb-0b95-4d45-8b56-ead8019e2a92"], "metadata": {"page_label": "462", "file_name": "Time Series.pdf"}}, "24232b86-b4df-4a0c-ae88-6db8fd096838": {"node_ids": ["454c1031-4ca1-4019-8e83-195c85ad9edf", "ebf5b508-8483-455d-a697-c2f7a81eed04"], "metadata": {"page_label": "463", "file_name": "Time Series.pdf"}}, "ba8740db-1722-4584-b585-a469555c0ef5": {"node_ids": ["822511ba-a3a0-4575-a7f3-6921fda4967d", "79f29ca5-b46f-4175-b8aa-a7f513b5dbe5"], "metadata": {"page_label": "464", "file_name": "Time Series.pdf"}}, "3a84ad94-6d87-4774-884a-487f13436df8": {"node_ids": ["ab0311a8-f633-4052-8abc-a4877db7fb17", "01b73d52-a1f6-46db-b1e0-dfb4c40e4e34"], "metadata": {"page_label": "465", "file_name": "Time Series.pdf"}}, "70f07b21-b7de-41dc-86f4-3cb847800e0f": {"node_ids": ["83721b6a-913f-4d5b-b4d7-37836f1de808", "605623b7-9e41-468f-a110-3c815fcd67b6"], "metadata": {"page_label": "466", "file_name": "Time Series.pdf"}}, "f298ea32-3817-4de2-958f-5f09c5674ab0": {"node_ids": ["97ca86dd-1809-41b5-9595-39d097001f1e", "b153b385-a273-49f0-9fd0-8ea1df4829bd"], "metadata": {"page_label": "467", "file_name": "Time Series.pdf"}}, "cf0f18f8-9c3e-4c99-9fd2-94b3913575dc": {"node_ids": ["94cd3887-69f9-4cd6-b672-162d18cf085c", "d53c3265-651b-4a0f-969e-d555e7950fc2"], "metadata": {"page_label": "468", "file_name": "Time Series.pdf"}}, "e22a1468-6f1a-4e87-a5dc-3cbb33449b07": {"node_ids": ["4a86b7b9-c47a-4308-b480-866e59b21c7b", "7cfd25f8-69ac-4f6b-bdde-3f6e77142b44"], "metadata": {"page_label": "469", "file_name": "Time Series.pdf"}}, "ec1565e1-aa8f-41e8-a78c-1be832aaff8f": {"node_ids": ["8332f834-b7d6-40fd-be14-452612491e6d", "4ac8c7c3-bfd8-4754-978c-d37287648cc3"], "metadata": {"page_label": "470", "file_name": "Time Series.pdf"}}, "2dc415f5-1f27-4d16-a0c1-c03f15ad0889": {"node_ids": ["bd7e9bd3-9ede-4022-9cff-14dcd16bca56", "db02def1-dc5f-4a97-82f5-e20f25c36ca3"], "metadata": {"page_label": "471", "file_name": "Time Series.pdf"}}, "7f77fd31-795f-4704-9908-667c24140881": {"node_ids": ["cae98f70-b3b3-4d9c-9dfb-67e5ca000312", "a17b4fab-1952-4e52-a7ec-9f34198adfcb"], "metadata": {"page_label": "472", "file_name": "Time Series.pdf"}}, "5a303b42-cd78-408e-b164-0c66108f5b16": {"node_ids": ["d008b1ee-788d-467f-88bf-d9da6886bc63", "9b1f12ca-379e-4036-b639-1bacbba77bd2"], "metadata": {"page_label": "473", "file_name": "Time Series.pdf"}}}}