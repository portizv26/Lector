{"docstore/metadata": {"81745cd0-1bf7-4fbd-88ac-74668c9bdd52": {"doc_hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "2456850a-65d9-4aae-8eac-2037e676405f": {"doc_hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "eb2959f7-1309-454e-a4c7-f48d9e283445": {"doc_hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "9b87ab90-37e6-4a3b-98dc-578421c56631": {"doc_hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d"}, "3a14d9d8-8dae-46ed-aa9f-203ef9a483a1": {"doc_hash": "eb4549aa3fe8970830ceb7ede336adc8a4e61c273f25cdb0e04a9a9db070c96a"}, "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4": {"doc_hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "19f775fe-746a-420a-a946-5981879a2641": {"doc_hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "c255f5fc-f07d-4daa-8f84-0d36de6c2941": {"doc_hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "a5617f37-08ac-4921-bb85-c5619d31fe0a": {"doc_hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "2e951868-2342-4a1d-9c18-bad4bc6a98cd": {"doc_hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "b5bd3b13-52c3-47c1-8340-782660816dd3": {"doc_hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "ec3a32df-42a7-4f8d-8320-9e16855fdedc": {"doc_hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "9df74eb1-5688-4979-a8aa-865fca7c833e": {"doc_hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "92189415-813c-448f-9325-5d744b42daa2": {"doc_hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "8da82a75-5028-45a7-b216-e3136b94e7c7": {"doc_hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2": {"doc_hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "c7f9d697-11df-4960-bb3d-59ed77b3e0dd": {"doc_hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "83dd834e-1c6c-4347-a22f-565adc1fae11": {"doc_hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "50f89fe7-04a7-406e-b5db-f905d82e29d4": {"doc_hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "9355e891-1acc-419d-8d45-c051be2b9689": {"doc_hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52": {"doc_hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "d61f6244-0721-4e88-bb6f-1621e60bd67d": {"doc_hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "c4260db6-594a-44d3-85dc-1f383c2273c2": {"doc_hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f"}, "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe": {"doc_hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "e6d26f89-b133-4052-b227-ae3409989906": {"doc_hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "21fd1fe9-c79c-4c93-a146-2c1dc4fb0f04": {"doc_hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d"}, "0e4c2413-6aad-42c9-9b2f-cb493f42a576": {"doc_hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7": {"doc_hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "08268d8c-aed2-42b4-9253-9fb1269c87cf": {"doc_hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5": {"doc_hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "d4cde2b0-39de-4bf5-8ae7-90e4641d506e": {"doc_hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "77a4db53-f42f-4356-838a-929b3102cd92": {"doc_hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "86e38ea7-a328-4b53-b1a6-21a614131f54": {"doc_hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "d15ab249-9163-4186-87e4-823febc0e4db": {"doc_hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "d904018d-af49-48f7-854a-71954d7293ed": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}, "a92d053d-c7b3-46e4-8786-3c779f2b1ff2": {"doc_hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "1828614e-2082-4396-b396-0068e7b6ff76": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}, "5bf9a414-1d20-4b5a-b249-686a7a543d47": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}, "5daa4e26-d5fe-4950-83a8-3e22305a1a94": {"doc_hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "d0a47d64-b47a-44fa-b78b-a4de739208ea": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}, "5cb5b8f4-921e-43ac-814b-d705dd939634": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}, "a21d0bb2-5c95-4512-9771-2015b84ef71b": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}, "fae20a98-def8-487f-a902-69e703dd131b": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}, "3836b47f-7bbc-4927-ada9-7dfca6564469": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}, "0301e2f3-4c02-48e1-ba86-844d0d5c458b": {"doc_hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "a86d6031-7944-48c7-8929-e81402eda8cb": {"doc_hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a": {"doc_hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "410c2ff4-68ff-43c0-827a-41580d9e4802": {"doc_hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "6a4c592e-0151-4012-8f25-fa4da077b701": {"doc_hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "3086518d-aacf-4f68-90bc-9c1990559fbc": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}, "a11a05f9-b233-4c1e-8b44-b6f8de747991": {"doc_hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "347d6ad0-37d8-4fa7-bdd8-66b6a6196086": {"doc_hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c": {"doc_hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "34b7f9c2-6a26-403d-8da7-b1639a6b08d0": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}, "e9762258-28bb-423f-ad27-7ef0ef76cd7e": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}, "c9aad5a8-0e25-4daf-bdaa-dee8e0d30b75": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}, "b92ce087-ec9b-4087-a88b-a778ba836acb": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}, "ee389505-386e-4390-9969-42d853b0d752": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}, "caa39d26-8851-4d00-8804-9fc5618760bd": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}, "a09d8f31-e978-4e2b-b1c9-fa78949ffc4e": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}, "c23fcaa9-1e1d-4364-ae60-b074cd1472e8": {"doc_hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236", "ref_doc_id": "81745cd0-1bf7-4fbd-88ac-74668c9bdd52"}, "ca29c6d2-8165-4fb8-b5c7-19a0be4485c5": {"doc_hash": "62c547d85576c12961a9bf34f375448cee6926eb61cf3957369adc560bc7b3ee", "ref_doc_id": "81745cd0-1bf7-4fbd-88ac-74668c9bdd52"}, "598882f9-dd8d-40c2-a43f-19a3a95789ec": {"doc_hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f", "ref_doc_id": "2456850a-65d9-4aae-8eac-2037e676405f"}, "7d97b121-9283-4ff0-84a0-1eff38df493f": {"doc_hash": "b8e0a1fdb26934487eb7d37ec325cc279afca2ad118e77e57728f9ccefd2a7bd", "ref_doc_id": "2456850a-65d9-4aae-8eac-2037e676405f"}, "193efdee-affc-47dd-b957-1495148e0469": {"doc_hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5", "ref_doc_id": "eb2959f7-1309-454e-a4c7-f48d9e283445"}, "09fc3f4f-ac00-464e-bf5e-3b0068422cac": {"doc_hash": "a57348a5b16f6044a35b8d5e29d600bc1116812b02bfa883f8461e8bd62d513b", "ref_doc_id": "eb2959f7-1309-454e-a4c7-f48d9e283445"}, "c9983646-8fb8-497c-b167-66e800fdcc82": {"doc_hash": "f48dafca25f69cc4f8bda1113a73f97757656b07c8e9d666730427beb72f22ce", "ref_doc_id": "eb2959f7-1309-454e-a4c7-f48d9e283445"}, "a2a4a5ea-d13c-4753-ad8c-0b61cb72d0da": {"doc_hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d", "ref_doc_id": "9b87ab90-37e6-4a3b-98dc-578421c56631"}, "9f7e9f0f-60c4-4f15-9742-d5b1a9582f46": {"doc_hash": "493f7df431511224349c07bd3d8f2485415412f03d4482c9997130b8f9b2065a", "ref_doc_id": "3a14d9d8-8dae-46ed-aa9f-203ef9a483a1"}, "c8a2a2b4-e978-40c0-a1e6-2fdbc010f120": {"doc_hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a", "ref_doc_id": "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4"}, "a64fab5e-0406-46f8-9055-d14711882c66": {"doc_hash": "16e463931f18f35b3642bdff70ab9cd6677d8115e9c41d75a045e80d955deb8d", "ref_doc_id": "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4"}, "114811d9-85e3-4cf6-9789-9d2dcc7106e4": {"doc_hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4", "ref_doc_id": "19f775fe-746a-420a-a946-5981879a2641"}, "d58e52a2-9c28-4857-895c-8364eef31bc0": {"doc_hash": "8c61445f10725a12da31535e506cd42c771cc8ed8d643bdd508caf4cf875a5bc", "ref_doc_id": "19f775fe-746a-420a-a946-5981879a2641"}, "2fadee76-3fc6-489e-affe-96aa5ea758d7": {"doc_hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef", "ref_doc_id": "c255f5fc-f07d-4daa-8f84-0d36de6c2941"}, "76da649e-54b2-4621-b782-5a3bb9f35beb": {"doc_hash": "5b66bcaed1b10281dd094449a89002c483217efa3b81238188ed5503d32f37a5", "ref_doc_id": "c255f5fc-f07d-4daa-8f84-0d36de6c2941"}, "32fc8f4e-0ebd-447d-ab6d-d63bfc7f831e": {"doc_hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2", "ref_doc_id": "a5617f37-08ac-4921-bb85-c5619d31fe0a"}, "4332bf06-23a4-4f9e-90cc-f37645ddd5fb": {"doc_hash": "bf3b68a485a80b23b90ff7f37b93d3218d1d06a14c50fd09fcc8c6ccdc459e0d", "ref_doc_id": "a5617f37-08ac-4921-bb85-c5619d31fe0a"}, "de67346e-2716-4865-a7c3-dca015ef3509": {"doc_hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d", "ref_doc_id": "2e951868-2342-4a1d-9c18-bad4bc6a98cd"}, "f45ea891-8def-4990-b182-1f239497975d": {"doc_hash": "6816d3e526562d03e7bb7b5b627b317558279d961a017c2face016e24e2c5d55", "ref_doc_id": "2e951868-2342-4a1d-9c18-bad4bc6a98cd"}, "dcaca609-a339-413f-9417-4112d5e26099": {"doc_hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6", "ref_doc_id": "b5bd3b13-52c3-47c1-8340-782660816dd3"}, "1370603c-d302-496a-ab5d-4d5d7bd0e02d": {"doc_hash": "9d000eff7221771de29f17ebb8c74a9251e8f673d5adeb3426e6acf9e412dcc1", "ref_doc_id": "b5bd3b13-52c3-47c1-8340-782660816dd3"}, "1391ba94-bcad-4c9a-ad5d-bdbaa25d36b0": {"doc_hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c", "ref_doc_id": "ec3a32df-42a7-4f8d-8320-9e16855fdedc"}, "9a12ce67-810f-4816-b5a6-24891d994b91": {"doc_hash": "fae0694d1c68f9999de65e230b991eb8e2d7873c7cd5e3bf8674332a81903573", "ref_doc_id": "ec3a32df-42a7-4f8d-8320-9e16855fdedc"}, "5dbffff8-d037-4461-a8b5-a910192784bd": {"doc_hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b", "ref_doc_id": "9df74eb1-5688-4979-a8aa-865fca7c833e"}, "d70b24b2-9928-432a-8f0d-9a1a91c7092d": {"doc_hash": "4cf7630f511417962f3022c1045bdbcd41b0dee280d4c29a02514511e87b742d", "ref_doc_id": "9df74eb1-5688-4979-a8aa-865fca7c833e"}, "4e6e6472-5c48-4a8e-bfcd-3390f6e0e046": {"doc_hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38", "ref_doc_id": "92189415-813c-448f-9325-5d744b42daa2"}, "040cae1b-d191-486d-9455-5d4bf878b3ea": {"doc_hash": "2247db82e909588f7e1fb81d0c1df4944125b192ceb6ee5b67115f8e4eff9494", "ref_doc_id": "92189415-813c-448f-9325-5d744b42daa2"}, "13798897-265c-4df0-b0cb-58b61d12451d": {"doc_hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46", "ref_doc_id": "8da82a75-5028-45a7-b216-e3136b94e7c7"}, "b7454e40-c40e-4760-8320-87393fba0652": {"doc_hash": "64e4e8e21187c88719f8bd26a78dcb7367ca20611759399abb905c3312480ded", "ref_doc_id": "8da82a75-5028-45a7-b216-e3136b94e7c7"}, "254d23e2-4b68-41fe-a42e-8447042e7214": {"doc_hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b", "ref_doc_id": "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2"}, "b39d6656-38ae-4098-8d6c-1de0263626cf": {"doc_hash": "dd7ef7a7d2efaafc620b351fcdfc4bcbdf00c1e9f829930705c2771e9beb5563", "ref_doc_id": "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2"}, "7487076c-1967-4678-a14f-c86cf13dcead": {"doc_hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0", "ref_doc_id": "c7f9d697-11df-4960-bb3d-59ed77b3e0dd"}, "51777b76-56dd-4fd4-8d28-4dc30864db57": {"doc_hash": "00ab25eb19df3021ff031935473ece053588d06119c2ed16eb5a3649570fae8e", "ref_doc_id": "c7f9d697-11df-4960-bb3d-59ed77b3e0dd"}, "7e464763-6c28-40ef-9d6c-ab5e84d8b075": {"doc_hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead", "ref_doc_id": "83dd834e-1c6c-4347-a22f-565adc1fae11"}, "85c176b1-8119-4831-b3fe-6a680fe02035": {"doc_hash": "9994902f833106d18962d57156121dfd17dc7db1179df4de4c9cafd5038f2e75", "ref_doc_id": "83dd834e-1c6c-4347-a22f-565adc1fae11"}, "fdac7e1e-859a-4913-8521-caa2a30b2d6b": {"doc_hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c", "ref_doc_id": "50f89fe7-04a7-406e-b5db-f905d82e29d4"}, "b71157bb-690f-44d9-a23e-dbd99c686820": {"doc_hash": "84cb3e3b20784a5525040ca81c11620d3b413b825c313408b32de07a4c1719f0", "ref_doc_id": "50f89fe7-04a7-406e-b5db-f905d82e29d4"}, "e2e37e5f-1ba6-455c-9a86-7c060a21c5da": {"doc_hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b", "ref_doc_id": "9355e891-1acc-419d-8d45-c051be2b9689"}, "1ff8f34c-9eec-4f03-a610-841dc58c355e": {"doc_hash": "451a40404b3a65abf635cf105b00a994049037df0fd95184d74430c5a128364f", "ref_doc_id": "9355e891-1acc-419d-8d45-c051be2b9689"}, "0b699348-c595-48ba-8e34-08a194068eb5": {"doc_hash": "e756b48f7472c8c72c2ece9915a51c41e8a0e0bb28c9f7b68b4d096aec6c7f4d", "ref_doc_id": "9355e891-1acc-419d-8d45-c051be2b9689"}, "f4745372-c56a-435b-88ac-3b4d570e0370": {"doc_hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487", "ref_doc_id": "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52"}, "6e7fcb45-c5c5-4cb9-85a6-b5d91947530e": {"doc_hash": "e4893cc68ead0f9694ee193961cbb971ade24d0b5116048750159b217eeeabf5", "ref_doc_id": "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52"}, "483d22db-5260-43f2-af63-1e2bfee184d1": {"doc_hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9", "ref_doc_id": "d61f6244-0721-4e88-bb6f-1621e60bd67d"}, "3116bcda-b1ca-41c2-a33a-4b34f9154094": {"doc_hash": "8740eae52f0851067935d98196be784a47e0200857182a1cf0e0737488576e32", "ref_doc_id": "d61f6244-0721-4e88-bb6f-1621e60bd67d"}, "db902509-1cbb-4601-b036-b3b94682f814": {"doc_hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f", "ref_doc_id": "c4260db6-594a-44d3-85dc-1f383c2273c2"}, "dff121da-ad44-45e4-937c-2bbcb34a28a1": {"doc_hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b", "ref_doc_id": "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe"}, "d2e6aea4-c820-4237-8dbf-0f4d6addc68d": {"doc_hash": "a6edad30fa68fb24106f83183fd4ad1478266e13a46f11d9253455dbdb3ada62", "ref_doc_id": "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe"}, "5bc3ddbc-348b-4558-8f6e-e0a6e567aa4a": {"doc_hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba", "ref_doc_id": "e6d26f89-b133-4052-b227-ae3409989906"}, "f083b3e1-dbd6-488d-86c9-e36f2a442076": {"doc_hash": "97af896fed7cf6c5f05d10c37fb01aabc2d1d19890ae90819523af311c12335d", "ref_doc_id": "e6d26f89-b133-4052-b227-ae3409989906"}, "de81bd72-e92b-4660-9211-c8ee9f220638": {"doc_hash": "bca252312a9115dc73e51e3cc89d7f05fa363cd56bf0ead9c21fdad68da29238", "ref_doc_id": "e6d26f89-b133-4052-b227-ae3409989906"}, "60d5ebe6-9667-46c1-889e-c8d87f6cee52": {"doc_hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d", "ref_doc_id": "21fd1fe9-c79c-4c93-a146-2c1dc4fb0f04"}, "1cf809ca-64fa-40f0-9765-2d7bed70a722": {"doc_hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394", "ref_doc_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576"}, "b546b495-5330-414f-a767-832cf1360cd9": {"doc_hash": "dd4d4ebc565a9a9f858d5488a296870789ed38ecb2782dc5b057af11892b487a", "ref_doc_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576"}, "d935f193-6229-4fda-903a-0edf140774bb": {"doc_hash": "ceae6ea619af24fe24061387a6e1d56217c4d318b5ba0e440e105fc3b3e72a5e", "ref_doc_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576"}, "2e081ef4-24e3-4c4d-94b6-28decf74f920": {"doc_hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334", "ref_doc_id": "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7"}, "38da4b93-3a72-4c12-affe-d9c5bfc8f8c9": {"doc_hash": "ab17db18b0c1783371d15d2925b6c54f3096ec78d373c5d3e385e9357585d07f", "ref_doc_id": "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7"}, "84ca4505-a90d-4dd9-ae66-6b8908ca4715": {"doc_hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f", "ref_doc_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf"}, "b6441978-722d-4618-a32f-3d44db20d1a6": {"doc_hash": "73b7e691658d09e837770507349b4729d6cab31dce0f76f008f14964b32b7c0a", "ref_doc_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf"}, "1c0957ca-5dec-4d24-8864-2416c3c32dfa": {"doc_hash": "104fed68931a393e7207381a1283083e8515a13c59e0dcd0e46bfdb98e5a0c46", "ref_doc_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf"}, "139af044-4eb0-46df-9cf7-57eac878b175": {"doc_hash": "23744ce073b7cf82e01c31666e4973e024f40dacdd37216737a627702a659d76", "ref_doc_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf"}, "638d296f-3f65-4cf3-861c-0e4f1ee1cf2a": {"doc_hash": "784c85c089b92fbd649ed2ecaccb10fd8db10264eef1a8e8c32dd25b2889431e", "ref_doc_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf"}, "08395673-3918-4a6a-be76-55677d1b7b72": {"doc_hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737", "ref_doc_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5"}, "46de05ab-c6fe-46ab-bbe5-0b28c0333771": {"doc_hash": "51cc4d501cb4b8ea6ad5a522bad17fd6a163a3bf690f49dc9aa9f697f8e6b3a1", "ref_doc_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5"}, "62b80d0a-b9f9-40a5-8cba-6b6f0af10540": {"doc_hash": "98c7bc4b0d359841f364423ed8c96fa12500469d406f12aa651cd4612d334e12", "ref_doc_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5"}, "5f82c25e-e80d-4f46-92a8-17282ca85b55": {"doc_hash": "423ec4410c54c58ea8cfd534543bfed8d179fd717027bea92ede3087ac9fd740", "ref_doc_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5"}, "c066405b-531f-4635-9414-e6276de78b29": {"doc_hash": "0950004d8e20f1619cba8cdceb886c025bed794217bed7d3e6540c811853572e", "ref_doc_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5"}, "f46c1b14-611f-4233-8b26-de370fecc8ff": {"doc_hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491", "ref_doc_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e"}, "ad6b7b0e-5058-4e1a-8844-4783867f0856": {"doc_hash": "10bbc90f10e916cc57ea273d623136d751cf9e30a4ce39413b72667a8fc70a1a", "ref_doc_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e"}, "16be1f30-ceac-4b5e-9d9d-16e98e075545": {"doc_hash": "f119e82f822f157f87167ae8585d7d6b14c9893aeeee830684d37f32c5a9c6a7", "ref_doc_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e"}, "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58": {"doc_hash": "dbbf578008b2706bb3a3de23fdea9181f1077384a32406fa20b1ceed2d1227a8", "ref_doc_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e"}, "7425ce7f-c899-4926-b6d6-adfb50948bba": {"doc_hash": "2a55621b781c711b211036a39b704ae9c5a00aa33228595ef875bf9812b3236e", "ref_doc_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e"}, "692ad769-f565-4860-a8fc-f16d2970f1c9": {"doc_hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7", "ref_doc_id": "77a4db53-f42f-4356-838a-929b3102cd92"}, "a0c81609-7666-4f30-b0c9-bed065dfe2e7": {"doc_hash": "3b2299445322a2bc35e4619461230d271255fa531429be3c746c5a232375565a", "ref_doc_id": "77a4db53-f42f-4356-838a-929b3102cd92"}, "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793": {"doc_hash": "f395e1f1c3ad5727d0874acb6e1e9069ed88f61c24dee43903ba22edfb954bd6", "ref_doc_id": "77a4db53-f42f-4356-838a-929b3102cd92"}, "d65554cf-768d-469d-9658-4308f214680b": {"doc_hash": "d2b74fa8d1bb6c9fef6df46a83cb84d3eaa84979488cec071a44688b3ded98e0", "ref_doc_id": "77a4db53-f42f-4356-838a-929b3102cd92"}, "9e63e483-f011-4e84-84b0-51256bf47d20": {"doc_hash": "df21c5021b889e556e88a6772bc7c224a879754fd40dd7ff4e1d210aeafd7ece", "ref_doc_id": "77a4db53-f42f-4356-838a-929b3102cd92"}, "c312ba61-6eb2-4b47-a32e-baed6f6c350e": {"doc_hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7", "ref_doc_id": "86e38ea7-a328-4b53-b1a6-21a614131f54"}, "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac": {"doc_hash": "b179875b5ed80bc0742443c66e62a2250ec105ec2fceefa5c57b6333f0f8149f", "ref_doc_id": "86e38ea7-a328-4b53-b1a6-21a614131f54"}, "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6": {"doc_hash": "156fbdd6d1a06ce6d81a7b1735681584f6fdf0d4c82e0a951b72af2c10ef05ff", "ref_doc_id": "86e38ea7-a328-4b53-b1a6-21a614131f54"}, "a234f387-581a-47f1-83af-8a98dfd38cec": {"doc_hash": "dbbb55f44b80b62a4b3b91e82e09a54c406f897d8df41e387b85b789460e6a28", "ref_doc_id": "86e38ea7-a328-4b53-b1a6-21a614131f54"}, "2b510519-2c5e-488b-aaa4-fbf7a9789991": {"doc_hash": "f30e7a4c0bbee8baabc5cdc98fe6ecc7b104c97cb2f1593b8e304837ac6699cb", "ref_doc_id": "86e38ea7-a328-4b53-b1a6-21a614131f54"}, "c6ba7ed5-2ab8-4068-9576-3c6baa96aed9": {"doc_hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac", "ref_doc_id": "d15ab249-9163-4186-87e4-823febc0e4db"}, "ef521592-a2e7-4340-ab49-87747949fafe": {"doc_hash": "e7967e36e953bce7e0f2da22a256c7b60de17d354d0a36184eee40b73f3fb6af", "ref_doc_id": "d15ab249-9163-4186-87e4-823febc0e4db"}, "81421553-3b86-42b5-bf2e-1b7138f2b853": {"doc_hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "ref_doc_id": "d904018d-af49-48f7-854a-71954d7293ed"}, "b5560d09-046f-4c51-8380-90520ee9abd1": {"doc_hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "ref_doc_id": "a92d053d-c7b3-46e4-8786-3c779f2b1ff2"}, "115e2887-4896-4804-b552-b4e4f031bc94": {"doc_hash": "da59e72ec43c4cc43f5dc0e7aa51d49574c8bfe6c96935a091ea78024ed6f742", "ref_doc_id": "a92d053d-c7b3-46e4-8786-3c779f2b1ff2"}, "4061679f-82cd-42d7-a83e-c2188dd0546b": {"doc_hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "ref_doc_id": "1828614e-2082-4396-b396-0068e7b6ff76"}, "4ce061cb-d032-4bcf-91d2-48f358f2db7c": {"doc_hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "ref_doc_id": "5bf9a414-1d20-4b5a-b249-686a7a543d47"}, "deb06ccc-8dbf-4b10-8d2a-50a3dfa2bf79": {"doc_hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "ref_doc_id": "5daa4e26-d5fe-4950-83a8-3e22305a1a94"}, "dbe138dc-27d1-40cd-82ae-a26f6325c2ba": {"doc_hash": "483a7130fee7a8acde98cbaaac6885d09238336fe2e79045d1481a3829e68b4e", "ref_doc_id": "5daa4e26-d5fe-4950-83a8-3e22305a1a94"}, "0c2aed0c-19a2-49ac-b20f-edab59243b01": {"doc_hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "ref_doc_id": "d0a47d64-b47a-44fa-b78b-a4de739208ea"}, "cc74ac2b-f0c4-4dbd-979e-e1d4bde5829f": {"doc_hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "ref_doc_id": "5cb5b8f4-921e-43ac-814b-d705dd939634"}, "2297bd97-eaa8-472f-b652-ca88608aaf1f": {"doc_hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "ref_doc_id": "a21d0bb2-5c95-4512-9771-2015b84ef71b"}, "26f1f084-44a3-4b39-a03f-0dfebd9afb57": {"doc_hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "ref_doc_id": "fae20a98-def8-487f-a902-69e703dd131b"}, "3aa3e455-37c8-436b-9a39-a86c924314f9": {"doc_hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "ref_doc_id": "3836b47f-7bbc-4927-ada9-7dfca6564469"}, "72810aca-171e-45e0-86d5-d5d6a5cc8224": {"doc_hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "ref_doc_id": "0301e2f3-4c02-48e1-ba86-844d0d5c458b"}, "1889aed4-8ab1-413d-ba27-60b9f3577d0c": {"doc_hash": "0912fda46fc21c82a29cad3bedd983c2e7705ff52368765cba7cf3f268ab170a", "ref_doc_id": "0301e2f3-4c02-48e1-ba86-844d0d5c458b"}, "de12318d-d007-4b53-9030-1ac11fb79da4": {"doc_hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "ref_doc_id": "a86d6031-7944-48c7-8929-e81402eda8cb"}, "9d54f5cb-4543-41b5-99b5-2891300b7452": {"doc_hash": "83ce0e2565f8105c6b7fcc9cbea794fd662e9fb249ad38202dc006acd075c770", "ref_doc_id": "a86d6031-7944-48c7-8929-e81402eda8cb"}, "966b471a-3089-490c-8a0f-abb31d721c0e": {"doc_hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "ref_doc_id": "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a"}, "704e88cd-f1bc-4a76-ad6e-d5d1f352ba36": {"doc_hash": "dd82271dac46266cb3bde99bdd0369381e17ecd0fdf1617aa58cf24ff1dd8f7e", "ref_doc_id": "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a"}, "82c0ed52-9311-4ebc-9757-ee0a5366c107": {"doc_hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "ref_doc_id": "410c2ff4-68ff-43c0-827a-41580d9e4802"}, "2c22439d-abc5-4f02-86d1-674b55702e41": {"doc_hash": "58b9fbfae73e0447be7f222296d5dd748e8a3187e15e9372af79bfd311bff37d", "ref_doc_id": "410c2ff4-68ff-43c0-827a-41580d9e4802"}, "24271fce-0aef-4b36-bd9d-5e329ae162bf": {"doc_hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "ref_doc_id": "6a4c592e-0151-4012-8f25-fa4da077b701"}, "c9e82794-4c0d-4fae-8a62-6b75440d6850": {"doc_hash": "5d7d521755825863b13d02bd87c1feb00216922fec7ecbee98a5932e0f4074ba", "ref_doc_id": "6a4c592e-0151-4012-8f25-fa4da077b701"}, "7298e8d9-a332-440b-aa04-b94bd357478d": {"doc_hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "ref_doc_id": "3086518d-aacf-4f68-90bc-9c1990559fbc"}, "6d4e2671-af99-4cea-89b1-4d3cb83c1b71": {"doc_hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "ref_doc_id": "a11a05f9-b233-4c1e-8b44-b6f8de747991"}, "a7809498-b4a3-48cb-a20c-63c3a0cdb239": {"doc_hash": "119572f66488246d97df820cc009226802ac1e06e3d272b748b600bc58725323", "ref_doc_id": "a11a05f9-b233-4c1e-8b44-b6f8de747991"}, "2b32b82d-0bca-485f-a959-b39af5c9ae69": {"doc_hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "ref_doc_id": "347d6ad0-37d8-4fa7-bdd8-66b6a6196086"}, "8e8a509b-fe89-4043-b5b3-baeffd1cfac9": {"doc_hash": "af6d37ffabf23b16cf006b58ec3a8b37d390263c8d16f66a531fa99f5f071c00", "ref_doc_id": "347d6ad0-37d8-4fa7-bdd8-66b6a6196086"}, "c322b139-26f8-42ff-a193-2c4f2e7cd4aa": {"doc_hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "ref_doc_id": "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c"}, "bc3543fc-9560-447f-a0c7-2e1fa2a23e46": {"doc_hash": "cf780e542324252bff936e6d6909e22ac4592bb0e9b966fbf86b4478bdbde1ba", "ref_doc_id": "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c"}, "bf2da5cf-8744-469a-9063-f614a0612590": {"doc_hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "ref_doc_id": "34b7f9c2-6a26-403d-8da7-b1639a6b08d0"}, "d07687e2-37d1-48fe-a725-67e4b0859386": {"doc_hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "ref_doc_id": "e9762258-28bb-423f-ad27-7ef0ef76cd7e"}, "fd8d547a-e8c9-4e39-a044-3a9b70d3d588": {"doc_hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "ref_doc_id": "c9aad5a8-0e25-4daf-bdaa-dee8e0d30b75"}, "c86c275c-6617-4d6b-8698-c6218241d5dc": {"doc_hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "ref_doc_id": "b92ce087-ec9b-4087-a88b-a778ba836acb"}, "71f4dc22-f1bb-4723-9362-bd012126c23c": {"doc_hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "ref_doc_id": "ee389505-386e-4390-9969-42d853b0d752"}, "f585cddf-1482-487f-8059-e55daf73b770": {"doc_hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "ref_doc_id": "caa39d26-8851-4d00-8804-9fc5618760bd"}, "d2c56a30-c946-48df-b908-c921747c9548": {"doc_hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "ref_doc_id": "a09d8f31-e978-4e2b-b1c9-fa78949ffc4e"}}, "docstore/data": {"c23fcaa9-1e1d-4364-ae60-b074cd1472e8": {"__data__": {"id_": "c23fcaa9-1e1d-4364-ae60-b074cd1472e8", "embedding": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81745cd0-1bf7-4fbd-88ac-74668c9bdd52", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "3": {"node_id": "ca29c6d2-8165-4fb8-b5c7-19a0be4485c5", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "62c547d85576c12961a9bf34f375448cee6926eb61cf3957369adc560bc7b3ee"}}, "hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236", "text": "IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005 645\nSurvey of Clustering Algorithms\nRui Xu , Student Member, IEEE and Donald Wunsch II , Fellow, IEEE\nAbstract\u2014 Data analysis plays an indispensable role for un-\nderstanding various phenomena. Cluster analysis, primitiveexploration with little or no prior knowledge, consists of researchdeveloped across a wide variety of communities. The diversity,on one hand, equips us with many tools. On the other hand,the profusion of options causes confusion. We survey clusteringalgorithms for data sets appearing in statistics, computer science,and machine learning, and illustrate their applications in somebenchmark data sets, the traveling salesman problem, and bioin-formatics, a new \ufb01eld attracting intensive efforts. Several tightlyrelated topics, proximity measure, and cluster validation, are also\ndiscussed.\nIndex Terms\u2014 Adaptive resonance theory (ART), clustering,\nclustering algorithm, cluster validation, neural networks, prox-imity, self-organizing feature map (SOFM).\nI. INTRODUCTION\nWE ARE living in a world full of data. Every day, people\nencounter a large amount of information and store or\nrepresent it as data, for further analysis and management. Oneof the vital means in dealing with these data is to classify or\ngroup them into a set of categories or clusters. Actually, as one\nof the most primitive activities of human beings [14], classi-\ufb01cation plays an important and indispensable role in the longhistory of human development. In order to learn a new object\nor understand a new phenomenon, people always try to seek\nthe features that can describe it, and further compare it withother known objects or phenomena, based on the similarity ordissimilarity, generalized as proximity, according to some cer-\ntain standards or rules. \u201cBasically, classi\ufb01cation systems are ei-\nther supervised or unsupervised, depending on whether they as-sign new inputs to one of a \ufb01nite number of discrete supervised\nclasses or unsupervised categories, respectively [38], [60], [75].\nIn supervised classi\ufb01cation, the mapping from a set of input datavectors (\n, where\n is the input space dimensionality), to\na \ufb01nite set of discrete class labels (\n , where\n is\nthe total number of class types), is modeled in terms of some\nmathematical function\n , where\n is a vector of\nadjustable parameters. The values of these parameters are de-termined (optimized) by an inductive learning algorithm (also\ntermed inducer), whose aim is to minimize an empirical risk\nfunctional (related to an inductive principle) on a \ufb01nite data setof input\u2013output examples,\n, where\n is\nthe \ufb01nite cardinality of the available representative data set [38],\nManuscript received March 31, 2003; revised September 28, 2004. This work\nwas supported in part by the National Science Foundation and in part by the\nM. K. Finley Missouri Endowment.\nThe authors are with the Department of Electrical and Computer Engineering,\nUniversity of Missouri-Rolla, Rolla, MO 65409 USA (e-mail: rxu@umr.edu;\ndwunsch@ece.umr.edu).\nDigital Object Identi\ufb01er 10.1109/TNN.2005.845141[60], [167]. When the inducer reaches convergence or termi-\nnates, an induced classi\ufb01er is generated [167].\nIn unsupervised classi\ufb01cation, called clustering or ex-\nploratory data analysis, no labeled data are available [88],[150]. The goal of clustering is to separate a \ufb01nite unlabeleddata set into a \ufb01nite and discrete set of \u201cnatural,\u201d hidden data\nstructures, rather than", "start_char_idx": 0, "end_char_idx": 3458, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca29c6d2-8165-4fb8-b5c7-19a0be4485c5": {"__data__": {"id_": "ca29c6d2-8165-4fb8-b5c7-19a0be4485c5", "embedding": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "81745cd0-1bf7-4fbd-88ac-74668c9bdd52", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "32b57ddc8607a7e637a8628e158ec6cf08c43f0b1cd1b4856046d59123c3a752"}, "2": {"node_id": "c23fcaa9-1e1d-4364-ae60-b074cd1472e8", "node_type": null, "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}, "hash": "afb03408995abfe7da83b815b8b389767a38995b7e852845a52f80a1d4915236"}}, "hash": "62c547d85576c12961a9bf34f375448cee6926eb61cf3957369adc560bc7b3ee", "text": "authors are with the Department of Electrical and Computer Engineering,\nUniversity of Missouri-Rolla, Rolla, MO 65409 USA (e-mail: rxu@umr.edu;\ndwunsch@ece.umr.edu).\nDigital Object Identi\ufb01er 10.1109/TNN.2005.845141[60], [167]. When the inducer reaches convergence or termi-\nnates, an induced classi\ufb01er is generated [167].\nIn unsupervised classi\ufb01cation, called clustering or ex-\nploratory data analysis, no labeled data are available [88],[150]. The goal of clustering is to separate a \ufb01nite unlabeleddata set into a \ufb01nite and discrete set of \u201cnatural,\u201d hidden data\nstructures, rather than provide an accurate characterization\nof unobserved samples generated from the same probabilitydistribution [23], [60]. This can make the task of clustering falloutside of the framework of unsupervised predictive learning\nproblems, such as vector quantization [60] (see Section II-C),\nprobability density function estimation [38] (see Section II-D),[60], and entropy maximization [99]. It is noteworthy thatclustering differs from multidimensional scaling (perceptual\nmaps), whose goal is to depict all the evaluated objects in a\nway that minimizes the topographical distortion while using asfew dimensions as possible. Also note that, in practice, many(predictive) vector quantizers are also used for (nonpredictive)\nclustering analysis [60].\nNonpredictive clustering is a subjective process in nature,\nwhich precludes an absolute judgment as to the relative ef\ufb01-cacy of all clustering techniques [23], [152]. As pointed out by\nBacker and Jain [17], \u201cin cluster analysis a group of objects is\nsplit up into a number of more or less homogeneous subgroupson the basis of an often subjectively chosen measure of sim-ilarity (i.e., chosen subjectively based on its ability to create\n\u201cinteresting\u201d clusters), such that the similarity between objects\nwithin a subgroup is larger than the similarity between objectsbelonging to different subgroups\u201d\u201d\n1.\nClustering algorithms partition data into a certain number\nof clusters (groups, subsets, or categories). There is no univer-\nsally agreed upon de\ufb01nition [88]. Most researchers describe acluster by considering the internal homogeneity and the externalseparation [111], [124], [150], i.e., patterns in the same cluster\nshould be similar to each other, while patterns in different clus-\nters should not. Both the similarity and the dissimilarity shouldbe examinable in a clear and meaningful way. Here, we givesome simple mathematical descriptions of several types of clus-\ntering, based on the descriptions in [124].\nGiven a set of input patterns\n,\nwhere\n and each measure\nis said to be a feature (attribute, dimension, or variable).\n\u2022 (Hard) partitional clustering attempts to seek a\n -par-\ntition of\n , such that\n1)\n ;\n2)\n ;\n3)\n and\n .\n1The preceding quote is taken verbatim from verbiage suggested by the\nanonymous associate editor, a suggestion which we gratefully acknowledge.\n1045-9227/$20.00 \u00a9 2005 IEEE", "start_char_idx": 2870, "end_char_idx": 5813, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "598882f9-dd8d-40c2-a43f-19a3a95789ec": {"__data__": {"id_": "598882f9-dd8d-40c2-a43f-19a3a95789ec", "embedding": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2456850a-65d9-4aae-8eac-2037e676405f", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "3": {"node_id": "7d97b121-9283-4ff0-84a0-1eff38df493f", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "b8e0a1fdb26934487eb7d37ec325cc279afca2ad118e77e57728f9ccefd2a7bd"}}, "hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f", "text": "646 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 1. Clustering procedure. The typical cluster analysis consists of four steps with a feedback pathway. These steps are closely related to each oth er and affect\nthe derived clusters.\n\u2022) Hierarchical clustering attempts to construct a\ntree-like nested structure partition of\n, such that\n, and\n imply\n or\n for all\n.\nFor hard partitional clustering, each pattern only belongs to\none cluster. However, a pattern may also be allowed to belongto all clusters with a degree of membership,\n, which\nrepresents the membership coef \ufb01cient of the\n th object in the\nth cluster and satis \ufb01es the following two constraints:\nand\nas introduced in fuzzy set theory [293]. This is known as fuzzy\nclustering, reviewed in Section II-G.\nFig. 1 depicts the procedure of cluster analysis with four basic\nsteps.\n1) Feature selection or extraction . As pointed out by Jain\net al. [151], [152] and Bishop [38], feature selection\nchoosesdistinguishingfeaturesfromasetofcandidates,\nwhile feature extraction utilizes some transformationsto generate useful and novel features from the originalones. Both are very crucial to the effectiveness of clus-\ntering applications. Elegant selection of features can\ngreatly decrease the workload and simplify the subse-quentdesignprocess.Generally,idealfeaturesshouldbeof use in distinguishing patterns belonging to different\nclusters, immune to noise, easy to extract and interpret.\nWe elaborate the discussion on feature extraction inSection II-L, in the context of data visualization anddimensionality reduction. More information on feature\nselection can be found in [38], [151], and [250].\n2) Clustering algorithm design or selection . The step is\nusually combined with the selection of a correspondingproximity measure and the construction of a criterion\nfunction. Patterns are grouped according to whether\nthey resemble each other. Obviously, the proximitymeasure directly affects the formation of the resultingclusters. Almost all clustering algorithms are explicitly\nor implicitly connected to some de \ufb01nition of proximity\nmeasure. Some algorithms even work directly on theproximity matrix, as de \ufb01ned in Section II-A. Once\na proximity measure is chosen, the construction of aclustering criterion function makes the partition of\nclusters an optimization problem, which is well de \ufb01ned\nmathematically, and has rich solutions in the literature.Clusteringisubiquitous,andawealthofclusteringalgo-\nrithmshasbeendevelopedto solvedifferentproblemsin\nspeci \ufb01c\ufb01elds.However,thereisnoclusteringalgorithm\nthat can be universally used to solve all problems. \u201cIt has\nbeen very dif \ufb01cult to develop a uni \ufb01ed framework for\nreasoning about it (clustering) at a technical level, and\nprofoundly diverse approaches to clustering \u201d[166], as\nproved through an impossibility theorem. Therefore, itis important to carefully investigate the characteristics\nof the problem at hand, in order to select or design an\nappropriate clustering strategy.\n3) Cluster validation . Given a data set, each clustering\nalgorithm can always generate a division, no matter\nwhether the structure exists or not. Moreover, different\napproaches usually lead to different clusters; and evenfor the same algorithm, parameter identi \ufb01cation or\nthe presentation order of input patterns may affect the\n\ufb01nal results. Therefore, effective evaluation standards\nand criteria are important to provide the users with adegree of con \ufb01dence for the clustering results derived\nfrom the used algorithms. These assessments should\nbe", "start_char_idx": 0, "end_char_idx": 3560, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7d97b121-9283-4ff0-84a0-1eff38df493f": {"__data__": {"id_": "7d97b121-9283-4ff0-84a0-1eff38df493f", "embedding": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2456850a-65d9-4aae-8eac-2037e676405f", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "d529da8b761f88408bd55460edabc6393d1e085c32e35c8e80a238a20b7246be"}, "2": {"node_id": "598882f9-dd8d-40c2-a43f-19a3a95789ec", "node_type": null, "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}, "hash": "3ccff81937e8d9644e76646a626c4ea21f3a5c4955c1c8597c19994158ee0a3f"}}, "hash": "b8e0a1fdb26934487eb7d37ec325cc279afca2ad118e77e57728f9ccefd2a7bd", "text": "diverse approaches to clustering \u201d[166], as\nproved through an impossibility theorem. Therefore, itis important to carefully investigate the characteristics\nof the problem at hand, in order to select or design an\nappropriate clustering strategy.\n3) Cluster validation . Given a data set, each clustering\nalgorithm can always generate a division, no matter\nwhether the structure exists or not. Moreover, different\napproaches usually lead to different clusters; and evenfor the same algorithm, parameter identi \ufb01cation or\nthe presentation order of input patterns may affect the\n\ufb01nal results. Therefore, effective evaluation standards\nand criteria are important to provide the users with adegree of con \ufb01dence for the clustering results derived\nfrom the used algorithms. These assessments should\nbe objective and have no preferences to any algorithm.\nAlso, they should be useful for answering questionslike how many clusters are hidden in the data, whetherthe clusters obtained are meaningful or just an artifact\nof the algorithms, or why we choose some algorithm\ninstead of another. Generally, there are three categoriesof testing criteria: external indices, internal indices,and relative indices. These are de \ufb01ned on three types\nof clustering structures, known as partitional clus-\ntering, hierarchical clustering, and individual clusters[150]. Tests for the situation, where no clusteringstructure exists in the data, are also considered [110],\nbut seldom used, since users are con \ufb01dent of the pres-\nence of clusters. External indices are based on someprespeci \ufb01ed structure, which is the re \ufb02ection of prior\ninformation on the data, and used as a standard to\nvalidate the clustering solutions. Internal tests are not\ndependent on external information (prior knowledge).On the contrary, they examine the clustering structuredirectly from the original data. Relative criteria place", "start_char_idx": 2766, "end_char_idx": 4647, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "193efdee-affc-47dd-b957-1495148e0469": {"__data__": {"id_": "193efdee-affc-47dd-b957-1495148e0469", "embedding": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2959f7-1309-454e-a4c7-f48d9e283445", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "3": {"node_id": "09fc3f4f-ac00-464e-bf5e-3b0068422cac", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "a57348a5b16f6044a35b8d5e29d600bc1116812b02bfa883f8461e8bd62d513b"}}, "hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 647\nthe emphasis on the comparison of different clustering\nstructures, in order to provide a reference, to decidewhich one may best reveal the characteristics of the\nobjects. We will not survey the topic in depth and refer\ninterested readers to [74], [110], and [150]. However,we will cover more details on how to determine thenumber of clusters in Section II-M. Some more recent\ndiscussion can be found in [22], [37], [121], [180],\nand [181]. Approaches for fuzzy clustering validityare reported in [71], [104], [123], and [220].\n4) Results interpretation. The ultimate goal of clustering\nis to provide users with meaningful insights from the\noriginal data, so that they can effectively solve theproblems encountered. Experts in the relevant \ufb01elds in-\nterpret the data partition. Further analyzes, even exper-\niments, may be required to guarantee the reliability of\nextracted knowledge.\nNote that the \ufb02ow chart also includes a feedback pathway.\nClusteranalysisisnotaone-shotprocess.Inmanycircumstances,\nit needs a series of trials and repetitions. Moreover, there are no\nuniversal and effective criteria to guide the selection of featuresand clustering schemes. Validation criteria provide some insightson the quality of clustering solutions. But even how to choose the\nappropriate criterion is still a problem requiring more efforts.\nClustering has been applied in a wide variety of \ufb01elds,\nranging from engineering (machine learning, arti \ufb01cial intelli-\ngence, pattern recognition, mechanical engineering, electrical\nengineering), computer sciences (web mining, spatial database\nanalysis, textual document collection, image segmentation),life and medical sciences (genetics, biology, microbiology,paleontology, psychiatry, clinic, pathology), to earth sciences\n(geography. geology, remote sensing), social sciences (soci-\nology, psychology, archeology, education), and economics(marketing, business) [88], [127]. Accordingly, clustering isalso known as numerical taxonomy, learning without a teacher\n(or unsupervised learning), typological analysis and partition.\nThe diversity re \ufb02ects the important position of clustering in\nscienti \ufb01c research. On the other hand, it causes confusion, due\nto the differing terminologies and goals. Clustering algorithms\ndeveloped to solve a particular problem, in a specialized \ufb01eld,\nusually make assumptions in favor of the application of interest.These biases inevitably affect performance in other problemsthat do not satisfy these premises. For example, the\n-means\nalgorithm is based on the Euclidean measure and, hence, tends\nto generate hyperspherical clusters. But if the real clusters arein other geometric forms,\n-means may no longer be effective,\nand we need to resort to other schemes. This situation also\nholds true for mixture-model clustering, in which a model is \ufb01t\nto data in advance.\nClustering has a long history, with lineage dating back to Aris-\ntotle [124]. General references on clustering techniques include\n[14], [75], [77], [88], [111], [127], [150], [161], [259]. Important\nsurvey papers on clustering techniques also exist in the literature.Starting from a statistical pattern recognition viewpoint, Jain,Murty,andFlynnreviewedtheclusteringalgorithmsandotherim-\nportant issues related to cluster analysis [152], while Hansen and\nJaumard described the clustering problems under a mathematicalprogramming scheme [124]. Kolatch and He investigated appli-cationsofclusteringalgorithmsforspatialdatabasesystems[171]\nand information retrieval", "start_char_idx": 0, "end_char_idx": 3551, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09fc3f4f-ac00-464e-bf5e-3b0068422cac": {"__data__": {"id_": "09fc3f4f-ac00-464e-bf5e-3b0068422cac", "embedding": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2959f7-1309-454e-a4c7-f48d9e283445", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "2": {"node_id": "193efdee-affc-47dd-b957-1495148e0469", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "6263f15c6fd1579a37da53e5d067afcf86d8558b261afea3855ba26ae05e12a5"}, "3": {"node_id": "c9983646-8fb8-497c-b167-66e800fdcc82", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "f48dafca25f69cc4f8bda1113a73f97757656b07c8e9d666730427beb72f22ce"}}, "hash": "a57348a5b16f6044a35b8d5e29d600bc1116812b02bfa883f8461e8bd62d513b", "text": "model is \ufb01t\nto data in advance.\nClustering has a long history, with lineage dating back to Aris-\ntotle [124]. General references on clustering techniques include\n[14], [75], [77], [88], [111], [127], [150], [161], [259]. Important\nsurvey papers on clustering techniques also exist in the literature.Starting from a statistical pattern recognition viewpoint, Jain,Murty,andFlynnreviewedtheclusteringalgorithmsandotherim-\nportant issues related to cluster analysis [152], while Hansen and\nJaumard described the clustering problems under a mathematicalprogramming scheme [124]. Kolatch and He investigated appli-cationsofclusteringalgorithmsforspatialdatabasesystems[171]\nand information retrieval [133], respectively. Berkhin further ex-panded the topic to the whole \ufb01eld of data mining [33]. Murtagh\nreported the advances in hierarchical clustering algorithms [210]\nandBaraldisurveyedseveralmodelsforfuzzyandneuralnetworkclustering [24]. Some more survey papers can also be found in[25], [40], [74], [89], and [151]. In addition to the review papers,\ncomparative research on clustering algorithms is also signi \ufb01cant.\nRauber, Paralic, and Pampalk presented empirical results for \ufb01ve\ntypical clustering algorithms [231]. Wei, Lee, and Hsu placed theemphasisonthecomparisonoffastalgorithmsforlargedatabases\n[280]. Scheunders compared several clustering techniques for\ncolor image quantization, with emphasis on computational timeandthepossibilityofobtainingglobaloptima[239].Applicationsand evaluations of different clustering algorithms for the analysis\nof gene expression data from DNA microarray experiments were\ndescribed in [153], [192], [246], and [271]. Experimental evalua-tionondocumentclusteringtechniques,basedonhierarchicaland\n-means clustering algorithms, were summarized by Steinbach,\nKarypis, and Kumar [261].\nIn contrast to the above, the purpose of this paper is to pro-\nvide a comprehensive and systematic description of the in \ufb02u-\nential and important clustering algorithms rooted in statistics,computer science, and machine learning, with emphasis on new\nadvances in recent years.\nThe remainder of the paper is organized as follows. In Sec-\ntion II, we review clustering algorithms, based on the natures\nof generated clusters and techniques and theories behind them.Furthermore, we discuss approaches for clustering sequentialdata, large data sets, data visualization, and high-dimensional\ndata through dimension reduction. Two important issues on\ncluster analysis, including proximity measure and how tochoose the number of clusters, are also summarized in thesection. This is the longest section of the paper, so, for conve-\nnience, we give an outline of Section II in bullet form here:\nII. Clustering Algorithms\n\u2022 A. Distance and Similarity Measures\n(See also Table I)\n\u2022 B. Hierarchical\n\u2014 Agglomerative\nSingle linkage, complete linkage, group average\nlinkage, median linkage, centroid linkage, Ward \u2019s\nmethod, balanced iterative reducing and clusteringusing hierarchies (BIRCH), clustering using rep-resentatives (CURE), robust clustering using links(ROCK)\n\u2014 Divisive\ndivisive analysis (DIANA), monothetic analysis\n(MONA)\n\u2022 C. Squared Error-Based (Vector Quantization)\n\u2014\n -means, iterative self-organizing data analysis\ntechnique (ISODATA), genetic\n -means algorithm\n(GKA), partitioning around medoids (PAM)\n\u2022 D. pdf Estimation via Mixture Densities\n\u2014 Gaussian mixture density decomposition", "start_char_idx": 2930, "end_char_idx": 6340, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c9983646-8fb8-497c-b167-66e800fdcc82": {"__data__": {"id_": "c9983646-8fb8-497c-b167-66e800fdcc82", "embedding": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "eb2959f7-1309-454e-a4c7-f48d9e283445", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "1ddcd76dbfbadad01d38ae850b5ade65259c8ed1d978961c882ef932aeb90d09"}, "2": {"node_id": "09fc3f4f-ac00-464e-bf5e-3b0068422cac", "node_type": null, "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}, "hash": "a57348a5b16f6044a35b8d5e29d600bc1116812b02bfa883f8461e8bd62d513b"}}, "hash": "f48dafca25f69cc4f8bda1113a73f97757656b07c8e9d666730427beb72f22ce", "text": "also Table I)\n\u2022 B. Hierarchical\n\u2014 Agglomerative\nSingle linkage, complete linkage, group average\nlinkage, median linkage, centroid linkage, Ward \u2019s\nmethod, balanced iterative reducing and clusteringusing hierarchies (BIRCH), clustering using rep-resentatives (CURE), robust clustering using links(ROCK)\n\u2014 Divisive\ndivisive analysis (DIANA), monothetic analysis\n(MONA)\n\u2022 C. Squared Error-Based (Vector Quantization)\n\u2014\n -means, iterative self-organizing data analysis\ntechnique (ISODATA), genetic\n -means algorithm\n(GKA), partitioning around medoids (PAM)\n\u2022 D. pdf Estimation via Mixture Densities\n\u2014 Gaussian mixture density decomposition (GMDD),\nAutoClass\n\u2022 E. Graph Theory-Based\n\u2014 Chameleon, Delaunay triangulation graph (DTG),\nhighly connected subgraphs (HCS), clustering iden-", "start_char_idx": 6327, "end_char_idx": 7104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2a4a5ea-d13c-4753-ad8c-0b61cb72d0da": {"__data__": {"id_": "a2a4a5ea-d13c-4753-ad8c-0b61cb72d0da", "embedding": null, "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9b87ab90-37e6-4a3b-98dc-578421c56631", "node_type": null, "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}, "hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d"}}, "hash": "0805e3dcceaf648114148058ae5e108b6281a5ebac0b345b1dd498788064b55d", "text": "648 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nTABLE I\nSIMILARITY AND DISSIMILARITY MEASURE FOR QUANTITATIVE FEATURES\nti\ufb01cation via connectivity kernels (CLICK), cluster\naf\ufb01nity search technique (CAST)\n\u2022 F. Combinatorial Search Techniques-Based\n\u2014 Genetically guided algorithm (GGA), TS clustering,\nSA clustering\n\u2022 G. Fuzzy\n\u2014 Fuzzy\n -means (FCM), mountain method (MM), pos-\nsibilistic\n -means clustering algorithm (PCM), fuzzy\n-shells (FCS)\n\u2022 H. Neural Networks-Based\n\u2014 Learning vector quantization (LVQ), self-organizing\nfeature map (SOFM), ART, simpli \ufb01ed ART (SART),\nhyperellipsoidal clustering network (HEC), self-split-ting competitive learning network (SPLL)\n\u2022 I. Kernel-Based\n\u2014 Kernel\n -means, support vector clustering (SVC)\n\u2022 J. Sequential Data\n\u2014 Sequence Similarity\n\u2014 Indirect sequence clustering\n\u2014 Statistical sequence clustering\n\u2022 K. Large-Scale Data Sets (See also Table II)\n\u2014 CLARA, CURE, CLARANS, BIRCH, DBSCAN,\nDENCLUE, WaveCluster, FC, ART\n\u2022 L. Data visualization and High-dimensional Data\n\u2014 PCA, ICA, Projection pursuit, Isomap, LLE,\nCLIQUE, OptiGrid, ORCLUS\n\u2022 M. How Many Clusters?Applications in two benchmark data sets, the traveling\nsalesman problem, and bioinformatics are illustrated in Sec-tion III. We conclude the paper in Section IV .\nII. C\nLUSTERING ALGORITHMS\nDifferent starting points and criteria usually lead to different\ntaxonomies of clustering algorithms [33], [88], [124], [150],[152], [171]. A rough but widely agreed frame is to classify\nclustering techniques as hierarchical clustering and parti-\ntional clustering, based on the properties of clusters generated[88], [152]. Hierarchical clustering groups data objects witha sequence of partitions, either from singleton clusters to a\ncluster including all individuals or vice versa, while partitional\nclustering directly divides data objects into some prespeci \ufb01ed\nnumber of clusters without the hierarchical structure. Wefollow this frame in surveying the clustering algorithms in the\nliterature. Beginning with the discussion on proximity measure,\nwhich is the basis for most clustering algorithms, we focus onhierarchical clustering and classical partitional clustering algo-rithms in Section II-B \u2013D. Starting from part E, we introduce\nand analyze clustering algorithms based on a wide variety of\ntheories and techniques, including graph theory, combinato-rial search techniques, fuzzy set theory, neural networks, andkernels techniques. Compared with graph theory and fuzzy set", "start_char_idx": 0, "end_char_idx": 2491, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9f7e9f0f-60c4-4f15-9742-d5b1a9582f46": {"__data__": {"id_": "9f7e9f0f-60c4-4f15-9742-d5b1a9582f46", "embedding": null, "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3a14d9d8-8dae-46ed-aa9f-203ef9a483a1", "node_type": null, "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}, "hash": "eb4549aa3fe8970830ceb7ede336adc8a4e61c273f25cdb0e04a9a9db070c96a"}}, "hash": "493f7df431511224349c07bd3d8f2485415412f03d4482c9997130b8f9b2065a", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 649\nTABLE II\nCOMPUTATIONAL COMPLEXITY OF CLUSTERING ALGORITHMS\ntheory, which had already been widely used in cluster analysis\nbefore the 1980s, the other techniques have been \ufb01nding their\napplications in clustering just in the recent decades. In spite ofthe short history, much progress has been achieved. Note that\nthese techniques can be used for both hierarchical and parti-\ntional clustering. Considering the more frequent requirement oftackling sequential data sets, large-scale, and high-dimensionaldata sets in many current applications, we review clustering\nalgorithms for them in the following three parts. We focus\nparticular attention on clustering algorithms applied in bioin-formatics. We offer more detailed discussion on how to identifyappropriate number of clusters, which is particularly important\nin cluster validity, in the last part of the section.\nA. Distance and Similarity Measures\nIt is natural to ask what kind of standards we should use to\ndetermine the closeness, or how to measure the distance (dis-\nsimilarity) or similarity between a pair of objects, an object anda cluster, or a pair of clusters. In the next section on hierarchicalclustering, we will illustrate linkage metrics for measuring prox-\nimity between clusters. Usually, a prototype is used to represent\na cluster so that it can be further processed like other objects.Here, we focus on reviewing measure approaches between in-dividuals due to the previous consideration.A data object is described by a set of features, usually repre-\nsented as a multidimensional vector. The features can be quan-titative or qualitative, continuous or binary, nominal or ordinal,\nwhich determine the corresponding measure mechanisms.\nA distance or dissimilarity function on a data set\nis de\ufb01ned\nto satisfy the following conditions.\n1) Symmetry.\n ;\n2) Positivity.\n for all\n and\n .\nIf conditions\n3) Triangle inequality.\nfor all\n and\nand (4) Re \ufb02exivity.\n also\nhold, it is called a metric.\nLikewise, a similarity function is de \ufb01ned to satisfy the con-\nditions in the following.\n1) Symmetry.\n ;\n2) Positivity.\n , for all\n and\n .\nIf it also satis \ufb01es conditions\n3)\nfor all\n and\nand (4)\n , it is called a simi-\nlarity metric.\nFor a data set with\n input patterns, we can de \ufb01ne an\nsymmetric matrix, called proximity matrix, whose\n th\nelement represents the similarity or dissimilarity measure for\nthe\nth and\n th patterns\n .\nTypically, distance functions are used to measure continuous\nfeatures, while similarity measures are more important for qual-\nitative variables. We summarize some typical measures for con-tinuous features in Table I. The selection of different measuresis problem dependent. For binary features, a similarity measure\nis commonly used (dissimilarity measures can be obtained by\nsimply using\n). Suppose we use two binary sub-\nscripts to count features in two objects.\n and\n represent\nthe number of simultaneous absence or presence of features in\ntwo objects, and\n and\n count the features present only in\none object. Then two types of commonly used similarity mea-sures for data points\nand\n are illustrated in the following.\n\u2022\nsimple matching coef \ufb01cient\nRogers and Tanimoto measure.\nGower and Legendre measure\nThese measures compute the match between two objects\ndirectly. Unmatched pairs are weighted based on their\ncontribution to the similarity.\n\u2022\nJaccard coef \ufb01cient\nSokal and Sneath measure.\nGower and Legendre measure", "start_char_idx": 0, "end_char_idx": 3464, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8a2a2b4-e978-40c0-a1e6-2fdbc010f120": {"__data__": {"id_": "c8a2a2b4-e978-40c0-a1e6-2fdbc010f120", "embedding": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "3": {"node_id": "a64fab5e-0406-46f8-9055-d14711882c66", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "16e463931f18f35b3642bdff70ab9cd6677d8115e9c41d75a045e80d955deb8d"}}, "hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a", "text": "650 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nThese measures focus on the co-occurrence features while\nignoring the effect of co-absence.\nFor nominal features that have more than two states, a simple\nstrategy needs to map them into new binary features [161], while\na more effective method utilizes the matching criterion\nwhere\nif\nand\n do not match\nif\nand\n match\n[88]. Ordinal features order multiple states according to some\nstandard and can be compared by using continuous dissimi-larity measures discussed in [161]. Edit distance for alphabetic\nsequences is discussed in Section II-J. More discussion on se-\nquences and strings comparisons can be found in [120] and[236].\nGenerally, for objects consisting of mixed variables, we can\nmap all these variables into the interval (0, 1) and use mea-\nsures like the Euclidean metric. Alternatively, we can trans-form them into binary variables and use binary similarity func-tions. The drawback of these methods is the information loss.\nA more powerful method was described by Gower in the formof\n, where\n indicates the\nsimilarity for the\n th feature and\n is a 0 \u20131 coef \ufb01cient based\non whether the measure of the two objects is missing [88], [112].\nB. Hierarchical Clustering\nHierarchical clustering (HC) algorithms organize data into a\nhierarchical structure according to the proximity matrix. The re-\nsults of HC are usually depicted by a binary tree or dendrogram.The root node of the dendrogram represents the whole data set\nand each leaf node is regarded as a data object. The interme-\ndiate nodes, thus, describe the extent that the objects are prox-imal to each other; and the height of the dendrogram usuallyexpresses the distance between each pair of objects or clusters,\nor an object and a cluster. The ultimate clustering results can\nbe obtained by cutting the dendrogram at different levels. Thisrepresentation provides very informative descriptions and visu-alization for the potential data clustering structures, especially\nwhen real hierarchical relations exist in the data, like the data\nfrom evolutionary research on different species of organizms.HC algorithms are mainly classi \ufb01ed as agglomerative methods\nand divisive methods. Agglomerative clustering starts with\nclusters and each of them includes exactly one object. A seriesof merge operations are then followed out that \ufb01nally lead all\nobjects to the same group. Divisive clustering proceeds in anopposite way. In the beginning, the entire data set belongs to\na cluster and a procedure successively divides it until all clus-\nters are singleton clusters. For a cluster with\nobjects, there\nare\n possible two-subset divisions, which is very ex-\npensive in computation [88]. Therefore, divisive clustering is\nnot commonly used in practice. We focus on the agglomera-\ntive clustering in the following discussion and some of divisiveclustering applications for binary data can be found in [88]. Twodivisive clustering algorithms, named MONA and DIANA, are\ndescribed in [161].The general agglomerative clustering can be summarized by\nthe following procedure.\n1) Start with\nsingleton clusters. Calculate the prox-\nimity matrix for the\n clusters.\n2) Search the minimal distance\nwhere\n is the distance function discussed be-\nfore, in the proximity matrix, and combine cluster\nand\n to form a new cluster.\n3) Update the proximity matrix by computing the dis-\ntances between the new cluster and the other clusters.\n4) Repeat steps 2) \u20133) until all objects are in the same\ncluster.\nBased on the different de \ufb01nitions for distance between two\nclusters, there are many agglomerative", "start_char_idx": 0, "end_char_idx": 3610, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a64fab5e-0406-46f8-9055-d14711882c66": {"__data__": {"id_": "a64fab5e-0406-46f8-9055-d14711882c66", "embedding": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "243d25c07321bee1459ee1ea7915622a7791acd164dfa9d2d61719d61b0d7f3f"}, "2": {"node_id": "c8a2a2b4-e978-40c0-a1e6-2fdbc010f120", "node_type": null, "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}, "hash": "9f018c9efe7910b449c040a723ea8bc73c389ae78c0ef054ef0698b2f51a625a"}}, "hash": "16e463931f18f35b3642bdff70ab9cd6677d8115e9c41d75a045e80d955deb8d", "text": "[88]. Twodivisive clustering algorithms, named MONA and DIANA, are\ndescribed in [161].The general agglomerative clustering can be summarized by\nthe following procedure.\n1) Start with\nsingleton clusters. Calculate the prox-\nimity matrix for the\n clusters.\n2) Search the minimal distance\nwhere\n is the distance function discussed be-\nfore, in the proximity matrix, and combine cluster\nand\n to form a new cluster.\n3) Update the proximity matrix by computing the dis-\ntances between the new cluster and the other clusters.\n4) Repeat steps 2) \u20133) until all objects are in the same\ncluster.\nBased on the different de \ufb01nitions for distance between two\nclusters, there are many agglomerative clustering algorithms.\nThe simplest and most popular methods include single linkage\n[256] and complete linkage technique [258]. For the singlelinkage method, the distance between two clusters is deter-mined by the two closest objects in different clusters, so it\nis also called nearest neighbor method. On the contrary, the\ncomplete linkage method uses the farthest distance of a pair ofobjects to de \ufb01ne inter-cluster distance. Both the single linkage\nand the complete linkage method can be generalized by the\nrecurrence formula proposed by Lance and Williams [178] as\nwhere\n is the distance function and\n , and\n are\ncoef\ufb01cients that take values dependent on the scheme used.\nThe formula describes the distance between a cluster\n and a\nnew cluster formed by the merge of two clusters\n and\n . Note\nthat when\n , and\n , the formula\nbecomes\nwhich corresponds to the single linkage method. When\nand\n , the formula is\nwhich corresponds to the complete linkage method.\nSeveral more complicated agglomerative clustering algo-\nrithms, including group average linkage, median linkage,centroid linkage, and Ward \u2019s method, can also be constructed\nby selecting appropriate coef \ufb01cients in the formula. A detailed\ntable describing the coef \ufb01cient values for different algorithms\nis offered in [150] and [210]. Single linkage, complete linkageand average linkage consider all points of a pair of clusters,\nwhen calculating their inter-cluster distance, and are also called\ngraph methods. The others are called geometric methods sincethey use geometric centers to represent clusters and determinetheir distances. Remarks on important features and properties\nof these methods are summarized in [88]. More inter-cluster", "start_char_idx": 2927, "end_char_idx": 5314, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "114811d9-85e3-4cf6-9789-9d2dcc7106e4": {"__data__": {"id_": "114811d9-85e3-4cf6-9789-9d2dcc7106e4", "embedding": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19f775fe-746a-420a-a946-5981879a2641", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "3": {"node_id": "d58e52a2-9c28-4857-895c-8364eef31bc0", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "8c61445f10725a12da31535e506cd42c771cc8ed8d643bdd508caf4cf875a5bc"}}, "hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 651\ndistance measures, especially the mean-based ones, were intro-\nduced by Yager, with further discussion on their possible effectto control the hierarchical clustering process [289].\nThe common criticism for classical HC algorithms is that they\nlack robustness and are, hence, sensitive to noise and outliers.Once an object is assigned to a cluster, it will not be considered\nagain, which means that HC algorithms are not capable of cor-\nrecting possible previous misclassi \ufb01cation. The computational\ncomplexity for most of HC algorithms is at least\nand\nthis high cost limits their application in large-scale data sets.\nOther disadvantages of HC include the tendency to form spher-\nical shapes and reversal phenomenon, in which the normal hier-archical structure is distorted.\nIn recent years, with the requirement for handling large-scale\ndata sets in data mining and other \ufb01elds, many new HC tech-\nniques have appeared and greatly improved the clustering per-formance. Typical examples include CURE [116], ROCK [117],\nChameleon [159], and BIRCH [295].\nThe main motivations of BIRCH lie in two aspects, the ability\nto deal with large data sets and the robustness to outliers [295].\nIn order to achieve these goals, a new data structure, clustering\nfeature (CF) tree, is designed to store the summaries of theoriginal data. The CF tree is a height-balanced tree, with eachinternal vertex composed of entries de \ufb01ned as\nchild\n, where\n is a representation of the cluster\n and\nis de \ufb01ned as\n , where\n is the number of\ndata objects in the cluster,\n is the linear sum of the objects,\nand SS is the squared sum of the objects, child\n is a pointer to the\nth child node, and\n is a threshold parameter that determines\nthe maximum number of entries in the vertex, and each leafcomposed of entries in the form of\n, where\nis the threshold parameter that controls the maximum number ofentriesintheleaf.Moreover,theleavesmustfollowtherestrictionthat the diameter\nof each entry in the leaf is less than a threshold\n . The CF\ntree structure captures the important clustering information of\nthe original data while reducing the required storage. Outliersare eliminated from the summaries by identifying the objectssparsely distributed in the feature space. After the CF tree is\nbuilt, an agglomerative HC is applied to the set of summaries to\nperform global clustering. An additional step may be performedto re \ufb01ne the clusters. BIRCH can achieve a computational\ncomplexity of\n.\nNoticing the restriction of centroid-based HC, which is\nunable to identify arbitrary cluster shapes, Guha, Rastogi, andShim developed a HC algorithm, called CURE, to explore more\nsophisticated cluster shapes [116]. The crucial feature of CURE\nlies in the usage of a set of well-scattered points to representeach cluster, which makes it possible to \ufb01nd rich cluster shapes\nother than hyperspheres and avoids both the chaining effect\n[88] of the minimum linkage method and the tendency to favor\nclusters with similar sizes of centroid. These representativepoints are further shrunk toward the cluster centroid accordingto an adjustable parameter in order to weaken the effects of\noutliers. CURE utilizes random sample (and partition) strategy\nto reduce computational complexity. Guha et al. also proposed\nanother agglomerative HC algorithm, ROCK, to group datawith qualitative attributes [117]. They used a novel measure\u201clink\u201dto describe the relation between a pair of objects and their\ncommon neighbors. Like CURE, a random sample strategy isused to", "start_char_idx": 0, "end_char_idx": 3562, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d58e52a2-9c28-4857-895c-8364eef31bc0": {"__data__": {"id_": "d58e52a2-9c28-4857-895c-8364eef31bc0", "embedding": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "19f775fe-746a-420a-a946-5981879a2641", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "2bbd038e62214f59141cfb30a69faf65643422df5a7e983822a055c13d5bf8d0"}, "2": {"node_id": "114811d9-85e3-4cf6-9789-9d2dcc7106e4", "node_type": null, "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}, "hash": "21c8d77e02d89cef5f9c5866895c2998869c0b0170919dfd3162e9510f4e37a4"}}, "hash": "8c61445f10725a12da31535e506cd42c771cc8ed8d643bdd508caf4cf875a5bc", "text": "points to representeach cluster, which makes it possible to \ufb01nd rich cluster shapes\nother than hyperspheres and avoids both the chaining effect\n[88] of the minimum linkage method and the tendency to favor\nclusters with similar sizes of centroid. These representativepoints are further shrunk toward the cluster centroid accordingto an adjustable parameter in order to weaken the effects of\noutliers. CURE utilizes random sample (and partition) strategy\nto reduce computational complexity. Guha et al. also proposed\nanother agglomerative HC algorithm, ROCK, to group datawith qualitative attributes [117]. They used a novel measure\u201clink\u201dto describe the relation between a pair of objects and their\ncommon neighbors. Like CURE, a random sample strategy isused to handle large data sets. Chameleon is constructed from\ngraph theory and will be discussed in Section II-E.\nRelative hierarchical clustering (RHC) is another exploration\nthat considers both the internal distance (distance between apair of clusters which may be merged to yield a new cluster)\nand the external distance (distance from the two clusters to the\nrest), and uses the ratio of them to decide the proximities [203].Leung et al. showed an interesting hierarchical clustering based\non scale-space theory [180]. They interpreted clustering using\na blurring process, in which each datum is regarded as a light\npoint in an image, and a cluster is represented as a blob. Liand Biswas extended agglomerative HC to deal with both nu-meric and nominal data. The proposed algorithm, called simi-\nlarity-based agglomerative clustering (SBAC), employs a mixed\ndata measure scheme that pays extra attention to less commonmatches of feature values [183]. Parallel techniques for HC arediscussed in [69] and [217], respectively.\nC. Squared Error\u2014Based Clustering (Vector Quantization)\nIn contrast to hierarchical clustering, which yields a succes-\nsive level of clusters by iterative fusions or divisions, partitionalclustering assigns a set of objects into\nclusters with no hier-\narchical structure. In principle, the optimal partition, based on\nsome speci \ufb01c criterion, can be found by enumerating all pos-\nsibilities. But this brute force method is infeasible in practice,due to the expensive computation [189]. Even for a small-scale\nclustering problem (organizing 30 objects into 3 groups), the\nnumber of possible partitions is\n. Therefore, heuristic\nalgorithms have been developed in order to seek approximatesolutions.\nOne of the important factors in partitional clustering is the\ncriterion function [124]. The sum of squared error function isone of the most widely used criteria. Suppose we have a set ofobjects\n, and we want to organize them\ninto\n subsets\n . The squared error criterion\nthen is de \ufb01ned as\nwhere\na partition matrix;\nif\n cluster\notherwisewith\ncluster prototype or centroid (means) matrix;\nsample mean for the\n th cluster;\nnumber of objects in the\n th cluster.\nNote the relation between the sum of squared error criterion\nand the scatter matrices de \ufb01ned in multiclass discriminant anal-\nysis [75],", "start_char_idx": 2802, "end_char_idx": 5875, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2fadee76-3fc6-489e-affe-96aa5ea758d7": {"__data__": {"id_": "2fadee76-3fc6-489e-affe-96aa5ea758d7", "embedding": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c255f5fc-f07d-4daa-8f84-0d36de6c2941", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "3": {"node_id": "76da649e-54b2-4621-b782-5a3bb9f35beb", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "5b66bcaed1b10281dd094449a89002c483217efa3b81238188ed5503d32f37a5"}}, "hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef", "text": "652 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nwhere\ntotal scatter matrix;\nwithin-class scatter matrix;\nbetween-class scatter matrix; and\nmean vector for the whole data set.\nIt is not dif \ufb01cult to see that the criterion based on the trace\nof\n is the same as the sum of squared error criterion. To\nminimize the squared error criterion is equivalent to minimizing\nthe trace of\n or maximizing the trace of\n . We can obtain\na rich class of criterion functions based on the characteristics of\nand\n [75].\nThe\n -means algorithm is the best-known squared\nerror-based clustering algorithm [94], [191].\n1) Initialize a\n -partition randomly or based on some\nprior knowledge. Calculate the cluster prototype ma-\ntrix\n .\n2) Assign each object in the data set to the nearest cluster\n, i.e.\nif\nfor\n and\n3) Recalculate the cluster prototype matrix based on the\ncurrent partition.\n4) Repeat steps 2) \u20133) until there is no change for each\ncluster.\nThe\n -means algorithm is very simple and can be easily\nimplemented in solving many practical problems. It can workvery well for compact and hyperspherical clusters. The time\ncomplexity of\n-means is\n . Since\n and\n are usu-\nally much less than\n -means can be used to cluster large\ndata sets. Parallel techniques for\n -means are developed that\ncan largely accelerate the algorithm [262]. The drawbacks of\n-means are also well studied, and as a result, many variants of\n-means have appeared in order to overcome these obstacles.\nWe summarize some of the major disadvantages with the pro-posed improvement in the following.\n1) There is no ef \ufb01cient and universal method for iden-\ntifying the initial partitions and the number of clus-ters\n. The convergence centroids vary with different\ninitial points. A general strategy for the problem is\nto run the algorithm many times with random initialpartitions. Pe \u00f1a, Lozano, and Larra \u00f1aga compared the\nrandom method with other three classical initial parti-\ntion methods by Forgy [94], Kaufman [161], and Mac-\nQueen [191], based on the effectiveness, robustness,and convergence speed criteria [227]. According totheir experimental results, the random and Kaufman \u2019s\nmethod work much better than the other two under the\n\ufb01rst two criteria and by further considering the conver-\ngence speed, they recommended Kaufman \u2019s method.\nBradley and Fayyad presented a re \ufb01nement algorithm\nthat\ufb01rst utilizes\n-means\n times to\n random sub-\nsets from the original data [43]. The set formed fromthe union of the solution (centroids of the\nclusters)of the\n subsets is clustered\n times again, setting\neach subset solution as the initial guess. The startingpoints for the whole data are obtained by choosing the\nsolution with minimal sum of squared distances. Likas,\nVlassis, and Verbeek proposed a global\n-means algo-\nrithm consisting of a series of\n -means clustering pro-\ncedures with the number of clusters varying from 1 to\n[186]. After \ufb01nding the centroid for only one cluster\nexisting, at each\n , the previous\ncentroids are \ufb01xed and the new centroid is selected by\nexamining all data points. The authors claimed that the\nalgorithm is independent of the initial partitions and\nprovided accelerating strategies. But the problem oncomputational complexity exists, due to the require-ment for executing\n-means\n times for each value\nof\n.\nAn interesting technique, called ISODATA, devel-\noped by Ball and", "start_char_idx": 0, "end_char_idx": 3375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "76da649e-54b2-4621-b782-5a3bb9f35beb": {"__data__": {"id_": "76da649e-54b2-4621-b782-5a3bb9f35beb", "embedding": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c255f5fc-f07d-4daa-8f84-0d36de6c2941", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "d8facc4ef390103bdb2358edb99ed4499ce07f18cbab8c414efe20020eb1ff5f"}, "2": {"node_id": "2fadee76-3fc6-489e-affe-96aa5ea758d7", "node_type": null, "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}, "hash": "5cd7243532b15e1db547bdf8f0eb09c742a1415e05502bffd45f8cdb2e58c9ef"}}, "hash": "5b66bcaed1b10281dd094449a89002c483217efa3b81238188ed5503d32f37a5", "text": "with minimal sum of squared distances. Likas,\nVlassis, and Verbeek proposed a global\n-means algo-\nrithm consisting of a series of\n -means clustering pro-\ncedures with the number of clusters varying from 1 to\n[186]. After \ufb01nding the centroid for only one cluster\nexisting, at each\n , the previous\ncentroids are \ufb01xed and the new centroid is selected by\nexamining all data points. The authors claimed that the\nalgorithm is independent of the initial partitions and\nprovided accelerating strategies. But the problem oncomputational complexity exists, due to the require-ment for executing\n-means\n times for each value\nof\n.\nAn interesting technique, called ISODATA, devel-\noped by Ball and Hall [21], deals with the estimationof\n. ISODATA can dynamically adjust the number of\nclusters by merging and splitting clusters according to\nsome prede \ufb01ned thresholds (in this sense, the problem\nof identifying the initial number of clusters becomesthat of parameter (threshold) tweaking). The new\nis\nused as the expected number of clusters for the next it-\neration.\n2) The iteratively optimal procedure of\n -means cannot\nguarantee convergence to a global optimum. The sto-\nchastic optimal techniques, like simulated annealing\n(SA) and genetic algorithms (also see part II.F), can\ufb01nd the global optimum with the price of expensive\ncomputation. Krishna and Murty designed new opera-\ntors in their hybrid scheme, GKA, in order to achieve\nglobal search and fast convergence [173]. The de \ufb01ned\nbiased mutation operator is based on the Euclideandistance between an object and the centroids and aims\nto avoid getting stuck in a local optimum. Another\noperator, the\n-means operator (KMO), replaces the\ncomputationally expensive crossover operators andalleviates the complexities coming with them. An\nadaptive learning rate strategy for the online mode\n-means is illustrated in [63]. The learning rate is\nexclusively dependent on the within-group variationsand can be adjusted without involving any user activi-\nties. The proposed enhanced LBG (ELBG) algorithm\nadopts a roulette mechanism typical of genetic algo-rithms to become near-optimal and therefore, is notsensitive to initialization [222].\n3)\n-means is sensitive to outliers and noise. Even if an\nobject is quite far away from the cluster centroid, it isstill forced into a cluster and, thus, distorts the clustershapes. ISODATA [21] and PAM [161] both consider\nthe effect of outliers in clustering procedures. ISO-\nDATA gets rid of clusters with few objects. The split-ting operation of ISODATA eliminates the possibilityof elongated clusters typical of\n-means. PAM utilizes\nreal data points (medoids) as the cluster prototypes and\navoids the effect of outliers. Based on the same con-sideration, a\n-medoids algorithm is presented in [87]", "start_char_idx": 2691, "end_char_idx": 5467, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "32fc8f4e-0ebd-447d-ab6d-d63bfc7f831e": {"__data__": {"id_": "32fc8f4e-0ebd-447d-ab6d-d63bfc7f831e", "embedding": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5617f37-08ac-4921-bb85-c5619d31fe0a", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "3": {"node_id": "4332bf06-23a4-4f9e-90cc-f37645ddd5fb", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "bf3b68a485a80b23b90ff7f37b93d3218d1d06a14c50fd09fcc8c6ccdc459e0d"}}, "hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 653\nby searching the discrete 1-medians as the cluster cen-\ntroids.\n4) The de \ufb01nition of \u201cmeans \u201dlimits the application\nonly to numerical variables. The\n -medoids algo-\nrithm mentioned previously is a natural choice, whenthe computation of means is unavailable, since themedoids do not need any computation and always exist\n[161]. Huang [142] and Gupta et al. [118] de \ufb01ned dif-\nferent dissimilarity measures to extend\n-means\nto categorical variables. For Huang \u2019s method, the\nclustering goal is to minimize the cost function\n, where\nand\nwith a set of\n -dimensional vectors\n, where\n .\nEach vector\n is known as a mode and is de \ufb01ned to\nminimize the sum of distances\n . The\nproposed\n -modes algorithm operates in a similar\nway as\n -means.\nSeveral recent advances on\n -means and other squared-error\nbased clustering algorithms with their applications can be foundin [125], [155], [222], [223], [264], and [277].\nD. Mixture Densities-Based Clustering (pdf Estimation via\nMixture Densities)\nIn the probabilistic view, data objects are assumed to be gen-\nerated according to several probability distributions. Data pointsin different clusters were generated by different probability dis-tributions. They can be derived from different types of density\nfunctions (e.g., multivariate Gaussian or\n-distribution), or the\nsame families, but with different parameters. If the distributionsare known, \ufb01nding the clusters of a given data set is equivalent\nto estimating the parameters of several underlying models. Sup-\npose the prior probability (also known as mixing probability)\nfor cluster\n (here,\n is assumed to\nbe known and methods for estimating\n are discussed in Sec-\ntion II-M) and the conditional probability density\n(also known as component density), where\n is the unknown\nparameter vector, are known. Then, the mixture probability den-sity for the whole data set is expressed as\nwhere\n , and\n . As long as\nthe parameter vector\n is decided, the posterior probability for\nassigning a data point to a cluster can be easily calculated withBayes \u2019s theorem. Here, the mixtures can be constructed with\nany types of components, but more commonly, multivariateGaussian densities are used due to its complete theory and\nanalytical tractability [88], [297].\nMaximum likelihood (ML) estimation is an important statis-\ntical approach for parameter estimation [75] and it considers the\nbest estimate as the one that maximizes the probability of gen-erating all the observations, which is given by the joint densityfunction\nor, in a logarithm form\nThe best estimate can be achieved by solving the log-likelihood\nequations\n .\nUnfortunately, since the solutions of the likelihood equa-\ntions cannot be obtained analytically in most circumstances[90], [197], iteratively suboptimal approaches are required to\napproximate the ML estimates. Among these methods, the\nexpectation-maximization (EM) algorithm is the most popular[196]. EM regards the data set as incomplete and divideseach data point\ninto two parts\n , where\nrepresents the observable features and\nis the missing data, where\n chooses value 1 or 0 according\nto whether\n belongs to the component\n or not. Thus, the\ncomplete data log-likelihood is\nThe standard EM algorithm generates a series of parameter\nestimates\n , where\n represents the reaching of\nthe convergence criterion, through the following steps:\n1) initialize\n and set\n ;\n2) e-step:", "start_char_idx": 0, "end_char_idx": 3432, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4332bf06-23a4-4f9e-90cc-f37645ddd5fb": {"__data__": {"id_": "4332bf06-23a4-4f9e-90cc-f37645ddd5fb", "embedding": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a5617f37-08ac-4921-bb85-c5619d31fe0a", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "b4947ae526abb9965563d9c1623c69b71b1bb6b62d1ee0e59b18e5114943f32c"}, "2": {"node_id": "32fc8f4e-0ebd-447d-ab6d-d63bfc7f831e", "node_type": null, "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}, "hash": "a78d2866de3726e61691d5fa7024fdb32336199c28978053cf243afec7bce2f2"}}, "hash": "bf3b68a485a80b23b90ff7f37b93d3218d1d06a14c50fd09fcc8c6ccdc459e0d", "text": "of the likelihood equa-\ntions cannot be obtained analytically in most circumstances[90], [197], iteratively suboptimal approaches are required to\napproximate the ML estimates. Among these methods, the\nexpectation-maximization (EM) algorithm is the most popular[196]. EM regards the data set as incomplete and divideseach data point\ninto two parts\n , where\nrepresents the observable features and\nis the missing data, where\n chooses value 1 or 0 according\nto whether\n belongs to the component\n or not. Thus, the\ncomplete data log-likelihood is\nThe standard EM algorithm generates a series of parameter\nestimates\n , where\n represents the reaching of\nthe convergence criterion, through the following steps:\n1) initialize\n and set\n ;\n2) e-step: Compute the expectation of the complete data\nlog-likelihood\n3) m-step: Select a new parameter estimate that maxi-\nmizes the\n -function,\n ;\n4) Increase\n ; repeat steps 2) \u20133) until the conver-\ngence condition is satis \ufb01ed.\nThe major disadvantages for EM algorithm are the sensitivity\nto the selection of initial parameters, the effect of a singular co-\nvariance matrix, the possibility of convergence to a local op-\ntimum, and the slow convergence rate [96], [196]. Variants ofEM for addressing these problems are discussed in [90] and[196].\nA valuable theoretical note is the relation between the EM\nalgorithm and the\n-means algorithm. Celeux and Govaert\nproved that classi \ufb01cation EM (CEM) algorithm under a spher-\nical Gaussian mixture is equivalent to the\n -means algorithm\n[58].", "start_char_idx": 2693, "end_char_idx": 4215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de67346e-2716-4865-a7c3-dca015ef3509": {"__data__": {"id_": "de67346e-2716-4865-a7c3-dca015ef3509", "embedding": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e951868-2342-4a1d-9c18-bad4bc6a98cd", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "3": {"node_id": "f45ea891-8def-4990-b182-1f239497975d", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "6816d3e526562d03e7bb7b5b627b317558279d961a017c2face016e24e2c5d55"}}, "hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d", "text": "654 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFraley and Raftery described a comprehensive mix-\nture-model based clustering scheme [96], which was im-plemented as a software package, known as MCLUST [95]. In\nthis case, the component density is multivariate Gaussian, with\na mean vector\nand a covariance matrix\n as the parameters\nto be estimated. The covariance matrix for each component canfurther be parameterized by virtue of eigenvalue decomposi-\ntion, represented as\n, where\n is a scalar,\n is\nthe orthogonal matrix of eigenvectors, and\n is the diagonal\nmatrix based on the eigenvalues of\n [96]. These three elements\ndetermine the geometric properties of each component. After\nthe maximum number of clusters and the candidate models are\nspeci \ufb01ed, an agglomerative hierarchical clustering was used to\nignite the EM algorithm by forming an initial partition, whichincludes at most the maximum number of clusters, for each\nmodel. The optimal clustering result is achieved by checking\nthe Bayesian information criterion (BIC) value discussed inSection II-M. GMDD is also based on multivariate Gaussiandensities and is designed as a recursive algorithm that sequen-\ntially estimates each component [297]. GMDD views data\npoints that are not generated from a distribution as noise andutilizes an enhanced model- \ufb01tting estimator to construct each\ncomponent from the contaminated model. AutoClass considers\nmore families of probability distributions (e.g., Poisson and\nBernoulli) for different data types [59]. A Bayesian approach isused in AutoClass to \ufb01nd out the optimal partition of the given\ndata based on the prior probabilities. Its parallel realization is\ndescribed in [228]. Other important algorithms and programs\ninclude Multimix [147], EM based mixture program (EMMIX)[198], and Snob [278].\nE. Graph Theory-Based Clustering\nThe concepts and properties of graph theory [126] make it\nvery convenient to describe clustering problems by means ofgraphs. Nodes\nof a weighted graph\n correspond to data\npoints in the pattern space and edges\n re\ufb02ect the proximities\nbetween each pair of data points. If the dissimilarity matrix is\nde\ufb01ned as\nif\notherwise\nwhere\n is a threshold value, the graph is simpli \ufb01ed to an\nunweighted threshold graph. Both the single linkage HC and\nthe complete linkage HC can be described on the basis ofthe threshold graph. Single linkage clustering is equivalent toseeking maximally connected subgraphs (components) while\ncomplete linkage clustering corresponds to \ufb01nding maximally\ncomplete subgraphs (cliques) [150]. Jain and Dubes illustratedand discussed more applications of graph theory (e.g., Hubert \u2019s\nalgorithm and Johnson \u2019s algorithm) for hierarchical clustering\nin [150]. Chameleon [159] is a newly developed agglomerative\nHC algorithm based on the\n-nearest-neighbor graph, in which\nan edge is eliminated if both vertices are not within the\nclosest points related to each other. At the \ufb01rst step, Chameleon\ndivides the connectivity graph into a set of subclusters with the\nminimal edge cut. Each subgraph should contain enough nodesin order for effective similarity computation. By combiningboth the relative interconnectivity and relative closeness, whichmake Chameleon \ufb02exible enough to explore the characteristics\nof potential clusters, Chameleon merges these small subsetsand, thus, comes up with the ultimate clustering solutions.\nHere, the relative interconnectivity (or closeness) is obtained\nby normalizing the sum of weights (or average weight) of", "start_char_idx": 0, "end_char_idx": 3514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f45ea891-8def-4990-b182-1f239497975d": {"__data__": {"id_": "f45ea891-8def-4990-b182-1f239497975d", "embedding": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2e951868-2342-4a1d-9c18-bad4bc6a98cd", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "580f46a4f466376a6aad98a138fa17778126b60692afc89c3f8730100e157788"}, "2": {"node_id": "de67346e-2716-4865-a7c3-dca015ef3509", "node_type": null, "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}, "hash": "e5c7c586b61e84a906cb9e7e525dd2d97dcbebdd5e62bcf0c3b7d4491c43c46d"}}, "hash": "6816d3e526562d03e7bb7b5b627b317558279d961a017c2face016e24e2c5d55", "text": "is a newly developed agglomerative\nHC algorithm based on the\n-nearest-neighbor graph, in which\nan edge is eliminated if both vertices are not within the\nclosest points related to each other. At the \ufb01rst step, Chameleon\ndivides the connectivity graph into a set of subclusters with the\nminimal edge cut. Each subgraph should contain enough nodesin order for effective similarity computation. By combiningboth the relative interconnectivity and relative closeness, whichmake Chameleon \ufb02exible enough to explore the characteristics\nof potential clusters, Chameleon merges these small subsetsand, thus, comes up with the ultimate clustering solutions.\nHere, the relative interconnectivity (or closeness) is obtained\nby normalizing the sum of weights (or average weight) of theedges connecting the two clusters over the internal connectivity(or closeness) of the clusters. DTG is another important graph\nrepresentation for HC analysis. Cherng and Lo constructed a\nhypergraph (each edge is allowed to connect more than twovertices) from the DTG and used a two-phase algorithm that issimilar to Chameleon to \ufb01nd clusters [61]. Another DTG-based\napplication, known as AMOEBA algorithm, is presented in\n[86].\nGraph theory can also be used for nonhierarchical clusters.\nZahn \u2019s clustering algorithm seeks connected components as\nclusters by detecting and discarding inconsistent edges in the\nminimum spanning tree [150]. Hartuv and Shamir treated clus-ters as HCS, where \u201chighly connected \u201dmeans the connectivity\n(the minimum number of edges needed to disconnect a graph)\nof the subgraph is at least half as great as the number of the\nvertices [128]. A minimum cut (mincut) procedure, whichaims to separate a graph with a minimum number of edges, isused to \ufb01nd these HCSs recursively. Another algorithm, called\nCLICK, is based on the calculation of the minimum weight\ncut to form clusters [247]. Here, the graph is weighted and theedge weights are assigned a new interpretation, by combiningprobability and graph theory. The edge weight between node\nand\n is de \ufb01ned as shown in\nbelong to the same cluster\ndoes not belong to the same cluster\nwhere\n represents the similarity between the two nodes.\nCLICK further assumes that the similarity values within clus-ters and between clusters follow Gaussian distributions with\ndifferent means and variances, respectively. Therefore, the\nprevious equation can be rewritten by using Bayes \u2019theorem as\nwhere\n is the prior probability that two objects belong to\nthe same cluster and\n are the means and\nvariances for between-cluster similarities and within-clusters\nsimilarities, respectively. These parameters can be estimatedeither from prior knowledge, or by using parameter estimationmethods [75]. CLICK recursively checks the current subgraph,\nand generates a kernel list, which consists of the components\nsatisfying some criterion function. Subgraphs that include onlyone node are regarded as singletons, and are separated forfurther manipulation. Using the kernels as the basic clusters,\nCLICK carries out a series of singleton adoptions and cluster\nmerge to generate the resulting clusters. Additional heuristicsare provided to accelerate the algorithm performance.\nSimilarly, CAST considers a probabilistic model in designing\na graph theory-based clustering algorithm [29]. Clusters are\nmodeled as corrupted clique graphs, which, in ideal conditions,are regarded as a set of disjoint cliques. The effect of noiseis incorporated by adding or removing edges from the ideal", "start_char_idx": 2746, "end_char_idx": 6252, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dcaca609-a339-413f-9417-4112d5e26099": {"__data__": {"id_": "dcaca609-a339-413f-9417-4112d5e26099", "embedding": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5bd3b13-52c3-47c1-8340-782660816dd3", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "3": {"node_id": "1370603c-d302-496a-ab5d-4d5d7bd0e02d", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "9d000eff7221771de29f17ebb8c74a9251e8f673d5adeb3426e6acf9e412dcc1"}}, "hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 655\nmodel, with a probability\n . Proofs were given for recovering\nthe uncorrupted graph with a high probability. CAST is theheuristic implementation of the original theoretical version.\nCAST creates clusters sequentially, and each cluster begins\nwith a random and unassigned data point. The relation betweena data point\nand a cluster\n being built is determined by the\naf\ufb01nity, de \ufb01ned as\n , and the af \ufb01nity threshold\nparameter\n . When\n , it means that the data point is\nhighly related to the cluster and vice versa. CAST alternatelyadds high af \ufb01nity data points or deletes low af \ufb01nity data points\nfrom the cluster until no more changes occur.\nF . Combinatorial Search Techniques-Based Clustering\nThe basic object of search techniques is to \ufb01nd the global\nor approximate global optimum for combinatorial optimization\nproblems, which usually have NP-hard complexity and need to\nsearch an exponentially large solution space. Clustering can beregarded as a category of optimization problems. Given a set ofdata points\n, clustering algorithms aim\nto organize them into\n subsets\n that optimize\nsome criterion function. The possible partition for\n points into\nclusters is given by the formula [189]\nAs shown before, even for small\n and\n , the computa-\ntional complexity is extremely expensive, not to mention thelarge-scale clustering problems frequently encountered in recentdecades. Simple local search techniques, like hill-climbing al-\ngorithms, are utilized to \ufb01nd the partitions, but they are easily\nstuck in local minima and therefore cannot guarantee optimality.More complex search methods (e.g., evolutionary algorithms(EAs) [93], SA [165], and Tabu search (TS) [108] are known as\nstochastic optimization methods, while deterministic annealing\n(DA) [139], [234] is the most typical deterministic search tech-nique) can explore the solution space more \ufb02exibly and ef \ufb01-\nciently.\nInspired by the natural evolution process, evolutionary com-\nputation, which consists of genetic algorithms (GAs), evolutionstrategies (ESs), evolutionary programming (EP), and geneticprogramming (GP), optimizes a population of structure by using\na set of evolutionary operators [93]. An optimization function,\ncalled the \ufb01tness function, is the standard for evaluating the opti-\nmizing degree of the population, in which each individual has itscorresponding \ufb01tness. Selection, recombination, and mutation\nare the most widely used evolutionary operators. The selection\noperator ensures the continuity of the population by favoring thebest individuals in the next generation. The recombination andmutation operators support the diversity of the population by ex-\nerting perturbations on the individuals. Among many EAs, GAs\n[140] are the most popular approaches applied in cluster anal-ysis. In GAs, each individual is usually encoded as a binary bitstring, called a chromosome. After an initial population is gener-\nated according to some heuristic rules or just randomly, a series\nof operations, including selection, crossover and mutation, areiteratively applied to the population until the stop condition issatis\ufb01ed.Hall, \u00d6zyurt, and Bezdek proposed a GGA that can be re-\ngarded as a general scheme for center-based (hard or fuzzy)clustering problems [122]. Fitness functions are reformulated\nfrom the standard sum of squared error criterion function in\norder to adapt the change of the construction of the optimiza-tion problem (only the prototype matrix is needed)\nfor hard clustering\nfor fuzzy clustering\nwhere\n , is the distance between\nthe\nth", "start_char_idx": 0, "end_char_idx": 3583, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1370603c-d302-496a-ab5d-4d5d7bd0e02d": {"__data__": {"id_": "1370603c-d302-496a-ab5d-4d5d7bd0e02d", "embedding": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5bd3b13-52c3-47c1-8340-782660816dd3", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "b36f3d40f003a2fa383f4b32dc176c45cbeb8e1ab98db9d23c7af2f63d906bd7"}, "2": {"node_id": "dcaca609-a339-413f-9417-4112d5e26099", "node_type": null, "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}, "hash": "9ae06ab2811d7d1507267de12f9f127f1b315dd251866cf0374d7ba44f4b80f6"}}, "hash": "9d000eff7221771de29f17ebb8c74a9251e8f673d5adeb3426e6acf9e412dcc1", "text": "In GAs, each individual is usually encoded as a binary bitstring, called a chromosome. After an initial population is gener-\nated according to some heuristic rules or just randomly, a series\nof operations, including selection, crossover and mutation, areiteratively applied to the population until the stop condition issatis\ufb01ed.Hall, \u00d6zyurt, and Bezdek proposed a GGA that can be re-\ngarded as a general scheme for center-based (hard or fuzzy)clustering problems [122]. Fitness functions are reformulated\nfrom the standard sum of squared error criterion function in\norder to adapt the change of the construction of the optimiza-tion problem (only the prototype matrix is needed)\nfor hard clustering\nfor fuzzy clustering\nwhere\n , is the distance between\nthe\nth cluster and the\n th data object, and\n is the fuzzi \ufb01cation\nparameter.\nGGA proceeds with the following steps.\n1) Choose appropriate parameters for the algorithm. Ini-\ntialize the population randomly with\n individuals,\neach of which represents a\n prototype matrix and\nis encoded as gray codes. Calculate the \ufb01tness value for\neach individual.\n2) Use selection (tournament selection) operator to\nchoose parental members for reproduction.\n3) Use crossover (two-point crossover) and mutation (bit-\nwise mutation) operator to generate offspring from theindividuals chosen in step 2).\n4) Determine the next generation by keeping the individ-\nuals with the highest \ufb01tness.\n5) Repeat steps 2) \u20134) until the termination condition is\nsatis\ufb01ed.\nOther GAs-based clustering applications have appeared\nbased on a similar framework. They are different in themeaning of an individual in the population, encoding methods,\ufb01tness function de \ufb01nition, and evolutionary operators [67],\n[195], [273]. The algorithm CLUSTERING in [273] includes\na heuristic scheme for estimating the appropriate number ofclusters in the data. It also uses a nearest-neighbor algorithmto divide data into small subsets, before GAs-based clustering,\nin order to reduce the computational complexity. GAs are very\nuseful for improving the performance of\n-means algorithms.\nBabu and Murty used GAs to \ufb01nd good initial partitions [15].\nKrishna and Murty combined GA with\n -means and devel-\noped GKA algorithm that can \ufb01nd the global optimum [173].\nAs indicated in Section II-C, the algorithm ELBG uses theroulette mechanism to address the problems due to the badinitialization [222]. It is worthwhile to note that ELBG are\nequivalent to another algorithm, fully automatic clustering\nsystem (FACS) [223], in terms of quantization level detection.The difference lies in the input parameters employed (ELBGadopts the number of quantization levels, while FACS uses\nthe desired distortion error). Except the previous applications,\nGAs can also be used for hierarchical clustering. Lozano andLarra \u00f1ag discussed the properties of ultrametric distance [127]\nand reformulated the hierarchical clustering as an optimization", "start_char_idx": 2824, "end_char_idx": 5750, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1391ba94-bcad-4c9a-ad5d-bdbaa25d36b0": {"__data__": {"id_": "1391ba94-bcad-4c9a-ad5d-bdbaa25d36b0", "embedding": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec3a32df-42a7-4f8d-8320-9e16855fdedc", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "3": {"node_id": "9a12ce67-810f-4816-b5a6-24891d994b91", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "fae0694d1c68f9999de65e230b991eb8e2d7873c7cd5e3bf8674332a81903573"}}, "hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c", "text": "656 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nproblem that tries to \ufb01nd the closest ultrametic distance for a\ngiven dissimilarity with Euclidean norm [190]. They suggestedan order-based GA to solve the problem. Clustering algorithms\nbased on ESs and EP are described and analyzed in [16] and\n[106], respectively.\nTS is a combinatory search technique that uses the tabu list to\nguide the search process consisting of a sequence of moves. The\ntabu list stores part or all of previously selected moves according\nto the speci \ufb01ed size. These moves are forbidden in the current\nsearch and are called tabu. In the TS clustering algorithm devel-oped by Al-Sultan [9], a set of candidate solutions are generated\nfrom the current solution with some strategy. Each candidate so-\nlution represents the allocations of\ndata objects in\n clusters.\nThe candidate with the optimal cost function is selected as thecurrent solution and appended to the tabu list, if it is not already\nin the tabu list or meets the aspiration criterion, which can over-\nrule the tabu restriction. Otherwise, the remaining candidatesare evaluated in the order of their cost function values, until allthese conditions are satis \ufb01ed. When all the candidates are tabu, a\nnew set of candidate solutions are created followed by the same\nsearch process. The search process proceeds until the maximumnumber of iterations is reached. Sung and Jin \u2019s method includes\nmore elaborate search processes with the packing and releasing\nprocedures [266]. They also used a secondary tabu list to keep\nthe search from trapping into the potential cycles. A fuzzy ver-sion of TS clustering can be found in [72].\nSA is also a sequential and global search technique and is mo-\ntivated by the annealing process in metallurgy [165]. SA allowsthe search process to accept a worse solution with a certain prob-ability. The probability is controlled by a parameter, known as\ntemperature\nand is usually expressed as\n , where\nis the change of the energy (cost function). The tempera-\nture\n goes through an annealing schedule from initial high to\nultimate low values, which means that SA attempts to explore\nsolution space more completely at high temperatures while fa-\nvors the solutions that lead to lower energy at low temperatures.SA-based clustering was reported in [47] and [245]. The formerillustrated an application of SA clustering to evaluate different\nclustering criteria and the latter investigated the effects of input\nparameters to the clustering performance.\nHybrid approaches that combine these search techniques are\nalso proposed. A tabu list is used in a GA clustering algorithm to\npreserve the variety of the population and avoid repeating com-\nputation [243]. An application of SA for improving TS was re-ported in [64]. The algorithm further reduces the possible movesto local optima.\nThe main drawback that plagues the search techniques-based\nclustering algorithms is the parameter selection. More oftenthan not, search techniques introduce more parameters thanother methods (like\n-means). There are no theoretic guide-\nlines to select the appropriate and effective parameters. Hall\net al. provided some methods for setting parameters in their\nGAs-based clustering framework [122], but most of thesecriteria are still obtained empirically. The same situation exists\nfor TS and SA clustering [9], [245]. Another problem is the\ncomputational complexity paid for the convergence to globaloptima. High computational requirement limits their applica-tions in large-scale data sets.G. Fuzzy Clustering\nExcept for GGA, the clustering techniques we have discussed\nso far are referred to as hard or crisp clustering, which means\nthat each object is assigned to only one", "start_char_idx": 0, "end_char_idx": 3735, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9a12ce67-810f-4816-b5a6-24891d994b91": {"__data__": {"id_": "9a12ce67-810f-4816-b5a6-24891d994b91", "embedding": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ec3a32df-42a7-4f8d-8320-9e16855fdedc", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "9d0365ef0e21379e5e06bf7b3cda221f7a83bf264bf49eea336edd7d079548d8"}, "2": {"node_id": "1391ba94-bcad-4c9a-ad5d-bdbaa25d36b0", "node_type": null, "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}, "hash": "c7bd37cdb61e280fb87eefe366c185dab393ccc1208a714a13c17116fcf0783c"}}, "hash": "fae0694d1c68f9999de65e230b991eb8e2d7873c7cd5e3bf8674332a81903573", "text": "oftenthan not, search techniques introduce more parameters thanother methods (like\n-means). There are no theoretic guide-\nlines to select the appropriate and effective parameters. Hall\net al. provided some methods for setting parameters in their\nGAs-based clustering framework [122], but most of thesecriteria are still obtained empirically. The same situation exists\nfor TS and SA clustering [9], [245]. Another problem is the\ncomputational complexity paid for the convergence to globaloptima. High computational requirement limits their applica-tions in large-scale data sets.G. Fuzzy Clustering\nExcept for GGA, the clustering techniques we have discussed\nso far are referred to as hard or crisp clustering, which means\nthat each object is assigned to only one cluster. For fuzzy clus-tering, this restriction is relaxed, and the object can belong to all\nof the clusters with a certain degree of membership [293]. This\nis particularly useful when the boundaries among the clustersare not well separated and ambiguous. Moreover, the member-ships may help us discover more sophisticated relations between\na given object and the disclosed clusters.\nFCM is one of the most popular fuzzy clustering algorithms\n[141]. FCM can be regarded as a generalization of ISODATA[76] and was realized by Bezdek [35]. FCM attempts to \ufb01nd a\npartition (\nfuzzy clusters) for a set of data points\nwhile minimizing the cost function\nwhere\nis the fuzzy partition matrix and\nis the membership coef \ufb01cient of the\nth object in the\n th cluster;\nis the cluster prototype (mean or center)\nmatrix;\nis the fuzzi \ufb01cation parameter and usually\nis set to 2 [129];\nis the distance measure between\n and\n.\nWe summarize the standard FCM as follows, in which the\nEuclidean or\n norm distance function is used.\n1) Select appropriate values for\n , and a small positive\nnumber\n . Initialize the prototype matrix\n randomly.\nSet step variable\n .\n2) Calculate (at\n ) or update (at\n ) the member-\nship matrix\n by\nfor\n and\n3) Update the prototype matrix\n by\nfor\n4) Repeat steps 2) \u20133) until\n .\nNumerous FCM variants and other fuzzy clustering algo-\nrithms have appeared as a result of the intensive investigation\non the distance measure functions, the effect of weightingexponent on fuzziness control, the optimization approaches forfuzzy partition, and improvements of the drawbacks of FCM\n[84], [141].\nLike its hard counterpart, FCM also suffers from the presence\nof noise and outliers and the dif \ufb01culty to identify the initial par-\ntitions. Yager and Filev proposed a MM in order to estimate the", "start_char_idx": 2973, "end_char_idx": 5525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5dbffff8-d037-4461-a8b5-a910192784bd": {"__data__": {"id_": "5dbffff8-d037-4461-a8b5-a910192784bd", "embedding": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9df74eb1-5688-4979-a8aa-865fca7c833e", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "3": {"node_id": "d70b24b2-9928-432a-8f0d-9a1a91c7092d", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "4cf7630f511417962f3022c1045bdbcd41b0dee280d4c29a02514511e87b742d"}}, "hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 657\ncenters of clusters [290]. Candidate centers consist of a set of\nvertices that are formed by building a grid on the pattern space.The mountain function for a vertex\nis de \ufb01ned as\nwhere\n is the distance between the\n th data object and\nthe\nth node, and\n is a positive constant. Therefore, the closer\na data object is to a vertex, the more the data object contributes\nto the mountain function. The vertex\n with the maximum\nvalue of mountain function\n is selected as the \ufb01rst center.\nA procedure, called mountain destruction, is performed to get\nrid of the effects of the selected center. This is achieved by sub-\ntracting the mountain function value for each of the rest ver-tices with an amount dependent on the current maximum moun-tain function value and the distance between the vertex and the\ncenter. The process iterates until the ratio between the current\nmaximum and\nis below some threshold. The connection\nof MM with several other fuzzy clustering algorithms was fur-ther discussed in [71]. Gath and Geva described an initialization\nstrategy of unsupervised tracking of cluster prototypes in their\n2-layer clustering scheme, in which FCM and fuzzy ML esti-mation are effectively combined [102].\nKersten suggested that city block distance (or\nnorm) could\nimprove the robustness of FCM to outliers [163]. Furthermore,\nHathaway, Bezdek, and Hu extended FCM to a more universalcase by using Minkowski distance (or\nnorm,\n ) and\nseminorm\n for the models that operate either di-\nrectly on the data objects or indirectly on the dissimilarity mea-\nsures [130]. According to their empirical results, the object databased models, with\nand\n norm, are recommended. They\nalso pointed out the possible improvement of models for other\nnorm with the price of more complicated optimization oper-\nations. PCM is another approach for dealing with outliers [175].Under this model, the memberships are interpreted by a possi-bilistic view, i.e., \u201cthe compatibilities of the points with the class\nprototypes \u201d[175]. The effect of noise and outliers is abated with\nthe consideration of typicality. In this case, the \ufb01rst condition for\nthe membership coef \ufb01cient described in Section I is relaxed to\n. Accordingly, the cost function is reformu-\nlated as\nwhere\n are some positive constants. The additional term\ntends to give credits to memberships with large values. A\nmodi \ufb01ed version in order to \ufb01nd appropriate clusters is pro-\nposed in [294]. Dav \u00e9and Krishnapuram further elaborated\nthe discussion on fuzzy clustering robustness and indicated itsconnection with robust statistics [71]. Relations among some\nwidely used fuzzy clustering algorithms were discussed and\ntheir similarities to some robust statistical methods were alsoreviewed. They reached a uni \ufb01ed framework as the conclusion\nfor the previous discussion and proposed generic algorithms\nfor robust clustering.The standard FCM alternates the calculation of the member-\nship and prototype matrix, which causes a computational burdenfor large-scale data sets. Kolen and Hutcheson accelerated the\ncomputation by combining updates of the two matrices [172].\nHung and Yang proposed a method to reduce computationaltime by identifying more accurate cluster centers [146]. FCMvariants were also developed to deal with other data types, such\nas symbolic data [81] and data with missing values [129].\nA family of fuzzy\n-shells algorithms has also appeared to de-\ntect different types of cluster shapes, especially contours (lines,circles, ellipses, rings, rectangles, hyperbolas) in", "start_char_idx": 0, "end_char_idx": 3574, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d70b24b2-9928-432a-8f0d-9a1a91c7092d": {"__data__": {"id_": "d70b24b2-9928-432a-8f0d-9a1a91c7092d", "embedding": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9df74eb1-5688-4979-a8aa-865fca7c833e", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "b6026dd84a710330da2171b8a2198ec06e626a8ca6b71c8db636ca53322353ba"}, "2": {"node_id": "5dbffff8-d037-4461-a8b5-a910192784bd", "node_type": null, "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}, "hash": "08e534ae1d6c5bc379a712f623f20242e94992a4250387bebfa87471d8e4858b"}}, "hash": "4cf7630f511417962f3022c1045bdbcd41b0dee280d4c29a02514511e87b742d", "text": "the previous discussion and proposed generic algorithms\nfor robust clustering.The standard FCM alternates the calculation of the member-\nship and prototype matrix, which causes a computational burdenfor large-scale data sets. Kolen and Hutcheson accelerated the\ncomputation by combining updates of the two matrices [172].\nHung and Yang proposed a method to reduce computationaltime by identifying more accurate cluster centers [146]. FCMvariants were also developed to deal with other data types, such\nas symbolic data [81] and data with missing values [129].\nA family of fuzzy\n-shells algorithms has also appeared to de-\ntect different types of cluster shapes, especially contours (lines,circles, ellipses, rings, rectangles, hyperbolas) in a two-dimen-\nsional data space. They use the \u201cshells \u201d(curved surfaces [70])\nas the cluster prototypes instead of points or surfaces in tra-ditional fuzzy clustering algorithms. In the case of FCS [36],[70], the proposed cluster prototype is represented as a\n-di-\nmensional hyperspherical shell\n (\n for circles),\nwhere\n is the center, and\n is the radius. A dis-\ntance function is de \ufb01ned as\n to\nmeasure the distance from a data object\n to the prototype\n .\nSimilarly, other cluster shapes can be achieved by de \ufb01ning ap-\npropriate prototypes and corresponding distance functions, ex-ample including fuzzy\n-spherical shells (FCSS) [176], fuzzy\n-rings (FCR) [193], fuzzy\n -quadratic shells (FCQS) [174], and\nfuzzy\n -rectangular shells (FCRS) [137]. See [141] for further\ndetails.\nFuzzy set theories can also be used to create hierarchical\ncluster structure. Geva proposed a hierarchical unsupervised\nfuzzy clustering (HUFC) algorithm [104], which can effec-\ntively explore data structure at different levels like HC, whileestablishing the connections between each object and cluster inthe hierarchy with the memberships. This design makes HUFC\novercome one of the major disadvantages of HC, i.e., HC\ncannot reassign an object once it is designated into a cluster.Fuzzy clustering is also closely related to neural networks [24],and we will see more discussions in the following section.\nH. Neural Networks-Based Clustering\nNeural networks-based clustering has been dominated by\nSOFMs and adaptive resonance theory (ART), both of whichare reviewed here, followed by a brief discussion of otherapproaches.\nIn competitive neural networks, active neurons reinforce\ntheir neighborhood within certain regions, while suppressingthe activities of other neurons (so-called on-center/off-surroundcompetition). Typical examples include LVQ and SOFM [168],\n[169]. Intrinsically, LVQ performs supervised learning, and is\nnot categorized as a clustering algorithm [169], [221]. But itslearning properties provide an insight to describe the potentialdata structure using the prototype vectors in the competitive\nlayer. By pointing out the limitations of LVQ, including sen-\nsitivity to initiation and lack of a de \ufb01nite clustering object,\nPal, Bezdek, and Tsao proposed a general LVQ algorithmfor clustering, known as GLVQ [221] (also see [157] for its\nimproved version GLVQ-F). They constructed the clustering\nproblem as an optimization process based on minimizing aloss function, which is de \ufb01ned on the locally weighted error\nbetween the input pattern and the winning prototype. They also\nshowed the relations between LVQ and the online\n-means", "start_char_idx": 2833, "end_char_idx": 6203, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4e6e6472-5c48-4a8e-bfcd-3390f6e0e046": {"__data__": {"id_": "4e6e6472-5c48-4a8e-bfcd-3390f6e0e046", "embedding": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92189415-813c-448f-9325-5d744b42daa2", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "3": {"node_id": "040cae1b-d191-486d-9455-5d4bf878b3ea", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "2247db82e909588f7e1fb81d0c1df4944125b192ceb6ee5b67115f8e4eff9494"}}, "hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38", "text": "658 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nalgorithm. Soft LVQ algorithms, e.g., fuzzy algorithms for\nLVQ (FALVQ), were discussed in [156].\nThe objective of SOFM is to represent high-dimensional\ninput patterns with prototype vectors that can be visualized in\na usually two-dimensional lattice structure [168], [169]. Eachunit in the lattice is called a neuron, and adjacent neurons areconnected to each other, which gives the clear topology of\nhow the network \ufb01ts itself to the input space. Input patterns\nare fully connected to all neurons via adaptable weights, andduring the training process, neighboring input patterns areprojected into the lattice, corresponding to adjacent neurons. In\nthis sense, some authors prefer to think of SOFM as a method\nto displaying latent data structure in a visual way rather than aclustering approach [221]. Basic SOFM training goes throughthe following steps.\n1) De \ufb01ne the topology of the SOFM; Initialize the proto-\ntype vectors\nrandomly.\n2) Present an input pattern\n to the network; Choose\nthe winning node\n that is closest to\n , i.e.,\n.\n3) Update prototype vectors\nwhere\n is the neighborhood function that is often\nde\ufb01ned as\nwhere\n is the monotonically decreasing learning\nrate,\n represents the position of corresponding neuron,\nand\n is the monotonically decreasing kernel width\nfunction, or\nif node\n belongs to the neighborhood\nof the winning node\notherwise\n4) Repeat steps 2) \u20133) until no change of neuron position\nthat is more than a small positive number is observed.\nWhile SOFM enjoy the merits of input space density ap-\nproximation and independence of the order of input patterns, a\nnumber of user-dependent parameters cause problems when ap-plied in real practice. Like the\n-means algorithm, SOFM need\nto prede \ufb01ne the size of the lattice, i.e., the number of clusters,\nwhich is unknown for most circumstances. Additionally, trained\nSOFM may be suffering from input space density misrepresen-tation [132], where areas of low pattern density may be over-rep-resented and areas of high density under-represented. Kohonen\nreviewed a variety of variants of SOFM in [169], which improve\ndrawbacks of basic SOFM and broaden its applications. SOFMcan also be integrated with other clustering approaches (e.g.,\n-means algorithm or HC) to provide more effective and faster\nclustering. [263] and [276] illustrate two such hybrid systems.\nART was developed by Carpenter and Grossberg, as a so-\nlution to the plasticity and stability dilemma [51], [53], [113].ART can learn arbitrary input patterns in a stable, fast, andself-organizing way, thus, overcoming the effect of learning in-\nstability that plagues many other competitive networks. ART isnot, as is popularly imagined, a neural network architecture. It\nis a learning theory, that resonance in neural circuits can trigger\nfast learning. As such, it subsumes a large family of currentand future neural networks architectures, with many variants.ART1 is the \ufb01rst member, which only deals with binary input\npatterns [51], although it can be extended to arbitrary input\npatterns by a variety of coding mechanisms. ART2 extendsthe applications to analog input patterns [52] and ART3 intro-duces a new mechanism originating from elaborate biological\nprocesses to achieve more ef \ufb01cient parallel search in hierar-\nchical structures [54]. By incorporating two ART modules,which receive input patterns\nART\n and corresponding labels\nART\n , respectively, with an", "start_char_idx": 0, "end_char_idx": 3472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "040cae1b-d191-486d-9455-5d4bf878b3ea": {"__data__": {"id_": "040cae1b-d191-486d-9455-5d4bf878b3ea", "embedding": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92189415-813c-448f-9325-5d744b42daa2", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "76f7bd151b41fddb53f34a1d8905f6c81f0d8413686b04f289c7541248f5e2c9"}, "2": {"node_id": "4e6e6472-5c48-4a8e-bfcd-3390f6e0e046", "node_type": null, "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}, "hash": "f75dca5a8c17e2957540c153da00601181553eb5f33d178344cca40ffa3b0b38"}}, "hash": "2247db82e909588f7e1fb81d0c1df4944125b192ceb6ee5b67115f8e4eff9494", "text": "It\nis a learning theory, that resonance in neural circuits can trigger\nfast learning. As such, it subsumes a large family of currentand future neural networks architectures, with many variants.ART1 is the \ufb01rst member, which only deals with binary input\npatterns [51], although it can be extended to arbitrary input\npatterns by a variety of coding mechanisms. ART2 extendsthe applications to analog input patterns [52] and ART3 intro-duces a new mechanism originating from elaborate biological\nprocesses to achieve more ef \ufb01cient parallel search in hierar-\nchical structures [54]. By incorporating two ART modules,which receive input patterns\nART\n and corresponding labels\nART\n , respectively, with an inter-ART module, the resulting\nARTMAP system can be used for supervised classi \ufb01cations\n[56]. The match tracking strategy ensures the consistency ofcategory prediction between two ART modules by dynamicallyadjusting the vigilance parameter of ART\n. Also see fuzzy\nARTMAP in [55]. A similar idea, omitting the inter-ART\nmodule, is known as LAPART [134].\nThe basic ART1 architecture consists of two-layer nodes, the\nfeature representation \ufb01eld\n and the category representation\n\ufb01eld\n . They are connected by adaptive weights, bottom-up\nweight matrix\n and top-down weight matrix\n . The pro-\ntotypes of clusters are stored in layer\n . After it is activated\naccording to the winner-takes-all competition, an expectation\nis re\ufb02ected in layer\n , and compared with the input pattern.\nThe orienting subsystem with the speci \ufb01ed vigilance parameter\ndetermines whether the expectation and the\ninput are closely matched, and therefore controls the generation\nof new clusters. It is clear that the larger\n is, the more clusters\nare generated. Once weight adaptation occurs, both bottom-upand top-down weights are updated simultaneously. This is calledresonance, from which the name comes. The ART1 algorithm\ncan be described as follows.\n1) Initialize weight matrices\nand\n as\n, where\n are sorted in a descending order and sat-\nis\ufb01es\n for\n and any\nbinary input pattern\n , and\n ;\n2) For a new pattern\n , calculate the input from layer\nto layer\n as\nif\nis an uncommitted node\n\ufb01rst activated\nif\nis a committed node\nwhere\n represents the logic AND operation.\n3) Activate layer\n by choosing node\n with the winner-\ntakes-all rule\n .\n4) Compare the expectation from layer\n with the input\npattern. If\n , go to step 5a), other-\nwise go to step 5b).", "start_char_idx": 2772, "end_char_idx": 5196, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13798897-265c-4df0-b0cb-58b61d12451d": {"__data__": {"id_": "13798897-265c-4df0-b0cb-58b61d12451d", "embedding": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8da82a75-5028-45a7-b216-e3136b94e7c7", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "3": {"node_id": "b7454e40-c40e-4760-8320-87393fba0652", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "64e4e8e21187c88719f8bd26a78dcb7367ca20611759399abb905c3312480ded"}}, "hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 659\n5)\na) Update the corresponding weights for the active\nnode as\n new\n old\nold\n and\n new\n old\n.;\nb) Send a reset signal to disable the current active node\nby the orienting subsystem and return to step 3).\n6) Present another input pattern, return to step 2) until all\npatterns are processed.\nNote the relation between ART network and other clustering\nalgorithms described in traditional and statistical language.\nMoore used several clustering algorithms to explain the clus-tering behaviors of ART1 and therefore induced and proved anumber of important properties of ART1, notably its equiva-\nlence to varying\n-means clustering [204]. She also showed\nhow to adapt these algorithms under the ART1 framework. In[284] and [285], the ease with which ART may be used forhierarchical clustering is also discussed.\nFuzzy ART (FA) bene \ufb01ts the incorporation of fuzzy set theory\nand ART [57]. FA maintains similar operations to ART1 and\nuses the fuzzy set operators to replace the binary operators, sothat it can work for all real data sets. FA exhibits many desirablecharacteristics such as fast and stable learning and atypical pat-\ntern detection. Huang et al. investigated and revealed more prop-\nerties of FA classi \ufb01ed as template, access, reset, and the number\nof learning epochs [143]. The criticisms for FA are mostly fo-cused on its inef \ufb01ciency in dealing with noise and the de \ufb01-\nciency of hyperrectangular representation for clusters in many\ncircumstances [23], [24], [281]. Williamson described GaussianART (GA) to overcome these shortcomings [281], in which eachcluster is modeled with Gaussian distribution and represented as\na hyperellipsoid geometrically. GA does not inherit the of \ufb02ine\nfast learning property of FA, as indicated by Anagnostopoulos et\nal.[13], who proposed different ART architectures: hypersphere\nART (HA) [12] for hyperspherical clusters and ellipsoid ART\n(EA) [13] for hyperellipsoidal clusters, to explore a more ef \ufb01-\ncient representation of clusters, while keeping important prop-erties of FA. Baraldi and Alpaydin proposed SART followingtheir general ART clustering networks frame, which is described\nthrough a feedforward architecture combined with a match com-\nparison mechanism [23]. As speci \ufb01c examples, they illustrated\nsymmetric fuzzy ART (SFART) and fully self-organizing SART(FOSART) networks. These networks outperform ART1 and FA\naccording to their empirical studies [23].\nIn addition to these, many other neural network architectures\nare developed for clustering. Most of these architectures uti-lize prototype vectors to represent clusters, e.g., cluster detec-\ntion and labeling network (CDL) [82], HEC [194], and SPLL\n[296]. HEC uses a two-layer network architecture to estimatethe regularized Mahalanobis distance, which is equated to theEuclidean distance in a transformed whitened space. CDL is\nalso a two-layer network with an inverse squared Euclidean\nmetric. CDL requires the match between the input patterns andthe prototypes above a threshold, which is dynamically adjusted.SPLL emphasizes initiation independent and adaptive genera-\ntion of clusters. It begins with a random prototype in the input\nspace and iteratively chooses and divides prototypes until no fur-ther split is available. The divisibility of a prototype is based onthe consideration that each prototype represents only one natural\nFig. 2. ART1 architecture. Two layers are included in the attentional\nsubsystem, connected via", "start_char_idx": 0, "end_char_idx": 3500, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b7454e40-c40e-4760-8320-87393fba0652": {"__data__": {"id_": "b7454e40-c40e-4760-8320-87393fba0652", "embedding": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8da82a75-5028-45a7-b216-e3136b94e7c7", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "2cb16b33dfc49d166eabf2970744ea75acb1f1d4601f45a6ab15342b0fef171c"}, "2": {"node_id": "13798897-265c-4df0-b0cb-58b61d12451d", "node_type": null, "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}, "hash": "3f1cd5a5d408f043aeef216418d680534ba2878990deeb714f126a95c995af46"}}, "hash": "64e4e8e21187c88719f8bd26a78dcb7367ca20611759399abb905c3312480ded", "text": "SPLL\n[296]. HEC uses a two-layer network architecture to estimatethe regularized Mahalanobis distance, which is equated to theEuclidean distance in a transformed whitened space. CDL is\nalso a two-layer network with an inverse squared Euclidean\nmetric. CDL requires the match between the input patterns andthe prototypes above a threshold, which is dynamically adjusted.SPLL emphasizes initiation independent and adaptive genera-\ntion of clusters. It begins with a random prototype in the input\nspace and iteratively chooses and divides prototypes until no fur-ther split is available. The divisibility of a prototype is based onthe consideration that each prototype represents only one natural\nFig. 2. ART1 architecture. Two layers are included in the attentional\nsubsystem, connected via bottom-up and top-down adaptive weights. Theirinteractions are controlled by the orienting subsystem through a vigilance\nparameter.\ncluster, instead of the combinations of several clusters. Simpson\nemployed hyperbox fuzzy sets to characterize clusters [100],\n[249]. Each hyperbox is delineated by a min and max point, and\ndata points build their relations with the hyperbox through themembership function. The learning process experiences a se-ries of expansion and contraction operations, until all clusters\nare stable.\nI. Kernel-Based Clustering\nKernel-based learning algorithms [209], [240], [274] are\nbased on Cover \u2019s theorem. By nonlinearly transforming a set\nof complex and nonlinearly separable patterns into a higher-di-\nmensional feature space, we can obtain the possibility to\nseparate these patterns linearly [132]. The dif \ufb01culty of curse\nof dimensionality can be overcome by the kernel trick, arisingfrom Mercer \u2019s theorem [132]. By designing and calculating\nan inner-product kernel, we can avoid the time-consuming,\nsometimes even infeasible process to explicitly describe thenonlinear mapping and compute the corresponding points in\nthe transformed space.\nIn [241], Sch \u00f6lkopf, Smola, and M \u00fcller depicted a kernel-\n-means algorithm in the online mode. Suppose we have a set of\npatterns\n and a nonlinear map\n. Here,\n represents a feature space with arbitrarily high di-\nmensionality. The object of the algorithm is to \ufb01nd\n centers so\nthat we can minimize the distance between the mapped patterns\nand their closest center\nwhere\n is the center for the\n th cluster and lies in a span of\n, and\n is the inner-\nproduct kernel.\nDe\ufb01ne the cluster assignment variable\nif\n belongs to cluster\notherwise.", "start_char_idx": 2712, "end_char_idx": 5209, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "254d23e2-4b68-41fe-a42e-8447042e7214": {"__data__": {"id_": "254d23e2-4b68-41fe-a42e-8447042e7214", "embedding": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "3": {"node_id": "b39d6656-38ae-4098-8d6c-1de0263626cf", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "dd7ef7a7d2efaafc620b351fcdfc4bcbdf00c1e9f829930705c2771e9beb5563"}}, "hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b", "text": "660 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nThen the kernel-\n -means algorithm can be formulated as the\nfollowing.\n1) Initialize the centers\n with the \ufb01rst\n , ob-\nservation patterns;\n2) Take a new pattern\n and calculate\n as\nshown in the equation at the bottom of the page.\n3) Update the mean vector\n whose corresponding\nis 1\nwhere\n .\n4) Adapt the coef \ufb01cients\n for each\n as\nfor\nfor\n5) Repeat steps 2) \u20134) until convergence is achieved.\nTwo variants of kernel-\n -means were introduced in [66],\nmotivated by SOFM and ART networks. These variants con-\nsider effects of neighborhood relations, while adjusting the\ncluster assignment variables, and use a vigilance parameter tocontrol the process of producing mean vectors. The authors alsoillustrated the application of these approaches in case based\nreasoning systems.\nAn alternative kernel-based clustering approach is in [107].\nThe problem was formulated to determine an optimal partition\nto minimize the trace of within-group scatter matrix in the\nfeature space\nwhere\n,\nand\n is the total number of patterns in the\n th cluster.\nNote that the kernel function utilized in this case is the radial\nbasis function (RBF) and\n can be interpreted as a mea-\nsure of the denseness for the\n th cluster.\nBen-Hur et al. presented a new clustering algorithm, SVC,\nin order to \ufb01nd a set of contours used as the cluster bound-\naries in the original data space [31], [32]. These contours can\nbe formed by mapping back the smallest enclosing sphere in\nthe transformed feature space. RBF is chosen in this algorithm,and, by adjusting the width parameter of RBF, SVC can form ei-\nther agglomerative or divisive hierarchical clusters. When somepoints are allowed to lie outside the hypersphere, SVC can deal\nwith outliers effectively. An extension, called multiple spheres\nsupport vector clustering, was proposed in [62], which combinesthe concept of fuzzy membership.\nKernel-based clustering algorithms have many advantages.\n1) It is more possible to obtain a linearly separable hyper-\nplane in the high-dimensional, or even in \ufb01nite feature\nspace.\n2) They can form arbitrary clustering shapes other than\nhyperellipsoid and hypersphere.\n3) Kernel-based clustering algorithms, like SVC, have the\ncapability of dealing with noise and outliers.\n4) For SVC, there is no requirement for prior knowledge\nto determine the system topological structure. In [107],the kernel matrix can provide the means to estimate thenumber of clusters.\nMeanwhile, there are also some problems requiring further\nconsideration and investigation. Like many other algorithms,how to determine the appropriate parameters, for example, thewidth of Gaussian kernel, is not trivial. The problem of compu-\ntational complexity may become serious for large data sets.\nThe process of constructing the sum-of-squared clustering\nalgorithm [107] and\n-means algorithm [241] presents a good\nexample to reformulate more powerful nonlinear versions\nfor many existing linear algorithms, provided that the scalar\nproduct can be obtained. Theoretically, it is important to investi-gate whether these nonlinear variants can keep some useful andessential properties of the original algorithms and how Mercer\nkernels contribute to the improvement of the algorithms. The\neffect of different types of kernel functions, which are rich inthe literature, is also an interesting topic for further exploration.\nJ. Clustering Sequential Data\nSequential data are sequences with variable length and many\nother distinct characteristics, e.g.,", "start_char_idx": 0, "end_char_idx": 3533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b39d6656-38ae-4098-8d6c-1de0263626cf": {"__data__": {"id_": "b39d6656-38ae-4098-8d6c-1de0263626cf", "embedding": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "fe4e7e96eee675cd1537aa469a451f111855b8adf94b4494b8b31120fb7fcf99"}, "2": {"node_id": "254d23e2-4b68-41fe-a42e-8447042e7214", "node_type": null, "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}, "hash": "8468c96246e0d53924b6dc75c7db10ea8e7d7442de9ca5585493b6d92e8e9a6b"}}, "hash": "dd7ef7a7d2efaafc620b351fcdfc4bcbdf00c1e9f829930705c2771e9beb5563", "text": "of compu-\ntational complexity may become serious for large data sets.\nThe process of constructing the sum-of-squared clustering\nalgorithm [107] and\n-means algorithm [241] presents a good\nexample to reformulate more powerful nonlinear versions\nfor many existing linear algorithms, provided that the scalar\nproduct can be obtained. Theoretically, it is important to investi-gate whether these nonlinear variants can keep some useful andessential properties of the original algorithms and how Mercer\nkernels contribute to the improvement of the algorithms. The\neffect of different types of kernel functions, which are rich inthe literature, is also an interesting topic for further exploration.\nJ. Clustering Sequential Data\nSequential data are sequences with variable length and many\nother distinct characteristics, e.g., dynamic behaviors, timeconstraints, and large volume [120], [265]. Sequential data can\nbe generated from: DNA sequencing, speech processing, text\nmining, medical diagnosis, stock market, customer transactions,web data mining, and robot sensor analysis, to name a few [78],[265]. In recent decades, sequential data grew explosively. For\nexample, in genetics, the recent statistics released on October\n15, 2004 (Release 144.0) shows that there are 43 194 602655 bases from 38 941 263 sequences in GenBank database[103] and release 45.0 of SWISSPROT on October 25, 2004\ncontains 59 631 787 amino acids in 163 235 sequence entries\n[267]. Cluster analysis explores potential patterns hidden in thelarge number of sequential data in the context of unsupervisedlearning and therefore provides a crucial way to meet the cur-\nrent challenges. Generally, strategies for sequential clustering\nmostly fall into three categories.\nif\notherwise", "start_char_idx": 2714, "end_char_idx": 4463, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7487076c-1967-4678-a14f-c86cf13dcead": {"__data__": {"id_": "7487076c-1967-4678-a14f-c86cf13dcead", "embedding": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7f9d697-11df-4960-bb3d-59ed77b3e0dd", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "3": {"node_id": "51777b76-56dd-4fd4-8d28-4dc30864db57", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "00ab25eb19df3021ff031935473ece053588d06119c2ed16eb5a3649570fae8e"}}, "hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 661\n1) Sequence Similarity: The \ufb01rst scheme is based on the\nmeasure of the distance (or similarity) between each pair of se-quences. Then, proximity-based clustering algorithms, either hi-\nerarchical or partitional, can group sequences. Since many se-\nquential data are expressed in an alphabetic form, like DNAor protein sequences, conventional measure methods are inap-propriate. If a sequence comparison is regarded as a process of\ntransforming a given sequence to another with a series of substi-\ntution, insertion, and deletion operations, the distance betweenthe two sequences can be de \ufb01ned by virtue of the minimum\nnumber of required operations. A common analysis processes is\nalignment, illustrated in Fig. 3. The de \ufb01ned distance is known\nas edit distance or Levenshtein distance [120], [236]. These editoperations are weighted (punished or rewarded) according tosome prior domain knowledge and the distance herein is equiva-\nlent to the minimum cost to complete the transformation. In this\nsense, the similarity or distance between two sequences can bereformulated as an optimal alignment problem, which \ufb01ts well\nin the framework of dynamic programming.\nGiven two sequences,\nand\n, the basic dynamic program-\nming-based sequence alignment algorithm, also known asthe Needleman-Wunsch algorithm, can be depicted by the\nfollowing recursive equation [78], [212]:\nwhere\n is de \ufb01ned as the best alignment score be-\ntween sequence segment\n of\n and\nof\n , and\n ,o r\nrepresent the cost for aligning\n to\n , aligning\n to\na gap (denoted as\n ), or aligning\n to a gap, respectively. The\ncomputational results for each position at\n and\n are recorded\nin an array with a pointer that stores current optimal operationsand provides an effective path in backtracking the alignment.\nThe Needleman-Wunsch algorithm considers the comparison\nof the whole length of two sequences and therefore performsa global optimal alignment. However, it is also important to\ufb01nd local similarity among sequences in many circumstances.\nThe Smith-Waterman algorithm achieves that by allowing the\nbeginning of a new alignment during the recursive computa-tion, and the stop of an alignment anywhere in the dynamicprogramming matrix [78], [251]. This change is summarized\nin the following:\nFor both the global and local alignment algorithms, the com-\nputation complexity is\n , which is very expensive, es-\npecially for a clustering problem that requires an all-against-all\npairwise comparison. A wealth of speed-up methods has been\ndeveloped to improve the situation [78], [120]. We will seemore discussion in Section III-E in the context of biologicalsequences analysis. Other examples include applications for\nspeech recognition [236] and navigation pattern mining [131].\nFig. 3. Illustration of a sequence alignment. Series of edit operations\nis performed to change the sequence CLUSTERING into the sequence\nCLASSIFICATION.\n2) Indirect Sequence Clustering: The second approach\nemploys an indirect strategy, which begins with the extraction\nof a set of features from the sequences. All the sequencesare then mapped into the transformed feature space, whereclassical vector space-based clustering algorithms can be\nused to form clusters. Obviously, feature extraction becomes\nthe essential factor that decides the effectiveness of thesealgorithms. Guralnik and Karypis discussed the potential de-pendency between two sequential patterns and suggested both\nthe global and the local approaches to prune the initial feature\nsets in order to better represent sequences in the new featurespace [119]. Morzy et al. utilized the sequential patterns as\nthe basic element in the agglomerative", "start_char_idx": 0, "end_char_idx": 3702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51777b76-56dd-4fd4-8d28-4dc30864db57": {"__data__": {"id_": "51777b76-56dd-4fd4-8d28-4dc30864db57", "embedding": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c7f9d697-11df-4960-bb3d-59ed77b3e0dd", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "340b333c0a2430343e628fae96879d810a1370be5f4c57021749f015e9605c26"}, "2": {"node_id": "7487076c-1967-4678-a14f-c86cf13dcead", "node_type": null, "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}, "hash": "6391a3b56a85bf13ab7684669b3047987ff3f81803a0a4a5f2ab00f73da840b0"}}, "hash": "00ab25eb19df3021ff031935473ece053588d06119c2ed16eb5a3649570fae8e", "text": "performed to change the sequence CLUSTERING into the sequence\nCLASSIFICATION.\n2) Indirect Sequence Clustering: The second approach\nemploys an indirect strategy, which begins with the extraction\nof a set of features from the sequences. All the sequencesare then mapped into the transformed feature space, whereclassical vector space-based clustering algorithms can be\nused to form clusters. Obviously, feature extraction becomes\nthe essential factor that decides the effectiveness of thesealgorithms. Guralnik and Karypis discussed the potential de-pendency between two sequential patterns and suggested both\nthe global and the local approaches to prune the initial feature\nsets in order to better represent sequences in the new featurespace [119]. Morzy et al. utilized the sequential patterns as\nthe basic element in the agglomerative hierarchical clustering\nand de \ufb01ned a co-occurrence measure, as the standard of fusion\nof smaller clusters [207]. These methods greatly reduce thecomputational complexities and can be applied to large-scalesequence databases. However, the process of feature selection\ninevitably causes the loss of some information in the original\nsequences and needs extra attention.\n3) Statistical Sequence Clustering: Typically, the \ufb01rst two\napproaches are used to deal with sequential data composed of\nalphabets, while the third paradigm, which aims to construct\nstatistical models to describe the dynamics of each group of se-quences, can be applied to numerical or categorical sequences.The most important method is hidden Markov models (HMMs)\n[214], [219], [253], which \ufb01rst gained its popularity in the appli-\ncation of speech recognition [229]. A discrete HMM describesan unobservable stochastic process consisting of a set of states,each of which is related to another stochastic process that emits\nobservable symbols. Therefore, the HMM is completely speci-\n\ufb01ed by the following.\n1) A \ufb01nite set\nwith\n states.\n2) A discrete set\n with\n observa-\ntion symbols.\n3) A state transition distribution\n , where\nth state at time\n th state at time\n4) A symbol emission distribution\n , where\nat\n th state at\n5) An initial state distribution\n , where\nth state at\nAfter an initial state is selected according to the initial dis-\ntribution\n , a symbol is emitted with emission distribution\n .\nThe next state is decided by the state transition distribution\nand it also generates a symbol based on\n . The process repeats\nuntil reaching the last state. Note that the procedure generates", "start_char_idx": 2867, "end_char_idx": 5364, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7e464763-6c28-40ef-9d6c-ab5e84d8b075": {"__data__": {"id_": "7e464763-6c28-40ef-9d6c-ab5e84d8b075", "embedding": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83dd834e-1c6c-4347-a22f-565adc1fae11", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "3": {"node_id": "85c176b1-8119-4831-b3fe-6a680fe02035", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "9994902f833106d18962d57156121dfd17dc7db1179df4de4c9cafd5038f2e75"}}, "hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead", "text": "662 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\na sequence of symbol observations instead of states, which is\nwhere the name \u201chidden \u201dcomes from. HMMs are well founded\ntheoretically [229]. Dynamic programming and EM algorithm\nare developed to solve the three basic problems of HMMs as the\nfollowing.\n1) Likelihood (forward or backward algorithm). Com-\npute the probability of an observation sequence given\na model.\n2) State interpretation (Vertbi algorithm). Find an op-\ntimal state sequence by optimizing some criterion\nfunction given the observation sequence and the\nmodel.\n3) Parameter estimation (Baum \u2013Welch algorithm). De-\nsign suitable model parameters to maximize the prob-\nability of observation sequence under the model.\nThe equivalence between an HMM and a recurrent\nback-propagation network was elucidated in [148], and auniversal framework was constructed to describe both the\ncomputational and the structural properties of the HMM and\nthe neural network.\nSmyth proposed an HMM-based clustering model, which,\nsimilar to the theories introduced in mixture densities-based\nclustering, assumes that each cluster is generated based on some\nprobability distribution [253]. Here, HMMs are used rather thanthe common Gaussian or\n-distribution. In addition to the form\nof\ufb01nite mixture densities, the mixture model can also be de-\nscribed by means of an HMM with the transition matrix\nwhere\n is the transition distribution for the\n th\ncluster. The initial distribution of the HMM is determined basedon the prior probability for each cluster. The basic learning\nprocess starts with a parameter initialization scheme to form\na rough partition with the log-likelihood of each sequenceserving as the distance measure. The partition is further re-\ufb01ned by training the overall HMM over all sequences with\nthe classical EM algorithm. A Monte-Carlo cross validation\nmethod was used to estimate the possible number of clusters.An application with a modi \ufb01ed HMM model that considers\nthe effect of context for clustering facial display sequences is\nillustrated in [138]. Oates et al. addressed the initial problem by\npregrouping the sequences with the agglomerative hierarchicalclustering, which operates on the proximity matrix determinedby the dynamic time warping (DTW) technique [214]. The area\nformed between one original sequence and a new sequence,\ngenerated by warping the time dimension of another originalsequence, re \ufb02ects the similarity of the two sequences. Li and\nBiswas suggested several objective criterion functions based\non posterior probability and information theory for structural\nselection of HMMs and cluster validity [182]. More recentadvances on HMMs and other related topics are reviewed in[30].\nOther model-based sequence clustering includes mixtures of\n\ufb01rst-order Markov chain [255] and a linear model like autore-\ngressive moving average (ARMA) model [286]. Usually, theyare combined with EM for parameter estimation [286]. Smyth\n[255] and Cadez et al. [50] further generalize a universal prob-\nabilistic framework to model mixed data measurement, which\nincludes both conventional static multivariate vectors and dy-\nnamic sequence data.\nThe paradigm models clusters directly from original data\nwithout additional process that may cause information loss.\nThey provide more intuitive ways to capture the dynamics\nof data and more \ufb02exible means to deal with variable length\nsequences. However, determining the number of model com-ponents remains a complicated and uncertain process [214],\n[253]. Also, the model selected is required to have suf \ufb01cient\ncomplexity, in order to interpret the characteristics of data.\nK. Clustering Large-Scale Data", "start_char_idx": 0, "end_char_idx": 3685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "85c176b1-8119-4831-b3fe-6a680fe02035": {"__data__": {"id_": "85c176b1-8119-4831-b3fe-6a680fe02035", "embedding": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "83dd834e-1c6c-4347-a22f-565adc1fae11", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "3d3d5dc4acb59f243e20141f5d0b7fe8b6cad37a789c48763517e30a8f591139"}, "2": {"node_id": "7e464763-6c28-40ef-9d6c-ab5e84d8b075", "node_type": null, "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}, "hash": "fac561486dd81b090b2fb97bf637681dcb429c38f3ced278347f49be0a646ead"}}, "hash": "9994902f833106d18962d57156121dfd17dc7db1179df4de4c9cafd5038f2e75", "text": "theyare combined with EM for parameter estimation [286]. Smyth\n[255] and Cadez et al. [50] further generalize a universal prob-\nabilistic framework to model mixed data measurement, which\nincludes both conventional static multivariate vectors and dy-\nnamic sequence data.\nThe paradigm models clusters directly from original data\nwithout additional process that may cause information loss.\nThey provide more intuitive ways to capture the dynamics\nof data and more \ufb02exible means to deal with variable length\nsequences. However, determining the number of model com-ponents remains a complicated and uncertain process [214],\n[253]. Also, the model selected is required to have suf \ufb01cient\ncomplexity, in order to interpret the characteristics of data.\nK. Clustering Large-Scale Data Sets\nScalability becomes more and more important for clustering\nalgorithms with the increasing complexity of data, mainly man-ifesting in two aspects: enormous data volume and high dimen-sionality. Examples, illustrated in the sequential clustering sec-\ntion, are just some of the many applications that require this ca-\npability. With the further advances of database and Internet tech-nologies, clustering algorithms will face more severe challengesin handling the rapid growth of data. We summarize the com-\nputational complexity of some typical and classical clustering\nalgorithms in Table II with several newly proposed approachesspeci \ufb01cally designed to deal with large-scale data sets. Several\npoints can be generalized through the table.\n1) Obviously, classical hierarchical clustering algo-\nrithms, including single-linkage, complete linkage,\naverage linkage, centroid linkage and median linkage,are not appropriate for large-scale data sets due to thequadratic computational complexities in both execu-\ntion time and store space.\n2)\n-means algorithm has a time complexity of\nand space complexity of\n . Since\n is usu-\nally much larger than both\n and\n , the complexity be-\ncomes near linear to the number of samples in the data\nsets.\n -means algorithm is effective in clustering large-\nscale data sets, and efforts have been made in order toovercome its disadvantages [142], [218].\n3) Many novel algorithms have been developed to cluster\nlarge-scale data sets, especially in the context of datamining [44], [45], [85], [135], [213], [248]. Many ofthem can scale the computational complexity linearly\nto the input size and demonstrate the possibility of han-\ndling very large data sets.\na) Random sampling approach, e.g., CLARA clus-\ntering large applications (CLARA) [161] and CURE\n[116]. The key point lies that the appropriate sample\nsizes can effectively maintain the important geomet-rical properties of clusters. Furthermore, Chernoffboundscanprovideestimationforthelowerboundof\nthe minimum sample size, given the low probability\nthat points in each cluster are missed in the sampleset [116]. CLARA represents each cluster with a", "start_char_idx": 2909, "end_char_idx": 5828, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fdac7e1e-859a-4913-8521-caa2a30b2d6b": {"__data__": {"id_": "fdac7e1e-859a-4913-8521-caa2a30b2d6b", "embedding": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50f89fe7-04a7-406e-b5db-f905d82e29d4", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "3": {"node_id": "b71157bb-690f-44d9-a23e-dbd99c686820", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "84cb3e3b20784a5525040ca81c11620d3b413b825c313408b32de07a4c1719f0"}}, "hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 663\nmedoid while CURE chooses a set of well-scattered\nand center-shrunk points.\nb) Randomized search approach, e.g., clustering\nlarge applications based on randomized search\n(CLARANS) [213]. CLARANS sees the clusteringas a search process in a graph, in which each nodecorresponds to a set of\nmedoids. It begins with an\narbitrary node as the current node and examines a set\nof neighbors, de \ufb01ned as the node consisting of only\none different data object, to seek a better solution,i.e., any neighbor, with a lower cost, becomes the\ncurrent node. If the maximum number of neighbors,\nspeci \ufb01ed by the user, has been reached, the current\nnode is accepted as a winning node. This processiterates several times as speci \ufb01ed by users. Though\nCLARANS achieves better performance than algo-\nrithms like CLARA, the total computational time isstill quadratic, which makes CLARANS not quiteeffective in very large data sets.\nc) Condensation-based approach, e.g., BIRCH [295].\nBIRCH generates and stores the compact sum-maries of the original data in a CF tree, as discussedin Section II-B. This new data structure ef \ufb01ciently\ncaptures the clustering information and largely\nreduces the computational burden. BIRCH wasgeneralized into a broader framework in [101] withtwo algorithms realization, named as BUBBLE and\nBUBBLE-FM.\nd) Density-based approach, e.g., density based spatial\nclustering of applications with noise (DBSCAN)[85] and density-based clustering (DENCLUE)\n[135]. DBSCAN requires that the density in a\nneighborhood for an object should be high enoughif it belongs to a cluster. DBSCAN creates a newcluster from a data object by absorbing all objects in\nits neighborhood. The neighborhood needs to sat-\nisfy a user-speci \ufb01ed density threshold. DBSCAN\nuses a\n-tree structure for more ef \ufb01cient queries.\nDENCLUE seeks clusters with local maxima of\nthe overall density function, which re \ufb02ects the\ncomprehensive in \ufb02uence of data objects to their\nneighborhoods in the corresponding data space.\ne) Grid-based approach, e.g., WaveCluster [248] and\nfractal clustering (FC) [26]. WaveCluster assigns\ndataobjectstoasetofunitsdividedintheoriginalfea-ture space, and employs wavelet transforms on theseunits, to map objects into the frequency domain. The\nkey idea is that clusters can be easily distinguished in\nthe transformed space. FC combines the concepts ofboth incremental clustering and fractal dimension.Data objects are incrementally added to the clusters,\nspeci \ufb01ed through an initial process, and represented\nas cells in a grid, with the condition that the fractaldimension of cluster needs to keep relatively stable.\n4) Most algorithms listed previously lack the capability of\ndealing with data with high dimensionality. Their per-\nformances degenerate with the increase of dimension-ality. Some algorithms, like FC and DENCLUE, haveshown some successful applications in such cases, but\nthese are still far from completely effective.\nIn addition to the aforementioned approaches, several other\ntechniquesalsoplaysigni \ufb01cantrolesinclusteringlarge-scaledata\nsets. Parallel algorithms can more effectively use computational\nresources,andgreatlyimproveoverallperformanceinthecontext\nofboth time andspace complexity [69], [217], [262]. Incrementalclustering techniques do not require the storage of the entire dataset, and can handle it in", "start_char_idx": 0, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b71157bb-690f-44d9-a23e-dbd99c686820": {"__data__": {"id_": "b71157bb-690f-44d9-a23e-dbd99c686820", "embedding": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "50f89fe7-04a7-406e-b5db-f905d82e29d4", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "f977ffef4c4f6bf6f96effcd02ae6cce0e1e7062af6013da7120d408699b8643"}, "2": {"node_id": "fdac7e1e-859a-4913-8521-caa2a30b2d6b", "node_type": null, "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}, "hash": "5d9b2f3d5f087b05ddb82323b0ee8bdf95abb7f9e8b03905b904813dac6cb95c"}}, "hash": "84cb3e3b20784a5525040ca81c11620d3b413b825c313408b32de07a4c1719f0", "text": "of cluster needs to keep relatively stable.\n4) Most algorithms listed previously lack the capability of\ndealing with data with high dimensionality. Their per-\nformances degenerate with the increase of dimension-ality. Some algorithms, like FC and DENCLUE, haveshown some successful applications in such cases, but\nthese are still far from completely effective.\nIn addition to the aforementioned approaches, several other\ntechniquesalsoplaysigni \ufb01cantrolesinclusteringlarge-scaledata\nsets. Parallel algorithms can more effectively use computational\nresources,andgreatlyimproveoverallperformanceinthecontext\nofboth time andspace complexity [69], [217], [262]. Incrementalclustering techniques do not require the storage of the entire dataset, and can handle it in a one-pattern-at-a-time way. If the pat-\ntern displays enough closeness to a cluster according to some\nprede \ufb01ned criteria, it is assigned to the cluster. Otherwise, a new\ncluster is created to represent the object. A typical example isthe ART family [51] \u2013[53] discussed in Section II-H. Most incre-\nmental clustering algorithms are dependent on the order of the\ninput patterns [51], [204]. Bradley, Fayyad, and Reina proposeda scalable clustering framework, considering seven relevant im-portant characteristics in dealing with large databases [44]. Ap-\nplications of the framework were illustrated for the\n-means al-\ngorithm and EM mixture models [44], [45].\nL. Exploratory Data Visualization and High-Dimensional\nData Analysis Through Dimensionality Reduction\nFor most of the algorithms summarized in Table II, although\nthey can deal with large-scale data, they are not suf \ufb01cient for\nanalyzing high-dimensional data. The term, \u201ccurse of dimen-\nsionality, \u201dwhich was \ufb01rst used by Bellman to indicate the ex-\nponential growth of complexity in the case of multivariate func-\ntion estimation under a high dimensionality situation [28], isgenerally used to describe the problems accompanying high di-mensional spaces [34], [132]. It is theoretically proved that the\ndistance between the nearest points is no different from that\nof other points when the dimensionality of the space is highenough [34]. Therefore, clustering algorithms that are based onthe distance measure may no longer be effective in a high dimen-\nsional space. Fortunately, in practice, many high-dimensional\ndata usually have an intrinsic dimensionality that is much lowerthan the original dimension [60]. Dimension reduction is impor-tant in cluster analysis, which not only makes the high-dimen-\nsional data addressable and reduces the computational cost, but\nprovides users with a clearer picture and visual examination ofthe data of interest. However, dimensionality reduction methodsinevitably cause some loss of information, and may damage the\ninterpretability of the results, even distort the real clusters.\nOne natural strategy for dimensionality reduction is to\nextract important components from original data, which cancontribute to the division of clusters. Principle component\nanalysis (PCA) or Karhunen-Lo \u00e9ve transformation is one of\nthe typical approaches, which is concerned with constructinga linear combination of a set of vectors that can best describethe variance of data. Given the\ninput pattern matrix\n, the linear mapping\nprojects\n into a low-dimensional\nsubspace, where\n is the resulting\n matrix and\n is the\nprojection matrix whose columns are the eigenvectors\nthat correspond to the\n largest eigenvalues of the\n co-\nvariance matrix\n , calculated from the whole data set (hence,\nthe column vectors of\n are orthonormal). PCA estimates the\nmatrix\n while minimizing the sum of squares of the error", "start_char_idx": 2625, "end_char_idx": 6275, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e2e37e5f-1ba6-455c-9a86-7c060a21c5da": {"__data__": {"id_": "e2e37e5f-1ba6-455c-9a86-7c060a21c5da", "embedding": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9355e891-1acc-419d-8d45-c051be2b9689", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "3": {"node_id": "1ff8f34c-9eec-4f03-a610-841dc58c355e", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "451a40404b3a65abf635cf105b00a994049037df0fd95184d74430c5a128364f"}}, "hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b", "text": "664 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nof approximating the input vectors. In this sense, PCA can\nbe realized through a three-layer neural network, called anauto-associative multilayer perceptron, with linear activation\nfunctions [19], [215]. In order to extract more complicated\nnonlinear data structure, nonlinear PCA was developed and oneof the typical examples is kernel PCA. As methods discussedin Section II-I, kernel PCA \ufb01rst maps the input patterns into a\nfeature space. The similar steps are then applied to solve the\neigenvalue problem with the new covariance matrix in the fea-ture space. In another way, extra hidden layers with nonlinearactivation functions can be added into the auto-associative\nnetwork for this purpose [38], [75].\nPCA is appropriate for Gaussian distributions since it relies on\nsecond-order relationships in the covariance matrix, Other linear\ntransforms, like independent component analysis (ICA) and pro-jection pursuit, which use higher order statistical information,are more suited for non-Gaussian distributions [60], [151]. The\nbasic goal of ICA is to \ufb01nd the components that are most statis-\ntically independent from each other [149], [154]. In the contextof blind source separation, ICA aims to separate the independentsource signals from the mixed observation signal. This problem\ncan be formulated in several different ways [149], and one of\nthe simplest form (without considering noise) is represented as\n, where\n is the\n -dimensional observable vector,\nis the\n -dimensional source vector assumed to be statistically\nindependent, and\n is a nonsingular\n mixing matrix. ICA\ncan also be realized by virtueof multilayer perceptrons, and[158]illustrates one of such examples. The proposed ICA networkincludes whitening, separation, and basis vectors estimation\nlayers, with corresponding learning algorithms. The authors\nalso indicated its connection to the auto-associative multilayerperceptron. Projection pursuit is another statistical technique forseeking low-dimensional projection structures for multivariate\ndata [97], [144]. Generally, projection pursuit regards the normal\ndistribution as the least interesting projections and optimizessome certain indices that measure the degree of nonnormality[97]. PCA can be considered as a special example of projection\npursuit, as indicated in [60]. More discussions on the relations\namong PCA, ICA, projection pursuit, and other relevant tech-niques are offered in [149] and [158].\nDifferent from PCA, ICA, and projection pursuit, Multidi-\nmensional scaling (MDS) is a nonlinear projection technique\n[75], [292]. The basic idea of MDS lies in \ufb01tting original mul-\ntivariate data into a low-dimensional structure while aiming tomaintain the proximity information. The distortion is measuredthrough some criterion functions, e.g., in the sense of sum\nof squared error between the real distance and the projection\ndistance. The isometric feature mapping (Isomap) algorithmis another nonlinear technique, based on MDS [270]. Isomapestimates the geodesic distance between a pair of points, which\nis the shortest path between the points on a manifold, by virtue\nof the measured input-space distances, e.g., the Euclideandistance usually used. This extends the capability of MDS toexplore more complex nonlinear structures in the data. Locally\nlinear embedding (LLE) algorithm addresses the nonlinear\ndimensionality reduction problem from a different startingpoint [235]. LLE emphasizes the local linearity of the manifoldand assumes that the local relations in the original data space(\n-dimensional) are also preserved in the projected low-di-\nmensional space (\n -dimensional). This is represented through\na weight matrix, describing how each", "start_char_idx": 0, "end_char_idx": 3754, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ff8f34c-9eec-4f03-a610-841dc58c355e": {"__data__": {"id_": "1ff8f34c-9eec-4f03-a610-841dc58c355e", "embedding": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9355e891-1acc-419d-8d45-c051be2b9689", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "2": {"node_id": "e2e37e5f-1ba6-455c-9a86-7c060a21c5da", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "0f673ef769e6ae1d41f93727fe01ffb6e7ef0144e416a45ef684d794dd986f5b"}, "3": {"node_id": "0b699348-c595-48ba-8e34-08a194068eb5", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "e756b48f7472c8c72c2ece9915a51c41e8a0e0bb28c9f7b68b4d096aec6c7f4d"}}, "hash": "451a40404b3a65abf635cf105b00a994049037df0fd95184d74430c5a128364f", "text": "mapping (Isomap) algorithmis another nonlinear technique, based on MDS [270]. Isomapestimates the geodesic distance between a pair of points, which\nis the shortest path between the points on a manifold, by virtue\nof the measured input-space distances, e.g., the Euclideandistance usually used. This extends the capability of MDS toexplore more complex nonlinear structures in the data. Locally\nlinear embedding (LLE) algorithm addresses the nonlinear\ndimensionality reduction problem from a different startingpoint [235]. LLE emphasizes the local linearity of the manifoldand assumes that the local relations in the original data space(\n-dimensional) are also preserved in the projected low-di-\nmensional space (\n -dimensional). This is represented through\na weight matrix, describing how each point is related to the\nreconstruction of another data point. Therefore, the procedure\nfor dimensional reduction can be constructed as the problemthat \ufb01nding\n-dimensional vectors\n so that the criterion\nfunction\n is minimized. Another inter-\nesting nonlinear dimensionality reduction approach, known as\nLaplace eigenmap algorithm, is presented in [27].\nAs discussed in Section II-H, SOFM also provide good visu-\nalization for high-dimensional input patterns [168]. SOFM map\ninput patterns into a one or usually two dimensional lattice struc-\nture, consisting of nodes associated with different clusters. Anapplication for clustering of a large set of documental data isillustrated in [170], in which 6 840 568 patent abstracts were\nprojected onto a SOFM with 1 002 240 nodes.\nSubspace-based clustering addresses the challenge by ex-\nploring the relations of data objects under different combina-tions of features. clustering in quest (CLIQUE) [3] employs a\nbottom-up scheme to seek dense rectangular cells in all sub-\nspaces with high density of points. Clusters are generated as theconnected components in a graph whose vertices stand for thedense units. The resulting minimal description of the clusters is\nobtained through the merge of these rectangles. OptiGrid [136] is\ndesigned to obtain an optimal grid-partitioning. This is achievedby constructing the best cutting hyperplanes through a set ofprojections. The time complexity for OptiGrid is in the interval\nof\nand\n . ORCLUS (arbitrarily ORiented\nprojected CLUster generation) [2] de \ufb01nes a generalized pro-\njected cluster as a densely distributed subset of data objects ina subspace, along with a subset of vectors that represent the\nsubspace. The dimensionality of the subspace is prespeci \ufb01ed by\nusers as an input parameter, and several strategies are proposedin guidance of its selection. The algorithm begins with a setof randomly selected\nseeds with the full dimensionality.\nThis dimensionality and the number of clusters are decayed\naccording to some factors at each iteration, until the numberof clusters reaches the prede \ufb01ned values. Each repetition con-\nsists of three basic operations, known as assignment, vector\n\ufb01nding, and merge. ORCLUS has the overall time complexity of\nand space complexity of\n .\nObviously, the scalability to large data sets relies on the numberof initial seeds\n. A generalized subspace clustering model,\npCluster was proposed in [279]. These pClusters are formed\nby a depth- \ufb01rst clustering algorithm. Several other interesting\napplications, including a Clindex (CLustering for INDEXing)scheme and wavelet transform, are shown in [184] and [211],\nrespectively.\nM. How Many Clusters?\nThe clustering process partitions data into an appropriate\nnumber of subsets. Although for some applications, users candetermine the number of clusters,\n, in terms of their expertise,\nunder more circumstances, the value of\n is unknown and\nneeds to be estimated exclusively", "start_char_idx": 3066, "end_char_idx": 6808, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0b699348-c595-48ba-8e34-08a194068eb5": {"__data__": {"id_": "0b699348-c595-48ba-8e34-08a194068eb5", "embedding": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9355e891-1acc-419d-8d45-c051be2b9689", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "9176bffa391f70b4c1eb70b13b2a5fb968a00f79b9dd8204184b47974728fdad"}, "2": {"node_id": "1ff8f34c-9eec-4f03-a610-841dc58c355e", "node_type": null, "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}, "hash": "451a40404b3a65abf635cf105b00a994049037df0fd95184d74430c5a128364f"}}, "hash": "e756b48f7472c8c72c2ece9915a51c41e8a0e0bb28c9f7b68b4d096aec6c7f4d", "text": "has the overall time complexity of\nand space complexity of\n .\nObviously, the scalability to large data sets relies on the numberof initial seeds\n. A generalized subspace clustering model,\npCluster was proposed in [279]. These pClusters are formed\nby a depth- \ufb01rst clustering algorithm. Several other interesting\napplications, including a Clindex (CLustering for INDEXing)scheme and wavelet transform, are shown in [184] and [211],\nrespectively.\nM. How Many Clusters?\nThe clustering process partitions data into an appropriate\nnumber of subsets. Although for some applications, users candetermine the number of clusters,\n, in terms of their expertise,\nunder more circumstances, the value of\n is unknown and\nneeds to be estimated exclusively from the data themselves.\nMany clustering algorithms ask\n to be provided as an input\nparameter, and it is obvious that the quality of resulting clustersis largely dependent on the estimation of\n. A division with", "start_char_idx": 6758, "end_char_idx": 7709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f4745372-c56a-435b-88ac-3b4d570e0370": {"__data__": {"id_": "f4745372-c56a-435b-88ac-3b4d570e0370", "embedding": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "3": {"node_id": "6e7fcb45-c5c5-4cb9-85a6-b5d91947530e", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "e4893cc68ead0f9694ee193961cbb971ade24d0b5116048750159b217eeeabf5"}}, "hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 665\ntoo many clusters complicates the result, therefore, makes it\nhard to interpret and analyze, while a division with too fewclusters causes the loss of information and misleads the \ufb01nal\ndecision. Dubes called the problem of determining the number\nof clusters \u201cthe fundamental problem of cluster validity \u201d[74].\nA large number of attempts have been made to estimate the\nappropriate\nand some of representative examples are illus-\ntrated in the following.\n1) Visualization of the data set . For the data points that\ncan be effectively projected onto a two-dimensional\nEuclidean space, which are commonly depicted with\na histogram or scatterplot, direct observations can pro-vide good insight on the value of\n. However, the com-\nplexity of most real data sets restricts the effectiveness\nof the strategy only to a small scope of applications.\n2) Construction of certain indices (or stopping rules) .\nThese indices usually emphasize the compactness ofintra-cluster and isolation of inter-cluster and consider\nthe comprehensive effects of several factors, including\nthe de \ufb01ned squared error, the geometric or statistical\nproperties of the data, the number of patterns, the dis-similarity (or similarity), and the number of clusters.\nMilligan and Cooper compared and ranked 30 indices\naccording to their performance over a series of arti \ufb01-\ncial data sets [202]. Among these indices, the Cali \u00f1ski\nand Harabasz index [74] achieve the best performance\nand can be represented as\nCH\nwhere\n is the total number of patterns and\nand\n are the trace of the between and within\nclass scatter matrix, respectively. The\n that maxi-\nmizes the value of CH\n is selected as the optimal.\nIt is worth noting that these indices may be data de-pendent. The good performance of an index for cer-tain data does not guarantee the same behavior with\ndifferent data. As pointed out by Everitt, Landau, and\nLeese, \u201cit is advisable not to depend on a single rule\nfor selecting the number of groups, but to synthesizethe results of several techniques \u201d[88].\n3) Optimization of some criterion functions under prob-\nabilistic mixture-model framework . In a statistical\nframework, \ufb01nding the correct number of clusters\n(components)\n, is equivalent to \ufb01tting a model with\nobserved data and optimizing some criterion [197].\nUsually, the EM algorithm is used to estimate themodel parameters for a given\n, which goes through\na prede \ufb01ned range of values. The value of\n that\nmaximizes (or minimizes) the de \ufb01ned criterion is\nregarded as optimal. Smyth presented a Monte-Carlocross-validation method, which randomly divides datainto training and test sets\ntimes according to a cer-\ntain fraction\n (\n works well from the empirical\nresults) [252]. The\n is selected either directly based\non the criterion function or some posterior probabili-ties calculated.A large number of criteria, which combine concepts\nfrom information theory, have been proposed in theliterature. Typical examples include,\n\u2022 Akaike \u2019s information criterion (AIC) [4], [282]\nAIC\nwhere\n is the total number of patterns,\n is the\nnumber of parameters for each cluster,\n is the total\nnumber of parameters estimated, and\n is the max-\nimum log-likelihood.\n is selected with the minimum\nvalue of AIC\n .\n\u2022 Bayesian inference criterion (BIC) [226], [242]\nBIC\nis selected with the maximum value of BIC\n .\nMore criteria, such as minimum description length\n(MDL) [114], [233], minimum message length (MML)\n[114], [216], cross", "start_char_idx": 0, "end_char_idx": 3492, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e7fcb45-c5c5-4cb9-85a6-b5d91947530e": {"__data__": {"id_": "6e7fcb45-c5c5-4cb9-85a6-b5d91947530e", "embedding": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "47e5e6d01db90f7747b58d8b1786eb89fcbe51722cca6487c80d81b23989772f"}, "2": {"node_id": "f4745372-c56a-435b-88ac-3b4d570e0370", "node_type": null, "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}, "hash": "1269b9526f9ecb1ded7cdacfe4515eaf20b4369b8c0ff770022c62547b004487"}}, "hash": "e4893cc68ead0f9694ee193961cbb971ade24d0b5116048750159b217eeeabf5", "text": "posterior probabili-ties calculated.A large number of criteria, which combine concepts\nfrom information theory, have been proposed in theliterature. Typical examples include,\n\u2022 Akaike \u2019s information criterion (AIC) [4], [282]\nAIC\nwhere\n is the total number of patterns,\n is the\nnumber of parameters for each cluster,\n is the total\nnumber of parameters estimated, and\n is the max-\nimum log-likelihood.\n is selected with the minimum\nvalue of AIC\n .\n\u2022 Bayesian inference criterion (BIC) [226], [242]\nBIC\nis selected with the maximum value of BIC\n .\nMore criteria, such as minimum description length\n(MDL) [114], [233], minimum message length (MML)\n[114], [216], cross validation-based information crite-\nrion (CVIC) [254] and covariance in \ufb02ation criterion\n(CIC) [272], with their characteristics, are summarizedin [197]. Like the previous discussion for validation\nindex, there is no criterion that is superior to others in\ngeneral case. The selection of different criteria is stilldependent on the data at hand.\n4) Other heuristic approaches based on a variety of tech-\nniques and theories . Girolami performed eigenvalue\ndecomposition on the kernel matrix in the high-dimen-sional feature space and used the dominant\ncompo-\nnents in the decomposition summation as an indication\nof the possible existence of\n clusters [107]. Kothari\nand Pitts described a scale-based method, in which thedistance from a cluster centroid to other clusters inits neighborhood is considered (added as a regulariza-\ntion term in the original squared error criterion, Sec-\ntion II-C) [160]. The neighborhood of clusters work asa scale parameter and the\nthat is persistent in the\nlargest interval of the neighborhood parameter is re-\ngarded as the optimal.\nBesides the previous methods, constructive clustering algo-\nrithms can adaptively and dynamically adjust the number of\nclusters rather than use a prespeci \ufb01ed and \ufb01xed number. ART\nnetworks generate a new cluster, only when the match betweenthe input pattern and the expectation is below some prespeci \ufb01ed\ncon\ufb01dence value [51]. A functionally similar mechanism is\nused in the CDL network [82]. The robust competitive clus-\ntering algorithm (RCA) describes a competitive agglomerationprocess that progresses in stages, and clusters that lose in thecompetition are discarded, and absorbed into other clusters [98].\nThis process is generalized in [42], which attains the number\nof clusters by balancing the effect between the complexityand the \ufb01delity. Another learning scheme, SPLL iteratively\ndivides cluster prototypes from a single prototype until no\nmore prototypes satisfy the split criterion [296]. Several other\nconstructive clustering algorithms, including the FACS andplastic neural gas, can be accessed in [223] and [232], re-spectively. Obviously, the problem of determining the number", "start_char_idx": 2828, "end_char_idx": 5654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "483d22db-5260-43f2-af63-1e2bfee184d1": {"__data__": {"id_": "483d22db-5260-43f2-af63-1e2bfee184d1", "embedding": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d61f6244-0721-4e88-bb6f-1621e60bd67d", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "3": {"node_id": "3116bcda-b1ca-41c2-a33a-4b34f9154094", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "8740eae52f0851067935d98196be784a47e0200857182a1cf0e0737488576e32"}}, "hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9", "text": "666 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nof clusters is converted into a parameter selection problem,\nand the resulting number of clusters is largely dependent onparameter tweaking.\nIII. A\nPPLICATIONS\nWe illustrate applications of clustering techniques in three as-\npects. The \ufb01rst is for two classical benchmark data sets that are\nwidely used in pattern recognition and machine learning. Then,we show an application of clustering for the traveling salesmanproblem. The last topic is on bioinformatics. We deal with clas-\nsical benchmarks in Sections III-A and III-B and the traveling\nsalesman problem in Section III-C. A more extensive discussionof bioinformatics is in Sections III-D and III-E.\nA. Benchmark Data Sets \u2014I\nRIS\nThe iris data set [92] is one of the most popular data\nsets to examine the performance of novel methods in pat-tern recognition and machine learning. It can be down-loaded from the UCI Machine Learning Repository at\nhttp://www.ics.uci.edu/~mlearn/MLRepository.html. There are\nthree categories in the data set (i.e., iris setosa, iris versicolorand iris virginical), each having 50 patterns with four features[i.e., sepal length (SL), sepal width (SW), petal length (PL),\nand petal width (PW)]. Iris setosa can be linearly separated\nfrom iris versicolor and iris virginical, while iris versicolor andiris virginical are not linearly separable (see Fig. 4(a), in whichonly three features are used). Fig. 4(b) depicts the clustering\nresult with a standard\n-means algorithm. It is clear to see that\n-means can correctly differentiate iris setosa from the other\ntwo iris plants. But for iris versicolor and virginical, there exist16 misclassi \ufb01cations. This result is similar to those (around\n15 errors) obtained from other classical clustering algorithms\n[221]. Table III summarizes some of the clustering resultsreported in the literature. From the table, we can see that manynewly developed approaches can greatly improve the clustering\nperformance on iris data set (around 5 misclassi \ufb01cations); some\neven can achieve 100% accuracy. Therefore, the data can bewell classi \ufb01ed with appropriate methods.\nB. Benchmark Data Sets \u2014M\nUSHROOM\nUnlike the iris data set, all of the features of the mushroom\ndata set, which can also be accessible at the UCI Machine\nLearning Repository, are nominal rather than numerical. These23 species of gilled mushrooms are categorized as either edible\nor poisonous. The total number of instances is 8 124 with 4\n208 being edible and 3 916 poisonous. The 22 features aresummarized in Table IV with corresponding possible values.Table V illustrates some experimental results in the literature.\nAs indicated in [117] and [277], traditional clustering strategies,\nlike\n-means and hierarchical clustering, work poorly on the\ndata set. The accuracy for\n -means is just around 69% [277]\nand the clusters formed by classical HC are mixed with nearly\nsimilar proportion of both edible and poisonous objects [117].\nThe results reported in the newly developed algorithms, whichare speci \ufb01cally used for tackling categorical or mixture data,\ngreatly improve the situation [117], [183]. The algorithm ROCK\nFig. 4. (a) Iris data sets. There are three iris categories, each having 50 samples\nwith 4 features. Here, only three features are used: PL, PW, and SL. (b) /75-means\nclustering result with 16 classi \ufb01cation errors observed.\nTABLE III\nSOME CLUSTERING RESULTS FOR THE", "start_char_idx": 0, "end_char_idx": 3439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3116bcda-b1ca-41c2-a33a-4b34f9154094": {"__data__": {"id_": "3116bcda-b1ca-41c2-a33a-4b34f9154094", "embedding": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d61f6244-0721-4e88-bb6f-1621e60bd67d", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "8f37bb90b13bd395fc2f4659bc023956b00e81998adf683929c3dc1ccb6cb8fc"}, "2": {"node_id": "483d22db-5260-43f2-af63-1e2bfee184d1", "node_type": null, "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}, "hash": "9743564f7a0a857c76d8a31acdfd372dd76e001bd3b1e2e9ee78bf0ba1846ad9"}}, "hash": "8740eae52f0851067935d98196be784a47e0200857182a1cf0e0737488576e32", "text": "clustering, work poorly on the\ndata set. The accuracy for\n -means is just around 69% [277]\nand the clusters formed by classical HC are mixed with nearly\nsimilar proportion of both edible and poisonous objects [117].\nThe results reported in the newly developed algorithms, whichare speci \ufb01cally used for tackling categorical or mixture data,\ngreatly improve the situation [117], [183]. The algorithm ROCK\nFig. 4. (a) Iris data sets. There are three iris categories, each having 50 samples\nwith 4 features. Here, only three features are used: PL, PW, and SL. (b) /75-means\nclustering result with 16 classi \ufb01cation errors observed.\nTABLE III\nSOME CLUSTERING RESULTS FOR THE IRISDATASET\ndivides objects into 21 clusters with most of them (except one)\nconsisting of only one category, which increases the accuracy\nalmost to 99%. The algorithm SBAC works on a subset of\n200 randomly selected objects, 100 for each category and thegeneral results show the correct partition of 3 clusters (two foredible mushrooms, one for poisonous ones). In both studies, the", "start_char_idx": 2769, "end_char_idx": 3821, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db902509-1cbb-4601-b036-b3b94682f814": {"__data__": {"id_": "db902509-1cbb-4601-b036-b3b94682f814", "embedding": null, "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c4260db6-594a-44d3-85dc-1f383c2273c2", "node_type": null, "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}, "hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f"}}, "hash": "23be5151a06c86657e00f09c5cefbba20ed48146c4f4302abd9d9af92a856d0f", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 667\nTABLE IV\nFEATURES FOR THE MUSHROOM DATASET\nconstitution of each feature for generated clusters is also illus-\ntrated and it is observed that some features, like cap-shape andring-type, represent themselves identically for both categories\nand, thus, suggest poor performance of traditional approaches.\nMeanwhile, feature odor shows good discrimination for thedifferent types of mushrooms. Usually, value almond, anise,or none indicates the edibility of mushrooms, while value\npungent, foul, or \ufb01shy means the high possibility of presence\nof poisonous contents in the mushrooms.\nC. Traveling Salesman Problem\nThe traveling salesman problem (TSP) is one of the most\nstudied examples in an important class of problems known as\nNP-complete problems. Given a complete undirected graph\n, where\n is a set of vertices and\n is a set of\nedges each relating two vertices with an associated nonnegativeinteger cost, the most general form of the TSP is equivalent\nto\ufb01nding any Hamiltonian cycle, which is a tour over\nthat\nbegins and ends at the same vertex and visits other verticesexactly once. The more common form of the problem is theTABLE V\nSOME CLUSTERING RESULTS FOR THE MUSHROOM DATASET\noptimization problem of trying to \ufb01nd the shortest Hamiltonian\ncycle, and in particular, the most common is the Euclidean\nversion, where the vertices and edges all lie in the plane.Mulder and Wunsch applied a divide-and-conquer clusteringtechnique, with ART networks, to scale the problem to a mil-\nlion cities [208]. The divide and conquer paradigm gives the\n\ufb02exibility to hierarchically break large problems into arbitrarily\nsmall clusters depending on what tradeoff between accuracy\nand speed is desired. In addition, the subproblems provide an\nexcellent opportunity to take advantage of parallel systemsfor further optimization. As the \ufb01rst stage of the algorithm,\nthe ART network is used to sort the cities into clusters. The\nvigilance parameter is used to set a maximum distance from the\ncurrent pattern. A vigilance parameter between 0 and 1 is usedas a percentage of the global space to determine the vigilancedistance. Values were chosen based on the desired number and\nsize of individual clusters. The clusters were then each passed to\na version of the Lin-Kernighan (LK) algorithm [187]. The laststep combines the subtours back into one complete tour. Tourswith good quality for city levels up to 1 000 000 were obtained\nwithin 25 minutes on a 2 GHz AMD Athlon MP processor with\n512 M of DDR RAM. Fig. 5 shows the visualizing results for 1000, 10 000, and 1 000 000 cities, respectively.\nIt is worthwhile to emphasize the relation between the TSP\nand very large-scale integrated (VLSI) circuit clustering, which\npartitions a sophisticated system into smaller and simpler sub-circuits to facilitate the circuit design. The object of the par-titions is to minimize the number of connections among the\ncomponents. One strategy for solving the problem is based on\ngeometric representations, either linear or multidimensional [8].Alpert and Kahng considered a solution to the problem as the\u201cinverse \u201dof the divide-and-conquer TSP method and used a\nlinear tour of the modules to form the subcircuit partitions [7].\nThey adopted the space \ufb01lling curve heuristic for the TSP to con-\nstruct the tour so that connected modules are still close in thegenerated tour. A dynamic programming method was used to\ngenerate the resulting partitions. More detailed discussion on\nVLSI circuit clustering can be found in the survey by Alpertand Kahng [7].", "start_char_idx": 0, "end_char_idx": 3579, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dff121da-ad44-45e4-937c-2bbcb34a28a1": {"__data__": {"id_": "dff121da-ad44-45e4-937c-2bbcb34a28a1", "embedding": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "3": {"node_id": "d2e6aea4-c820-4237-8dbf-0f4d6addc68d", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "a6edad30fa68fb24106f83183fd4ad1478266e13a46f11d9253455dbdb3ada62"}}, "hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b", "text": "668 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 5. Clustering divide-and-conquer TSP resulting tours for (a) 1 k, (b) 10 k,\n(c) 1 M cities. The clustered LK algorithm achieves a signi \ufb01cant speedup and\nshows good scalability.\nD. Bioinformatics \u2014Gene Expression Data\nRecently, advances in genome sequencing projects and DNA\nmicroarray technologies have been achieved. The \ufb01rst draft of\nthe human genome sequence project was completed in 2001,\nseveral years earlier than expected [65], [275]. The genomic se-\nquence data for other organizms (e.g., Drosophila melanogaster\nandEscherichia coli ) are also abundant. DNA microarray tech-\nnologies provide an effective and ef \ufb01cient way to measure gene\nexpression levels of thousands of genes simultaneously under\ndifferent conditions and tissues, which makes it possible to in-vestigate gene activities from the angle of the whole genome[79], [188]. With sequences and gene expression data in hand,\nto investigate the functions of genes and identify their roles in\nthe genetic process become increasingly important. Analyzesunder traditional laboratory techniques are time-consuming andexpensive. They fall far behind the explosively increasing gen-\neration of new data. Among the large number of computational\nmethods used to accelerate the exploration of life science, clus-tering can reveal the hidden structures of biological data, and isparticularly useful for helping biologists investigate and under-\nstand the activities of uncharacterized genes and proteins and\nfurther, the systematic architecture of the whole genetic net-work. We demonstrate the applications of clustering algorithmsin bioinformatics from two aspects. The \ufb01rst part is based on\nthe analysis of gene expression data generated from DNA mi-\ncroarray technologies. The second part describes clustering pro-cesses that directly work on linear DNA or protein sequences.The assumption is that functionally similar genes or proteins\nusually share similar patterns or primary sequence structures.DNA microarray technologies generate many gene ex-\npression pro \ufb01les. Currently, there are two major microarray\ntechnologies based on the nature of the attached DNA: cDNAwith length varying from several hundred to thousand bases,\nor oligonucleotides containing 20 \u201330 bases. For cDNA tech-\nnologies, a DNA microarray consists of a solid substrate to\nwhich a large amount of cDNA clones are attached according\nto a certain order [79]. Fluorescently labeled cDNA, obtainedfrom RNA samples of interest through the process of reverse\ntranscription, is hybridized with the array. A reference sample\nwith a different \ufb02uorescent label is also needed for comparison.\nImage analysis techniques are then used to measure the \ufb02uores-\ncence of each dye, and the ratio re \ufb02ects relative levels of gene\nexpression. For a high-density oligonucleotide microarray,\noligonucleotides are \ufb01xed on a chip through photolithography\nor solid-phase DNA synthesis [188]. In this case, absolute\ngene expression levels are obtained. After the normalization\nof the \ufb02uorescence intensities, the gene expression pro \ufb01les\nare represented as a matrix\n, where\n is the ex-\npression level of the\n th gene in the\n th condition, tissue, or\nexperimental stage. Gene expression data analysis consists of a\nthree-level framework based on the complexity, ranging from\nthe investigation of single gene activities to the inference of the\nentire genetic network [20]. The intermediate level explores\nthe relations and interactions between genes under differentconditions, and attracts more attention currently. Generally,\ncluster analysis of gene expression data is composed of two\naspects:", "start_char_idx": 0, "end_char_idx": 3678, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d2e6aea4-c820-4237-8dbf-0f4d6addc68d": {"__data__": {"id_": "d2e6aea4-c820-4237-8dbf-0f4d6addc68d", "embedding": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "95db2a39c65df656e70d6388fbd6acad2e136a7bcb975658a9a9d5397215ca83"}, "2": {"node_id": "dff121da-ad44-45e4-937c-2bbcb34a28a1", "node_type": null, "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}, "hash": "3833c3249e47f5f630e0a572a31282895c86079d5d7973605ea90e6b9bef3b4b"}}, "hash": "a6edad30fa68fb24106f83183fd4ad1478266e13a46f11d9253455dbdb3ada62", "text": "are \ufb01xed on a chip through photolithography\nor solid-phase DNA synthesis [188]. In this case, absolute\ngene expression levels are obtained. After the normalization\nof the \ufb02uorescence intensities, the gene expression pro \ufb01les\nare represented as a matrix\n, where\n is the ex-\npression level of the\n th gene in the\n th condition, tissue, or\nexperimental stage. Gene expression data analysis consists of a\nthree-level framework based on the complexity, ranging from\nthe investigation of single gene activities to the inference of the\nentire genetic network [20]. The intermediate level explores\nthe relations and interactions between genes under differentconditions, and attracts more attention currently. Generally,\ncluster analysis of gene expression data is composed of two\naspects: clustering genes [80], [206], [260], [268], [283], [288]\nor clustering tissues or experiments [5], [109], [238].\nResults of gene clustering may suggest that genes in the same\ngroup have similar functions, or they share the same transcrip-tional regulation mechanism. Cluster analysis, for groupingfunctionally similar genes, gradually became popular after\nthe successful application of the average linkage hierarchical\nclustering algorithm for the expression data of budding yeastSaccharomyces cerevisiae and reaction of human \ufb01broblasts to\nserum by Eisen et al. [80]. They used the Pearson correlation\ncoef\ufb01cient to measure the similarity between two genes, and\nprovided a very informative visualization of the clustering re-sults. Their results demonstrate that functionally similar genestend to reside in the same clusters formed by their expression\npattern, even under a relatively small set of conditions. Herwig\net al. developed a variant of\n-means algorithm to cluster a set\nof 2 029 human cDNA clones and adopted mutual informationas the similarity measure [230]. Tomayo et al. [268] made\nuse of SOFM to cluster gene expression data and its applica-\ntion in hematopoietic differentiation provided new insight forfurther research. Graph theories based clustering algorithms,like CAST [29] and CLICK [247], showed very promising\nperformances in tackling different types of gene expression\ndata. Since many genes usually display more than one function,fuzzy clustering may be more effective in exposing these rela-tions [73]. Gene expression data is also important to elucidate\nthe genetic regulation mechanism in a cell. By examining\nthe corresponding DNA sequences in the control regions of acluster of co-expressed genes, we may identify potential shortand consensus sequence patterns, known as motifs, and further\ninvestigate their interaction with transcriptional binding factors,", "start_char_idx": 2898, "end_char_idx": 5569, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5bc3ddbc-348b-4558-8f6e-e0a6e567aa4a": {"__data__": {"id_": "5bc3ddbc-348b-4558-8f6e-e0a6e567aa4a", "embedding": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d26f89-b133-4052-b227-ae3409989906", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "3": {"node_id": "f083b3e1-dbd6-488d-86c9-e36f2a442076", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "97af896fed7cf6c5f05d10c37fb01aabc2d1d19890ae90819523af311c12335d"}}, "hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 669\nleading to different gene activities. Spellman et al. clustered\n800 genes according to their expression during the yeast cellcycle [260]. Analyzes of 8 major gene clusters unravel the\nconnection between co-expression and co-regulation. Tavazoie\net al. partitioned 3 000 genes into 30 clusters with the\n-means\nalgorithm [269]. For each cluster, 600 base pairs upstreamsequences of the genes were searched for potential motifs. 18\nmotifs were found from 12 clusters in their experiments and 7\nof them can be veri \ufb01ed according to previous empirical results\nin the literature. A more comprehensive investigation can befound in [206].\nAs to another application, clustering tissues or experiments\nare valuable in identifying samples that are in the different dis-ease states, discovering, or predicting different cancer types, andevaluating the effects of novel drugs and therapies [5], [109],\n[238]. Golub et al. described the restriction of traditional cancer\nclassi \ufb01cation methods, which are mostly dependent on mor-\nphological appearance of tumors, and divided cancer classi \ufb01-\ncation into two challenges: class discovery and class predic-\ntion. They utilized SOFM to discriminate two types of human\nacute leukemias: acute myeloid leukemia (AML) and acute lym-phoblastic leukemia (ALL) [109]. According to their results,two subsets of ALL, with different origin of lineage, can be\nwell separated. Alon et al. performed a two-way clustering for\nboth tissues and genes and revealed the potential relations, rep-resented as visualizing patterns, among them [6]. Alizadeh et\nal.demonstrated the effectiveness of molecular classi \ufb01cation of\ncancers by their gene expression pro \ufb01les and successfully dis-\ntinguished two molecularly distinct subtypes of diffuse largeB-cell lymphoma, which cause high percentage failure in clin-ical treatment [5]. Furthermore, Scherf et al. constructed a gene\nexpression database to study the relationship between genes and\ndrugs for 60 human cancer cell lines, which provides an im-portant criterion for therapy selection and drug discovery [238].Other applications of clustering algorithms for tissue classi \ufb01-\ncation include: mixtures of multivariate Gaussian distributions\n[105], ellipsoidal ART [287], and graph theory-based methods[29], [247]. In most of these applications, important genes thatare tightly related to the tumor types are identi \ufb01ed according to\ntheir expression differentiation under different cancerous cate-\ngories, which are in accord with our prior recognition of rolesof these genes, to a large extent [5], [109]. For example, Alon et\nal.found that 5 of 20 statistically signi \ufb01cant genes were muscle\ngenes, and the corresponding muscle indices provided an expla-\nnation for false classi \ufb01cations [6].\nFig. 7 illustrates an application of hierarchical clustering\nand SOFM for gene expression data. This data set is on the\ndiagnostic research of small round blue-cell tumors (SRBCT \u2019s)\nof childhood and consists of 83 samples from four categories,known as Burkitt lymphomas (BL), the Ewing family of tumors(EWS), neuroblastoma (NB), and rhabdomyosarcoma (RMS),\nand 5 non-SRBCT samples [164]. Gene expression levels of\n6 567 genes were measured using cDNA microarray for eachsample, 2 308 of which passed the \ufb01lter and were kept for fur-\nther analyzes. These genes are further ranked according to the\nscores calculated by some criterion functions", "start_char_idx": 0, "end_char_idx": 3454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f083b3e1-dbd6-488d-86c9-e36f2a442076": {"__data__": {"id_": "f083b3e1-dbd6-488d-86c9-e36f2a442076", "embedding": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d26f89-b133-4052-b227-ae3409989906", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "2": {"node_id": "5bc3ddbc-348b-4558-8f6e-e0a6e567aa4a", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "51c8f4a8ff52a9f53834279c5cd12f323019e74303ab95d1ac3f674d1489e9ba"}, "3": {"node_id": "de81bd72-e92b-4660-9211-c8ee9f220638", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "bca252312a9115dc73e51e3cc89d7f05fa363cd56bf0ead9c21fdad68da29238"}}, "hash": "97af896fed7cf6c5f05d10c37fb01aabc2d1d19890ae90819523af311c12335d", "text": "\ufb01cations [6].\nFig. 7 illustrates an application of hierarchical clustering\nand SOFM for gene expression data. This data set is on the\ndiagnostic research of small round blue-cell tumors (SRBCT \u2019s)\nof childhood and consists of 83 samples from four categories,known as Burkitt lymphomas (BL), the Ewing family of tumors(EWS), neuroblastoma (NB), and rhabdomyosarcoma (RMS),\nand 5 non-SRBCT samples [164]. Gene expression levels of\n6 567 genes were measured using cDNA microarray for eachsample, 2 308 of which passed the \ufb01lter and were kept for fur-\nther analyzes. These genes are further ranked according to the\nscores calculated by some criterion functions [109]. Generally,\nthese criterion functions attempt to seek a subset of genes thatcontribute most to the discrimination of different cancer types.This can be regarded as a feature selection process. However,\nproblems like how many genes are really required, and whetherthese genes selected are really biologically meaningful, are\nstill not answered satisfactorily. Hierarchical clustering was\nperformed by the program CLUSTER and the results werevisualized by the program TreeView, developed by Eisen inStanford University. Fig. 7(a) and (b) depicts the clustering\nresults for both the top 100 genes, selected by the Fisher\nscores, and the samples. Graphic visualization is achieved byassociating each data point with a certain color according to thecorresponding scale. Some clustering patterns are clearly dis-\nplayed in the image. Fig. 7(c) depicts a 5-by-5 SOFM topology\nfor all genes, with each cluster represented by the centroid(mean) for each feature (sample). 25 clusters are generatedand the number of genes in each cluster is also indicated.\nThe software package GeneCluster, developed by Whitehead\nInstitute/MIT Center for Genome Research (WICGR), wasused in this analysis.\nAlthough clustering techniques have already achieved many\nimpressive results in the analysis of gene expression data, thereare still many problems that remain open. Gene expression data\nsets usually are characterized as\n1) small set samples with high-dimensional features;\n2) high redundancy;\n3) inherent noise;4) sparsity of the data.\nMost of the published data sets include usually less than 20\nsamples for each tumor type, but with as many as thousands ofgene measures [80], [109], [238], [268]. This is partly causedby the lag of experimental condition (e.g., sample collection), in\ncontrast to the rapid advancement of microarray and sequencing\ntechnologies. In order to evaluate existing algorithms morereasonably and develop more effective new approaches, moredata with enough samples or more conditional observations are\nneeded. But from the trend of gene chip technologies, which\nalso follows Moore \u2019s law for semiconductor chips [205], the\ncurrent status will still exist for a long time. This problem ismore serious in the application of gene expression data for\ncancer research, in which clustering algorithms are required to\nbe capable of effectively \ufb01nding potential patterns under a large\nnumber of irrelevant factors, as a result of the introduction oftoo many genes. At the same time, feature selection, which is\nalso called informative gene selection in the context, also plays\na very important role. Without any doubt, clustering algorithmsshould be feasible in both time and space complexity. Due tothe nature of the manufacture process of the microarray chip,\nnoise can be inevitably introduced into the expression data\nduring different stages. Accordingly, clustering algorithmsshould have noise and outlier detection mechanisms in order toremove their effects. Furthermore, different algorithms usually\nform different clusters for the same data set, which is a general\nproblem in cluster analysis. How to evaluate the quality of thegenerated clusters of genes, and how to", "start_char_idx": 2892, "end_char_idx": 6729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de81bd72-e92b-4660-9211-c8ee9f220638": {"__data__": {"id_": "de81bd72-e92b-4660-9211-c8ee9f220638", "embedding": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e6d26f89-b133-4052-b227-ae3409989906", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "a80dbb45c440938597d3a76e1350d3eed2fa99b3ca490ddb9f52def65244dff8"}, "2": {"node_id": "f083b3e1-dbd6-488d-86c9-e36f2a442076", "node_type": null, "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}, "hash": "97af896fed7cf6c5f05d10c37fb01aabc2d1d19890ae90819523af311c12335d"}}, "hash": "bca252312a9115dc73e51e3cc89d7f05fa363cd56bf0ead9c21fdad68da29238", "text": "effectively \ufb01nding potential patterns under a large\nnumber of irrelevant factors, as a result of the introduction oftoo many genes. At the same time, feature selection, which is\nalso called informative gene selection in the context, also plays\na very important role. Without any doubt, clustering algorithmsshould be feasible in both time and space complexity. Due tothe nature of the manufacture process of the microarray chip,\nnoise can be inevitably introduced into the expression data\nduring different stages. Accordingly, clustering algorithmsshould have noise and outlier detection mechanisms in order toremove their effects. Furthermore, different algorithms usually\nform different clusters for the same data set, which is a general\nproblem in cluster analysis. How to evaluate the quality of thegenerated clusters of genes, and how to choose appropriatealgorithms for a speci \ufb01ed application, are particularly crucial\nfor gene expression data research, because sometimes, even\nbiologists cannot identify the real patterns from the artifacts ofthe clustering algorithms, due to the limitations of biological", "start_char_idx": 6450, "end_char_idx": 7564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60d5ebe6-9667-46c1-889e-c8d87f6cee52": {"__data__": {"id_": "60d5ebe6-9667-46c1-889e-c8d87f6cee52", "embedding": null, "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "21fd1fe9-c79c-4c93-a146-2c1dc4fb0f04", "node_type": null, "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}, "hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d"}}, "hash": "7dde70763ad7a05aa86668c11182248e7dc9c65ecb7f134558534ef3e6fb2c6d", "text": "670 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 6. Basic procedure of cDNA microarray technology [68]. Fluorescently labeled cDNAs, obtained from target and reference samples through revers e\ntranscription, are hybridized with the microarray, which is comprised of a large amount of cDNA clones. Image analysis measures the ratio of the two dy es.\nComputational methods, e.g., hierarchical clustering, further disclose the relations among genes and corresponding conditions.\nFig. 7. Hierarchical and SOFM clustering of SRBCT \u2019s gene expression data set. (a) Hierarchical clustering result for the 100 selected genes under 83 tissue\nsamples. The gene expression matrix is visualized through a color scale. (b) Hierarchical clustering result for the 83 tissue samples. Here, the dime nsion is 100 as\n100 genes are selected like in (a). (c) SOFM clustering result for the 2308 genes. A 5 /25 SOFM is used and 25 clusters are formed. Each cluster is represented by\nthe average values.", "start_char_idx": 0, "end_char_idx": 1004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1cf809ca-64fa-40f0-9765-2d7bed70a722": {"__data__": {"id_": "1cf809ca-64fa-40f0-9765-2d7bed70a722", "embedding": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "3": {"node_id": "b546b495-5330-414f-a767-832cf1360cd9", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "dd4d4ebc565a9a9f858d5488a296870789ed38ecb2782dc5b057af11892b487a"}}, "hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 671\nknowledge. Some recent results can be accessed in [29], [247],\nand [291].\nE. Bioinformatics \u2014DNA or Protein Sequences Clustering\nDNA (deoxyribonucleic acid) is the hereditary material ex-\nisting in all living cells. A DNA molecule is a double helix con-\nsisting of two strands, each of which is a linear sequence com-posed of four different nucleotides \u2014adenine, guanine, thymine,\nand cytosine, abbreviated as the letters A, G, T, and C, respec-\ntively. Each letter in a DNA sequence is also called a base.\nProteins determine most of cells \u2019structures, functions, prop-\nerties, and regulatory mechanisms. The primary structure of aprotein is also a linear and alphabetic chain with the difference\nthat each unit represents an amino acid, which has twenty types\nin total. Proteins are encoded by certain segments of DNA se-quences through a two-stage process (transcription and trans-lation). These segments are known as genes or coding regions.\nInvestigation of the relations between DNA and proteins, as well\nas their own functions and properties, is one of the important re-search directions in both genetics and bioinformatics.\nThe similarity between newly sequenced genes or proteins\nand annotated genes or proteins usually offers a cue to identifytheir functions. Searching corresponding databases for a new\nDNA or protein sequence has already become routine in genetic\nresearch. In contrast to sequence comparison and search, clusteranalysis provides a more effective means to discover compli-cated relations among DNA and protein sequences. We summa-\nrize the following clustering applications for DNA and protein\nsequences:\n1) function recognition of uncharacterized genes or pro-\nteins [119];\n2) structure identi \ufb01cation of large-scale DNA or protein\ndatabases [237], [257];\n3) redundancy decrease of large-scale DNA or protein\ndatabases [185];\n4) domain identi \ufb01cation [83], [115];\n5) expressed sequence tag (EST) clustering [49], [200].\nAs described in Section II-J, classical dynamic programming\nalgorithms for global and local sequence alignment are too in-\ntensive in computational complexity. This becomes worse be-cause of the existence of a large volume of nucleic acids andamino acids in the current DNA or protein databases, e.g., bac-\nteria genomes are from 0.5 to 10 Mbp, fungi genomes range\nfrom 10 to 50 Mbp, while the human genome is around 3 310Mbp [18] (Mbp means million base pairs). Thus, conventionaldynamic programming algorithms are computationally infea-\nsible. In practice, sequence comparison or proximity measure\nis achieved via some heuristics. Well-known examples includeBLAST and FASTA with many variants [10], [11], [224]. Thekey idea of these methods is to identify regions that may have\npotentially high matches, with a list of prespeci \ufb01ed high-scoring\nwords, at an early stage. Therefore, further search only needs tofocus on these regions with expensive but accurate algorithms.Recognizing the bene \ufb01t coming from the separation of word\nmatching and sequence alignment to computational burden re-\nduction, Miller, Gurd, and Brass described three algorithms fo-cusing on speci \ufb01c problems [199]. The implementation of thescheme for large database vs. database comparison exhibits an\napparent improvement in computation time. Kent and Zahler de-signed a three-pass algorithm, called wobble aware bulk aligner\n(WABA) [162], for aligning large-scale genomic sequences", "start_char_idx": 0, "end_char_idx": 3461, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b546b495-5330-414f-a767-832cf1360cd9": {"__data__": {"id_": "b546b495-5330-414f-a767-832cf1360cd9", "embedding": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "2": {"node_id": "1cf809ca-64fa-40f0-9765-2d7bed70a722", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "a9373f707c8b9706603b936002bbc1888df8c10eff1e95d63d52f3190a02f394"}, "3": {"node_id": "d935f193-6229-4fda-903a-0edf140774bb", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "ceae6ea619af24fe24061387a6e1d56217c4d318b5ba0e440e105fc3b3e72a5e"}}, "hash": "dd4d4ebc565a9a9f858d5488a296870789ed38ecb2782dc5b057af11892b487a", "text": "methods is to identify regions that may have\npotentially high matches, with a list of prespeci \ufb01ed high-scoring\nwords, at an early stage. Therefore, further search only needs tofocus on these regions with expensive but accurate algorithms.Recognizing the bene \ufb01t coming from the separation of word\nmatching and sequence alignment to computational burden re-\nduction, Miller, Gurd, and Brass described three algorithms fo-cusing on speci \ufb01c problems [199]. The implementation of thescheme for large database vs. database comparison exhibits an\napparent improvement in computation time. Kent and Zahler de-signed a three-pass algorithm, called wobble aware bulk aligner\n(WABA) [162], for aligning large-scale genomic sequences of\ndifferent species, which employs a seven-state pairwise hiddenMarkov model [78] for more effective alignments. In [201],Miller summarized the current research status of genomic se-\nquence comparison and suggested valuable directions for fur-\nther research efforts.\nMany clustering techniques have been applied to organize\nDNA or protein sequence data. Some directly operate on a\nproximity measure; some are based on feature extraction,\nwhile others are constructed on statistical models. Somervuoand Kohonen illustrated an application of SOFM to clusterprotein sequences in SWISSPROT database [257]. FASTA\nwas used to calculate the sequence similarity. The resulting\ntwo-dimensional SOFM provides a visualized representationof the relations within the entire sequence database. Basedon the similarity measure of gapped BLAST, Sasson et al.\nutilized an agglomerative hierarchical clustering paradigm to\ncluster all protein sequences in SWISSPROT [237]. The effectsof four merging rules, different from the interpretation ofcluster centers, on the resulting protein clusters were examined.\nThe advantages as well as the potential risk of the concept,\ntransitivity, were also elucidated in the paper. According tothe transitivity relation, two sequences that do not show highsequence similarity by virtue of direct comparison, may be\nhomologous (having a common ancestor) if there exists an\nintermediate sequence similar to both of them. This makes itpossible to detect remote homologues that can not be observedby similarity comparison. However, unrelated sequences may\nbe clustered together due to the effects of these intermediate\nsequences [237]. Bolten et al. addressed the problem with the\nconstruction a directed graph, in which each protein sequencecorresponds to a vertex and edges are weighted based on the\nalignment score between two sequences and self alignment\nscore of each sequence [41]. Clusters were formed throughthe search of strongly connected components (SCCs), each ofwhich is a maximal subset of vertices and for each pair of ver-\ntices\nand\n in the subset, there exist two directed paths from\nto\nand vice versa. A minimum normalized cut algorithm for\ndetecting protein families and a minimum spanning tree (MST)application for seeking domain information were presented in\n[1] and [115], respectively. In contrast with the aforementioned\nproximity-based methods, Guralnik and Karypis transformedprotein or DNA sequences into a new feature space, basedon the detected subpatterns working as the sequence features,\nand clustered with the\n-means algorithm [119]. The method\nis immune from all-against-all expensive sequence compar-ison and suitable for analyzing large-scale databases. Kroghdemonstrated the power of hidden Markov models (HMMs)\nin biological sequences modeling and clustering of protein\nfamilies [177]. Fig. 8 depicts a typical structure of HMM, inwhich match states (abbreviated with letter M), insert states (I)and delete states (D) are represented as rectangles, diamonds,\nand circles, respectively [78], [177]. These states", "start_char_idx": 2835, "end_char_idx": 6623, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d935f193-6229-4fda-903a-0edf140774bb": {"__data__": {"id_": "d935f193-6229-4fda-903a-0edf140774bb", "embedding": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0e4c2413-6aad-42c9-9b2f-cb493f42a576", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "4b568e212fc507329cccb12245fd1ba66ca096c54675f70ab3155b381e808b70"}, "2": {"node_id": "b546b495-5330-414f-a767-832cf1360cd9", "node_type": null, "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}, "hash": "dd4d4ebc565a9a9f858d5488a296870789ed38ecb2782dc5b057af11892b487a"}}, "hash": "ceae6ea619af24fe24061387a6e1d56217c4d318b5ba0e440e105fc3b3e72a5e", "text": "In contrast with the aforementioned\nproximity-based methods, Guralnik and Karypis transformedprotein or DNA sequences into a new feature space, basedon the detected subpatterns working as the sequence features,\nand clustered with the\n-means algorithm [119]. The method\nis immune from all-against-all expensive sequence compar-ison and suitable for analyzing large-scale databases. Kroghdemonstrated the power of hidden Markov models (HMMs)\nin biological sequences modeling and clustering of protein\nfamilies [177]. Fig. 8 depicts a typical structure of HMM, inwhich match states (abbreviated with letter M), insert states (I)and delete states (D) are represented as rectangles, diamonds,\nand circles, respectively [78], [177]. These states correspond\nto substitution, insertion, and deletion in edit operations. Forconvenience, a begin state and an end state are added to the", "start_char_idx": 6511, "end_char_idx": 7386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2e081ef4-24e3-4c4d-94b6-28decf74f920": {"__data__": {"id_": "2e081ef4-24e3-4c4d-94b6-28decf74f920", "embedding": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "3": {"node_id": "38da4b93-3a72-4c12-affe-d9c5bfc8f8c9", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "ab17db18b0c1783371d15d2925b6c54f3096ec78d373c5d3e385e9357585d07f"}}, "hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334", "text": "672 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\nFig. 8. HMM architecture [177]. There are three different states, match (M),\ninsert (I), and delete (D), corresponding to substitution, insertion, and deletion\noperation, respectively. A begin (B) and end (E) state are also introduced to\nrepresent the start and end of the process. This process goes through a series of\nstates according to the transition probability, and emits either 4-letter nucleotide\nor 20-letter amino acid alphabet based on the emission probability.\nmodel, denoted by letter B and E. Letters, either from the 4-letter\nnucleotide alphabet or from 20-letter amino acid alphabet,\nare generated from match and insert states according to some\nemission probability distributions. Delete states do not produceany symbols, and are used to skip the match states.\nHMMs are\nrequiredinorderto describe\n clusters,orfamilies (subfamilies),\nwhich are regarded as a mixture model and proceeded with\nan EM learning algorithm similar to single HMM case. Anexample for clustering subfamilies of 628 globins shows theencouraging results. Further discussion can be found in [78]\nand [145].\nIV . C\nONCLUSION\nAs an important tool for data exploration, cluster analysis\nexamines unlabeled data, by either constructing a hierarchicalstructure, or forming a set of groups according to a prespeci \ufb01ed\nnumber. This process includes a series of steps, ranging from\npreprocessing and algorithm development, to solution validityand evaluation. Each of them is tightly related to each otherand exerts great challenges to the scienti \ufb01c disciplines. Here, we\nplace the focus on the clustering algorithms and review a wide\nvariety of approaches appearing in the literature. These algo-rithms evolve from different research communities, aim to solvedifferent problems, and have their own pros and cons. Though\nwe have already seen many examples of successful applications\nof cluster analysis, there still remain many open problems dueto the existence of many inherent uncertain factors. These prob-lems have already attracted and will continue to attract intensive\nefforts from broad disciplines. We summarize and conclude the\nsurvey with listing some important issues and research trendsfor cluster algorithms.\n1) There is no clustering algorithm that can be univer-\nsally used to solve all problems. Usually, algorithmsare designed with certain assumptions and favor sometype of biases. In this sense, it is not accurate to say\n\u201cbest\u201din the context of clustering algorithms, although\nsome comparisons are possible. These comparisons aremostly based on some speci \ufb01c applications, under cer-\ntain conditions, and the results may become quite dif-\nferent if the conditions change.\n2) New technology has generated more complex and\nchallenging tasks, requiring more powerful clusteringalgorithms. The following properties are important to\nthe ef \ufb01ciency and effectiveness of a novel algorithm.\nI) generate arbitrary shapes of clusters rather than be\ncon\ufb01ned to some particular shape;\nII) handle large volume of data as well as high-dimen-\nsional features with acceptable time and storagecomplexities;\nIII) detect and remove possible outliers and noise;\nIV) decrease the reliance of algorithms on users-de-\npendent parameters;\nV) have the capability of dealing with newly occur-\nring data without relearning from the scratch;\nVI) be immune to the effects of order of input patterns;VII) provide some insight for the number of potential\nclusters without prior knowledge;\nVIII) show good data visualization and provide users\nwith results that can simplify further analysis;\nIX) be capable of handling both numerical and", "start_char_idx": 0, "end_char_idx": 3669, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "38da4b93-3a72-4c12-affe-d9c5bfc8f8c9": {"__data__": {"id_": "38da4b93-3a72-4c12-affe-d9c5bfc8f8c9", "embedding": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "ba9ead6a5f76f17b5dfca27a817550e36039bd7e2202ba47c9a9fdaffaf3ed18"}, "2": {"node_id": "2e081ef4-24e3-4c4d-94b6-28decf74f920", "node_type": null, "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}, "hash": "95d3e3d19e3bb2e44502abc9c3ca966e73a9d1b558d68b6ab9bb30df3b115334"}}, "hash": "ab17db18b0c1783371d15d2925b6c54f3096ec78d373c5d3e385e9357585d07f", "text": "generate arbitrary shapes of clusters rather than be\ncon\ufb01ned to some particular shape;\nII) handle large volume of data as well as high-dimen-\nsional features with acceptable time and storagecomplexities;\nIII) detect and remove possible outliers and noise;\nIV) decrease the reliance of algorithms on users-de-\npendent parameters;\nV) have the capability of dealing with newly occur-\nring data without relearning from the scratch;\nVI) be immune to the effects of order of input patterns;VII) provide some insight for the number of potential\nclusters without prior knowledge;\nVIII) show good data visualization and provide users\nwith results that can simplify further analysis;\nIX) be capable of handling both numerical and nom-\ninal data or be easily adaptable to some other data\ntype.\nOf course, some more detailed requirements for spe-ci\ufb01c applications will affect these properties.\n3) At the preprocessing and post-processing phase, fea-\nture selection/extraction (as well as standardization\nand normalization) and cluster validation are as impor-tant as the clustering algorithms. Choosing appropriateand meaningful features can greatly reduce the burden\nof subsequent designs and result evaluations re \ufb02ect\nthe degree of con \ufb01dence to which we can rely on the\ngenerated clusters. Unfortunately, both processes lackuniversal guidance. Ultimately, the tradeoff among\ndifferent criteria and methods is still dependent on the\napplications themselves.\nA\nCKNOWLEDGMENT\nThe authors would like to thank the Eisen Laboratory in Stan-\nford University for use of their CLUSTER and TreeView soft-\nware and Whitehead Institute/MIT Center for Genome Research\nfor use of their GeneCluster software. They would also like tothank S. Mulder for the part on the traveling salesman problemand also acknowledge extensive comments from the reviewers\nand the anonymous associate editor.\nR\nEFERENCES\n[1] F. Abascal and A. Valencia, \u201cClustering of proximal sequence space\nfor the identi \ufb01cation of protein families, \u201dBioinformatics , vol. 18, pp.\n908\u2013921, 2002.\n[2] C. Aggarwal and P. Yu, \u201cRede \ufb01ning clustering for high-dimensional ap-\nplications, \u201dIEEE Trans. Knowl. Data Eng. , vol. 14, no. 2, pp. 210 \u2013225,\nFeb. 2002.\n[3] R. Agrawal, J. Gehrke, D. Gunopulos, and P. Raghavan, \u201cAutomatic\nsubspace clustering of high dimensional data for data mining applica-\ntions, \u201dinProc. ACM SIGMOD Int. Conf. Management of Data , 1998,\npp. 94 \u2013105.\n[4] H. Akaike, \u201cA new look at the statistical model identi \ufb01cation, \u201dIEEE\nTrans. Autom. Control , vol. AC-19, no. 6, pp. 716 \u2013722, Dec. 1974.\n[5] A. Alizadeh et al. ,\u201cDistinct types of diffuse large B-cell Lymphoma\nidenti \ufb01ed by gene expression pro \ufb01ling,\u201dNature , vol. 403, pp. 503 \u2013511,\n2000.\n[6] U. Alon, N. Barkai, D. Notterman, K. Gish, S. Ybarra, D. Mack, and\nA. Levine, \u201cBroad patterns of gene expression revealed by clustering\nanalysis of tumor and normal colon tissues probed by oligonucleotidearrays, \u201dProc. Nat. Acad. Sci. USA , pp. 6745 \u20136750, 1999.", "start_char_idx": 2950, "end_char_idx": 5927, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84ca4505-a90d-4dd9-ae66-6b8908ca4715": {"__data__": {"id_": "84ca4505-a90d-4dd9-ae66-6b8908ca4715", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "3": {"node_id": "b6441978-722d-4618-a32f-3d44db20d1a6", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "73b7e691658d09e837770507349b4729d6cab31dce0f76f008f14964b32b7c0a"}}, "hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 673\n[7] C. Alpert and A. Kahng, \u201cMulti-way partitioning via space \ufb01lling curves\nand dynamic programming, \u201dinProc. 31st ACM/IEEE Design Automa-\ntion Conf. , 1994, pp. 652 \u2013657.\n[8] ,\u201cRecent directions in netlist partitioning: A survey, \u201dVLSI J. , vol.\n19, pp. 1 \u201381, 1995.\n[9] K. Al-Sultan, \u201cA Tabu search approach to the clustering problem, \u201dPat-\ntern Recognit. , vol. 28, no. 9, pp. 1443 \u20131451, 1995.\n[10] S. Altschul et al. ,\u201cGapped BLAST and PSI-BLAST: A new generation\nof protein database search programs, \u201dNucleic Acids Res. , vol. 25, pp.\n3389 \u20133402, 1997.\n[11] S. Altschul et al. ,\u201cBasic local alignment search tool, \u201dJ. Molec. Biol. ,\nvol. 215, pp. 403 \u2013410, 1990.\n[12] G. Anagnostopoulos and M. Georgiopoulos, \u201cHypersphere ART and\nARTMAP for unsupervised and supervised incremental learning, \u201din\nProc. IEEE-INNS-ENNS Int. Joint Conf. Neural Networks (IJCNN \u201900),\nvol. 6, Como, Italy, pp. 59 \u201364.\n[13] ,\u201cEllipsoid ART and ARTMAP for incremental unsupervised and\nsupervised learning, \u201dinProc. IEEE-INNS-ENNS Int. Joint Conf. Neural\nNetworks (IJCNN \u201901), vol. 2, Washington, DC, 2001, pp. 1221 \u20131226.\n[14] M. Anderberg, Cluster Analysis for Applications . New York: Aca-\ndemic, 1973.\n[15] G. Babu and M. Murty, \u201cA near-optimal initial seed value selection in\n/75-means algorithm using a genetic algorithm, \u201dPattern Recognit. Lett. ,\nvol. 14, no. 10, pp. 763 \u2013769, 1993.\n[16] ,\u201cClustering with evolution strategies, \u201dPattern Recognit. , vol. 27,\nno. 2, pp. 321 \u2013329, 1994.\n[17] E. Backer and A. Jain, \u201cA clustering performance measure based on\nfuzzy set decomposition, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol.\nPAMI-3, no. 1, pp. 66 \u201375, Jan. 1981.\n[18] P. Baldi and S. Brunak, Bioinformatics: The Machine Learning Ap-\nproach , 2nd ed. Cambridge, MA: MIT Press, 2001.\n[19] P. Baldi and K. Hornik, \u201cNeural networks and principal component anal-\nysis: Learning from examples without local minima, \u201dNeural Netw. , vol.\n2, pp. 53 \u201358, 1989.\n[20] P. Baldi and A. Long, \u201cA Bayesian framework for the analysis of mi-\ncroarray expression data: Regularized t-test and statistical inferences of\ngene changes, \u201dBioinformatics , vol. 17, pp. 509 \u2013519, 2001.\n[21] G. Ball and D. Hall, \u201cA clustering technique for summarizing multi-\nvariate data, \u201dBehav. Sci. , vol. 12, pp. 153 \u2013155, 1967.\n[22] S. Bandyopadhyay and U. Maulik, \u201cNonparametric genetic clustering:\nComparison of validity indices, \u201dIEEE Trans. Syst., Man, Cybern. C,\nAppl. Rev. , vol. 31, no. 1, pp. 120 \u2013125, Feb.", "start_char_idx": 0, "end_char_idx": 2525, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b6441978-722d-4618-a32f-3d44db20d1a6": {"__data__": {"id_": "b6441978-722d-4618-a32f-3d44db20d1a6", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "84ca4505-a90d-4dd9-ae66-6b8908ca4715", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "1c4b13699c836af13f7af69f7c9c3095e282a39e6f4c7ea39ed3fd5557a6c02f"}, "3": {"node_id": "1c0957ca-5dec-4d24-8864-2416c3c32dfa", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "104fed68931a393e7207381a1283083e8515a13c59e0dcd0e46bfdb98e5a0c46"}}, "hash": "73b7e691658d09e837770507349b4729d6cab31dce0f76f008f14964b32b7c0a", "text": "1989.\n[20] P. Baldi and A. Long, \u201cA Bayesian framework for the analysis of mi-\ncroarray expression data: Regularized t-test and statistical inferences of\ngene changes, \u201dBioinformatics , vol. 17, pp. 509 \u2013519, 2001.\n[21] G. Ball and D. Hall, \u201cA clustering technique for summarizing multi-\nvariate data, \u201dBehav. Sci. , vol. 12, pp. 153 \u2013155, 1967.\n[22] S. Bandyopadhyay and U. Maulik, \u201cNonparametric genetic clustering:\nComparison of validity indices, \u201dIEEE Trans. Syst., Man, Cybern. C,\nAppl. Rev. , vol. 31, no. 1, pp. 120 \u2013125, Feb. 2001.\n[23] A. Baraldi and E. Alpaydin, \u201cConstructive feedforward ART clustering\nnetworks \u2014Part I and II, \u201dIEEE Trans. Neural Netw. , vol. 13, no. 3, pp.\n645\u2013677, May 2002.\n[24] A. Baraldi and P. Blonda, \u201cA survey of fuzzy clustering algorithms for\npattern recognition \u2014Part I and II, \u201dIEEE Trans. Syst., Man, Cybern. B,\nCybern. , vol. 29, no. 6, pp. 778 \u2013801, Dec. 1999.\n[25] A. Baraldi and L. Schenato, \u201cSoft-to-hard model transition in clustering:\nA review, \u201d, Tech. Rep. TR-99-010, 1999.\n[26] D. Barbar \u00e1and P. Chen, \u201cUsing the fractal dimension to cluster datasets, \u201d\ninProc. 6th ACM SIGKDD Int. Conf. Knowledge Discovery and Data\nMining , 2000, pp. 260 \u2013264.\n[27] M. Belkin and P. Niyogi, \u201cLaplacian eigenmaps and spectral techniques\nfor embedding and clustering, \u201dinAdvances in Neural Information\nProcessing Systems , T. G. Dietterich, S. Becker, and Z. Ghahramani,\nEds. Cambridge, MA: MIT Press, 2002, vol. 14.\n[28] R. Bellman, Adaptive Control Processes: A Guided Tour . Princeton,\nNJ: Princeton Univ. Press, 1961.\n[29] A. Ben-Dor, R. Shamir, and Z. Yakhini, \u201cClustering gene expression\npatterns, \u201dJ. Comput. Biol. , vol. 6, pp. 281 \u2013297, 1999.\n[30] Y . Bengio, \u201cMarkovian models for sequential data, \u201dNeural Comput.\nSurv. , vol. 2, pp. 129 \u2013162, 1999.\n[31] A. Ben-Hur, D. Horn, H. Siegelmann, and V . Vapnik, \u201cSupport vector\nclustering, \u201dJ. Mach. Learn. Res. , vol. 2, pp. 125 \u2013137, 2001.\n[32] ,\u201cA support vector clustering method, \u201dinProc. Int. Conf. Pattern\nRecognition , vol. 2, 2000, pp. 2724 \u20132727.\n[33] P. Berkhin. (2001) Survey of clustering data mining techniques. [On-\nline]. Available: http://www.accrue.com/products/rp_cluster_review.pdf\nhttp://citeseer.nj.nec.com/berkhin02survey.html\n[34] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft, \u201cWhen is nearest\nneighbor meaningful, \u201dinProc. 7th Int. Conf. Database Theory , 1999,\npp. 217 \u2013235.\n[35] J. Bezdek, Pattern Recognition with Fuzzy Objective Function Algo-\nrithms . New York: Plenum, 1981.[36] J. Bezdek and R.", "start_char_idx": 2071, "end_char_idx": 4596, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c0957ca-5dec-4d24-8864-2416c3c32dfa": {"__data__": {"id_": "1c0957ca-5dec-4d24-8864-2416c3c32dfa", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "b6441978-722d-4618-a32f-3d44db20d1a6", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "73b7e691658d09e837770507349b4729d6cab31dce0f76f008f14964b32b7c0a"}, "3": {"node_id": "139af044-4eb0-46df-9cf7-57eac878b175", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "23744ce073b7cf82e01c31666e4973e024f40dacdd37216737a627702a659d76"}}, "hash": "104fed68931a393e7207381a1283083e8515a13c59e0dcd0e46bfdb98e5a0c46", "text": "2, 2000, pp. 2724 \u20132727.\n[33] P. Berkhin. (2001) Survey of clustering data mining techniques. [On-\nline]. Available: http://www.accrue.com/products/rp_cluster_review.pdf\nhttp://citeseer.nj.nec.com/berkhin02survey.html\n[34] K. Beyer, J. Goldstein, R. Ramakrishnan, and U. Shaft, \u201cWhen is nearest\nneighbor meaningful, \u201dinProc. 7th Int. Conf. Database Theory , 1999,\npp. 217 \u2013235.\n[35] J. Bezdek, Pattern Recognition with Fuzzy Objective Function Algo-\nrithms . New York: Plenum, 1981.[36] J. Bezdek and R. Hathaway, \u201cNumerical convergence and interpretation\nof the fuzzy /99-shells clustering algorithms, \u201dIEEE Trans. Neural Netw. ,\nvol. 3, no. 5, pp. 787 \u2013793, Sep. 1992.\n[37] J. Bezdek and N. Pal, \u201cSome new indexes of cluster validity, \u201dIEEE\nTrans. Syst., Man, Cybern. B, Cybern. , vol. 28, no. 3, pp. 301 \u2013315, Jun.\n1998.\n[38] C. Bishop, Neural Networks for Pattern Recognition . New York: Ox-\nford Univ. Press, 1995.\n[39] L. Bobrowski and J. Bezdek, \u201cc-Means clustering with the /108\nand /108\nnorms, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 21, no. 3, pp. 545 \u2013554,\nMay-Jun. 1991.\n[40] H. Bock, \u201cProbabilistic models in cluster analysis, \u201dComput. Statist.\nData Anal. , vol. 23, pp. 5 \u201328, 1996.\n[41] E. Bolten, A. Sxhliep, S. Schneckener, D. Schomburg, and R. Schrader,\n\u201cClustering protein sequences \u2014Structure prediction by transitive ho-\nmology, \u201dBioinformatics , vol. 17, pp. 935 \u2013941, 2001.\n[42] N. Boujemaa, \u201cGeneralized competitive clustering for image segmen-\ntation, \u201dinProc. 19th Int. Meeting North American Fuzzy Information\nProcessing Soc. (NAFIPS \u201900), Atlanta, GA, 2000, pp. 133 \u2013137.\n[43] P. Bradley and U. Fayyad, \u201cRe\ufb01ning initial points for /75-means clus-\ntering, \u201dinProc. 15th Int. Conf. Machine Learning , 1998, pp. 91 \u201399.\n[44] P. Bradley, U. Fayyad, and C. Reina, \u201cScaling clustering algorithms to\nlarge databases, \u201dinProc. 4th Int. Conf. Knowledge Discovery and Data\nMining (KDD \u201998), 1998, pp. 9 \u201315.\n[45] ,\u201cClustering very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[46] ,\u201cClustering very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[47] D. Brown and C. Huntley, \u201cA practical application of simulated an-\nnealing to clustering, \u201dPattern Recognit. , vol. 25, no. 4, pp. 401 \u2013412,\n1992.\n[48] C. Burges, \u201cA tutorial on support vector machines for pattern recogni-\ntion,\u201dData Mining Knowl. Discov. , vol. 2, pp. 121 \u2013167,", "start_char_idx": 4606, "end_char_idx": 7092, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "139af044-4eb0-46df-9cf7-57eac878b175": {"__data__": {"id_": "139af044-4eb0-46df-9cf7-57eac878b175", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "1c0957ca-5dec-4d24-8864-2416c3c32dfa", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "104fed68931a393e7207381a1283083e8515a13c59e0dcd0e46bfdb98e5a0c46"}, "3": {"node_id": "638d296f-3f65-4cf3-861c-0e4f1ee1cf2a", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "784c85c089b92fbd649ed2ecaccb10fd8db10264eef1a8e8c32dd25b2889431e"}}, "hash": "23744ce073b7cf82e01c31666e4973e024f40dacdd37216737a627702a659d76", "text": "very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[46] ,\u201cClustering very large databases using EM mixture models, \u201din\nProc. 15th Int. Conf. Pattern Recognition , vol. 2, 2000, pp. 76 \u201380.\n[47] D. Brown and C. Huntley, \u201cA practical application of simulated an-\nnealing to clustering, \u201dPattern Recognit. , vol. 25, no. 4, pp. 401 \u2013412,\n1992.\n[48] C. Burges, \u201cA tutorial on support vector machines for pattern recogni-\ntion,\u201dData Mining Knowl. Discov. , vol. 2, pp. 121 \u2013167, 1998.\n[49] J. Burke, D. Davison, and W. Hide, \u201cd2Cluster: A validated method for\nclustering EST and full-length cDNA sequences, \u201dGenome Res. , vol. 9,\npp. 1135 \u20131142, 1999.\n[50] I. Cadez, S. Gaffney, and P. Smyth, \u201cA general probabilistic framework\nfor clustering individuals and objects, \u201dinProc. 6th ACM SIGKDD Int.\nConf. Knowledge Discovery and Data Mining , 2000, pp. 140 \u2013149.\n[51] G. Carpenter and S. Grossberg, \u201cA massively parallel architecture for\na self-organizing neural pattern recognition machine, \u201dComput. Vis.\nGraph. Image Process. , vol. 37, pp. 54 \u2013115, 1987.\n[52] ,\u201cART2: Self-organization of stable category recognition codes for\nanalog input patterns, \u201dAppl. Opt. , vol. 26, no. 23, pp. 4919 \u20134930, 1987.\n[53] ,\u201cThe ART of adaptive pattern recognition by a self-organizing\nneural network, \u201dIEEE Computer , vol. 21, no. 3, pp. 77 \u201388, Mar. 1988.\n[54] ,\u201cART3: Hierarchical search using chemical transmitters in self-\norganizing pattern recognition architectures, \u201dNeural Netw. , vol. 3, no.\n23, pp. 129 \u2013152, 1990.\n[55] G. Carpenter, S. Grossberg, N. Markuzon, J. Reynolds, and D. Rosen,\n\u201cFuzzy ARTMAP: A neural network architecture for incremental super-\nvised learning of analog multidimensional maps, \u201dIEEE Trans. Neural\nNetw. , vol. 3, no. 5, pp. 698 \u2013713, 1992.\n[56] G. Carpenter, S. Grossberg, and J. Reynolds, \u201cARTMAP: Supervised\nreal-time learning and classi \ufb01cation of nonstationary data by a self-or-\nganizing neural network, \u201dNeural Netw. , vol. 4, no. 5, pp. 169 \u2013181, 1991.\n[57] G. Carpenter, S. Grossberg, and D. Rosen, \u201cFuzzy ART: Fast stable\nlearning and categorization of analog patterns by an adaptive resonance\nsystem, \u201dNeural Netw. , vol. 4, pp. 759 \u2013771, 1991.\n[58] G. Celeux and G. Govaert, \u201cA classi \ufb01cation EM algorithm for clustering\nand two stochastic versions, \u201dComput. Statist. Data Anal. , vol. 14, pp.\n315\u2013332, 1992.\n[59] P. Cheeseman and J. Stutz, \u201cBayesian classi \ufb01cation (AutoClass):\nTheory and results, \u201dinAdvances in Knowledge Discovery and Data\nMining , U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,\nEds. Menlo Park, CA:", "start_char_idx": 7080, "end_char_idx": 9713, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "638d296f-3f65-4cf3-861c-0e4f1ee1cf2a": {"__data__": {"id_": "638d296f-3f65-4cf3-861c-0e4f1ee1cf2a", "embedding": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "08268d8c-aed2-42b4-9253-9fb1269c87cf", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "645b083c319a8631a5bad4852cfae3f5bd99fb73844a8d1e5b30e4275c198f09"}, "2": {"node_id": "139af044-4eb0-46df-9cf7-57eac878b175", "node_type": null, "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}, "hash": "23744ce073b7cf82e01c31666e4973e024f40dacdd37216737a627702a659d76"}}, "hash": "784c85c089b92fbd649ed2ecaccb10fd8db10264eef1a8e8c32dd25b2889431e", "text": "stable\nlearning and categorization of analog patterns by an adaptive resonance\nsystem, \u201dNeural Netw. , vol. 4, pp. 759 \u2013771, 1991.\n[58] G. Celeux and G. Govaert, \u201cA classi \ufb01cation EM algorithm for clustering\nand two stochastic versions, \u201dComput. Statist. Data Anal. , vol. 14, pp.\n315\u2013332, 1992.\n[59] P. Cheeseman and J. Stutz, \u201cBayesian classi \ufb01cation (AutoClass):\nTheory and results, \u201dinAdvances in Knowledge Discovery and Data\nMining , U. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy,\nEds. Menlo Park, CA: AAAI Press, 1996, pp. 153 \u2013180.\n[60] V . Cherkassky and F. Mulier, Learning From Data: Concepts, Theory,\nand Methods . New York: Wiley, 1998.\n[61] J. Cherng and M. Lo, \u201cA hypergraph based clustering algorithm for spa-\ntial data sets, \u201dinProc. IEEE Int. Conf. Data Mining (ICDM \u201901), 2001,\npp. 83 \u201390.\n[62] J. Chiang and P. Hao, \u201cA new kernel-based fuzzy clustering approach:\nSupport vector clustering with cell growing, \u201dIEEE Trans. Fuzzy Syst. ,\nvol. 11, no. 4, pp. 518 \u2013527, Aug. 2003.\n[63] C. Chinrungrueng and C. S \u00e9quin, \u201cOptimal adaptive /75-means algo-\nrithm with dynamic adjustment of learning rate, \u201dIEEE Trans. Neural\nNetw. , vol. 6, no. 1, pp. 157 \u2013169, Jan. 1995.", "start_char_idx": 9651, "end_char_idx": 10848, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "08395673-3918-4a6a-be76-55677d1b7b72": {"__data__": {"id_": "08395673-3918-4a6a-be76-55677d1b7b72", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "3": {"node_id": "46de05ab-c6fe-46ab-bbe5-0b28c0333771", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "51cc4d501cb4b8ea6ad5a522bad17fd6a163a3bf690f49dc9aa9f697f8e6b3a1"}}, "hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737", "text": "674 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[64] S. Chu and J. Roddick, \u201cA clustering algorithm using the Tabu search\napproach with simulated annealing, \u201dinData Mining II \u2014Proceedings\nof Second International Conference on Data Mining Methods and\nDatabases , N. Ebecken and C. Brebbia, Eds, Cambridge, U.K., 2000,\npp. 515 \u2013523.\n[65] I. H. G. S. Consortium, \u201cInitial sequencing and analysis of the human\ngenome, \u201dNature , vol. 409, pp. 860 \u2013921, 2001.\n[66] J. Corchado and C. Fyfe, \u201cA comparison of kernel methods for instan-\ntiating case based reasoning systems, \u201dComput. Inf. Syst. , vol. 7, pp.\n29\u201342, 2000.\n[67] M. Cowgill, R. Harvey, and L. Watson, \u201cA genetic algorithm approach\nto cluster analysis, \u201dComput. Math. Appl. , vol. 37, pp. 99 \u2013108, 1999.\n[68] C. Cummings and D. Relman, \u201cUsing DNA microarray to study host-\nmicrobe interactions, \u201dGenomics , vol. 6, no. 5, pp. 513 \u2013525, 2000.\n[69] E. Dahlhaus, \u201cParallel algorithms for hierarchical clustering and appli-\ncations to split decomposition and parity graph recognition, \u201dJ. Algo-\nrithms , vol. 36, no. 2, pp. 205 \u2013240, 2000.\n[70] R. Dav \u00e9,\u201cAdaptive fuzzy /99-shells clustering and detection of ellipses, \u201d\nIEEE Trans. Neural Netw. , vol. 3, no. 5, pp. 643 \u2013662, Sep. 1992.\n[71] R. Dav \u00e9and R. Krishnapuram, \u201cRobust clustering methods: A uni \ufb01ed\nview, \u201dIEEE Trans. Fuzzy Syst. , vol. 5, no. 2, pp. 270 \u2013293, May 1997.\n[72] M. Delgado, A. Sk \u00e1rmeta, and H. Barber \u00e1,\u201cA Tabu search approach\nto the fuzzy clustering problem, \u201dinProc. 6th IEEE Int. Conf. Fuzzy\nSystems , vol. 1, 1997, pp. 125 \u2013130.\n[73] D. Demb \u00e9l\u00e9and P. Kastner, \u201cFuzzy /99-means method for clustering mi-\ncroarray data, \u201dBioinformatics , vol. 19, no. 8, pp. 973 \u2013980, 2003.\n[74] Handbook of Pattern Recognition and Computer Vision , C. Chen, L.\nPau, and P. Wang, Eds., World Scienti \ufb01c, Singapore, 1993, pp. 3 \u201332. R.\nDubes, \u201cCluster analysis and related issue \u201d.\n[75] R. Duda, P. Hart, and D. Stork, Pattern Classi\ufb01cation , 2nd ed. New\nYork: Wiley, 2001.\n[76] J. Dunn, \u201cA fuzzy relative of the ISODATA process and its use in de-\ntecting compact well separated clusters, \u201dJ. Cybern. , vol. 3, no. 3, pp.\n32\u201357, 1974.\n[77] B. Duran and P. Odell, Cluster Analysis: A Survey . New York:\nSpringer-Verlag, 1974.\n[78] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison, Biological Sequence\nAnalysis: Probabilistic Models of Proteins and Nucleic Acids . Cam-\nbridge, U.K.: Cambridge Univ. Press, 1998.\n[79] M. Eisen and P. Brown, \u201cDNA", "start_char_idx": 0, "end_char_idx": 2471, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46de05ab-c6fe-46ab-bbe5-0b28c0333771": {"__data__": {"id_": "46de05ab-c6fe-46ab-bbe5-0b28c0333771", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "08395673-3918-4a6a-be76-55677d1b7b72", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "08cdb91ea9f2284c8bad0e6fcc6605004475125dbf7a9a6b4fe3604dd8598737"}, "3": {"node_id": "62b80d0a-b9f9-40a5-8cba-6b6f0af10540", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "98c7bc4b0d359841f364423ed8c96fa12500469d406f12aa651cd4612d334e12"}}, "hash": "51cc4d501cb4b8ea6ad5a522bad17fd6a163a3bf690f49dc9aa9f697f8e6b3a1", "text": "2nd ed. New\nYork: Wiley, 2001.\n[76] J. Dunn, \u201cA fuzzy relative of the ISODATA process and its use in de-\ntecting compact well separated clusters, \u201dJ. Cybern. , vol. 3, no. 3, pp.\n32\u201357, 1974.\n[77] B. Duran and P. Odell, Cluster Analysis: A Survey . New York:\nSpringer-Verlag, 1974.\n[78] R. Durbin, S. Eddy, A. Krogh, and G. Mitchison, Biological Sequence\nAnalysis: Probabilistic Models of Proteins and Nucleic Acids . Cam-\nbridge, U.K.: Cambridge Univ. Press, 1998.\n[79] M. Eisen and P. Brown, \u201cDNA arrays for analysis of gene expression, \u201d\nMethods Enzymol. , vol. 303, pp. 179 \u2013205, 1999.\n[80] M. Eisen, P. Spellman, P. Brown, and D. Botstein, \u201cCluster analysis and\ndisplay of genome-wide expression patterns, \u201dinProc. Nat. Acad. Sci.\nUSA, vol. 95, 1998, pp. 14 863 \u201314 868.\n[81] Y . El-Sonbaty and M. Ismail, \u201cFuzzy clustering for symbolic data, \u201dIEEE\nTrans. Fuzzy Syst. , vol. 6, no. 2, pp. 195 \u2013204, May 1998.\n[82] T. Eltoft and R. deFigueiredo, \u201cA new neural network for cluster-de-\ntection-and-labeling, \u201dIEEE Trans. Neural Netw. , vol. 9, no. 5, pp.\n1021 \u20131035, Sep. 1998.\n[83] A. Enright and C. Ouzounis, \u201cGeneRAGE: A robust algorithm for se-\nquence clustering and domain detection, \u201dBioinformatics , vol. 16, pp.\n451\u2013457, 2000.\n[84] S. Eschrich, J. Ke, L. Hall, and D. Goldgof, \u201cFast accurate fuzzy clus-\ntering through data reduction, \u201dIEEE Trans. Fuzzy Syst. , vol. 11, no. 2,\npp. 262 \u2013270, Apr. 2003.\n[85] M. Ester, H. Kriegel, J. Sander, and X. Xu, \u201cA density-based algorithm\nfor discovering clusters in large spatial databases with noise, \u201dinProc.\n2nd Int. Conf. Knowledge Discovery and Data Mining (KDD\u201996) , 1996,\npp. 226 \u2013231.\n[86] V . Estivill-Castro and I. Lee, \u201cAMOEBA: Hierarchical clustering\nbased on spatial proximity using Delaunay diagram, \u201dinProc. 9th Int.\nSymp. Spatial Data Handling (SDH\u201999) , Beijing, China, 1999, pp.\n7a.26 \u20137a.41.\n[87] V . Estivill-Castro and J. Yang, \u201cA fast and robust general purpose clus-\ntering algorithm, \u201dinProc. 6th Paci\ufb01c Rim Int. Conf. Arti\ufb01cial Intelli-\ngence (PRICAI\u201900) , R. Mizoguchi and J. Slaney, Eds., Melbourne, Aus-\ntralia, 2000, pp. 208 \u2013218.\n[88] B. Everitt, S. Landau, and M. Leese, Cluster Analysis . London:\nArnold, 2001.\n[89] D. Fasulo, \u201cAn analysis of recent work on clustering algorithms, \u201dDept.\nComput. Sci. Eng., Univ. Washington, Seattle, WA, Tech. Rep. 01-03-02,\n1999.\n[90] M. Figueiredo and A. Jain, \u201cUnsupervised learning of", "start_char_idx": 2049, "end_char_idx": 4454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "62b80d0a-b9f9-40a5-8cba-6b6f0af10540": {"__data__": {"id_": "62b80d0a-b9f9-40a5-8cba-6b6f0af10540", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "46de05ab-c6fe-46ab-bbe5-0b28c0333771", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "51cc4d501cb4b8ea6ad5a522bad17fd6a163a3bf690f49dc9aa9f697f8e6b3a1"}, "3": {"node_id": "5f82c25e-e80d-4f46-92a8-17282ca85b55", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "423ec4410c54c58ea8cfd534543bfed8d179fd717027bea92ede3087ac9fd740"}}, "hash": "98c7bc4b0d359841f364423ed8c96fa12500469d406f12aa651cd4612d334e12", "text": "general purpose clus-\ntering algorithm, \u201dinProc. 6th Paci\ufb01c Rim Int. Conf. Arti\ufb01cial Intelli-\ngence (PRICAI\u201900) , R. Mizoguchi and J. Slaney, Eds., Melbourne, Aus-\ntralia, 2000, pp. 208 \u2013218.\n[88] B. Everitt, S. Landau, and M. Leese, Cluster Analysis . London:\nArnold, 2001.\n[89] D. Fasulo, \u201cAn analysis of recent work on clustering algorithms, \u201dDept.\nComput. Sci. Eng., Univ. Washington, Seattle, WA, Tech. Rep. 01-03-02,\n1999.\n[90] M. Figueiredo and A. Jain, \u201cUnsupervised learning of \ufb01nite mixture\nmodels, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 3, pp.\n381\u2013396, Mar. 2002.\n[91] D. Fisher, \u201cKnowledge acquisition via incremental conceptual clus-\ntering, \u201dMach. Learn. , vol. 2, pp. 139 \u2013172, 1987.[92] R. Fisher, \u201cThe use of multiple measurements in taxonomic problems, \u201d\nAnnu. Eugenics , pt. II, vol. 7, pp. 179 \u2013188, 1936.\n[93] D. Fogel, \u201cAn introduction to simulated evolutionary optimization, \u201d\nIEEE Trans. Neural Netw. , vol. 5, no. 1, pp. 3 \u201314, Jan. 1994.\n[94] E. Forgy, \u201cCluster analysis of multivariate data: Ef \ufb01ciency vs. inter-\npretability of classi \ufb01cations, \u201dBiometrics , vol. 21, pp. 768 \u2013780, 1965.\n[95] C. Fraley and A. Raftery, \u201cMCLUST: Software for model-based cluster\nanalysis, \u201dJ. Classi\ufb01cat. , vol. 16, pp. 297 \u2013306, 1999.\n[96] ,\u201cModel-Based clustering, discriminant analysis, and density esti-\nmation, \u201dJ. Amer. Statist. Assoc. , vol. 97, pp. 611 \u2013631, 2002.\n[97] J. Friedman, \u201cExploratory projection pursuit, \u201dJ. Amer. Statist. Assoc. ,\nvol. 82, pp. 249 \u2013266, 1987.\n[98] H. Frigui and R. Krishnapuram, \u201cA robust competitive clustering algo-\nrithm with applications in computer vision, \u201dIEEE Trans. Pattern Anal.\nMach. Intell. , vol. 21, no. 5, pp. 450 \u2013465, May 1999.\n[99] B. Fritzke. (1997) Some competitive learning methods. [Online]. Avail-\nable: http://www.neuroinformatik.ruhr-uni-bochum.de/ini/VDM/re-\nsearch/gsn/JavaPaper\n[100] B. Gabrys and A. Bargiela, \u201cGeneral fuzzy min-max neural network for\nclustering and classi \ufb01cation, \u201dIEEE Trans. Neural Netw. , vol. 11, no. 3,\npp. 769 \u2013783, May 2000.\n[101] V . Ganti, R. Ramakrishnan, J. Gehrke, A. Powell, and J. French, \u201cClus-\ntering large datasets in arbitrary metric spaces, \u201dinProc. 15th Int. Conf.\nData Engineering , 1999, pp. 502 \u2013511.\n[102] I. Gath and A. Geva, \u201cUnsupervised optimal fuzzy clustering, \u201dIEEE\nTrans. Pattern Anal. Mach. Intell. , vol. 11, no. 7, pp. 773 \u2013781, Jul. 1989.\n[103] GenBank Release Notes 144.0 .\n[104] A.", "start_char_idx": 4457, "end_char_idx": 6889, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5f82c25e-e80d-4f46-92a8-17282ca85b55": {"__data__": {"id_": "5f82c25e-e80d-4f46-92a8-17282ca85b55", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "62b80d0a-b9f9-40a5-8cba-6b6f0af10540", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "98c7bc4b0d359841f364423ed8c96fa12500469d406f12aa651cd4612d334e12"}, "3": {"node_id": "c066405b-531f-4635-9414-e6276de78b29", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "0950004d8e20f1619cba8cdceb886c025bed794217bed7d3e6540c811853572e"}}, "hash": "423ec4410c54c58ea8cfd534543bfed8d179fd717027bea92ede3087ac9fd740", "text": "and classi \ufb01cation, \u201dIEEE Trans. Neural Netw. , vol. 11, no. 3,\npp. 769 \u2013783, May 2000.\n[101] V . Ganti, R. Ramakrishnan, J. Gehrke, A. Powell, and J. French, \u201cClus-\ntering large datasets in arbitrary metric spaces, \u201dinProc. 15th Int. Conf.\nData Engineering , 1999, pp. 502 \u2013511.\n[102] I. Gath and A. Geva, \u201cUnsupervised optimal fuzzy clustering, \u201dIEEE\nTrans. Pattern Anal. Mach. Intell. , vol. 11, no. 7, pp. 773 \u2013781, Jul. 1989.\n[103] GenBank Release Notes 144.0 .\n[104] A. Geva, \u201cHierarchical unsupervised fuzzy clustering, \u201dIEEE Trans.\nFuzzy Syst. , vol. 7, no. 6, pp. 723 \u2013733, Dec. 1999.\n[105] D. Ghosh and A. Chinnaiyan, \u201cMixture modeling of gene expression\ndata from microarray experiments, \u201dBioinformatics , vol. 18, no. 2, pp.\n275\u2013286, 2002.\n[106] A. Ghozeil and D. Fogel, \u201cDiscovering patterns in spatial data using\nevolutionary programming, \u201dinProc. 1st Annu. Conf. Genetic Program-\nming , 1996, pp. 512 \u2013520.\n[107] M. Girolami, \u201cMercer kernel based clustering in feature space, \u201dIEEE\nTrans. Neural Netw. , vol. 13, no. 3, pp. 780 \u2013784, May 2002.\n[108] F. Glover, \u201cTabu search, part I, \u201dORSA J. Comput. , vol. 1, no. 3, pp.\n190\u2013206, 1989.\n[109] T. Golub, D. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. Mesirov,\nH. Coller, M. Loh, J. Downing, M. Caligiuri, C. Bloom \ufb01eld, and E.\nLander, \u201cMolecular classi \ufb01cation of cancer: Class discovery and class\nprediction by gene expression monitoring, \u201dScience , vol. 286, pp.\n531\u2013537, 1999.\n[110] A. Gordon, \u201cCluster validation, \u201dinData Science, Classi\ufb01cation, and Re-\nlated Methods , C. Hayashi, N. Ohsumi, K. Yajima, Y . Tanaka, H. Bock,\nand Y . Bada, Eds. New York: Springer-Verlag, 1998, pp. 22 \u201339.\n[111] ,Classi\ufb01cation , 2nd ed. London, U.K.: Chapman & Hall, 1999.\n[112] J. Gower, \u201cA general coef \ufb01cient of similarity and some of its properties, \u201d\nBiometrics , vol. 27, pp. 857 \u2013872, 1971.\n[113] S. Grossberg, \u201cAdaptive pattern recognition and universal encoding II:\nFeedback, expectation, olfaction, and illusions, \u201dBiol. Cybern. , vol. 23,\npp. 187 \u2013202, 1976.\n[114] P. Gr \u00fcnwald, P. Kontkanen, P. Myllym \u00e4ki, T. Silander, and H. Tirri,\n\u201cMinimum encoding approaches for predictive modeling, \u201dinProc. 14th\nInt. Conf. Uncertainty in AI (UAI\u201998) , 1998, pp. 183 \u2013192.\n[115] X. Guan and L. Du, \u201cDomain identi \ufb01cation by clustering sequence\nalignments, \u201dBioinformatics , vol. 14, pp. 783 \u2013788, 1998.\n[116] S. Guha, R. Rastogi, and K. Shim,", "start_char_idx": 6910, "end_char_idx": 9306, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c066405b-531f-4635-9414-e6276de78b29": {"__data__": {"id_": "c066405b-531f-4635-9414-e6276de78b29", "embedding": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "ffff7e6a4d601993a87afb5a3a4badabba6e8744ba869bc30b1b5242522002d7"}, "2": {"node_id": "5f82c25e-e80d-4f46-92a8-17282ca85b55", "node_type": null, "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}, "hash": "423ec4410c54c58ea8cfd534543bfed8d179fd717027bea92ede3087ac9fd740"}}, "hash": "0950004d8e20f1619cba8cdceb886c025bed794217bed7d3e6540c811853572e", "text": "encoding II:\nFeedback, expectation, olfaction, and illusions, \u201dBiol. Cybern. , vol. 23,\npp. 187 \u2013202, 1976.\n[114] P. Gr \u00fcnwald, P. Kontkanen, P. Myllym \u00e4ki, T. Silander, and H. Tirri,\n\u201cMinimum encoding approaches for predictive modeling, \u201dinProc. 14th\nInt. Conf. Uncertainty in AI (UAI\u201998) , 1998, pp. 183 \u2013192.\n[115] X. Guan and L. Du, \u201cDomain identi \ufb01cation by clustering sequence\nalignments, \u201dBioinformatics , vol. 14, pp. 783 \u2013788, 1998.\n[116] S. Guha, R. Rastogi, and K. Shim, \u201cCURE: An ef \ufb01cient clustering algo-\nrithm for large databases, \u201dinProc. ACM SIGMOD Int. Conf. Manage-\nment of Data , 1998, pp. 73 \u201384.\n[117] ,\u201cROCK: A robust clustering algorithm for categorical attributes, \u201d\nInf. Syst. , vol. 25, no. 5, pp. 345 \u2013366, 2000.\n[118] S. Gupata, K. Rao, and V . Bhatnagar, \u201c /75-means clustering algorithm\nfor categorical attributes, \u201dinProc. 1st Int. Conf. Data Warehousing and\nKnowledge Discovery (DaWaK\u201999) , Florence, Italy, 1999, pp. 203 \u2013208.\n[119] V . Guralnik and G. Karypis, \u201cA scalable algorithm for clustering sequen-\ntial data, \u201dinProc. 1st IEEE Int. Conf. Data Mining (ICDM\u201901) , 2001,\npp. 179 \u2013186.\n[120] D. Gus \ufb01eld,Algorithms on Strings, Trees, and Sequences: Computer\nScience and Computational Biology . Cambridge, U.K.: Cambridge\nUniv. Press, 1997.\n[121] M. Halkidi, Y . Batistakis, and M. Vazirgiannis, \u201cCluster validity\nmethods: Part I & II, \u201dSIGMOD Record , vol. 31, no. 2 \u20133, 2002.\n[122] L. Hall, I. \u00d6zyurt, and J. Bezdek, \u201cClustering with a genetically opti-\nmized approach, \u201dIEEE Trans. Evol. Comput. , vol. 3, no. 2, pp. 103 \u2013112,\n1999.", "start_char_idx": 9226, "end_char_idx": 10799, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f46c1b14-611f-4233-8b26-de370fecc8ff": {"__data__": {"id_": "f46c1b14-611f-4233-8b26-de370fecc8ff", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "3": {"node_id": "ad6b7b0e-5058-4e1a-8844-4783867f0856", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "10bbc90f10e916cc57ea273d623136d751cf9e30a4ce39413b72667a8fc70a1a"}}, "hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 675\n[123] R. Hammah and J. Curran, \u201cValidity measures for the fuzzy cluster anal-\nysis of orientations, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no.\n12, pp. 1467 \u20131472, Dec. 2000.\n[124] P. Hansen and B. Jaumard, \u201cCluster analysis and mathematical program-\nming, \u201dMath. Program. , vol. 79, pp. 191 \u2013215, 1997.\n[125] P. Hansen and N. Mladenovi \u00e6,\u201cJ-means: A new local search heuristic\nfor minimum sum of squares clustering, \u201dPattern Recognit. , vol. 34, pp.\n405\u2013413, 2001.\n[126] F. Harary, Graph Theory . Reading, MA: Addison-Wesley, 1969.\n[127] J. Hartigan, Clustering Algorithms . New York: Wiley, 1975.\n[128] E. Hartuv and R. Shamir, \u201cA clustering algorithm based on graph con-\nnectivity, \u201dInf. Process. Lett. , vol. 76, pp. 175 \u2013181, 2000.\n[129] R. Hathaway and J. Bezdek, \u201cFuzzy /99-means clustering of incomplete\ndata,\u201dIEEE Trans. Syst., Man, Cybern. , vol. 31, no. 5, pp. 735 \u2013744,\n2001.\n[130] R. Hathaway, J. Bezdek, and Y . Hu, \u201cGeneralized fuzzy /99-means clus-\ntering strategies using /76\nnorm distances, \u201dIEEE Trans. Fuzzy Syst. , vol.\n8, no. 5, pp. 576 \u2013582, Oct. 2000.\n[131] B. Hay, G. Wets, and K. Vanhoof, \u201cClustering navigation patterns on a\nwebsite using a sequence alignment method, \u201dinProc. Intelligent Tech-\nniques for Web Personalization: 17th Int. Joint Conf. Arti \ufb01cial Intelli-\ngence , vol. s.l, 2001, pp. 1 \u20136, 200.\n[132] S. Haykin, Neural Networks: A Comprehensive Foundation , 2nd\ned. Englewood Cliffs, NJ: Prentice-Hall, 1999.\n[133] Q. He, \u201cA review of clustering algorithms as applied to IR, \u201dUniv. Illinois\nat Urbana-Champaign, Tech. Rep. UIUCLIS-1999/6+IRG, 1999.\n[134] M. Healy, T. Caudell, and S. Smith, \u201cA neural architecture for pattern\nsequence veri \ufb01cation through inferencing, \u201dIEEE Trans. Neural Netw. ,\nvol. 4, no. 1, pp. 9 \u201320, Jan. 1993.\n[135] A. Hinneburg and D. Keim, \u201cAn ef \ufb01cient approach to clustering in large\nmultimedia databases with noise, \u201dinProc. 4th Int. Conf. Knowledge\nDiscovery and Data Mining (KDD \u201998), 1998, pp. 58 \u201365.\n[136] ,\u201cOptimal grid-clustering: Toward breaking the curse of dimen-\nsionality in high-dimensional clustering, \u201dinProc. 25th VLDB Conf. ,\n1999, pp. 506 \u2013517.\n[137] F. Hoeppner, \u201cFuzzy shell clustering algorithms in image processing:\nFuzzy /99-rectangular and 2-rectangular shells, \u201dIEEE Trans. Fuzzy Syst. ,\nvol. 5, no. 4, pp. 599 \u2013613, Nov. 1997.\n[138] J. Hoey, \u201cClustering contextual facial display sequences, \u201dinProc. 5th\nIEEE Int. Conf. Automatic Face and", "start_char_idx": 0, "end_char_idx": 2499, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ad6b7b0e-5058-4e1a-8844-4783867f0856": {"__data__": {"id_": "ad6b7b0e-5058-4e1a-8844-4783867f0856", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "f46c1b14-611f-4233-8b26-de370fecc8ff", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "7c5ca4996f192aad7ae021592c341686fbca4c808e89910fc96af036f2e15491"}, "3": {"node_id": "16be1f30-ceac-4b5e-9d9d-16e98e075545", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "f119e82f822f157f87167ae8585d7d6b14c9893aeeee830684d37f32c5a9c6a7"}}, "hash": "10bbc90f10e916cc57ea273d623136d751cf9e30a4ce39413b72667a8fc70a1a", "text": "and Data Mining (KDD \u201998), 1998, pp. 58 \u201365.\n[136] ,\u201cOptimal grid-clustering: Toward breaking the curse of dimen-\nsionality in high-dimensional clustering, \u201dinProc. 25th VLDB Conf. ,\n1999, pp. 506 \u2013517.\n[137] F. Hoeppner, \u201cFuzzy shell clustering algorithms in image processing:\nFuzzy /99-rectangular and 2-rectangular shells, \u201dIEEE Trans. Fuzzy Syst. ,\nvol. 5, no. 4, pp. 599 \u2013613, Nov. 1997.\n[138] J. Hoey, \u201cClustering contextual facial display sequences, \u201dinProc. 5th\nIEEE Int. Conf. Automatic Face and Gesture Recognition (FGR \u201902),\n2002, pp. 354 \u2013359.\n[139] T. Hofmann and J. Buhmann, \u201cPairwise data clustering by deterministic\nannealing, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 19, no. 1, pp.\n1\u201314, Jan. 1997.\n[140] J. Holland, Adaption in Natural and Arti \ufb01cial Systems . Ann Arbor, MI:\nUniv. Michigan Press, 1975.\n[141] F. H \u00f6ppner, F. Klawonn, and R. Kruse, Fuzzy Cluster Analysis: Methods\nfor Classi \ufb01cation, Data Analysis, and Image Recognition . New York:\nWiley, 1999.\n[142] Z. Huang, \u201cExtensions to the /75-means algorithm for clustering large\ndata sets with categorical values, \u201dData Mining Knowl. Discov. , vol. 2,\npp. 283 \u2013304, 1998.\n[143] J. Huang, M. Georgiopoulos, and G. Heileman, \u201cFuzzy ART properties, \u201d\nNeural Netw. , vol. 8, no. 2, pp. 203 \u2013213, 1995.\n[144] P. Huber, \u201cProjection pursuit, \u201dAnn. Statist. , vol. 13, no. 2, pp. 435 \u2013475,\n1985.\n[145] R. Hughey and A. Krogh, \u201cHidden Markov models for sequence anal-\nysis: Extension and analysis of the basic method, \u201dCABIOS , vol. 12, no.\n2, pp. 95 \u2013107, 1996.\n[146] M. Hung and D. Yang, \u201cAn ef \ufb01cient fuzzy /99-means clustering algo-\nrithm, \u201dinProc. IEEE Int. Conf. Data Mining , 2001, pp. 225 \u2013232.\n[147] L. Hunt and J. Jorgensen, \u201cMixture model clustering using the MUL-\nTIMIX program, \u201dAustralia and New Zealand J. Statist. , vol. 41, pp.\n153\u2013171, 1999.\n[148] J. Hwang, J. Vlontzos, and S. Kung, \u201cA systolic neural network archi-\ntecture for hidden Markov models, \u201dIEEE Trans. Acoust., Speech, Signal\nProcess. , vol. 37, no. 12, pp. 1967 \u20131979, Dec. 1989.\n[149] A. Hyv \u00e4rinen, \u201cSurvey of independent component analysis, \u201dNeural\nComput. Surv. , vol. 2, pp. 94 \u2013128, 1999.\n[150] A. Jain and R. Dubes, Algorithms for Clustering Data . Englewood\nCliffs, NJ: Prentice-Hall, 1988.\n[151] A. Jain, R. Duin, and J. Mao, \u201cStatistical pattern recognition: A review, \u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1, pp. 4 \u201337, 2000.\n[152] A. Jain, M. Murty, and P. Flynn, \u201cData clustering: A", "start_char_idx": 2063, "end_char_idx": 4535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "16be1f30-ceac-4b5e-9d9d-16e98e075545": {"__data__": {"id_": "16be1f30-ceac-4b5e-9d9d-16e98e075545", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "ad6b7b0e-5058-4e1a-8844-4783867f0856", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "10bbc90f10e916cc57ea273d623136d751cf9e30a4ce39413b72667a8fc70a1a"}, "3": {"node_id": "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "dbbf578008b2706bb3a3de23fdea9181f1077384a32406fa20b1ceed2d1227a8"}}, "hash": "f119e82f822f157f87167ae8585d7d6b14c9893aeeee830684d37f32c5a9c6a7", "text": "Signal\nProcess. , vol. 37, no. 12, pp. 1967 \u20131979, Dec. 1989.\n[149] A. Hyv \u00e4rinen, \u201cSurvey of independent component analysis, \u201dNeural\nComput. Surv. , vol. 2, pp. 94 \u2013128, 1999.\n[150] A. Jain and R. Dubes, Algorithms for Clustering Data . Englewood\nCliffs, NJ: Prentice-Hall, 1988.\n[151] A. Jain, R. Duin, and J. Mao, \u201cStatistical pattern recognition: A review, \u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 1, pp. 4 \u201337, 2000.\n[152] A. Jain, M. Murty, and P. Flynn, \u201cData clustering: A review, \u201dACM\nComput. Surv. , vol. 31, no. 3, pp. 264 \u2013323, 1999.[153] D. Jiang, C. Tang, and A. Zhang, \u201cCluster analysis for gene expression\ndata: A survey, \u201dIEEE Trans. Knowl. Data Eng. , vol. 16, no. 11, pp.\n1370 \u20131386, Nov. 2004.\n[154] C. Jutten and J. Herault, \u201cBlind separation of sources, Part I: An adaptive\nalgorithms based on neuromimetic architecture, \u201dSignal Process. , vol.\n24, no. 1, pp. 1 \u201310, 1991.\n[155] T. Kanungo, D. Mount, N. Netanyahu, C. Piatko, R. Silverman, and A.\nWu, \u201cAn ef \ufb01cient /75-means clustering algorithm: Analysis and imple-\nmentation, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 24, no. 7, pp.\n881\u2013892, Jul. 2000.\n[156] N. Karayiannis, \u201cA methodology for construction fuzzy algorithms for\nlearning vector quantization, \u201dIEEE Trans. Neural Netw. , vol. 8, no. 3,\npp. 505 \u2013518, May 1997.\n[157] N. Karayiannis, J. Bezdek, N. Pal, R. Hathaway, and P. Pai, \u201cRepairs\nto GLVQ: A new family of competitive learning schemes, \u201dIEEE Trans.\nNeural Netw. , vol. 7, no. 5, pp. 1062 \u20131071, Sep. 1996.\n[158] J. Karhunen, E. Oja, L. Wang, R. Vig \u00e1rio, and J. Joutsensalo, \u201cA class\nof neural networks for independent component analysis, \u201dIEEE Trans.\nNeural Netw. , vol. 8, no. 3, pp. 486 \u2013504, May 1997.\n[159] G. Karypis, E. Han, and V . Kumar, \u201cChameleon: Hierarchical clustering\nusing dynamic modeling, \u201dIEEE Computer , vol. 32, no. 8, pp. 68 \u201375,\nAug. 1999.\n[160] R. Kathari and D. Pitts, \u201cOn\ufb01nding the number of clusters, \u201dPattern\nRecognit. Lett. , vol. 20, pp. 405 \u2013416, 1999.\n[161] L. Kaufman and P. Rousseeuw, Finding Groups in Data: An Introduction\nto Cluster Analysis : Wiley, 1990.\n[162] W. Kent and A. Zahler, \u201cConservation, regulation, synteny, and introns\nin a large-scale C. Briggsae \u2014C. elegans genomic alignment, \u201dGenome\nRes., vol. 10, pp. 1115 \u20131125, 2000.\n[163] P. Kersten, \u201cImplementation issues in the fuzzy /99-medians clustering\nalgorithm, \u201dinProc. 6th IEEE Int. Conf. Fuzzy Systems , vol. 2, 1997,", "start_char_idx": 4555, "end_char_idx": 6992, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58": {"__data__": {"id_": "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "16be1f30-ceac-4b5e-9d9d-16e98e075545", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "f119e82f822f157f87167ae8585d7d6b14c9893aeeee830684d37f32c5a9c6a7"}, "3": {"node_id": "7425ce7f-c899-4926-b6d6-adfb50948bba", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "2a55621b781c711b211036a39b704ae9c5a00aa33228595ef875bf9812b3236e"}}, "hash": "dbbf578008b2706bb3a3de23fdea9181f1077384a32406fa20b1ceed2d1227a8", "text": "the number of clusters, \u201dPattern\nRecognit. Lett. , vol. 20, pp. 405 \u2013416, 1999.\n[161] L. Kaufman and P. Rousseeuw, Finding Groups in Data: An Introduction\nto Cluster Analysis : Wiley, 1990.\n[162] W. Kent and A. Zahler, \u201cConservation, regulation, synteny, and introns\nin a large-scale C. Briggsae \u2014C. elegans genomic alignment, \u201dGenome\nRes., vol. 10, pp. 1115 \u20131125, 2000.\n[163] P. Kersten, \u201cImplementation issues in the fuzzy /99-medians clustering\nalgorithm, \u201dinProc. 6th IEEE Int. Conf. Fuzzy Systems , vol. 2, 1997, pp.\n957\u2013962.\n[164] J. Khan, J. Wei, M. Ringn \u00e9r, L. Saal, M. Ladanyi, F. Westermann, F.\nBerthold, M. Schwab, C. Antonescu, C. Peterson, and P. Meltzer, \u201cClas-\nsi\ufb01cation and diagnostic prediction of cancers using gene expression\npro\ufb01ling and arti \ufb01cial neural networks, \u201dNature Med. , vol. 7, no. 6, pp.\n673\u2013679, 2001.\n[165] S. Kirkpatrick, C. Gelatt, and M. Vecchi, \u201cOptimization by simulated\nannealing, \u201dScience , vol. 220, no. 4598, pp. 671 \u2013680, 1983.\n[166] J. Kleinberg, \u201cAn impossibility theorem for clustering, \u201dinProc. 2002\nConf. Advances in Neural Information Processing Systems , vol. 15,\n2002, pp. 463 \u2013470.\n[167] R. Kohavi, \u201cA study of cross-validation and bootstrap for accuracy es-\ntimation and model selection, \u201dinProc. 14th Int. Joint Conf. Arti \ufb01cial\nIntelligence , 1995, pp. 338 \u2013345.\n[168] T. Kohonen, \u201cThe self-organizing map, \u201dProc. IEEE , vol. 78, no. 9, pp.\n1464 \u20131480, Sep. 1990.\n[169] ,Self-Organizing Maps , 3rd ed. New York: Springer-Verlag,\n2001.\n[170] T. Kohonen, S. Kaski, K. Lagus, J. Saloj \u00e4rvi, J. Honkela, V . Paatero, and\nA. Saarela, \u201cSelf organization of a massive document collection, \u201dIEEE\nTrans. Neural Netw. , vol. 11, no. 3, pp. 574 \u2013585, May 2000.\n[171] E. Kolatch. (2001) Clustering algorithms for spatial databases: A\nSurvey. [Online]. Available: http://citeseer.nj.nec.com/436 843.html\n[172] J. Kolen and T. Hutcheson, \u201cReducing the time complexity of the\nfuzzy /99-means algorithm, \u201dIEEE Trans. Fuzzy Syst. , vol. 10, no. 2, pp.\n263\u2013267, Apr. 2002.\n[173] K. Krishna and M. Murty, \u201cGenetic /75-means algorithm, \u201dIEEE Trans.\nSyst., Man, Cybern. B, Cybern. , vol. 29, no. 3, pp. 433 \u2013439, Jun. 1999.\n[174] R. Krishnapuram, H. Frigui, and O. Nasraoui, \u201cFuzzy and possiblistic\nshell clustering algorithms and their application to boundary detection\nand surface approximation \u2014Part I and II, \u201dIEEE Trans. Fuzzy Syst. , vol.\n3, no. 1, pp. 29 \u201360, Feb. 1995.\n[175] R. Krishnapuram and", "start_char_idx": 6966, "end_char_idx": 9406, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7425ce7f-c899-4926-b6d6-adfb50948bba": {"__data__": {"id_": "7425ce7f-c899-4926-b6d6-adfb50948bba", "embedding": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d4cde2b0-39de-4bf5-8ae7-90e4641d506e", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "4e5e8f5b5bce60a99d792aedac15b1b6e0a52c035d1ef6ba10e9b09c37c65d7b"}, "2": {"node_id": "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58", "node_type": null, "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}, "hash": "dbbf578008b2706bb3a3de23fdea9181f1077384a32406fa20b1ceed2d1227a8"}}, "hash": "2a55621b781c711b211036a39b704ae9c5a00aa33228595ef875bf9812b3236e", "text": "Syst. , vol. 10, no. 2, pp.\n263\u2013267, Apr. 2002.\n[173] K. Krishna and M. Murty, \u201cGenetic /75-means algorithm, \u201dIEEE Trans.\nSyst., Man, Cybern. B, Cybern. , vol. 29, no. 3, pp. 433 \u2013439, Jun. 1999.\n[174] R. Krishnapuram, H. Frigui, and O. Nasraoui, \u201cFuzzy and possiblistic\nshell clustering algorithms and their application to boundary detection\nand surface approximation \u2014Part I and II, \u201dIEEE Trans. Fuzzy Syst. , vol.\n3, no. 1, pp. 29 \u201360, Feb. 1995.\n[175] R. Krishnapuram and J. Keller, \u201cA possibilistic approach to clustering, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 1, no. 2, pp. 98 \u2013110, Apr. 1993.\n[176] R. Krishnapuram, O. Nasraoui, and H. Frigui, \u201cThe fuzzy /99spherical\nshells algorithm: A new approach, \u201dIEEE Trans. Neural Netw. , vol. 3,\nno. 5, pp. 663 \u2013671, Sep. 1992.\n[177] A. Krogh, M. Brown, I. Mian, K. Sj \u00f6lander, and D. Haussler, \u201cHidden\nMarkov models in computational biology: Applications to protein mod-\neling, \u201dJ. Molec. Biol. , vol. 235, pp. 1501 \u20131531, 1994.\n[178] G. Lance and W. Williams, \u201cA general theory of classi \ufb01cation sorting\nstrategies: 1. Hierarchical systems, \u201dComput. J. , vol. 9, pp. 373 \u2013380,\n1967.\n[179] M. Law and J. Kwok, \u201cRival penalized competitive learning for model-\nbased sequence clustering, \u201dinProc. 15th Int. Conf. Pattern Recognition ,\nvol. 2, 2000, pp. 195 \u2013198.", "start_char_idx": 9376, "end_char_idx": 10681, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "692ad769-f565-4860-a8fc-f16d2970f1c9": {"__data__": {"id_": "692ad769-f565-4860-a8fc-f16d2970f1c9", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77a4db53-f42f-4356-838a-929b3102cd92", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "3": {"node_id": "a0c81609-7666-4f30-b0c9-bed065dfe2e7", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "3b2299445322a2bc35e4619461230d271255fa531429be3c746c5a232375565a"}}, "hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7", "text": "676 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[180] Y . Leung, J. Zhang, and Z. Xu, \u201cClustering by scale-space \ufb01ltering, \u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 22, no. 12, pp. 1396 \u20131410,\nDec. 2000.\n[181] E. Levine and E. Domany, \u201cResampling method for unsupervised es-\ntimation of cluster validity, \u201dNeural Comput. , vol. 13, pp. 2573 \u20132593,\n2001.\n[182] C. Li and G. Biswas, \u201cTemporal pattern generation using hidden\nMarkov model based unsupervised classi \ufb01cation, \u201dinAdvances in\nIntelligent Data Analysis . ser. Lecture Notes in Computer Science, D.\nHand, K. Kok, and M. Berthold, Eds. New York: Springer-Verlag,\n1999, vol. 1642.\n[183] ,\u201cUnsupervised learning with mixed numeric and nominal data, \u201d\nIEEE Trans. Knowl. Data Eng. , vol. 14, no. 4, pp. 673 \u2013690, Jul.-Aug.\n2002.\n[184] C. Li, H. Garcia-Molina, and G. Wiederhold, \u201cClustering for approxi-\nmate similarity search in high-dimensional spaces, \u201dIEEE Trans. Knowl.\nData Eng. , vol. 14, no. 4, pp. 792 \u2013808, Jul.-Aug. 2002.\n[185] W. Li, L. Jaroszewski, and A. Godzik, \u201cClustering of highly homologous\nsequences to reduce the size of large protein databases, \u201dBioinformatics ,\nvol. 17, pp. 282 \u2013283, 2001.\n[186] A. Likas, N. Vlassis, and J. Verbeek, \u201cThe global /75-means clustering\nalgorithm, \u201dPattern Recognit. , vol. 36, no. 2, pp. 451 \u2013461, 2003.\n[187] S. Lin and B. Kernighan, \u201cAn effective heuristic algorithm for the trav-\neling salesman problem, \u201dOperat. Res. , vol. 21, pp. 498 \u2013516, 1973.\n[188] R. Lipshutz, S. Fodor, T. Gingeras, and D. Lockhart, \u201cHigh density\nsynthetic oligonucleotide arrays, \u201dNature Genetics , vol. 21, pp. 20 \u201324,\n1999.\n[189] G. Liu, Introduction to Combinatorial Mathematics . New York: Mc-\nGraw-Hill, 1968.\n[190] J. Lozano and P. Larra \u00f1aga, \u201cApplying genetic algorithms to search for\nthe best hierarchical clustering of a dataset, \u201dPattern Recognit. Lett. , vol.\n20, pp. 911 \u2013918, 1999.\n[191] J. MacQueen, \u201cSome methods for classi \ufb01cation and analysis of mul-\ntivariate observations, \u201dinProc. 5th Berkeley Symp. , vol. 1, 1967, pp.\n281\u2013297.\n[192] S. C. Madeira and A. L. Oliveira, \u201cBiclustering algorithms for biolog-\nical data analysis: A survey, \u201dIEEE/ACM Trans. Computat. Biol. Bioin-\nformatics , vol. 1, no. 1, pp. 24 \u201345, Jan. 2004.\n[193] Y . Man and I. Gath, \u201cDetection and separation of ring-shaped clusters\nusing fuzzy clustering, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 16,\nno. 8, pp. 855 \u2013861, Aug. 1994.\n[194] J. Mao and A. Jain, \u201cA self-organizing network for hyperellipsoidal", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0c81609-7666-4f30-b0c9-bed065dfe2e7": {"__data__": {"id_": "a0c81609-7666-4f30-b0c9-bed065dfe2e7", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77a4db53-f42f-4356-838a-929b3102cd92", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "692ad769-f565-4860-a8fc-f16d2970f1c9", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "90256ef4094b851f51a51f9946e52fa591ea79100587c45f03f7748ad32eaee7"}, "3": {"node_id": "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "f395e1f1c3ad5727d0874acb6e1e9069ed88f61c24dee43903ba22edfb954bd6"}}, "hash": "3b2299445322a2bc35e4619461230d271255fa531429be3c746c5a232375565a", "text": "Symp. , vol. 1, 1967, pp.\n281\u2013297.\n[192] S. C. Madeira and A. L. Oliveira, \u201cBiclustering algorithms for biolog-\nical data analysis: A survey, \u201dIEEE/ACM Trans. Computat. Biol. Bioin-\nformatics , vol. 1, no. 1, pp. 24 \u201345, Jan. 2004.\n[193] Y . Man and I. Gath, \u201cDetection and separation of ring-shaped clusters\nusing fuzzy clustering, \u201dIEEE Trans. Pattern Anal. Mach. Intell. , vol. 16,\nno. 8, pp. 855 \u2013861, Aug. 1994.\n[194] J. Mao and A. Jain, \u201cA self-organizing network for hyperellipsoidal clus-\ntering (HEC), \u201dIEEE Trans. Neural Netw. , vol. 7, no. 1, pp. 16 \u201329, Jan.\n1996.\n[195] U. Maulik and S. Bandyopadhyay, \u201cGenetic algorithm-based clustering\ntechnique, \u201dPattern Recognit. , vol. 33, pp. 1455 \u20131465, 2000.\n[196] G. McLachlan and T. Krishnan, The EM Algorithm and Exten-\nsions . New York: Wiley, 1997.\n[197] G. McLachlan and D. Peel, Finite Mixture Models . New York: Wiley,\n2000.\n[198] G. McLachlan, D. Peel, K. Basford, and P. Adams, \u201cThe EMMIX soft-\nware for the \ufb01tting of mixtures of normal and t-components, \u201dJ. Statist.\nSoftware , vol. 4, 1999.\n[199] C. Miller, J. Gurd, and A. Brass, \u201cA RAPID algorithm for sequence data-\nbase comparisons: Application to the identi \ufb01cation of vector contami-\nnation in the EMBL databases, \u201dBioinformatics , vol. 15, pp. 111 \u2013121,\n1999.\n[200] R. Miller et al. ,\u201cA comprehensive approach to clustering of expressed\nhuman gene sequence: The sequence tag alignment and consensus\nknowledge base, \u201dGenome Res. , vol. 9, pp. 1143 \u20131155, 1999.\n[201] W. Miller, \u201cComparison of genomic DNA sequences: Solved and un-\nsolved problems, \u201dBioinformatics , vol. 17, pp. 391 \u2013397, 2001.\n[202] G. Milligan and M. Cooper, \u201cAn examination of procedures for deter-\nmining the number of clusters in a data set, \u201dPsychometrika , vol. 50, pp.\n159\u2013179, 1985.\n[203] R. Mollineda and E. Vidal, \u201cA relative approach to hierarchical\nclustering, \u201dinPattern Recognition and Applications, Frontiers in\nArti\ufb01cial Intelligence and Applications , M. Torres and A. Sanfeliu,\nEds. Amsterdam, The Netherlands: IOS Press, 2000, vol. 56, pp.\n19\u201328.\n[204] B. Moore, \u201cART1 and pattern clustering, \u201dinProc. 1988 Connectionist\nModels Summer School , 1989, pp. 174 \u2013185.\n[205] S. Moore, \u201cMaking chips to probe genes, \u201dIEEE Spectr. , vol. 38, no. 3,\npp. 54 \u201360, Mar. 2001.\n[206] Y . Moreau, F. Smet, G. Thijs, K. Marchal, and B. Moor, \u201cFunctional\nbioinformatics of microarray data: From expression to regulation, \u201dProc.\nIEEE , vol. 90, no. 11, pp. 1722 \u20131743, Nov. 2002.[207] T. Morzy, M. Wojciechowski, and M. Zakrzewicz,", "start_char_idx": 2104, "end_char_idx": 4628, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793": {"__data__": {"id_": "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77a4db53-f42f-4356-838a-929b3102cd92", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "a0c81609-7666-4f30-b0c9-bed065dfe2e7", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "3b2299445322a2bc35e4619461230d271255fa531429be3c746c5a232375565a"}, "3": {"node_id": "d65554cf-768d-469d-9658-4308f214680b", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "d2b74fa8d1bb6c9fef6df46a83cb84d3eaa84979488cec071a44688b3ded98e0"}}, "hash": "f395e1f1c3ad5727d0874acb6e1e9069ed88f61c24dee43903ba22edfb954bd6", "text": "pp.\n19\u201328.\n[204] B. Moore, \u201cART1 and pattern clustering, \u201dinProc. 1988 Connectionist\nModels Summer School , 1989, pp. 174 \u2013185.\n[205] S. Moore, \u201cMaking chips to probe genes, \u201dIEEE Spectr. , vol. 38, no. 3,\npp. 54 \u201360, Mar. 2001.\n[206] Y . Moreau, F. Smet, G. Thijs, K. Marchal, and B. Moor, \u201cFunctional\nbioinformatics of microarray data: From expression to regulation, \u201dProc.\nIEEE , vol. 90, no. 11, pp. 1722 \u20131743, Nov. 2002.[207] T. Morzy, M. Wojciechowski, and M. Zakrzewicz, \u201cPattern-oriented\nhierarchical clustering, \u201dinProc. 3rd East Eur. Conf. Advances in\nDatabases and Information Systems , 1999, pp. 179 \u2013190.\n[208] S. Mulder and D. Wunsch, \u201cMillion city traveling salesman problem\nsolution by divide and conquer clustering with adaptive resonance neural\nnetworks, \u201dNeural Netw. , vol. 16, pp. 827 \u2013832, 2003.\n[209] K. M \u00fcller, S. Mika, G. R \u00e4tsch, K. Tsuda, and B. Sch \u00f6lkopf, \u201cAn intro-\nduction to kernel-based learning algorithms, \u201dIEEE Trans. Neural Netw. ,\nvol. 12, no. 2, pp. 181 \u2013201, Mar. 2001.\n[210] F. Murtagh, \u201cA survey of recent advances in hierarchical clustering al-\ngorithms, \u201dComput. J. , vol. 26, no. 4, pp. 354 \u2013359, 1983.\n[211] F. Murtagh and M. Berry, \u201cOvercoming the curse of dimensionality in\nclustering by means of the wavelet transform, \u201dComput. J. , vol. 43, no.\n2, pp. 107 \u2013120, 2000.\n[212] S. Needleman and C. Wunsch, \u201cA general method applicable to the\nsearch for similarities in the amino acid sequence of two proteins, \u201dJ.\nMolec. Biol. , vol. 48, pp. 443 \u2013453, 1970.\n[213] R. Ng and J. Han, \u201cCLARANS: A method for clustering objects for\nspatial data mining, \u201dIEEE Trans. Knowl. Data Eng. , vol. 14, no. 5, pp.\n1003 \u20131016, Sep.-Oct. 2002.\n[214] T. Oates, L. Firoiu, and P. Cohen, \u201cUsing dynamic time warping to boot-\nstrap HMM-based clustering of time series, \u201dinSequence Learning . ser.\nLNAI 1828, R. Sun and C. Giles, Eds. Berlin, Germany: Springer-Verlag, 2000, pp. 35 \u201352.\n[215] E. Oja, \u201cPrincipal components minor components, and linear neural net-\nworks, \u201dNeural Netw. , vol. 5, pp. 927 \u2013935, 1992.\n[216] J. Oliver, R. Baxter, and C. Wallace, \u201cUnsupervised learning using\nMML, \u201dinProc. 13th Int. Conf. Machine Learning (ICML \u201996), Lorenza,\nSaitta, 1996, pp. 364 \u2013372.\n[217] C. Olson, \u201cParallel algorithms for hierarchical clustering, \u201dParallel\nComput. , vol. 21, pp. 1313 \u20131325, 1995.\n[218] C. Ordonez and E. Omiecinski, \u201cEf\ufb01cient disk-based K-means clus-\ntering for relational databases, \u201dIEEE Trans. Knowl. Data Eng. , vol. 16,\nno.", "start_char_idx": 4639, "end_char_idx": 7116, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d65554cf-768d-469d-9658-4308f214680b": {"__data__": {"id_": "d65554cf-768d-469d-9658-4308f214680b", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77a4db53-f42f-4356-838a-929b3102cd92", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "f395e1f1c3ad5727d0874acb6e1e9069ed88f61c24dee43903ba22edfb954bd6"}, "3": {"node_id": "9e63e483-f011-4e84-84b0-51256bf47d20", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "df21c5021b889e556e88a6772bc7c224a879754fd40dd7ff4e1d210aeafd7ece"}}, "hash": "d2b74fa8d1bb6c9fef6df46a83cb84d3eaa84979488cec071a44688b3ded98e0", "text": "\u201dNeural Netw. , vol. 5, pp. 927 \u2013935, 1992.\n[216] J. Oliver, R. Baxter, and C. Wallace, \u201cUnsupervised learning using\nMML, \u201dinProc. 13th Int. Conf. Machine Learning (ICML \u201996), Lorenza,\nSaitta, 1996, pp. 364 \u2013372.\n[217] C. Olson, \u201cParallel algorithms for hierarchical clustering, \u201dParallel\nComput. , vol. 21, pp. 1313 \u20131325, 1995.\n[218] C. Ordonez and E. Omiecinski, \u201cEf\ufb01cient disk-based K-means clus-\ntering for relational databases, \u201dIEEE Trans. Knowl. Data Eng. , vol. 16,\nno. 8, pp. 909 \u2013921, Aug. 2004.\n[219] L. Owsley, L. Atlas, and G. Bernard, \u201cSelf-organizing feature maps\nand hidden Markov models for machine-tool monitoring, \u201dIEEE Trans.\nSignal Process. , vol. 45, no. 11, pp. 2787 \u20132798, Nov. 1997.\n[220] N. Pal and J. Bezdek, \u201cOn cluster validity for the fuzzy\n/99-means model, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 3, no. 3, pp. 370 \u2013379, Aug. 1995.\n[221] N. Pal, J. Bezdek, and E. Tsao, \u201cGeneralized clustering networks and\nKohonen \u2019s self-organizing scheme, \u201dIEEE Trans. Neural Netw. , vol. 4,\nno. 4, pp. 549 \u2013557, Jul. 1993.\n[222] G. Patan \u00e8and M. Russo, \u201cThe enhanced-LBG algorithm, \u201dNeural Netw. ,\nvol. 14, no. 9, pp. 1219 \u20131237, 2001.\n[223] ,\u201cFully automatic clustering system, \u201dIEEE Trans. Neural Netw. ,\nvol. 13, no. 6, pp. 1285 \u20131298, Nov. 2002.\n[224] W. Pearson, \u201cImproved tools for biological sequence comparison, \u201dProc.\nNat. Acad. Sci. , vol. 85, pp. 2444 \u20132448, 1988.\n[225] D. Peel and G. McLachlan, \u201cRobust mixture modeling using the t-dis-\ntribution, \u201dStatist. Comput. , vol. 10, pp. 339 \u2013348, 2000.\n[226] D. Pelleg and A. Moore, \u201cX-means: Extending /75-means with ef \ufb01cient\nestimation of the number of clusters, \u201dinProc. 17th Int. Conf. Machine\nLearning (ICML \u201900), 2000, pp. 727 \u2013734.\n[227] J. Pe \u00f1a, J. Lozano, and P. Larra \u00f1aga,\u201cAn empirical comparison of four\ninitialization methods for the /75-means algorithm, \u201dPattern Recognit.\nLett., vol. 20, pp. 1027 \u20131040, 1999.\n[228] C. Pizzuti and D. Talia, \u201cP-AutoClass: Scalable parallel clustering for\nmining large data sets, \u201dIEEE Trans. Knowl. Data Eng. , vol. 15, no. 3,\npp. 629 \u2013641, May-Jun. 2003.\n[229] L. Rabiner, \u201cA tutorial on hidden Markov models and selected applica-\ntions in speech recognition, \u201dProc. IEEE , vol. 77, no. 2, pp. 257 \u2013286,\nFeb. 1989.\n[230] Ralf-Herwig, A. Poustka, C. M \u00fcller, C. Bull, H. Lehrach, and\nJ. O \u2019Brien, \u201cLarge-scale clustering of cDNA- \ufb01ngerprinting data, \u201d\nGenome", "start_char_idx": 7111, "end_char_idx": 9488, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9e63e483-f011-4e84-84b0-51256bf47d20": {"__data__": {"id_": "9e63e483-f011-4e84-84b0-51256bf47d20", "embedding": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "77a4db53-f42f-4356-838a-929b3102cd92", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "5ef8d2a10e69c6d0ac3216fb8d42b3154dd36cf99e58325f275d17d424d5c9f0"}, "2": {"node_id": "d65554cf-768d-469d-9658-4308f214680b", "node_type": null, "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}, "hash": "d2b74fa8d1bb6c9fef6df46a83cb84d3eaa84979488cec071a44688b3ded98e0"}}, "hash": "df21c5021b889e556e88a6772bc7c224a879754fd40dd7ff4e1d210aeafd7ece", "text": "1999.\n[228] C. Pizzuti and D. Talia, \u201cP-AutoClass: Scalable parallel clustering for\nmining large data sets, \u201dIEEE Trans. Knowl. Data Eng. , vol. 15, no. 3,\npp. 629 \u2013641, May-Jun. 2003.\n[229] L. Rabiner, \u201cA tutorial on hidden Markov models and selected applica-\ntions in speech recognition, \u201dProc. IEEE , vol. 77, no. 2, pp. 257 \u2013286,\nFeb. 1989.\n[230] Ralf-Herwig, A. Poustka, C. M \u00fcller, C. Bull, H. Lehrach, and\nJ. O \u2019Brien, \u201cLarge-scale clustering of cDNA- \ufb01ngerprinting data, \u201d\nGenome Res. , pp. 1093 \u20131105, 1999.\n[231] A. Rauber, J. Paralic, and E. Pampalk, \u201cEmpirical evaluation of clus-\ntering algorithms, \u201dJ. Inf. Org. Sci. , vol. 24, no. 2, pp. 195 \u2013209, 2000.\n[232] S. Ridella, S. Rovetta, and R. Zunino, \u201cPlastic algorithm for adaptive\nvector quantization, \u201dNeural Comput. Appl. , vol. 7, pp. 37 \u201351, 1998.\n[233] J. Rissanen, \u201cFisher information and stochastic complexity, \u201dIEEE\nTrans. Inf. Theory , vol. 42, no. 1, pp. 40 \u201347, Jan. 1996.\n[234] K. Rose, \u201cDeterministic annealing for clustering, compression, classi \ufb01-\ncation, regression, and related optimization problems, \u201dProc. IEEE , vol.\n86, no. 11, pp. 2210 \u20132239, Nov. 1998.", "start_char_idx": 9413, "end_char_idx": 10553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c312ba61-6eb2-4b47-a32e-baed6f6c350e": {"__data__": {"id_": "c312ba61-6eb2-4b47-a32e-baed6f6c350e", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86e38ea7-a328-4b53-b1a6-21a614131f54", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "3": {"node_id": "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "b179875b5ed80bc0742443c66e62a2250ec105ec2fceefa5c57b6333f0f8149f"}}, "hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7", "text": "XU AND WUNSCH II: SURVEY OF CLUSTERING ALGORITHMS 677\n[235] S. Roweis and L. Saul, \u201cNonlinear dimensionality reduction by locally\nlinear embedding, \u201dScience , vol. 290, no. 5500, pp. 2323 \u20132326, 2000.\n[236] D. Sankoff and J. Kruskal, Time Warps, String Edits, and Macro-\nmolecules: The Theory and Practice of Sequence Comparison . Stan-\nford, CA: CSLI Publications, 1999.\n[237] O. Sasson, N. Linial, and M. Linial, \u201cThe metric space of pro-\nteins \u2014Comparative study of clustering algorithms, \u201dBioinformatics ,\nvol. 18, pp. s14 \u2013s21, 2002.\n[238] U. Scherf, D. Ross, M. Waltham, L. Smith, J. Lee, L. Tanabe, K. Kohn,\nW. Reinhold, T. Myers, D. Andrews, D. Scudiero, M. Eisen, E. Sausville,\nY . Pommier, D. Botstein, P. Brown, and J. Weinstein, \u201cA gene expression\ndatabase for the molecular pharmacology of cancer, \u201dNature Genetics ,\nvol. 24, no. 3, pp. 236 \u2013244, 2000.\n[239] P. Scheunders, \u201cA comparison of clustering algorithms applied to color\nimage quantization, \u201dPattern Recognit. Lett. , vol. 18, pp. 1379 \u20131384,\n1997.\n[240] B. Sch \u00f6lkopf and A. Smola, Learning with Kernels: Support Vector Ma-\nchines, Regularization, Optimization, and Beyond . Cambridge, MA:\nMIT Press, 2002.\n[241] B. Sch \u00f6lkopf, A. Smola, and K. M \u00fcller,\u201cNonlinear component analysis\nas a kernel eigenvalue problem, \u201dNeural Computat. , vol. 10, no. 5, pp.\n1299 \u20131319, 1998.\n[242] G. Schwarz, \u201cEstimating the dimension of a model, \u201dAnn. Statist. , vol.\n6, no. 2, pp. 461 \u2013464, 1978.\n[243] G. Scott, D. Clark, and T. Pham, \u201cA genetic clustering algorithm guided\nby a descent algorithm, \u201dinProc. Congr. Evolutionary Computation , vol.\n2, Piscataway, NJ, 2001, pp. 734 \u2013740.\n[244] P. Sebastiani, M. Ramoni, and P. Cohen, \u201cSequence learning via\nBayesian clustering by dynamics, \u201dinSequence Learning . ser. LNAI\n1828, R. Sun and C. Giles, Eds. Berlin, Germany: Springer-Verlag,\n2000, pp. 11 \u201334.\n[245] S. Selim and K. Alsultan, \u201cA simulated annealing algorithm for the clus-\ntering problems, \u201dPattern Recognit. , vol. 24, no. 10, pp. 1003 \u20131008,\n1991.\n[246] R. Shamir and R. Sharan, \u201cAlgorithmic approaches to clustering gene\nexpression data, \u201dinCurrent Topics in Computational Molecular Bi-\nology , T. Jiang, T. Smith, Y . Xu, and M. Zhang, Eds. Cambridge, MA:\nMIT Press, 2002, pp. 269 \u2013300.\n[247] R. Sharan and R. Shamir, \u201cCLICK: A clustering algorithm with appli-\ncations to gene expression analysis, \u201dinProc. 8th Int. Conf. Intelligent\nSystems for Molecular Biology , 2000, pp. 307 \u2013316.\n[248] G. Sheikholeslami, S. Chatterjee, and A. Zhang, \u201cWaveCluster: A multi-\nresolution clustering approach", "start_char_idx": 0, "end_char_idx": 2565, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac": {"__data__": {"id_": "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86e38ea7-a328-4b53-b1a6-21a614131f54", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "c312ba61-6eb2-4b47-a32e-baed6f6c350e", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "1a7a3f233f3afbf7829efd7136f3c90c7a5059fcbbc51e2354a6d1a131ca5ae7"}, "3": {"node_id": "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "156fbdd6d1a06ce6d81a7b1735681584f6fdf0d4c82e0a951b72af2c10ef05ff"}}, "hash": "b179875b5ed80bc0742443c66e62a2250ec105ec2fceefa5c57b6333f0f8149f", "text": "R. Shamir and R. Sharan, \u201cAlgorithmic approaches to clustering gene\nexpression data, \u201dinCurrent Topics in Computational Molecular Bi-\nology , T. Jiang, T. Smith, Y . Xu, and M. Zhang, Eds. Cambridge, MA:\nMIT Press, 2002, pp. 269 \u2013300.\n[247] R. Sharan and R. Shamir, \u201cCLICK: A clustering algorithm with appli-\ncations to gene expression analysis, \u201dinProc. 8th Int. Conf. Intelligent\nSystems for Molecular Biology , 2000, pp. 307 \u2013316.\n[248] G. Sheikholeslami, S. Chatterjee, and A. Zhang, \u201cWaveCluster: A multi-\nresolution clustering approach for very large spatial databases, \u201dinProc.\n24th VLDB Conf. , 1998, pp. 428 \u2013439.\n[249] P. Simpson, \u201cFuzzy min-max neural networks \u2014Part 2: Clustering, \u201d\nIEEE Trans. Fuzzy Syst. , vol. 1, no. 1, pp. 32 \u201345, Feb. 1993.\n[250] Handbook of Pattern Recognition and Computer Vision , C. Chen, L.\nPau, and P. Wang, Eds., World Scienti \ufb01c, Singapore, 1993, pp. 61 \u2013124.\nJ. Sklansky and W. Siedlecki, \u201cLarge-scale feature selection \u201d.\n[251] T. Smith and M. Waterman, \u201cNew stratigraphic correlation techniques, \u201d\nJ. Geology , vol. 88, pp. 451 \u2013457, 1980.\n[252] P. Smyth, \u201cClustering using Monte Carlo cross-validation, \u201dinProc. 2nd\nInt. Conf. Knowledge Discovery and Data Mining , 1996, pp. 126 \u2013133.\n[253] ,\u201cClustering sequences with hidden Markov models, \u201dinAdvances\nin Neural Information Processing , M. Mozer, M. Jordan, and T. Petsche,\nEds. Cambridge, MA: MIT Press, 1997, vol. 9, pp. 648 \u2013654.\n[254] ,\u201cModel selection for probabilistic clustering using cross validated\nlikelihood, \u201dStatist. Comput. , vol. 10, pp. 63 \u201372, 1998.\n[255] ,\u201cProbabilistic model-based clustering of multivariate and sequen-\ntial data, \u201dinProc. 7th Int. Workshop on Arti \ufb01cial Intelligence and Sta-\ntistics , 1999, pp. 299 \u2013304.\n[256] P. Sneath, \u201cThe application of computers to taxonomy, \u201dJ. Gen. Micro-\nbiol., vol. 17, pp. 201 \u2013226, 1957.\n[257] P. Somervuo and T. Kohonen, \u201cClustering and visualization of large pro-\ntein sequence databases by means of an extension of the self-organizingmap, \u201dinLNAI 1967 , 2000, pp. 76 \u201385.\n[258] T. Sorensen, \u201cA method of establishing groups of equal amplitude in\nplant sociology based on similarity of species content and its application\nto analyzes of the vegetation on Danish commons, \u201dBiologiske Skrifter ,\nvol. 5, pp. 1 \u201334, 1948.\n[259] H. Sp \u00e4th,Cluster Analysis Algorithms . Chichester, U.K.: Ellis Hor-\nwood, 1980.\n[260] P. Spellman, G. Sherlock, M. Ma, V . Iyer, K. Anders, M. Eisen, P. Brown,\nD. Botstein, and B. Futcher, \u201cComprehensive identi \ufb01cation of cell cycle-\nregulated genes of the Yeast Saccharomyces Cerevisiae by microarray\nhybridization, \u201dMol. Biol. Cell , vol. 9, pp. 3273 \u20133297,", "start_char_idx": 2100, "end_char_idx": 4753, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6": {"__data__": {"id_": "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86e38ea7-a328-4b53-b1a6-21a614131f54", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "b179875b5ed80bc0742443c66e62a2250ec105ec2fceefa5c57b6333f0f8149f"}, "3": {"node_id": "a234f387-581a-47f1-83af-8a98dfd38cec", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "dbbb55f44b80b62a4b3b91e82e09a54c406f897d8df41e387b85b789460e6a28"}}, "hash": "156fbdd6d1a06ce6d81a7b1735681584f6fdf0d4c82e0a951b72af2c10ef05ff", "text": "in\nplant sociology based on similarity of species content and its application\nto analyzes of the vegetation on Danish commons, \u201dBiologiske Skrifter ,\nvol. 5, pp. 1 \u201334, 1948.\n[259] H. Sp \u00e4th,Cluster Analysis Algorithms . Chichester, U.K.: Ellis Hor-\nwood, 1980.\n[260] P. Spellman, G. Sherlock, M. Ma, V . Iyer, K. Anders, M. Eisen, P. Brown,\nD. Botstein, and B. Futcher, \u201cComprehensive identi \ufb01cation of cell cycle-\nregulated genes of the Yeast Saccharomyces Cerevisiae by microarray\nhybridization, \u201dMol. Biol. Cell , vol. 9, pp. 3273 \u20133297, 1998.\n[261] \u201cTech. Rep. 00 \u2013034,\u201dUniv. Minnesota, Minneapolis, 2000.[262] K. Stoffel and A. Belkoniene, \u201cParallel\n/75-means clustering for\nlarge data sets, \u201dinProc. EuroPar \u201999 Parallel Processing , 1999, pp.\n1451 \u20131454.\n[263] M. Su and H. Chang, \u201cFast self-organizing feature map algorithm, \u201dIEEE\nTrans. Neural Netw. , vol. 11, no. 3, pp. 721 \u2013733, May 2000.\n[264] M. Su and C. Chou, \u201cA modi \ufb01ed version of the /75-means algorithm with\na distance based on cluster symmetry, \u201dIEEE Trans. Pattern Anal. Mach.\nIntell. , vol. 23, no. 6, pp. 674 \u2013680, Jun. 2001.\n[265] R. Sun and C. Giles, \u201cSequence learning: Paradigms, algorithms, and\napplications, \u201dinLNAI 1828 , . Berlin, Germany, 2000.\n[266] C. Sung and H. Jin, \u201cA Tabu-search-based heuristic for clustering, \u201dPat-\ntern Recognit. , vol. 33, pp. 849 \u2013858, 2000.\n[267] SWISS-PROT Protein Knowledgebase Release 45.0 Statistics .\n[268] P. Tamayo, D. Slonim, J. Mesirov, Q. Zhu, S. Kitareewan, E. Dmitro-\nvsky, E. Lander, and T. Golub, \u201cInterpreting patterns of gene expression\nwith self-organizing maps: Methods and application to hematopoietic\ndifferentiation, \u201dProc. Nat. Acad. Sci. , pp. 2907 \u20132912, 1999.\n[269] S. Tavazoie, J. Hughes, M. Campbell, R. Cho, and G. Church, \u201cSys-\ntematic determination of genetic network architecture, \u201dNature Genetics ,\nvol. 22, pp. 281 \u2013285, 1999.\n[270] J. Tenenbaum, V . Silva, and J. Langford, \u201cA global geometric frame-\nwork for nonlinear dimensionality reduction, \u201dScience , vol. 290, pp.\n2319 \u20132323, 2000.\n[271] R. Tibshirani, T. Hastie, M. Eisen, D. Ross, D. Botstein, and P. Brown,\n\u201cClustering methods for the analysis of DNA microarray data, \u201dDept.\nStatist., Stanford Univ., Stanford, CA, Tech. Rep..\n[272] R. Tibshirani and K. Knight, \u201cThe covariance in \ufb02ation criterion for\nadaptive model selection, \u201dJ. Roy. Statist. Soc. B , vol. 61, pp. 529 \u2013546,\n1999.\n[273] L. Tseng and S. Yang, \u201cA genetic approach to the automatic clustering\nproblem, \u201dPattern Recognit. , vol. 34, pp. 415 \u2013424, 2001.\n[274] V .", "start_char_idx": 4757, "end_char_idx": 7292, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a234f387-581a-47f1-83af-8a98dfd38cec": {"__data__": {"id_": "a234f387-581a-47f1-83af-8a98dfd38cec", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86e38ea7-a328-4b53-b1a6-21a614131f54", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "156fbdd6d1a06ce6d81a7b1735681584f6fdf0d4c82e0a951b72af2c10ef05ff"}, "3": {"node_id": "2b510519-2c5e-488b-aaa4-fbf7a9789991", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "f30e7a4c0bbee8baabc5cdc98fe6ecc7b104c97cb2f1593b8e304837ac6699cb"}}, "hash": "dbbb55f44b80b62a4b3b91e82e09a54c406f897d8df41e387b85b789460e6a28", "text": "2000.\n[271] R. Tibshirani, T. Hastie, M. Eisen, D. Ross, D. Botstein, and P. Brown,\n\u201cClustering methods for the analysis of DNA microarray data, \u201dDept.\nStatist., Stanford Univ., Stanford, CA, Tech. Rep..\n[272] R. Tibshirani and K. Knight, \u201cThe covariance in \ufb02ation criterion for\nadaptive model selection, \u201dJ. Roy. Statist. Soc. B , vol. 61, pp. 529 \u2013546,\n1999.\n[273] L. Tseng and S. Yang, \u201cA genetic approach to the automatic clustering\nproblem, \u201dPattern Recognit. , vol. 34, pp. 415 \u2013424, 2001.\n[274] V . Vapnik, Statistical Learning Theory . New York: Wiley, 1998.\n[275] J. Venter et al. ,\u201cThe sequence of the human genome, \u201dScience , vol. 291,\npp. 1304 \u20131351, 2001.\n[276] J. Vesanto and E. Alhoniemi, \u201cClustering of the self-organizing map, \u201d\nIEEE Trans. Neural Netw. , vol. 11, no. 3, pp. 586 \u2013600, May 2000.\n[277] K. Wagstaff, S. Rogers, and S. Schroedl, \u201cConstrained /75-means clus-\ntering with background knowledge, \u201dinProc. 8th Int. Conf. Machine\nLearning , 2001, pp. 577 \u2013584.\n[278] C. Wallace and D. Dowe, \u201cIntrinsic classi \ufb01cation by MML \u2014The SNOB\nprogram, \u201dinProc. 7th Australian Joint Conf. Arti \ufb01cial Intelligence ,\n1994, pp. 37 \u201344.\n[279] H. Wang, W. Wang, J. Yang, and P. Yu, \u201cClustering by pattern similarity\nin large data sets, \u201dinProc. ACM SIGMOD Int. Conf. Management of\nData , 2002, pp. 394 \u2013405.\n[280] C. Wei, Y . Lee, and C. Hsu, \u201cEmpirical comparison of fast clustering\nalgorithms for large data sets, \u201dinProc. 33rd Hawaii Int. Conf. System\nSciences , Maui, HI, 2000, pp. 1 \u201310.\n[281] J. Williamson, \u201cGaussian ARTMAP: A neural network for fast incre-\nmental learning of noisy multidimensional maps, \u201dNeural Netw. , vol. 9,\nno. 5, pp. 881 \u2013897, 1996.\n[282] M. Windham and A. Culter, \u201cInformation ratios for validating mixture\nanalysis, \u201dJ. Amer. Statist. Assoc. , vol. 87, pp. 1188 \u20131192, 1992.\n[283] S. Wu, A. W.-C. Liew, H. Yan, and M. Yang, \u201cCluster analysis of\ngene expression data based on self-splitting and merging competitive\nlearning, \u201dIEEE Trans. Inf. Technol. Biomed. , vol. 8, no. 1, pp. 5 \u201315,\nJan. 2004.\n[284] D. Wunsch, \u201cAn optoelectronic learning machine: Invention, experi-\nmentation, analysis of \ufb01rst hardware implementation of the ART1 neural\nnetwork, \u201dPh.D. dissertation, Univ. Washington, Seattle, WA, 1991.\n[285] D. Wunsch, T. Caudell, C. Capps, R. Marks, and R. Falk, \u201cAn optoelec-\ntronic implementation of the adaptive resonance neural network, \u201dIEEE\nTrans. Neural Netw. , vol. 4, no. 4, pp. 673 \u2013684, Jul. 1993.\n[286] Y . Xiong and D. Yeung, \u201cMixtures of ARMA models for model-based\ntime series", "start_char_idx": 7326, "end_char_idx": 9868, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b510519-2c5e-488b-aaa4-fbf7a9789991": {"__data__": {"id_": "2b510519-2c5e-488b-aaa4-fbf7a9789991", "embedding": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86e38ea7-a328-4b53-b1a6-21a614131f54", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "7966c48e559e893eacc4a78d8fbe912bc054a510e63c47d70eeb9fe8ef4191a8"}, "2": {"node_id": "a234f387-581a-47f1-83af-8a98dfd38cec", "node_type": null, "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}, "hash": "dbbb55f44b80b62a4b3b91e82e09a54c406f897d8df41e387b85b789460e6a28"}}, "hash": "f30e7a4c0bbee8baabc5cdc98fe6ecc7b104c97cb2f1593b8e304837ac6699cb", "text": "1, pp. 5 \u201315,\nJan. 2004.\n[284] D. Wunsch, \u201cAn optoelectronic learning machine: Invention, experi-\nmentation, analysis of \ufb01rst hardware implementation of the ART1 neural\nnetwork, \u201dPh.D. dissertation, Univ. Washington, Seattle, WA, 1991.\n[285] D. Wunsch, T. Caudell, C. Capps, R. Marks, and R. Falk, \u201cAn optoelec-\ntronic implementation of the adaptive resonance neural network, \u201dIEEE\nTrans. Neural Netw. , vol. 4, no. 4, pp. 673 \u2013684, Jul. 1993.\n[286] Y . Xiong and D. Yeung, \u201cMixtures of ARMA models for model-based\ntime series clustering, \u201dinProc. IEEE Int. Conf. Data Mining , 2002, pp.\n717\u2013720.\n[287] R. Xu, G. Anagnostopoulos, and D. Wunsch, \u201cTissue classi \ufb01cation\nthrough analysis of gene expression data using a new family of ARTarchitectures, \u201dinProc. Int. Joint Conf. Neural Networks (IJCNN \u201902),\nvol. 1, 2002, pp. 300 \u2013304.\n[288] Y . Xu, V . Olman, and D. Xu, \u201cClustering gene expression data using\ngraph-theoretic approach: An application of minimum spanning trees, \u201d\nBioinformatics , vol. 18, no. 4, pp. 536 \u2013545, 2002.", "start_char_idx": 9772, "end_char_idx": 10801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6ba7ed5-2ab8-4068-9576-3c6baa96aed9": {"__data__": {"id_": "c6ba7ed5-2ab8-4068-9576-3c6baa96aed9", "embedding": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d15ab249-9163-4186-87e4-823febc0e4db", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "3": {"node_id": "ef521592-a2e7-4340-ab49-87747949fafe", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "e7967e36e953bce7e0f2da22a256c7b60de17d354d0a36184eee40b73f3fb6af"}}, "hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac", "text": "678 IEEE TRANSACTIONS ON NEURAL NETWORKS, VOL. 16, NO. 3, MAY 2005\n[289] R. Yager, \u201cIntelligent control of the hierarchical agglomerative clus-\ntering process, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 30, no. 6, pp.\n835\u2013845, 2000.\n[290] R. Yager and D. Filev, \u201cApproximate clustering via the moun-\ntain method, \u201dIEEE Trans. Syst., Man, Cybern. , vol. 24, no. 8, pp.\n1279 \u20131284, 1994.\n[291] K. Yeung, D. Haynor, and W. Ruzzo, \u201cValidating clustering for gene\nexpression data, \u201dBioinformatics , vol. 17, no. 4, pp. 309 \u2013318, 2001.\n[292] F. Young and R. Hamer, Multidimensional Scaling: History, Theory, and\nApplications . Hillsdale, NJ: Lawrence Erlbaum, 1987.\n[293] L. Zadeh, \u201cFuzzy sets, \u201dInf. Control , vol. 8, pp. 338 \u2013353, 1965.\n[294] J. Zhang and Y . Leung, \u201cImproved possibilistic C-means clustering al-\ngorithms, \u201dIEEE Trans. Fuzzy Syst. , vol. 12, no. 2, pp. 209 \u2013217, Apr.\n2004.\n[295] T. Zhang, R. Ramakrishnan, and M. Livny, \u201cBIRCH: An ef \ufb01cient data\nclustering method for very large databases, \u201dinProc. ACM SIGMOD\nConf. Management of Data , 1996, pp. 103 \u2013114.\n[296] Y . Zhang and Z. Liu, \u201cSelf-splitting competitive learning: A new on-line\nclustering paradigm, \u201dIEEE Trans. Neural Networks , vol. 13, no. 2, pp.\n369\u2013380, Mar. 2002.\n[297] X. Zhuang, Y . Huang, K. Palaniappan, and Y . Zhao, \u201cGaussian mixture\ndensity modeling, decomposition, and applications, \u201dIEEE Trans. Image\nProcess. , vol. 5, no. 9, pp. 1293 \u20131302, Sep. 1996.\nRui Xu (S\u201900) received the B.E. degree in electrical\nengineering from Huazhong University of Science\nand Technology, Wuhan, Hubei, China, in 1997,\nand the M.E. degree in electrical engineering from\nSichuan University, Chengdu, Sichuan, in 2000.\nHe is currently pursuing the Ph.D. degree in the\nDepartment of Electrical and Computer Engineering,\nUniversity of Missouri-Rolla.\nHis research interests include machine learning,\nneural networks, pattern classi \ufb01cation and clustering,\nand bioinformatics.\nMr. Xu is a Student Member of the IEEE Computational Intelligence Society,\nEngineering in Medicine and Biology Society, and the International Society for\nComputational Biology.\nDonald C. Wunsch II (S\u201987\u2013M\u201992\u2013SM\u201994\u2013F\u201905)\nreceived the B.S. degree in applied mathematics\nfrom the University of New Mexico, Albuquerque,\nand the M.S. degree in applied mathematics and\nthe Ph.D. degree in electrical engineering from the\nUniversity of Washington, Seattle.\nHeis the Mary K. Finley Missouri Distinguished\nProfessor of Computer Engineering, University\nof Missouri-Rolla, where he has been since 1999.\nHis prior positions were Associate Professor and\nDirector of the Applied Computational Intelligence\nLaboratory, Texas Tech University, Lubbock; Senior Principal Scientist,\nBoeing; Consultant, Rockwell International; and Technician, International\nLaser Systems. He has well over 200 publications, and has attracted over $5\nmillion in research funding. He has produced eight", "start_char_idx": 0, "end_char_idx": 2904, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ef521592-a2e7-4340-ab49-87747949fafe": {"__data__": {"id_": "ef521592-a2e7-4340-ab49-87747949fafe", "embedding": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d15ab249-9163-4186-87e4-823febc0e4db", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "2245995ace73ac675cd548934ee82f8ac133f761b6952804c8d687c7f57b95f7"}, "2": {"node_id": "c6ba7ed5-2ab8-4068-9576-3c6baa96aed9", "node_type": null, "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}, "hash": "19a33312d85ee8bb142050b252a37a5cfa7173ca44b839a71cb484afead4aeac"}}, "hash": "e7967e36e953bce7e0f2da22a256c7b60de17d354d0a36184eee40b73f3fb6af", "text": "the B.S. degree in applied mathematics\nfrom the University of New Mexico, Albuquerque,\nand the M.S. degree in applied mathematics and\nthe Ph.D. degree in electrical engineering from the\nUniversity of Washington, Seattle.\nHeis the Mary K. Finley Missouri Distinguished\nProfessor of Computer Engineering, University\nof Missouri-Rolla, where he has been since 1999.\nHis prior positions were Associate Professor and\nDirector of the Applied Computational Intelligence\nLaboratory, Texas Tech University, Lubbock; Senior Principal Scientist,\nBoeing; Consultant, Rockwell International; and Technician, International\nLaser Systems. He has well over 200 publications, and has attracted over $5\nmillion in research funding. He has produced eight Ph.D. recipients \u2014four in\nelectrical engineering, three in computer engineering, and one in computer\nscience.\nDr. Wunsch has received the Halliburton Award for Excellence in Teaching\nand Research, and the National Science Foundation CAREER Award. He served\nas a V oting Member of the IEEE Neural Networks Council, Technical Program\nCo-Chair for IJCNN \u201902, General Chair for IJCNN \u201903, International Neural Net-\nworks Society Board of Governors Member, and is now President of the Inter-national Neural Networks Society.", "start_char_idx": 2169, "end_char_idx": 3424, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "81421553-3b86-42b5-bf2e-1b7138f2b853": {"__data__": {"id_": "81421553-3b86-42b5-bf2e-1b7138f2b853", "embedding": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d904018d-af49-48f7-854a-71954d7293ed", "node_type": null, "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098"}}, "hash": "2c40d7e43315a84d3c460e89b1ad33c7d76ca9d79b1493935778f4345a3d2098", "text": "Journalof Machine LearningResearch 3(2003)1157-1182 Submitted 11/02;Published3/03\nAnIntroduction to Variableand Feature Selection\nIsabelleGuyon ISABELLE @CLOPINET .COM\nClopinet\n955 Creston Road\nBerkeley, CA 94708-1501, USA\nAndr\u00b4eElisseeff ANDRE@TUEBINGEN .MPG.DE\nEmpirical Inference for Machine Learning and Perception De partment\nMax Planck InstituteforBiological Cybernetics\nSpemannstrasse 38\n72076 T\u00a8ubingen, Germany\nEditor:Leslie Pack Kaelbling\nAbstract\nVariable and feature selection have become the focus of much research in areas of application for\nwhich datasets with tens or hundreds of thousands of variabl es are available. These areas include\ntextprocessingofinternetdocuments,geneexpressionarr ayanalysis,andcombinatorialchemistry.\nThe objective of variable selection is three-fold: improvi ng the prediction performance of the pre-\ndictors,providingfasterandmorecost-effectivepredict ors,andprovidingabetterunderstandingof\nthe underlying process that generated the data. The contrib utions of this special issue cover a wide\nrange of aspects of such problems: providing a better de\ufb01nit ion of the objective function, feature\nconstruction, feature ranking, multivariate feature sele ction, ef\ufb01cient search methods, and feature\nvalidity assessment methods.\nKeywords: Variable selection, feature selection, space dimensional ity reduction, pattern discov-\nery, \ufb01lters, wrappers, clustering, information theory, su pport vector machines, model selection,\nstatistical testing, bioinformatics, computational biol ogy, gene expression, microarray, genomics,\nproteomics, QSAR, text classi\ufb01cation, information retrie val.\n1 Introduction\nAs of 1997, when a special issue on relevance including several pape rs on variable and feature\nselection was published (Blum and Langley, 1997, Kohavi and John, 19 97), few domains explored\nused more than 40 features. The situation has changed considerably in the past few years and, in\nthis special issue, most papers explore domains with hundreds to tens of tho usands of variables or\nfeatures:1Newtechniquesareproposedtoaddressthesechallengingtasksinvolvin gmanyirrelevant\nandredundant variablesandoftencomparably fewtrainingexamples.\nTwoexamplesaretypicalofthenewapplicationdomainsandserveusasillustr ationthroughout\nthis introduction. One is gene selection from microarray data and the other is te xt categorization.\nIn the gene selection problem, the variables are gene expression coef\ufb01c ients corresponding to the\n1. We call \u201cvariable\u201d the \u201craw\u201d input variables and \u201cfeatures\u201d variable s constructed for the input variables. We use\nwithoutdistinctiontheterms\u201cvariable\u201dand\u201cfeature\u201dwhenthereisnoimpac tontheselectionalgorithms,e.g.,when\nfeatures resulting from a pre-processing of input variables are explic itly computed. The distinction is necessary in\nthe case ofkernel methods forwhich features are notexplicitly compute d (seesection 5.3).\nc/circlecopyrt2003Isabelle Guyonand Andr \u00b4eElisseeff.", "start_char_idx": 0, "end_char_idx": 2946, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b5560d09-046f-4c51-8380-90520ee9abd1": {"__data__": {"id_": "b5560d09-046f-4c51-8380-90520ee9abd1", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a92d053d-c7b3-46e4-8786-3c779f2b1ff2", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "3": {"node_id": "115e2887-4896-4804-b552-b4e4f031bc94", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "da59e72ec43c4cc43f5dc0e7aa51d49574c8bfe6c96935a091ea78024ed6f742"}}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425", "text": "GUYON AND ELISSEEFF\nabundance of mRNA in a sample (e.g. tissue biopsy), for a number of patients . A typical clas-\nsi\ufb01cation task is to separate healthy patients from cancer patients, based on their gene expression\n\u201cpro\ufb01le\u201d. Usually fewer than 100 examples (patients) are available altogeth er for training and test-\ning. But, the number of variables in the raw data ranges from 6000 to 60,000 . Some initial \ufb01ltering\nusually brings the number of variables to a few thousand. Because the abu ndance of mRNA varies\nbyseveralordersofmagnitudedependingonthegene,thevariablesar eusuallystandardized. Inthe\ntext classi\ufb01cation problem, the documents are represented by a \u201cbag-of- words\u201d, that is a vector of\ndimensionthesizeofthevocabularycontainingwordfrequencycounts(p ropernormalizationofthe\nvariables also apply). Vocabularies of hundreds of thousands of wor ds are common, but an initial\npruning of the most and least frequent words may reduce the effective number of words to 15,000.\nLargedocumentcollections of5000to800,000documents areavailablefor research. Typicaltasks\ninclude the automatic sorting of URLs into a web directory and the detection of un solicited email\n(spam). Foralistofpubliclyavailabledatasetsusedinthisissue,seeTable1 attheendofthepaper.\nTherearemanypotentialbene\ufb01tsofvariableandfeatureselection: facilita tingdatavisualization\nanddataunderstanding,reducingthemeasurementandstoragerequire ments,reducingtrainingand\nutilization times, defying the curse of dimensionality to improve prediction performa nce. Some\nmethods put more emphasis on one aspect than another, and this is another p oint of distinction\nbetweenthisspecialissueandpreviouswork. Thepapersinthisissuefo cusmainlyonconstructing\nand selecting subsets of features that areusefulto build a good predictor. This contrasts with the\nproblemof\ufb01ndingorrankingallpotentiallyrelevantvariables. Selectingth emostrelevantvariables\nisusuallysuboptimalforbuildingapredictor,particularlyifthevariablesare redundant. Conversely,\na subset of useful variables may exclude many redundant, but relevan t, variables. For a discussion\nofrelevance vs.usefulnessandde\ufb01nitionsofthevariousnotionsofrelevance,seethere viewarticles\nof KohaviandJohn(1997)andBlum andLangley(1997).\nThis introduction surveys the papers presented in this special issue. The depth of treatment of\nvarioussubjectsre\ufb02ectstheproportionofpaperscoveringthem: thepro blemofsupervisedlearning\nis treated more extensively than that of unsupervised learning; classi\ufb01ca tion problems serve more\noftenasillustrationthanregressionproblems,andonlyvectorialinputdata isconsidered. Complex-\nity is progressively introduced throughout the sections: The \ufb01rst sectio n starts by describing \ufb01lters\nthat select variables by ranking them with correlation coef\ufb01cients (Section 2). Limitations of such\napproaches are illustrated by a set of constructed examples (Section 3). Subset selection methods\narethenintroduced(Section4). Theseinclude wrappermethods thatassesssubsetsofvariablesac-\ncording to their usefulness to a given predictor. We show how some embedd ed methods implement\nthesameidea,butproceedmoreef\ufb01cientlybydirectlyoptimizingatwo-partob jectivefunctionwith\nagoodness-of-\ufb01ttermandapenaltyforalargenumberofvariables. W ethenturntotheproblemof\nfeature construction, whose goals include increasing the predictor perf ormance and building more\ncompact feature subsets (Section 5). All of the previous steps bene\ufb01t f rom reliably assessing", "start_char_idx": 0, "end_char_idx": 3465, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "115e2887-4896-4804-b552-b4e4f031bc94": {"__data__": {"id_": "115e2887-4896-4804-b552-b4e4f031bc94", "embedding": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a92d053d-c7b3-46e4-8786-3c779f2b1ff2", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "bea9ef860c5258b8413d0487210b8fc0179bd103e32d7b26dae12e804da3a671"}, "2": {"node_id": "b5560d09-046f-4c51-8380-90520ee9abd1", "node_type": null, "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}, "hash": "28be8b26b4057d1ebf6a311ba3106248cef0dc0fc38e3b2339a4ae053cc24425"}}, "hash": "da59e72ec43c4cc43f5dc0e7aa51d49574c8bfe6c96935a091ea78024ed6f742", "text": "(Section 2). Limitations of such\napproaches are illustrated by a set of constructed examples (Section 3). Subset selection methods\narethenintroduced(Section4). Theseinclude wrappermethods thatassesssubsetsofvariablesac-\ncording to their usefulness to a given predictor. We show how some embedd ed methods implement\nthesameidea,butproceedmoreef\ufb01cientlybydirectlyoptimizingatwo-partob jectivefunctionwith\nagoodness-of-\ufb01ttermandapenaltyforalargenumberofvariables. W ethenturntotheproblemof\nfeature construction, whose goals include increasing the predictor perf ormance and building more\ncompact feature subsets (Section 5). All of the previous steps bene\ufb01t f rom reliably assessing the\nstatistical signi\ufb01cance of the relevance of features. We brie\ufb02y review mode l selection methods and\nstatisticaltestsusedtothateffect(Section6). Finally,weconcludethepap erwithadiscussionsec-\ntion in which we go over more advanced issues (Section 7). Because the or ganization of our paper\ndoes not follow the work \ufb02ow of building a machine learning application, we summa rize the steps\nthatmaybe takentosolveafeatureselectionprobleminacheck list2:\n2. We caution the reader that this check list is heuristic. The only recommen dation that is almost surely valid is to try\nthe simplestthings \ufb01rst.\n1158", "start_char_idx": 2786, "end_char_idx": 4070, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4061679f-82cd-42d7-a83e-c2188dd0546b": {"__data__": {"id_": "4061679f-82cd-42d7-a83e-c2188dd0546b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1828614e-2082-4396-b396-0068e7b6ff76", "node_type": null, "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c"}}, "hash": "5df708c3c2500ffbc05809c25e6150400d69e5745d8033bf30ce031daf84031c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n1.Do youhavedomain knowledge? Ifyes,constructabetter setof \u201cad hoc\u201dfeatures.\n2.Are yourfeaturescommensurate? If no,consider normalizingthem.\n3.Doyoususpectinterdependenceoffeatures? Ifyes,expandyourfeaturesetbyconstructing\nconjunctive features or products of features, as much as your compute r resources allow you\n(seeexampleof useinSection4.4).\n4.Do you need to prune the input variables (e.g. for cost, speed or data understanding rea-\nsons)? If no, construct disjunctive features or weighted sums of featu res (e.g. by clustering\nor matrixfactorization,seeSection5).\n5.Doyouneedtoassessfeaturesindividually (e.g. tounderstandtheirin\ufb02uenceonthesystem\nor because their number is so large that you need to do a \ufb01rst \ufb01ltering)? If y es, use a variable\nrankingmethod (Section2andSection7.2);else,doitanyway togetbaselin eresults.\n6.Do youneed apredictor? Ifno,stop.\n7.Do you suspect your data is \u201cdirty\u201d (has a few meaningless input patterns and/or noisy\noutputs or wrong class labels)? If yes, detect the outlier examples using the top ranking\nvariables obtainedinstep5as representation;check and/or discardthem.\n8.Doyouknowwhattotry\ufb01rst? Ifno,usealinearpredictor.3Useaforwardselectionmethod\n(Section 4.2) with the \u201cprobe\u201d method as a stopping criterion (Section 6) or us e the/lscript0-norm\nembedded method (Section 4.3). For comparison, following the ranking of ste p 5, construct\na sequence of predictors of same nature using increasing subsets of fe atures. Can you match\nor improve performance with a smaller subset? If yes, try a non-linear pred ictor with that\nsubset.\n9.Do you have new ideas, time, computational resources, and enoug h examples? If yes,\ncompare several feature selection methods, including your new idea, cor relation coef\ufb01cients,\nbackwardselectionandembeddedmethods(Section4). Uselinearandnon -linearpredictors.\nSelect thebestapproachwithmodel selection(Section6).\n10.Do you want a stable solution (to improve performance and/or understanding)? If yes, sub-\nsampleyour dataand redoyour analysisforseveral\u201cbootstraps\u201d(Se ction7.1).\n2 Variable Ranking\nMany variable selection algorithms include variable ranking as a principal or auxiliary selection\nmechanism because of its simplicity, scalability, and good empirical success. S everal papers in this\nissue use variable ranking as a baseline method (see, e.g., Bekkerman et a l., 2003, Caruana and\nde Sa, 2003, Forman, 2003, Weston et al., 2003). Variable ranking is no t necessarily used to build\npredictors. One of its common uses in the microarray analysis domain is to discov er a set of drug\nleads (see, e.g., et al., 1999): A ranking criterion is used to \ufb01nd genes tha t discriminate between\nhealthy and disease patients; such genes may code for \u201cdrugable\u201d prote ins, or proteins that may\n3. By \u201clinear predictor\u201d we mean linear in the parameters. Feature constr uction may render the predictor non-linear in\nthe inputvariables.\n1159", "start_char_idx": 0, "end_char_idx": 2962, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4ce061cb-d032-4bcf-91d2-48f358f2db7c": {"__data__": {"id_": "4ce061cb-d032-4bcf-91d2-48f358f2db7c", "embedding": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5bf9a414-1d20-4b5a-b249-686a7a543d47", "node_type": null, "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69"}}, "hash": "b49d03d7c6a42cb6e8964777fa30443d29be7739249aac2ede85f0db1da61a69", "text": "GUYON AND ELISSEEFF\nthemselves be used as drugs. Validating drug leads is a labor intensive pro blem in biology that is\noutside of the scope of machine learning, so we focus here on building pre dictors. We consider in\nthis section ranking criteria de\ufb01ned for individual variables, independe ntly of the context of others.\nCorrelationmethodsbelongtothatcategory. Wealsolimitourselvestosuperv isedlearningcriteria.\nWerefer thereader toSection7.2for adiscussionof othertechniques.\n2.1 Principleofthe Method andNotations\nConsider a set of mexamples {xk,yk}(k=1,...m) consisting of ninput variables xk,i(i=1,...n)\nand one output variable yk. Variable ranking makes use of a scoring function S(i)computed from\nthe values xk,iandyk,k=1,...m. By convention, we assume that a high score is indicative of a\nvaluable variable and that we sort variables in decreasing order of S(i). To use variable ranking to\nbuild predictors, nested subsets incorporating progressively more and more variables of decreasing\nrelevance are de\ufb01ned. We postpone until Section 6 the discussion of sele cting an optimum subset\nsize.\nFollowingthe classi\ufb01cationof Kohaviand John(1997),variableranking isa\ufb01ltermethod: itis\napreprocessingstep,independentofthechoiceofthepredictor. Still,u ndercertainindependenceor\northogonality assumptions, it may be optimal with respect to a given predictor. For instance, using\nFisher\u2019scriterion4torankvariablesinaclassi\ufb01cationproblemwherethecovariancematrixisdia g-\nonal is optimum for Fisher\u2019s linear discriminant classi\ufb01er (Duda et al., 2001 ). Even when variable\nranking is not optimal, it may be preferable to other variable subset selection methods because of\nitscomputationalandstatisticalscalability: Computationally,itisef\ufb01cientsinceitr equiresonlythe\ncomputation of nscores and sorting the scores; Statistically, it is robust against over\ufb01tting because\nitintroduces biasbutitmayhave considerablyless variance(Hastieetal., 2001).5\nWeintroducesomeadditionalnotation: Iftheinputvector xcanbeinterpretedastherealization\nof a random vector drawn from an underlying unknown distribution, we d enote by Xithe random\nvariablecorrespondingtothe ithcomponentof x. Similarly, Ywillbetherandomvariableofwhich\nthe outcome yis a realization. We further denote by xithemdimensional vector containing all\nthe realizations of the ithvariable for the training examples, and by ythemdimensional vector\ncontaining allthetargetvalues.\n2.2 Correlation Criteria\nLet us consider \ufb01rst the prediction of a continuous outcome y. The Pearson correlation coef\ufb01cient\nisde\ufb01ned as:\nR(i) =cov(Xi,Y)/radicalbig\nvar(Xi)var(Y), (1)\nwherecovdesignatesthecovariance and varthevariance. Theestimateof R(i)is givenby:\nR(i) =\u2211m\nk=1(xk,i\u2212\u00afxi)(yk\u2212\u00afy)/radicalbig\n\u2211m\nk=1(xk,i\u2212\u00afxi)2\u2211m\nk=1(yk\u2212\u00afy)2, (2)\n4. The ratioof thebetween class variance tothe within-class variance.\n5. The similarity of variable ranking to the ORDERED-FS algorithm (Ng, 1998 ) indicates that its sample complexity\nmay be logarithmic in the number of irrelevant features, compared to a po wer law for \u201cwrapper\u201d subset selection\nmethods. This would mean that variable ranking can tolerate a number of ir relevant variables exponential in the\nnumber of trainingexamples.\n1160", "start_char_idx": 0, "end_char_idx": 3210, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "deb06ccc-8dbf-4b10-8d2a-50a3dfa2bf79": {"__data__": {"id_": "deb06ccc-8dbf-4b10-8d2a-50a3dfa2bf79", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5daa4e26-d5fe-4950-83a8-3e22305a1a94", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "3": {"node_id": "dbe138dc-27d1-40cd-82ae-a26f6325c2ba", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "483a7130fee7a8acde98cbaaac6885d09238336fe2e79045d1481a3829e68b4e"}}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nwhere the bar notation stands for an average over the index k. This coef\ufb01cient is also the cosine\nbetween vectors xiandy, after they have been centered (their mean subtracted). Although the R(i)\nis derived from R(i)it may be used without assuming that the input values are realizations of a\nrandomvariable.\nIn linear regression, the coef\ufb01cient of determination, which is the square ofR(i), represents the\nfractionofthetotalvariancearoundthemeanvalue \u00af ythatisexplainedbythelinearrelationbetween\nxiandy. Therefore, using R(i)2as a variable ranking criterion enforces a ranking according to\ngoodness oflinear \ufb01tof individualvariables.6\nThe use of R(i)2can be extended to the case of two-class classi\ufb01cation, for which each cla ss\nlabel is mapped to a given value of y, e.g., \u00b11.R(i)2can then be shown to be closely related to\nFisher\u2019s criterion (Furey et al., 2000), to the T-test criterion, and other similar criteria (see, e.g.,\net al., 1999, Tusher et al., 2001, Hastie et al., 2001). As further develo ped in Section 6, the link\nto the T-test shows that the score R(i)may be used as a test statistic to assess the signi\ufb01cance of a\nvariable.\nCorrelation criteria such as R(i)can only detect linear dependencies between variable and tar-\nget. A simple way of lifting this restriction is to make a non-linear \ufb01t of the target with single\nvariablesandrankaccordingtothegoodnessof\ufb01t. Becauseoftherisk ofover\ufb01tting,onecanalter-\nnatively consider using non-linear preprocessing (e.g., squaring, tak ing the square root, the log, the\ninverse, etc.) and then using a simple correlation coef\ufb01cient. Correlation cr iteria are often used for\nmicroarraydataanalysis,as illustratedin thisissuebyWestonet al.(2003).\n2.3 SingleVariable Classi\ufb01ers\nAsalreadymentioned,using R(i)2asarankingcriterionfor regression enforcesarankingaccording\ntogoodnessoflinear\ufb01tofindividualvariables. Onecanextendtothe classi\ufb01cation casetheideaof\nselectingvariablesaccordingtotheirindividualpredictivepower,using ascriteriontheperformance\nofaclassi\ufb01erbuiltwithasinglevariable. Forexample,thevalueofthevariab leitself(oritsnegative,\ntoaccountforclasspolarity)canbeusedasdiscriminantfunction. Aclas si\ufb01erisobtainedbysetting\nathreshold \u03b8onthevalueofthevariable(e.g.,atthemid-pointbetweenthecenterofgrav ityofthe\ntwoclasses).\nThe predictive power of the variable can be measured in terms of error rate . But, various other\ncriteria can be de\ufb01ned that involve false positive classi\ufb01cation rate fprand false negative classi\ufb01-\ncation rate fnr. The tradeoff between fprandfnris monitored in our simple example by varying\nthe threshold \u03b8. ROC curves that plot \u201chit\u201d rate (1-fpr)as a function of \u201cfalse alarm\u201d rate fnrare\ninstrumental in de\ufb01ning criteria such as: The \u201cBreak Even Point\u201d (the hit ra te for a threshold value\ncorrespondingto fpr=fnr)andthe \u201cAreaUnder Curve\u201d(thearea under theROCcurve).\nIn the case where there is a large number of variables that separate the da ta perfectly, ranking\ncriteria based on classi\ufb01cation success rate cannot distinguish between th e top ranking variables.\nOnewillthenprefertouseacorrelationcoef\ufb01cientoranotherstatisticliketh emargin(thedistance\nbetween", "start_char_idx": 0, "end_char_idx": 3198, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dbe138dc-27d1-40cd-82ae-a26f6325c2ba": {"__data__": {"id_": "dbe138dc-27d1-40cd-82ae-a26f6325c2ba", "embedding": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5daa4e26-d5fe-4950-83a8-3e22305a1a94", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "91a80a2d1797ef8d0a957b16c54fe2befa8f33d1292313fdf8f6ac288672d22f"}, "2": {"node_id": "deb06ccc-8dbf-4b10-8d2a-50a3dfa2bf79", "node_type": null, "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}, "hash": "6a89760a32d8b17641a9eb49f3ef948f522dd9618036a25c21ba47efa004a086"}}, "hash": "483a7130fee7a8acde98cbaaac6885d09238336fe2e79045d1481a3829e68b4e", "text": "monitored in our simple example by varying\nthe threshold \u03b8. ROC curves that plot \u201chit\u201d rate (1-fpr)as a function of \u201cfalse alarm\u201d rate fnrare\ninstrumental in de\ufb01ning criteria such as: The \u201cBreak Even Point\u201d (the hit ra te for a threshold value\ncorrespondingto fpr=fnr)andthe \u201cAreaUnder Curve\u201d(thearea under theROCcurve).\nIn the case where there is a large number of variables that separate the da ta perfectly, ranking\ncriteria based on classi\ufb01cation success rate cannot distinguish between th e top ranking variables.\nOnewillthenprefertouseacorrelationcoef\ufb01cientoranotherstatisticliketh emargin(thedistance\nbetween theexamples of oppositeclassesthatareclosesttooneanother f oragivenvariable).\n6. Avariantofthisideaistousethemean-squared-error,but,ifthevar iablesarenotoncomparablescales,acomparison\nbetween mean-squared-errors is meaningless. Another variant is to u seR(i)to rank variables, not R(i)2. Positively\ncorrelated variables are then top ranked and negatively correlated var iables bottom ranked. With this method, one\ncan choose asubsetof variables withagiven proportion of positivelya nd negatively correlated variables.\n1161", "start_char_idx": 2583, "end_char_idx": 3721, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0c2aed0c-19a2-49ac-b20f-edab59243b01": {"__data__": {"id_": "0c2aed0c-19a2-49ac-b20f-edab59243b01", "embedding": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d0a47d64-b47a-44fa-b78b-a4de739208ea", "node_type": null, "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4"}}, "hash": "2b539d9ce43ce2166c74fc169ca471902da89c19d7f939445ed75741d94b74c4", "text": "GUYON AND ELISSEEFF\nThe criteria described in this section extend to the case of binary variables. Forman (2003)\npresentsinthisissueanextensivestudyofsuchcriteriaforbinaryvaria bleswithapplicationsintext\nclassi\ufb01cation.\n2.4 InformationTheoretic RankingCriteria\nSeveral approaches to the variable selection problem using information the oretic criteria have been\nproposed (as reviewed in this issue by Bekkerman et al., 2003, Dhillon et a l., 2003, Forman, 2003,\nTorkkola,2003). Manyrelyonempiricalestimatesofthemutualinformationbe tweeneachvariable\nandthe target:\nI(i) =Z\nxiZ\nyp(xi,y)logp(xi,y)\np(xi)p(y)dxdy, (3)\nwherep(xi)andp(y)are the probability densities of xiandy, andp(xi,y)is the joint density. The\ncriterion I(i)is a measure of dependency between the density of variable xiand the density of the\ntargety.\nThe dif\ufb01culty is that the densities p(xi),p(y)andp(xi,y)are all unknown and are hard to\nestimatefromdata. Thecaseofdiscreteornominalvariablesisprobablyea siestbecausetheintegral\nbecomes asum:\nI(i) =\u2211\nxi\u2211\nyP(X=xi,Y=y)logP(X=xi,Y=y)\nP(X=xi)P(Y=y). (4)\nThe probabilities are then estimated from frequency counts. For example, in a three-class\nproblem, if a variable takes 4 values, P(Y=y)represents the class prior probabilities (3 fre-\nquency counts), P(X=xi)represents the distribution of the input variable (4 frequency counts),\nandP(X=xi,Y=y)istheprobabilityofthejointobservations(12frequencycounts). Thees tima-\ntionobviouslybecomes harder withlarger numbersof classesandvariable values.\nThe case of continuous variables (and possibly continuous targets) is the hardest. One can\nconsider discretizing the variables or approximating their densities with a non- parametric method\nsuch as Parzen windows (see, e.g., Torkkola, 2003). Using the normal distribution to estimate\ndensities would bring us back to estimating the covariance between XiandY, thus giving us a\ncriterionsimilartoacorrelationcoef\ufb01cient.\n3 Small but RevealingExamples\nWe present a series of small examples that outline the usefulness and the limitatio ns of variable\nranking techniques and present several situations in which the variable d ependencies cannot be\nignored.\n3.1 Can PresumablyRedundantVariables Help Each Other?\nOnecommoncriticismofvariablerankingisthatitleadstotheselectionofaredu ndantsubset. The\nsame performance could possibly be achieved with a smaller subset of comple mentary variables.\nStill, one may wonder whether adding presumably redundant variables can result in a performance\ngain.\n1162", "start_char_idx": 0, "end_char_idx": 2498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc74ac2b-f0c4-4dbd-979e-e1d4bde5829f": {"__data__": {"id_": "cc74ac2b-f0c4-4dbd-979e-e1d4bde5829f", "embedding": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cb5b8f4-921e-43ac-814b-d705dd939634", "node_type": null, "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d"}}, "hash": "148b50b0481c20efd68bb116821affb69cc08fb3983122de5589dff56fadd61d", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22126\u22124\u221220246 \u22125 0 5\u22126\u22124\u221220246\u2212505\n(a) (b)\nFigure1: Informationgainfrompresumablyredundantvariables. (a)Atwoclassproblemwith\nindependently and identically distributed (i.i.d.) variables. Each class has a Ga ussian distribution\nwith no covariance. (b) The same example after a 45 degree rotation showin g that a combination\nof the two variables yields a separation improvement by a factor\u221a\n2. I.i.d. variables are not truly\nredundant.\nConsider the classi\ufb01cation problem of Figure 1. For each class, we drew at random m=100\nexamples,eachofthetwovariablesbeingdrawnindependentlyaccording toanormaldistributionof\nstandarddeviation1. Theclasscentersareplacedatcoordinates(-1; -1)and(1;1). Figure1.ashows\nthescatterplotinthetwo-dimensionalspaceoftheinputvariables. Wealsos howonthesame\ufb01gure\nhistogramsoftheprojectionsoftheexamplesontheaxes. Tofacilitateitsreadin g,thescatterplotis\nshowntwicewithanaxisexchange. Figure1.bshowsthesamescatterplotsaf teraforty\ufb01vedegree\nrotation. Inthisrepresentation,thex-axisprojectionprovidesabettersep arationofthetwoclasses:\nthe standard deviation of both classes is the same, but the distance between c enters in projection is\nnow 2\u221a\n2 instead of 2. Equivalently, if we rescale the x-axis by dividing by\u221a\n2 to obtain a feature\nthat is the average of the two input variables, the distance between centers is still 2, but the within\nclass standard deviation is reduced by a factor\u221a\n2. This is not so surprising, since by averaging n\ni.i.d. random variables we will obtain a reduction of standard deviation by a fa ctor of\u221an.Noise\nreduction and consequently better class separation may be obtain ed by adding variables that\nare presumably redundant. Variables that are independently and identically distributed are not\ntrulyredundant.\n3.2 HowDoes Correlation ImpactVariable Redundancy?\nAnother notion of redundancy is correlation. In the previous example, in s pite of the fact that the\nexamples are i.i.d. with respect to the class conditional distributions, the variab les are correlated\nbecause of the separation of the class center positions. One may wonder h ow variable redundancy\nis affected by adding within-class variable correlation. In Figure 2, the cla ss centers are positioned\n1163", "start_char_idx": 0, "end_char_idx": 2289, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2297bd97-eaa8-472f-b652-ca88608aaf1f": {"__data__": {"id_": "2297bd97-eaa8-472f-b652-ca88608aaf1f", "embedding": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a21d0bb2-5c95-4512-9771-2015b84ef71b", "node_type": null, "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3"}}, "hash": "29d5774944e2e403e0d708d6b2ebe5855d7b696464a2c0797f21bc01ec091ec3", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n\u22125 0 5 \u22125 0 5\u2212505\u2212505\n(a) (b)\nFigure2: Intra-classcovariance. Inprojectionontheaxes,thedistributionsofthetwovariablesare\nthe same as in the previous example. (a) The class conditional distributions hav e a high covariance\ninthedirectionofthelineofthetwoclasscenters. Thereisnosigni\ufb01cantgain inseparationbyusing\ntwo variables instead of just one. (b) The class conditional distributions ha ve a high covariance in\nthe direction perpendicular to the line of the two class centers. An important se paration gain is\nobtained byusingtwovariables insteadofone.\nsimilarly as in the previous example at coordinates (-1; -1) and (1; 1) but w e have added some\nvariableco-variance. Weconsider twocases:\nIn Figure 2.a, in the direction of the class center line, the standard deviation o f the class condi-\ntional distributions is\u221a\n2, while in the perpendicular direction it is a small value ( \u03b5=1/10). With\nthis construction, as \u03b5goes to zero, the input variables have the same separation power as in the\ncase of the example of Figure 1, with a standard deviation of the class distribu tions of one and a\ndistance of the class centers of 2. But the feature constructed as the sum of the input variables has\nnobetterseparationpower: astandarddeviationof\u221a\n2andaclasscenterseparationof2\u221a\n2(asim-\nple scaling that does not change the separation power). Therefore, in the limit of perfect variable\ncorrelation (zero variance in the direction perpendicular to the class cente r line), single variables\nprovide the same separation as the sum of the two variables. Perfectly correlated variables are\ntrulyredundant in thesensethatnoadditionalinformationis gained byaddingthem.\nIn contrast, in the example of Figure 2.b, the \ufb01rst principal direction of the c ovariance matrices\nof the class conditional densities is perpendicular to the class center line. In th is case, more is\ngainedbyaddingthetwovariablesthanintheexampleofFigure1. Onenotices thatinspiteoftheir\ngreatcomplementarity(inthesensethataperfectseparationcanbeachie vedinthetwo-dimensional\nspace spanned by the two variables), the two variables are (anti-)corre lated. More anti-correlation\nis obtained by making the class centers closer and increasing the ratio of the v ariances of the class\nconditional distributions. Very high variable correlation (or anti-correlation) does not mean\nabsenceof variablecomplementarity.\n1164", "start_char_idx": 0, "end_char_idx": 2412, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "26f1f084-44a3-4b39-a03f-0dfebd9afb57": {"__data__": {"id_": "26f1f084-44a3-4b39-a03f-0dfebd9afb57", "embedding": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "fae20a98-def8-487f-a902-69e703dd131b", "node_type": null, "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c"}}, "hash": "08a3ae10b4ab262ad7d78fc9eead92e3f33c6da8f0c9943545ffe07cf840290c", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nThe examples of Figure 1 and 2 all have variables with the same distribution of e xamples (in\nprojection on the axis). Therefore, methods that score variables individ ually and independently of\neach other areatlosstodeterminewhichcombination of variableswouldgive bestperformance.\n3.3 Can aVariable thatis Useless byItself beUsefulwith Oth ers?\nOneconcernaboutmultivariatemethodsisthattheyarepronetoover\ufb01tting. Theproblemisaggra-\nvated when the number of variables to select from is large compared to the nu mber of examples.\nIt is tempting to use a variable ranking method to \ufb01lter out the least promising varia bles before us-\ning a multivariate method. Still one may wonder whether one could potentially lose s ome valuable\nvariablesthroughthat \ufb01lteringprocess.\nWe constructed an example in Figure 3.a. In this example, the two class condition al distribu-\ntions have identical covariance matrices, and the principal directions are o riented diagonally. The\nclass centers are separated on one axis, but not on the other. By itself o ne variable is \u201cuseless\u201d.\nStill, the two dimensional separation is better than the separation using the \u201cusef ul\u201d variable alone.\nTherefore, a variable that is completely useless by itself can provide a signi\ufb01cant p erformance\nimprovementwhen takenwith others.\nThe next question is whether two variables that are useless by themselves c an provide a good\nseparation when taken together. We constructed an example of such a cas e, inspired by the famous\nXOR problem.7In Figure 3.b, we drew examples for two classes using four Gaussians pla ced on\nthe corners of a square at coordinates (0; 0), (0; 1), (1; 0), and ( 1; 1). The class labels of these four\n\u201cclumps\u201dareattributedaccordingtothetruthtableofthelogicalXORfunction: f(0;0)=0,f(0;1)=1,\nf(1; 0)=1; f(1; 1)=0. We notice that the projections on the axes provide no class separation. Yet,\nin the two dimensional space the classes can easily be separated (albeit not with a linear decision\nfunction).8Twovariables thatare uselessbythemselvescan beusefultogeth er.\n7. The XOR problem is sometimes referred to as the two-bit parity problem a nd is generalizable to more than two\ndimensions (n-bit parity problem). A related problem is the chessboard p roblem in which the two classes pave\nthe space with squares of uniformly distributed examples with alternating clas s labels. The latter problem is also\ngeneralizable to the multi-dimensional case. Similar examples are used in se veral papers in this issue (Perkins et al.,\n2003, Stoppiglia etal., 2003).\n8. Incidentally, thetwo variables are alsouncorrelated with oneanother.\n1165", "start_char_idx": 0, "end_char_idx": 2666, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3aa3e455-37c8-436b-9a39-a86c924314f9": {"__data__": {"id_": "3aa3e455-37c8-436b-9a39-a86c924314f9", "embedding": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3836b47f-7bbc-4927-ada9-7dfca6564469", "node_type": null, "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9"}}, "hash": "d29c985fd82db5ddade88b05b6116d0e3358027261d8f0a2f97f32cd6d484ec9", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22122\u22121 012\u2212505\u22122\u22121012\n\u22120.5 00.511.5 \u22120.5 00.511.5\u22120.500.511.5\u22120.500.511.5\n(a) (b)\nFigure 3: A variable useless by itself can be useful together with others. (a) One variable has\ncompletely overlapping class conditional densities. Still, using it jointly with the othe r variable\nimprovesclassseparabilitycomparedtousingtheothervariablealone. (b)X OR-likeorchessboard-\nlike problems. The classes consist of disjoint clumps such that in projection o n the axes the class\nconditional densities overlap perfectly. Therefore, individual variab les have no separation power.\nStill,takentogether,thevariablesprovidegood classseparability.\n4 Variable Subset Selection\nIn the previous section, we presented examples that illustrate the usefulnes s of selecting subsets\nof variables that together have good predictive power, as opposed to r anking variables according\nto their individual predictive power. We now turn to this problem and outline th e main directions\nthat have been taken to tackle it. They essentially divide into wrappers, \ufb01lter s, and embedded\nmethods. Wrappers utilize the learning machine of interest as a black box to score subsets of\nvariable according to their predictive power. Filtersselect subsets of variables as a pre-processing\nstep, independently of the chosen predictor. Embedded methods perform variable selection in the\nprocessof trainingandareusuallyspeci\ufb01ctogivenlearningmachines.\n4.1 WrappersandEmbeddedMethods\nThe wrapper methodology, recently popularized by Kohavi and John (1 997), offers a simple and\npowerful way to address the problem of variable selection, regardless of the chosen learning ma-\nchine. In fact, the learning machine is considered a perfect black box and the method lends itself\nto the use of off-the-shelf machine learning software packages. In its mos t general formulation, the\nwrapper methodology consists in using the prediction performance of a giv en learning machine to\nassess the relative usefulness of subsets of variables. In practice, o ne needs to de\ufb01ne: (i) how to\nsearch the space of all possible variable subsets; (ii) how to assess the p rediction performance of\na learning machine to guide the search and halt it; and (iii) which predictor to us e. An exhaustive\nsearch can conceivably be performed, if the number of variables is not too large. But, the problem\n1166", "start_char_idx": 0, "end_char_idx": 2358, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72810aca-171e-45e0-86d5-d5d6a5cc8224": {"__data__": {"id_": "72810aca-171e-45e0-86d5-d5d6a5cc8224", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0301e2f3-4c02-48e1-ba86-844d0d5c458b", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "3": {"node_id": "1889aed4-8ab1-413d-ba27-60b9f3577d0c", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "0912fda46fc21c82a29cad3bedd983c2e7705ff52368765cba7cf3f268ab170a"}}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nisknowntobeNP-hard(AmaldiandKann,1998)andthesearchbecomes quicklycomputationally\nintractable. A wide range of search strategies can be used, including bes t-\ufb01rst, branch-and-bound,\nsimulated annealing, genetic algorithms (see Kohavi and John, 1997, for a review). Performance\nassessments are usually done using a validation set or by cross-validation (see Section 6). As il-\nlustrated in this special issue, popular predictors include decision trees, n a\u00a8\u0131ve Bayes, least-square\nlinear predictors,and supportvector machines.\nWrappersareoftencriticizedbecausetheyseemtobea\u201cbruteforce\u201dme thodrequiringmassive\namountsofcomputation,butitisnotnecessarilyso. Ef\ufb01cientsearchstra tegiesmaybedevised. Us-\ningsuchstrategiesdoesnotnecessarilymeansacri\ufb01cingpredictionperf ormance. Infact,itappears\nto be the converse in some cases: coarse search strategies may alleviate the problem of over\ufb01tting,\nas illustrated for instance in this issue by the work of Reunanen (2003). Gr eedy search strategies\nseem to be particularly computationally advantageous and robust against o ver\ufb01tting. They come in\ntwo \ufb02avors: forward selection andbackward elimination . In forward selection, variables are pro-\ngressively incorporated into larger and larger subsets, whereas in ba ckward elimination one starts\nwith the set of all variables and progressively eliminates the least promising o nes.9Both methods\nyieldnestedsubsets of variables.\nBy using the learning machine as a black box, wrappers are remarkably un iversal and simple.\nBut embedded methods that incorporate variable selection as part of the tra ining process may be\nmore ef\ufb01cient in several respects: they make better use of the available da ta by not needing to split\nthetrainingdataintoatrainingandvalidationset;theyreachasolutionfasterby avoidingretraining\na predictor from scratch for every variable subset investigated. Embed ded methods are not new:\ndecision trees such as CART, for instance, have a built-in mechanism to per form variable selection\n(Breiman et al., 1984). The next two sections are devoted to two families of emb edded methods\nillustratedby algorithmspublishedinthisissue.\n4.2 NestedSubsetMethods\nSome embedded methods guide their search by estimating changes in the objectiv e function value\nincurredbymakingmovesinvariablesubsetspace. Combinedwithgreedys earchstrategies(back-\nwardeliminationor forwardselection) theyyieldnestedsubsetsof variables .10\nLet us call sthe number of variables selected at a given algorithm step and J(s)the value of\nthe objective function of the trained learning machine using such a variable s ubset. Predicting the\nchange intheobjectivefunctionis obtainedby:\n1.Finite difference calculation: The difference between J(s)andJ(s+1)orJ(s\u22121)is com-\nputed forthevariables thatarecandidates foraddition or removal.\n2.Quadratic approximation of the cost function: This method was originally proposed to\nprune weights in neural networks (LeCun et al., 1990). It can be used for backward elimi-\nnation of variables, via the pruning of the input variable weights wi. A second order Taylor\nexpansion of Jis made. At the optimum of J, the \ufb01rst-order term can be neglected, yield-\n9. The name greedy comes from the fact that one never revisits forme r decisions to include (or exclude) variables in\nlight of newdecisions.\n10. Thealgorithmspresentedinthissectionandinthefollowinggenerallyben", "start_char_idx": 0, "end_char_idx": 3418, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1889aed4-8ab1-413d-ba27-60b9f3577d0c": {"__data__": {"id_": "1889aed4-8ab1-413d-ba27-60b9f3577d0c", "embedding": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0301e2f3-4c02-48e1-ba86-844d0d5c458b", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "da437a5973c15848f8a41db6c4818c858fecc531e9d1f2a97b91936f11431b46"}, "2": {"node_id": "72810aca-171e-45e0-86d5-d5d6a5cc8224", "node_type": null, "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}, "hash": "89e96a0eece58e39032c5ed3ed0354bba07674104bbf9c27fbe83e78bd5f4fd2"}}, "hash": "0912fda46fc21c82a29cad3bedd983c2e7705ff52368765cba7cf3f268ab170a", "text": "between J(s)andJ(s+1)orJ(s\u22121)is com-\nputed forthevariables thatarecandidates foraddition or removal.\n2.Quadratic approximation of the cost function: This method was originally proposed to\nprune weights in neural networks (LeCun et al., 1990). It can be used for backward elimi-\nnation of variables, via the pruning of the input variable weights wi. A second order Taylor\nexpansion of Jis made. At the optimum of J, the \ufb01rst-order term can be neglected, yield-\n9. The name greedy comes from the fact that one never revisits forme r decisions to include (or exclude) variables in\nlight of newdecisions.\n10. Thealgorithmspresentedinthissectionandinthefollowinggenerallyben e\ufb01tfromvariablenormalization,exceptif\nthey have an internalnormalization mechanism likethe Gram-Schmidtortho gonalization procedure .\n1167", "start_char_idx": 2749, "end_char_idx": 3557, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "de12318d-d007-4b53-9030-1ac11fb79da4": {"__data__": {"id_": "de12318d-d007-4b53-9030-1ac11fb79da4", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a86d6031-7944-48c7-8929-e81402eda8cb", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "3": {"node_id": "9d54f5cb-4543-41b5-99b5-2891300b7452", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "83ce0e2565f8105c6b7fcc9cbea794fd662e9fb249ad38202dc006acd075c770"}}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668", "text": "GUYON AND ELISSEEFF\ning for variable ito the variation DJi= (1/2)\u22022J\n\u2202w2\ni(Dwi)2. The change in weight Dwi=wi\ncorrespondstoremovingvariable i.\n3.Sensitivity of the objective function calculation: The absolute value or the square of the\nderivativeof Jwithrespectto xi(orwithrespectto wi) isused.\nSome training algorithms lend themselves to using \ufb01nite differences (method 1) be cause exact\ndifferencescanbecomputedef\ufb01ciently,withoutretrainingnewmodelsfor eachcandidatevariable.\nSuch is the case for the linear least-square model: The Gram-Schmidt orthog onolization procedure\npermits the performance of forward variable selection by adding at each s tep the variable that most\ndecreasesthemean-squared-error. Twopapersinthisissuearedev otedtothistechnique(Stoppiglia\netal.,2003,RivalsandPersonnaz,2003). Forotheralgorithmslikeker nelmethods,approximations\nof the difference can be computed ef\ufb01ciently. Kernel methods are learnin g machines of the form\nf(x) =\u2211m\nk=1\u03b1kK(x,xk), whereKis the kernel function, which measures the similarity between x\nandxk(Schoelkopf and Smola, 2002). The variation in J(s)is computed by keeping the \u03b1kvalues\nconstant. This procedure originally proposed for SVMs (Guyon et al., 2 002) is used in this issueas\nabaselinemethod (Rakotomamonjy,2003, Westonetal.,2003).\nThe \u201coptimum brain damage\u201d (OBD) procedure (method 2) is mentioned in this iss ue in the\npaper of Rivals and Personnaz (2003). The case of linear predictor sf(x) =w\u00b7x+bis particularly\nsimple. The authors of the OBD algorithm advocate using DJiinstead of the magnitude of the\nweights |wi|as pruning criterion. However, for linear predictors trained with an objec tive function\nJthatisquadraticin withesetwocriteriaareequivalent. Thisisthecase,forinstance,fortheline ar\nleast square model using J=\u2211m\nk=1(w\u00b7xk+b\u2212yk)2and for the linear SVM or optimum margin\nclassi\ufb01er, which minimizes J= (1/2)||w||2, under constraints (Vapnik, 1982). Interestingly, for\nlinear SVMs the \ufb01nite difference method (method 1) and the sensitivity method (me thod 3) also\nboildowntoselectingthevariablewithsmallest |wi|foreliminationateachstep(Rakotomamonjy,\n2003).\nThesensitivityoftheobjectivefunctiontochangesin wi(method3)isusedtodeviseaforward\nselection procedure in one paper presented in this issue (Perkins et al., 2 003). Applications of this\nproceduretoalinearmodelwithacross-entropyobjectivefunctionarep resented. Intheformulation\nproposed, the criterion is the absolute value of\u2202J\n\u2202wi=\u2211m\nk=1\u2202J\n\u2202\u03c1k\u2202\u03c1k\n\u2202wi, where\u03c1k=ykf(xk). In the case\nof the linear model f(x) =w\u00b7x+b, the criterion has a simple geometrical interpretation: it is the\nthedotproductbetweenthegradientoftheobjectivefunctionwithrespe cttothemarginvaluesand\nthevector [\u2202\u03c1k\n\u2202wi=xk,iyk]k=1...m. For thecross-entropylossfunction,wehave:\u2202J\n\u2202\u03c1k=1\n1+e\u03c1k.\nAn interesting variant of the sensitivity analysis method is obtained by replacin g the objective\nfunction by the leave-one-out cross-validation error. For some learning machines and some ob-\njective", "start_char_idx": 0, "end_char_idx": 2985, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d54f5cb-4543-41b5-99b5-2891300b7452": {"__data__": {"id_": "9d54f5cb-4543-41b5-99b5-2891300b7452", "embedding": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a86d6031-7944-48c7-8929-e81402eda8cb", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "4bcb026482b723739ddcff6892209fae9b56c8023dfb7e8906ccc01d944b2faa"}, "2": {"node_id": "de12318d-d007-4b53-9030-1ac11fb79da4", "node_type": null, "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}, "hash": "30ff35b6d6ae40ee5d57a0c80f2ce161ae4f25619d741aab4c4b3f9fb41cd668"}}, "hash": "83ce0e2565f8105c6b7fcc9cbea794fd662e9fb249ad38202dc006acd075c770", "text": "where\u03c1k=ykf(xk). In the case\nof the linear model f(x) =w\u00b7x+b, the criterion has a simple geometrical interpretation: it is the\nthedotproductbetweenthegradientoftheobjectivefunctionwithrespe cttothemarginvaluesand\nthevector [\u2202\u03c1k\n\u2202wi=xk,iyk]k=1...m. For thecross-entropylossfunction,wehave:\u2202J\n\u2202\u03c1k=1\n1+e\u03c1k.\nAn interesting variant of the sensitivity analysis method is obtained by replacin g the objective\nfunction by the leave-one-out cross-validation error. For some learning machines and some ob-\njective functions, approximate or exact analytical formulas of the leave-o ne-out error are known.\nIn this issue, the case of the linear least-square model (Rivals and Perso nnaz, 2003) and SVMs\n(Rakotomamonjy, 2003) are treated. Approximations for non-linear least-s quares have also been\ncomputed elsewhere (Monari and Dreyfus, 2000). The proposal of Rakotomamonjy (2003) is to\ntrain non-linear SVMs (Boser et al., 1992, Vapnik, 1998) with a regular tr aining procedure and\nselect features with backward elimination like in RFE (Guyon et al., 2002). Th e variable ranking\ncriterion however is not computed using the sensitivity of the objective func tionJ, but that of a\nleave-one-outbound.\n1168", "start_char_idx": 2482, "end_char_idx": 3675, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "966b471a-3089-490c-8a0f-abb31d721c0e": {"__data__": {"id_": "966b471a-3089-490c-8a0f-abb31d721c0e", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "3": {"node_id": "704e88cd-f1bc-4a76-ad6e-d5d1f352ba36", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "dd82271dac46266cb3bde99bdd0369381e17ecd0fdf1617aa58cf24ff1dd8f7e"}}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n4.3 DirectObjective Optimization\nAlotofprogresshasbeenmadeinthisissuetoformalizetheobjectivefunction ofvariableselection\nand\ufb01ndalgorithmstooptimizeit. Generally,theobjectivefunctionconsistsoftwo termsthatcom-\npete with each other: (1) the goodness-of-\ufb01t (to be maximized), and (2) the number of variables\n(to be minimized). This approach bears similarity with two-part objective functio ns consisting of\na goodness-of-\ufb01t term and a regularization term, particularly when the ef fect of the regularization\nterm is to \u201cshrink\u201d parameter space. This correspondence is formally esta blished in the paper of\nWeston et al. (2003) for the particular case of classi\ufb01cation with linear pre dictorsf(x) =w\u00b7x+b,\nin the SVM framework (Boser et al., 1992, Vapnik, 1998). Shrinking reg ularizers of the type\n||w||p\np= (\u2211n\ni=1wp\ni)1/p(/lscriptp-norm) are used. In the limit as p\u21920, the /lscriptp-norm is just the number\nof weights, i.e., the number of variables. Weston et al. proceed with showing that the /lscript0-norm\nformulation of SVMs can be solved approximately with a simple modi\ufb01cation of the va nilla SVM\nalgorithm:\n1. Traina regularlinear SVM (using /lscript1-normor /lscript2-normregularization).\n2. Re-scaletheinputvariablesbymultiplyingthembytheabsolutevaluesofthe componentsof\nthe weightvector wobtained.\n3. Iteratethe\ufb01rst2steps untilconvergence.\nThemethodisreminiscentofbackwardeliminationproceduresbasedonthes mallest |wi|. Variable\nnormalizationis importantforsuchamethodtoworkproperly.\nWeston et al. note that, although their algorithm only approximately minimizes the /lscript0-norm, in\npracticeitmaygeneralizebetterthananalgorithmthatreallydidminimizethe /lscript0-norm,becausethe\nlatterwouldnotprovidesuf\ufb01cientregularization(alotofvarianceremain sbecausetheoptimization\nproblem has multiple solutions). The need for additional regularization is also stressed in the paper\nof Perkins et al. (2003). The authors use a three-part objective fun ction that includes goodness-\nof-\ufb01t, a regularization term ( /lscript1-norm or /lscript2-norm), and a penalty for large numbers of variables\n(/lscript0-norm). The authors propose a computationally ef\ufb01cient forward selectio n method to optimize\nsuchobjective.\nAnother paper in the issue, by Bi et al. (2003), uses /lscript1-norm SVMs, without iterative multi-\nplicative updates. The authors \ufb01nd that, for their application, the /lscript1-norm minimization suf\ufb01ces to\ndriveenoughweightstozero. Thisapproachwasalsotakeninthecontex tofleast-squareregression\nby other authors (Tibshirani, 1994). The number of variables can be fu rther reduced by backward\nelimination.\nTo our knowledge, no algorithm has been proposed to directly minimize the numb er of vari-\nablesfornon-linearpredictors. Instead,severalauthorshavesub stitutedfortheproblemofvariable\nselection that of variable scaling (Jebara and Jaakkola, 2000, Weston e t al., 2000, Grandvalet and\nCanu,2002). Thevariablescalingfactorsare\u201chyper-parameters\u201da djustedbymodelselection. The\nscaling factors obtained are used to assess variable relevance. A varia nt of the method consists\nof adjusting the scaling factors by", "start_char_idx": 0, "end_char_idx": 3166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "704e88cd-f1bc-4a76-ad6e-d5d1f352ba36": {"__data__": {"id_": "704e88cd-f1bc-4a76-ad6e-d5d1f352ba36", "embedding": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "381f6f14bc8d39dc298ede062c00ad5938dabdf1b887ed42a2928cc0ff12e439"}, "2": {"node_id": "966b471a-3089-490c-8a0f-abb31d721c0e", "node_type": null, "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}, "hash": "a9e4c973172f89e3740bed1ed4bea59b85b39fc0fa16fc8fdc1e6b7cbf878246"}}, "hash": "dd82271dac46266cb3bde99bdd0369381e17ecd0fdf1617aa58cf24ff1dd8f7e", "text": "tofleast-squareregression\nby other authors (Tibshirani, 1994). The number of variables can be fu rther reduced by backward\nelimination.\nTo our knowledge, no algorithm has been proposed to directly minimize the numb er of vari-\nablesfornon-linearpredictors. Instead,severalauthorshavesub stitutedfortheproblemofvariable\nselection that of variable scaling (Jebara and Jaakkola, 2000, Weston e t al., 2000, Grandvalet and\nCanu,2002). Thevariablescalingfactorsare\u201chyper-parameters\u201da djustedbymodelselection. The\nscaling factors obtained are used to assess variable relevance. A varia nt of the method consists\nof adjusting the scaling factors by gradient descent on a bound of the lea ve-one-out error (Weston\net al., 2000). This method is used as baseline method in the paper of Weston et al. (2003) in this\nissue.\n1169", "start_char_idx": 2525, "end_char_idx": 3341, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82c0ed52-9311-4ebc-9757-ee0a5366c107": {"__data__": {"id_": "82c0ed52-9311-4ebc-9757-ee0a5366c107", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410c2ff4-68ff-43c0-827a-41580d9e4802", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "3": {"node_id": "2c22439d-abc5-4f02-86d1-674b55702e41", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "58b9fbfae73e0447be7f222296d5dd748e8a3187e15e9372af79bfd311bff37d"}}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d", "text": "GUYON AND ELISSEEFF\n4.4 Filters for SubsetSelection\nSeveral justi\ufb01cations for the use of \ufb01lters for subset selection have bee n put forward in this special\nissue and elsewhere. It is argued that, compared to wrappers, \ufb01lters ar e faster. Still, recently pro-\nposed ef\ufb01cient embedded methods are competitive in that respect. Another a rgument is that some\n\ufb01lters (e.g. those based on mutual information criteria) provide a generic se lection of variables, not\ntunedfor/byagivenlearningmachine. Anothercompellingjusti\ufb01cationisthat\ufb01 lteringcanbeused\nas apreprocessingsteptoreducespacedimensionalityandovercomeove r\ufb01tting.\nIn that respect, it seems reasonable to use a wrapper (or embedded metho d) with a linearpre-\ndictor as a \ufb01lter and then train a more complex non-linear predictor on the resulting variables. An\nexampleofthisapproachisfoundinthepaperofBietal.(2003): alinear /lscript1-normSVMisusedfor\nvariable selection, but a non-linear /lscript1-norm SVM is used for prediction. The complexity of linear\n\ufb01lters can be ramped up by adding to the selection process products of inpu t variables (monomi-\nals of a polynomial) and retaining the variables that are part of any selected monomial. Another\npredictor, e.g., a neural network, is eventually substituted to the polynomial to perform predictions\nusing the selected variables (Rivals and Personnaz, 2003, Stoppiglia et al., 2003). In some cases\nhowever, one may on the contrary want to reduce the complexity of linear \ufb01lte rs to overcome over-\n\ufb01ttingproblems. Whenthenumberofexamplesissmallcomparedtothenumberofv ariables(inthe\ncase of microarray data for instance) one may need to resort to selecting v ariables with correlation\ncoef\ufb01cients (seeSection2.2).\nInformation theoretic \ufb01ltering methods such as Markov blanket11algorithms (Koller and Sa-\nhami,1996)constituteanotherbroadfamily. Thejusti\ufb01cationforclassi\ufb01cation problemsisthatthe\nmeasureofmutualinformationdoesnotrelyonanypredictionprocess,bu tprovidesaboundonthe\nerrorrateusinganyprediction schemeforthe givendistribution. Wedono t haveanyillustrationof\nsuchmethodsinthisissuefortheproblemofvariablesubsetselection. Wer efertheinterestedreader\ntoKollerandSahami(1996)andreferencestherein. However,theuse ofmutualinformationcriteria\nforindividualvariablerankingwascoveredinSection2andapplicationto featureconstructionand\nselectionareillustratedin Section5.\n5 FeatureConstruction and SpaceDimensionality Reduction\nIn some applications, reducing the dimensionality of the data by selecting a subs et of the original\nvariablesmaybeadvantageousforreasonsincludingtheexpenseofmak ing,storingandprocessing\nmeasurements. If these considerations are not of concern, other means of space dimensionality\nreductionshouldalsobeconsidered.\nThe art of machine learning starts with the design of appropriate data repre sentations. Better\nperformance is often achieved using features derived from the origina l input. Building a feature\nrepresentationisanopportunitytoincorporatedomainknowledgeintothedata andcanbeveryap-\nplicationspeci\ufb01c. Nonetheless,thereareanumberofgenericfeatureco nstructionmethods,includ-\ning: clustering;basiclineartransformsoftheinputvariables(PCA/SVD,L DA);moresophisticated\nlinear transforms like spectral transforms (Fourier, Hadamard), wavele t transforms or convolutions\nofkernels;andapplyingsimplefunctionstosubsetsofvariables,likeprod uctstocreatemonomials.\n11. The", "start_char_idx": 0, "end_char_idx": 3397, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2c22439d-abc5-4f02-86d1-674b55702e41": {"__data__": {"id_": "2c22439d-abc5-4f02-86d1-674b55702e41", "embedding": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "410c2ff4-68ff-43c0-827a-41580d9e4802", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "3e785043b10e866f8f7fb2f2e9f7e757568ccc0c27da7be7d9748fd5cdb5d521"}, "2": {"node_id": "82c0ed52-9311-4ebc-9757-ee0a5366c107", "node_type": null, "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}, "hash": "8d1a66639cfa066664174805baa479f570b274483291c6cc556018ac17463c8d"}}, "hash": "58b9fbfae73e0447be7f222296d5dd748e8a3187e15e9372af79bfd311bff37d", "text": "means of space dimensionality\nreductionshouldalsobeconsidered.\nThe art of machine learning starts with the design of appropriate data repre sentations. Better\nperformance is often achieved using features derived from the origina l input. Building a feature\nrepresentationisanopportunitytoincorporatedomainknowledgeintothedata andcanbeveryap-\nplicationspeci\ufb01c. Nonetheless,thereareanumberofgenericfeatureco nstructionmethods,includ-\ning: clustering;basiclineartransformsoftheinputvariables(PCA/SVD,L DA);moresophisticated\nlinear transforms like spectral transforms (Fourier, Hadamard), wavele t transforms or convolutions\nofkernels;andapplyingsimplefunctionstosubsetsofvariables,likeprod uctstocreatemonomials.\n11. The Markov blanket of a given variable xiis a set of variables not including xithat render xi\u201cunnecessary\u201d. Once\na Markov blanket is found, xican safely be eliminated. Furthermore, in a backward elimination procedu re, it will\nremain unnecessary atlater stages.\n1170", "start_char_idx": 2680, "end_char_idx": 3660, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "24271fce-0aef-4b36-bd9d-5e329ae162bf": {"__data__": {"id_": "24271fce-0aef-4b36-bd9d-5e329ae162bf", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a4c592e-0151-4012-8f25-fa4da077b701", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "3": {"node_id": "c9e82794-4c0d-4fae-8a62-6b75440d6850", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5d7d521755825863b13d02bd87c1feb00216922fec7ecbee98a5932e0f4074ba"}}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nTwodistinctgoalsmaybepursuedforfeatureconstruction: achievingbe streconstructionofthe\ndata or being most ef\ufb01cient for making predictions. The \ufb01rst problem is an unsupervised learning\nproblem. Itiscloselyrelatedtothatofdatacompressionandalotofalgorithms areusedacrossboth\n\ufb01elds. The second problem is supervised. Are there reasons to select features in an unsupervised\nmanner when the problem is supervised? Yes, possibly several: Some pro blems, e.g., in text pro-\ncessingapplications,comewithmoreunlabelleddatathanlabelleddata. Also,un supervisedfeature\nselectionis lesspronetoover\ufb01tting.\nInthisissue,fourpapersaddresstheproblemoffeatureconstruction . Allofthemtakeaninfor-\nmation theoretic approach to the problem. Two of them illustrate the use of cluster ing to construct\nfeatures (Bekkerman et al., 2003, Dhillon et al., 2003), one provides a n ew matrix factorization al-\ngorithm (Globerson and Tishby, 2003), and one provides a supervise d means of learning features\nfrom a variety of models (Torkkola, 2003). In addition, two papers whos e main focus is directed\ntovariableselectionalsoaddresstheselectionofmonomialsofapolynomialmode landthehidden\nunits of a neural network (Rivals and Personnaz, 2003, Stoppiglia et a l., 2003), and one paper ad-\ndresses the implicit feature selection in non-linear kernel methods for polyn omial kernels (Weston\netal.,2003).\n5.1 Clustering\nClustering has long been used for feature construction. The idea is to rep lace a group of \u201csimilar\u201d\nvariables by a cluster centroid, which becomes a feature. The most popula r algorithms include\nK-means andhierarchicalclustering. Forareview,see,e.g.,thetextboo k ofDuda etal.(2001).\nClustering is usually associated with the idea of unsupervised learning. It c an be useful to\nintroduce some supervision in the clustering procedure to obtain more discrimin ant features. This\nistheideaofdistributionalclustering(Pereiraetal.,1993),whichisdevelo pedintwopapersofthis\nissue. Distributional clustering is rooted in the information bottleneck (IB) theo ry of Tishby et al.\n(1999). Ifwecall \u02dcXtherandomvariablerepresentingtheconstructedfeatures,theIBmethod seeks\nto minimize the mutual information I(X,\u02dcX), while preserving the mutual information I(\u02dcX,Y). A\nglobalobjectivefunctionisbuiltbyintroducingaLagrangemultiplier \u03b2:J=I(X,\u02dcX)\u2212\u03b2I(\u02dcX,Y). So,\nthemethodsearchesforthesolutionthatachievesthelargestpossiblecomp ression,whileretaining\ntheessentialinformationabout thetarget.\nText processing applications are usual targets for such techniques. P atterns are full documents\nand variables come from a bag-of-words representation: Each variab le is associated to a word and\nis proportional to the fraction of documents in which that word appears. In application to feature\nconstruction, clustering methods group words, not documents. In text ca tegorization tasks, the su-\npervisioncomesfromtheknowledgeofdocumentcategories. Itisintroduc edbyreplacingvariable\nvectorscontainingdocumentfrequencycountsbyshortervariablevec torscontainingdocumentcat-\negoryfrequencycounts,i.e.,thewordsarerepresentedas distribution sover documentcategories.\nThe simplest implementation of this idea is presented in the paper of Dhillon et al. (2 003) in\nthis issue. It uses K-means clustering on variables represented by a vec tor of document category\nfrequencycounts.", "start_char_idx": 0, "end_char_idx": 3373, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c9e82794-4c0d-4fae-8a62-6b75440d6850": {"__data__": {"id_": "c9e82794-4c0d-4fae-8a62-6b75440d6850", "embedding": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6a4c592e-0151-4012-8f25-fa4da077b701", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "46769db10d2a5074ce37f026b92701f43f67c6234ce7699e4cef5e6c527c8422"}, "2": {"node_id": "24271fce-0aef-4b36-bd9d-5e329ae162bf", "node_type": null, "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}, "hash": "5a34d9367feb466cf693377fce4b5a7172119ab431ecf1b1b6e94278ce2116f5"}}, "hash": "5d7d521755825863b13d02bd87c1feb00216922fec7ecbee98a5932e0f4074ba", "text": "documents\nand variables come from a bag-of-words representation: Each variab le is associated to a word and\nis proportional to the fraction of documents in which that word appears. In application to feature\nconstruction, clustering methods group words, not documents. In text ca tegorization tasks, the su-\npervisioncomesfromtheknowledgeofdocumentcategories. Itisintroduc edbyreplacingvariable\nvectorscontainingdocumentfrequencycountsbyshortervariablevec torscontainingdocumentcat-\negoryfrequencycounts,i.e.,thewordsarerepresentedas distribution sover documentcategories.\nThe simplest implementation of this idea is presented in the paper of Dhillon et al. (2 003) in\nthis issue. It uses K-means clustering on variables represented by a vec tor of document category\nfrequencycounts. The(non-symmetric)similaritymeasureusedistheKullbac k-Leiblerdivergence\nK(xj,\u02dcxi) =exp(\u2212\u03b2\u2211kxk,jln(xk,j/\u02dcxk,i)). In the sum, the index kruns over document categories. A\nmore elaborate approach is taken by Bekkerman et al. (2003) who use a \u201c soft\u201d version of K-means\n(allowing words to belong to several clusters) and who progressively d ivide clusters by varying the\nLagrangemultiplier \u03b2monitoringthetradeoffbetween I(X,\u02dcX)andI(\u02dcX,Y). Inthisway,documents\n1171", "start_char_idx": 2591, "end_char_idx": 3832, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7298e8d9-a332-440b-aa04-b94bd357478d": {"__data__": {"id_": "7298e8d9-a332-440b-aa04-b94bd357478d", "embedding": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3086518d-aacf-4f68-90bc-9c1990559fbc", "node_type": null, "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64"}}, "hash": "86d18057d87ebd483fd69ee8a0d0b25d31a6c8ab2b24078679e6832411096d64", "text": "GUYON AND ELISSEEFF\narerepresentedasadistributionoverwordcentroids. Bothmethodsperf ormwell. Bekkermanetal.\nmentionthatfewwordsendupbelongingtoseveralclusters,hintingthat\u201ch ard\u201dclusterassignment\nmaybe suf\ufb01cient.\n5.2 Matrix Factorization\nAnother widely used method of feature construction is singular value decomp osition (SVD). The\ngoal of SVD is to form a set of features that are linear combinations of the o riginal variables,\nwhich provide the best possible reconstruction of the original data in the lea st square sense (Duda\net al., 2001). It is an unsupervised method of feature construction. In th is issue, the paper of\nGloberson and Tishby (2003) presents an information theoretic unsuper vised feature construction\nmethod: suf\ufb01cient dimensionality reduction (SDR). The most informative fea tures are extracted by\nsolving an optimization problem that monitors the tradeoff between data recons truction and data\ncompression, similar to the information bottleneck of Tishby et al. (1999); the f eatures are found\nas Lagrange multipliers of the objective optimized. Non-negative matrices P of dimension (m, n)\nrepresentingthejointdistributionoftworandomvariables(forinstancethe co-occurrenceofwords\nin documents) are considered. The features are extracted by information theoretic I-projections,\nyielding a reconstructed matrix of special exponential form \u02dcP= (1/Z)exp(\u03a6\u03a8). For a set of d\nfeatures,\u03a6isa(m,d+2)matrixwhose (d+1)thcolumnisonesand \u03a8isa(d+2,n)matrixwhose\n(d+2)thcolumnisones,and Zisanormalizationcoef\ufb01cient. SimilarlytoSVD,thesolutionshows\nthesymmetryofthe problemwithrespecttopatterns andvariables.\n5.3 SupervisedFeatureSelection\nWe review three approaches for selecting features in cases where fea tures should be distinguished\nfromvariables becausebothappear simultaneouslyinthesamesystem:\nNested subset methods. A number of learning machines extract features as part of the learn-\ning process. These include neural networks whose internal nodes ar e feature extractors. Thus,\nnode pruning techniques such as OBD LeCun et al. (1990) are feature selection algorithms. Gram-\nSchmidt orthogonalization is presented in this issue as an alternative to OBD (S toppiglia et al.,\n2003).\nFilters.Torkkola (2003) proposes a \ufb01lter method for constructing features usin g a mutual in-\nformation criterion. The author maximizes I(\u03c6,y)formdimensional feature vectors \u03c6and target\nvectorsy.12Modelling the feature density function with Parzen windows allows him to compute\nderivatives \u2202I/\u2202\u03c6ithat are transform independent. Combining them with the transform-depend ent\nderivatives \u2202\u03c6i/\u2202w, he devises a gradient descent algorithm to optimize the parameters wof the\ntransform(thatneed notbelinear):\nwt+1=wt+\u03b7\u2202I\n\u2202w=wt+\u03b7\u2202I\n\u2202\u03c6i\u2202\u03c6i\n\u2202w. (5)\nDirect objective optimization. Kernel methods possess an implicit feature space revealed by\nthe kernel expansion: k(x,x/prime) =\u03c6(x).\u03c6(x/prime), where\u03c6(x)is a feature vector of possibly in\ufb01nite di-\nmension. Selecting these implicit features may improve generalization, but does not change the\n12. Infact, theauthor uses aquadratic measure of divergence instea d of theusualmutual information.\n1172", "start_char_idx": 0, "end_char_idx": 3136, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6d4e2671-af99-4cea-89b1-4d3cb83c1b71": {"__data__": {"id_": "6d4e2671-af99-4cea-89b1-4d3cb83c1b71", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a11a05f9-b233-4c1e-8b44-b6f8de747991", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "3": {"node_id": "a7809498-b4a3-48cb-a20c-63c3a0cdb239", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "119572f66488246d97df820cc009226802ac1e06e3d272b748b600bc58725323"}}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nrunning time or help interpreting the prediction function. In this issue, Weston e t al. (2003) pro-\nposeamethodforselectingimplicitkernelfeaturesinthecaseofthepolynomial kernel,usingtheir\nframeworkof minimizationof the /lscript0-norm.\n6 Validation Methods\nWe group in this section all the issues related to out-of-sample performance pr ediction (generaliza-\ntion prediction) and model selection. These are involved in various aspects of variable and feature\nselection: to determine the number of variables that are \u201csigni\ufb01cant\u201d, to guide and halt the search\nfor good variable subsets, to choose hyperparameters, and to evaluate the \ufb01nal performance of the\nsystem.\nOne should \ufb01rst distinguish the problem of model selection from that of eva luating the \ufb01nal\nperformance of the predictor. For that last purpose, it is important to set aside an independent\ntest set. The remaining data is used both for training and performing model sele ction. Additional\nexperimentalsophisticationcanbeaddedbyrepeatingtheentireexperiment forseveraldrawingsof\nthetestset.13\nTo perform model selection (including variable/feature selection and hype rparameter optimiza-\ntion), the data not used for testing may be further split between \ufb01xed training and validation sets,\nor various methods of cross-validation can be used. The problem is then b rought back to that of\nestimating the signi\ufb01cance of differences in validation errors. For a \ufb01xed v alidation set, statistical\ntests can be used, but their validity is doubtful for cross-validation becau se independence assump-\ntions are violated. For a discussion of these issues, see for instance the w ork of Dietterich (1998)\nand Nadeau and Bengio (2001). If there are suf\ufb01ciently many examples, it may not be necessary to\nsplit the training data: Comparisons of training errors with statistical tests can b e used (see Rivals\nandPersonnaz,2003,inthisissue). Cross-validationcanbeextended totime-seriesdataand,while\ni.i.d.assumptionsdonotholdanymore,itisstillpossibletoestimategeneralizatione rrorcon\ufb01dence\nintervals(seeBengio andChapados,2003, inthisissue).\nChoosing what fraction of the data should be used for training and for va lidation is an open\nproblem. Manyauthorsresorttousingtheleave-one-outcross-valida tionprocedure,eventhoughit\nis known to be a high variance estimator of generalization error (Vapnik, 19 82) and to give overly\noptimistic results, particularly when data are not properly independently and identically sampled\nfrom the \u201dtrue\u201d distribution. The leave-one-out procedure consists of removing one example from\nthetrainingset,constructingthepredictoronthebasisonlyoftheremainingtra iningdata,thentest-\ningontheremovedexample. Inthisfashiononetestsallexamplesofthetraining dataandaverages\ntheresults. Aspreviouslymentioned,thereexistexactorapproximatefor mulasoftheleave-one-out\nerror for a number of learning machines (Monari and Dreyfus, 2000, Rivals and Personnaz, 2003,\nRakotomamonjy, 2003).\nLeave-one-out formulas can be viewed as corrected values of the train ing error. Many other\ntypes of penalization of the training error have been proposed in the literatu re (see, e.g., Vapnik,\n1998, Hastie et al., 2001). Recently, a new family of such methods called \u201cmetr ic-based methods\u201d\nhave been proposed (Schuurmans, 1997). The paper of Bengio and Chapados (2003) in this issue\n13. Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc", "start_char_idx": 0, "end_char_idx": 3446, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a7809498-b4a3-48cb-a20c-63c3a0cdb239": {"__data__": {"id_": "a7809498-b4a3-48cb-a20c-63c3a0cdb239", "embedding": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a11a05f9-b233-4c1e-8b44-b6f8de747991", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "50ed442baa3b6b83e662327187d68b3f9e21cecefeb91215a8128243fce62ea0"}, "2": {"node_id": "6d4e2671-af99-4cea-89b1-4d3cb83c1b71", "node_type": null, "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}, "hash": "3db00c22f8d96b46ef10e112427218a956cd4cf3f04ba604a2d3d0b0d4e71cee"}}, "hash": "119572f66488246d97df820cc009226802ac1e06e3d272b748b600bc58725323", "text": "mulasoftheleave-one-out\nerror for a number of learning machines (Monari and Dreyfus, 2000, Rivals and Personnaz, 2003,\nRakotomamonjy, 2003).\nLeave-one-out formulas can be viewed as corrected values of the train ing error. Many other\ntypes of penalization of the training error have been proposed in the literatu re (see, e.g., Vapnik,\n1998, Hastie et al., 2001). Recently, a new family of such methods called \u201cmetr ic-based methods\u201d\nhave been proposed (Schuurmans, 1997). The paper of Bengio and Chapados (2003) in this issue\n13. Inthelimit,thetestsetcanhaveonlyoneexampleandleave-out-outc anbecarriedoutasan\u201couterloop\u201d,outsidethe\nfeature/variableselectionprocess,toestimatethe\ufb01nalperformanceof thepredictor. Thiscomputationallyexpensive\nprocedure isusedincases where datais extremely scarce.\n1173", "start_char_idx": 2857, "end_char_idx": 3654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2b32b82d-0bca-485f-a959-b39af5c9ae69": {"__data__": {"id_": "2b32b82d-0bca-485f-a959-b39af5c9ae69", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "347d6ad0-37d8-4fa7-bdd8-66b6a6196086", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "3": {"node_id": "8e8a509b-fe89-4043-b5b3-baeffd1cfac9", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "af6d37ffabf23b16cf006b58ec3a8b37d390263c8d16f66a531fa99f5f071c00"}}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8", "text": "GUYON AND ELISSEEFF\nillustrates their application to variable selection. The authors make use of unlab elled data, which\nare readily available in the application considered, time series prediction with a h orizon. Consider\ntwomodels fAandfBtrainedwithnestedsubsetsofvariables A\u2282B. Wecall d(fA,fB)thediscrep-\nancy of the two models. The criterion involves the ratio dU(fA,fB)/dT(fA,fB), wheredU(fA,fB)is\ncomputed with unlabelled data and dT(fA,fB)is computed with training data. A ratio signi\ufb01cantly\nlarger thanonesheds doubtontheusefulnessof thevariablesinsubse tBthat arenotin A.\nFor variable ranking or nested subset ranking methods (Sections 2 and 4 .2), another statisti-\ncal approach can be taken. The idea is to introduce a probe in the data that is a random variable.\nRoughly speaking, variables that have a relevance smaller or equal to tha t of the probe should be\ndiscarded. Bi et al. (2003) consider a very simple implementation of that idea : they introduce in\ntheirdatathreeadditional\u201cfakevariables\u201ddrawnrandomlyfromaGauss iandistributionandsubmit\nthem to their variable selection process with the other \u201ctrue variables\u201d. Subs equently, they discard\nall the variables that are less relevant than one of the three fake variable s (according to their weight\nmagnitude criterion). Stoppiglia et al. (2003) propose a more sophisticated me thod for the Gram-\nSchmidt forward selection method. For a Gaussian distributed probe, they p rovide an analytical\nformula to compute the rank of the probe associated with a given risk of acce pting an irrelevant\nvariable. A non-parametric variant of the probe method consists in creating \u201cfake variables\u201d by\nrandomly shuf\ufb02ing real variable vectors. In a forward selection proce ss, the introduction of fake\nvariables does not disturb the selection because fake variables can be d iscarded when they are en-\ncountered. Atagivenstepintheforwardselectionprocess,letuscall ftthefractionoftruevariables\nselected so far (among all true variables) and ffthe fraction of fake variables encountered (among\nall fake variables). As a halting criterion one can place a threshold on the r atioff/ft, which is an\nupper bound on the fraction of falsely relevant variables in the subset s elected so far. The latter\nmethodhasbeenusedforvariableranking(Tusheretal.,2001). Itspa rametricversionforGaussian\ndistributionsusingtheT statisticas rankingcriterionisnothingbuttheT-test.\n7 Advanced Topics and Open Problems\n7.1 Varianceof Variable SubsetSelection\nMany methods of variable subset selection are sensitive to small perturbatio ns of the experimental\nconditions. If the data has redundant variables, different subsets of variables with identical predic-\ntive power may be obtained according to initial conditions of the algorithm, remov al or addition of\na few variables or training examples, or addition of noise. For some applicatio ns, one might want\nto purposely generate alternative subsets that can be presented to a sub sequent stage of processing.\nStill one might \ufb01nd this variance undesirable because (i) variance is often th e symptom of a \u201cbad\u201d\nmodel that does not generalize well; (ii) results are not reproducible; an d (iii) one subset fails to\ncapturethe\u201cwhole picture\u201d.\nOnemethodto\u201cstabilize\u201dvariableselectionexploredinthisissueistousesever al\u201cbootstraps\u201d\n(Bi et al., 2003). The variable selection process is repeated with sub-sa mples of the training data.\nThe union of the subsets of variables selected", "start_char_idx": 0, "end_char_idx": 3456, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8e8a509b-fe89-4043-b5b3-baeffd1cfac9": {"__data__": {"id_": "8e8a509b-fe89-4043-b5b3-baeffd1cfac9", "embedding": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "347d6ad0-37d8-4fa7-bdd8-66b6a6196086", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "b4a249e6fda3bd215f621d35bd876b1162c4ba6614dc2829c5507f247e56e5e1"}, "2": {"node_id": "2b32b82d-0bca-485f-a959-b39af5c9ae69", "node_type": null, "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}, "hash": "509e4a1b37777e4d5c69d201e1c452fc0c328abd0604531bb6bce44a263a7be8"}}, "hash": "af6d37ffabf23b16cf006b58ec3a8b37d390263c8d16f66a531fa99f5f071c00", "text": "al or addition of\na few variables or training examples, or addition of noise. For some applicatio ns, one might want\nto purposely generate alternative subsets that can be presented to a sub sequent stage of processing.\nStill one might \ufb01nd this variance undesirable because (i) variance is often th e symptom of a \u201cbad\u201d\nmodel that does not generalize well; (ii) results are not reproducible; an d (iii) one subset fails to\ncapturethe\u201cwhole picture\u201d.\nOnemethodto\u201cstabilize\u201dvariableselectionexploredinthisissueistousesever al\u201cbootstraps\u201d\n(Bi et al., 2003). The variable selection process is repeated with sub-sa mples of the training data.\nThe union of the subsets of variables selected in the various bootstraps is ta ken as the \ufb01nal \u201cstable\u201d\nsubset. This joint subset may be at least as predictive as the best bootstr ap subset. Analyzing the\nbehavior of the variables across the various bootstraps also provides f urther insight, as described\nin the paper. In particular, an index of relevance of individual variable s can be created considering\nhowfrequentlytheyappear inthebootstraps.\n1174", "start_char_idx": 2773, "end_char_idx": 3864, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c322b139-26f8-42ff-a193-2c4f2e7cd4aa": {"__data__": {"id_": "c322b139-26f8-42ff-a193-2c4f2e7cd4aa", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "3": {"node_id": "bc3543fc-9560-447f-a0c7-2e1fa2a23e46", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "cf780e542324252bff936e6d6909e22ac4592bb0e9b966fbf86b4478bdbde1ba"}}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nRelated ideas have been described elsewhere in the context of Bayesian variable selection (Je-\nbara and Jaakkola, 2000, Ng and Jordan, 2001, Vehtari and Lampine n, 2002). A distribution over\na population of models using various variable subsets is estimated. Variables a re then ranked ac-\ncording to the marginal distribution, re\ufb02ecting how often they appear in importa nt subsets (i.e.,\nassociatedwiththemostprobablemodels).\n7.2 Variable Rankingin the Contextof Others\nIn Section 2, we limited ourselves to presenting variable ranking methods using a criterion com-\nputed from single variables, ignoring the context of others. In Section 4.2 , we introduced nested\nsubsetmethodsthatprovideausefulrankingofsubsets,notof indiv idualvariables: somevariables\nmayhavealowrankbecausetheyareredundantandyetbehighlyrelev ant. BootstrapandBayesian\nmethods presented in Section 7.1, may be instrumental in producing a good var iable ranking incor-\nporatingthecontext ofothers.\nThe relief algorithm uses another approach based on the nearest-neighb or algorithm (Kira and\nRendell,1992). Foreachexample,theclosestexampleofthesameclass(n earesthit)andtheclosest\nexampleofadifferentclass(nearestmiss)areselected. Thescore S(i)oftheithvariableiscomputed\nas the average over all examples of magnitude of the difference between th e distance to the nearest\nhitandthe distancetothenearestmiss,inprojectiononthe ithvariable.\n7.3 UnsupervisedVariable Selection\nSometimes, no target yis provided, but one still would want to select a set of most signi\ufb01cant\nvariables with respect to a de\ufb01ned criterion. Obviously, there are as many criteria as problems\ncan be stated. Still, a number of variable ranking criteria are useful acros s applications, including\nsaliency,entropy,smoothness ,densityandreliability . A variable is salient if it has a high variance\nor a large range, compared to others. A variable has a high entropy if the d istribution of examples\nis uniform. In a time series, a variable is smooth if on average its local curvatur e is moderate. A\nvariableisinahigh-densityregionifitishighlycorrelatedwithmanyothervaria bles. Avariableis\nreliable if the measurement error bars computed by repeating measurements a re small compared to\nthevariabilityof thevariablevalues(as quanti\ufb01ed,e.g., byanANOVA statistic) .\nSeveral authors have also attempted to perform variable or feature selec tion for clustering ap-\nplications (see,e.g., XingandKarp,2001, Ben-Hur andGuyon, 2003, andreferences therein).\n7.4 Forward vs.BackwardSelection\nItisoftenarguedthatforwardselectioniscomputationallymoreef\ufb01cienttha nbackwardelimination\nto generate nested subsets of variables. However, the defenders of b ackward elimination argue that\nweaker subsets are found by forward selection because the importance of variables is not assessed\nin the context of other variables not included yet. We illustrate this latter argume nt by the example\nofFigure4. Inthatexample,onevariableseparatesthetwoclassesbetter byitselfthaneitherofthe\ntwootheronestakenaloneandwillthereforebeselected\ufb01rstbyforwar dselection. Atthenextstep,\nwhen it is complemented by either of the two other variables, the resulting class s eparation in two\ndimensions will not be as good as the one obtained jointly by the two variables tha t were discarded\nat the \ufb01rst step. A backward selection method may outsmart forward selectio n by eliminating at\nthe \ufb01rst step the variable that by itself provides the best", "start_char_idx": 0, "end_char_idx": 3494, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc3543fc-9560-447f-a0c7-2e1fa2a23e46": {"__data__": {"id_": "bc3543fc-9560-447f-a0c7-2e1fa2a23e46", "embedding": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "01c52126a5556eeb7880dd432dd38ee58fd0514b73c7a161d4ca806e688772ea"}, "2": {"node_id": "c322b139-26f8-42ff-a193-2c4f2e7cd4aa", "node_type": null, "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}, "hash": "4f7866d2a59bea11881d0690933afae4da9ad36fa2848270b24f742508a88c52"}}, "hash": "cf780e542324252bff936e6d6909e22ac4592bb0e9b966fbf86b4478bdbde1ba", "text": "subsets are found by forward selection because the importance of variables is not assessed\nin the context of other variables not included yet. We illustrate this latter argume nt by the example\nofFigure4. Inthatexample,onevariableseparatesthetwoclassesbetter byitselfthaneitherofthe\ntwootheronestakenaloneandwillthereforebeselected\ufb01rstbyforwar dselection. Atthenextstep,\nwhen it is complemented by either of the two other variables, the resulting class s eparation in two\ndimensions will not be as good as the one obtained jointly by the two variables tha t were discarded\nat the \ufb01rst step. A backward selection method may outsmart forward selectio n by eliminating at\nthe \ufb01rst step the variable that by itself provides the best separation to retain the two variables that\n1175", "start_char_idx": 2766, "end_char_idx": 3543, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bf2da5cf-8744-469a-9063-f614a0612590": {"__data__": {"id_": "bf2da5cf-8744-469a-9063-f614a0612590", "embedding": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "34b7f9c2-6a26-403d-8da7-b1639a6b08d0", "node_type": null, "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab"}}, "hash": "6f38e320c784fc49a6616370b8f7053c350ee07c01b99871c0c7bbf9d0aa81ab", "text": "GUYON AND ELISSEEFF\n\u22125 0 5 \u22125 0 5 \u22125 0 5\u2212505\u2212505\u2212505\nFigure 4: Forward or backward selection? Of the three variables of this example, the third one\nseparatesthetwoclassesbestbyitself(bottomrighthistogram). Itisthere forethebestcandidatein\na forward selection process. Still, the two other variables are better taken to gether than any subset\nof twoincludingit. Abackwardselectionmethod mayperformbetter inthis case.\ntogether perform best. Still, if for some reason we need to get down to a sing le variable, backward\neliminationwillhavegotten ridof thevariablethatworks bestonitsown.\n7.5 TheMulti-class Problem\nSome variable selection methods treat the multi-class case directly rather than de composing it into\nseveral two-class problems: All the methods based on mutual information crite ria extend naturally\nto the multi-class case (see in this issue Bekkerman et al., 2003, Dhillon et al., 20 03, Torkkola,\n2003). Multi-classvariablerankingcriteriaincludeFisher\u2019scriterion(the ratioofthebetweenclass\nvariancetothewithin-classvariance). ItiscloselyrelatedtotheFstatisticuse dintheANOVAtest,\nwhichisonewayofimplementingtheprobemethod(Section6)forthemulti-classca se. Wrappers\nor embedded methods depend upon the capability of the classi\ufb01er used to han dle the multi-class\ncase. Examplesofsuchclassi\ufb01ersincludelineardiscriminantanalysis(LD A),amulti-classversion\nof Fisher\u2019s linear discriminant (Duda et al., 2001), and multi-class SVMs (s ee, e.g., Weston et al.,\n2003).\nOne may wonder whether it is advantageous to use multi-class methods for var iable selection.\nOn one hand, contrary to what is generally admitted for classi\ufb01cation, the multi- class setting is\nin some sense easier for variable selection than the two-class case. This is b ecause the larger the\n1176", "start_char_idx": 0, "end_char_idx": 1771, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d07687e2-37d1-48fe-a725-67e4b0859386": {"__data__": {"id_": "d07687e2-37d1-48fe-a725-67e4b0859386", "embedding": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e9762258-28bb-423f-ad27-7ef0ef76cd7e", "node_type": null, "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9"}}, "hash": "7096dc65cca109bf6168001fa6b7373dfec12e13c81a541e377de70d637b80f9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nnumberofclasses,thelesslikelya\u201crandom\u201dsetoffeaturesprovideago odseparation. Toillustrate\nthis point, consider a simple example where all features are drawn independ ently from the same\ndistribution Pand the \ufb01rst of them is the target y. Assume that all these features correspond to\nrollingadiewith Qfacesntimes(nisthenumberofsamples). Theprobabilitythatone\ufb01xedfeature\n(except the \ufb01rst one) is exactly yis then (1/Q)n. Therefore, \ufb01nding the feature that corresponds to\nthe target ywhen it is embedded in a sea of noisy features is easier when Qis large. On the other\nhand, Forman (2003) points out in this issue that in the case of uneven distr ibutions across classes,\nmulti-classmethodsmayover-representabundantoreasilyseparablecla sses. Apossiblealternative\nis to mix ranked lists of several two-class problems. Weston et al. (2003) pr opose one such mixing\nstrategy.\n7.6 Selectionof Examples\nThedualproblemsoffeatureselection/constructionarethoseofpatterns election/construction. The\nsymmetry of the two problems is made explicit in the paper of Globerson and Tishb y (2003) in\nthis issue. Likewise, both Stoppiglia et al. (2003) and Weston et al. (2003) point out that their\nalgorithm also applies to the selection of examples in kernel methods. Others ha ve already pointed\noutthesimilarityandcomplementarityofthetwoproblems(BlumandLangley,1997 ). Inparticular,\nmislabeledexamplesmayinducethechoiceofwrongvariables. Conversely, ifthelabelingishighly\nreliable,selectingwrongvariablesassociatedwithaconfoundingfactorma ybeavoidedbyfocusing\noninformativepatterns thatareclosetothedecisionboundary(Guyonet al.,2002).\n7.7 InverseProblems\nMostofthespecialissueconcentratesontheproblemof\ufb01ndinga(small)s ubsetofvariablesuseful\ntobuildagoodpredictor. Insomeapplications,particularlyinbioinformatics,th isisnotnecessarily\nthe only goal of variable selection. In diagnosis problems, for instance, it is important to identify\nthe factors that triggered a particular disease or unravel the chain of ev ents from the causes to\nthe symptoms. But reverse engineering the system that produced the data is a more challenging\ntask than building a predictor. The readers interested in these issues can c onsult the literature on\ngene networks in the conference proceedings of the paci\ufb01c symposium o n biocomputing (PSB) or\nintelligent systems for molecular biology conference (ISMB) and the causa lity inference literature\n(see, e.g., Pearl, 2000). At the heart of this problem is the distinction betwe en correlation and\ncausality. Observational data such as the data available to machine learning r esearchers allow us\nonly to observe correlations. For example, observations can be made abou t correlations between\nexpression pro\ufb01les of given genes or between pro\ufb01les and symptoms, b ut a leap of faith is made\nwhendeciding whichgene activatedwhichother one andinturntriggeredth esymptom.\nIn this issue, the paper of Caruana and de Sa (2003) presents interestin g ideas about using\nvariables discarded by variable selection as additional outputs of a neura l network. They show im-\nprovedperformanceonsyntheticandrealdata. Theiranalysissuppor tstheideathatsomevariables\naremoreef\ufb01cientlyusedasoutputsthanasinputs. Thiscouldbeasteptowar ddistinguishingcauses\nfromconsequences.\n1177", "start_char_idx": 0, "end_char_idx": 3315, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fd8d547a-e8c9-4e39-a044-3a9b70d3d588": {"__data__": {"id_": "fd8d547a-e8c9-4e39-a044-3a9b70d3d588", "embedding": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c9aad5a8-0e25-4daf-bdaa-dee8e0d30b75", "node_type": null, "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad"}}, "hash": "3db2b5f483e07cde1c62a8a9a9112c4c9d2f62044e933e768bf9524ff314bfad", "text": "GUYON AND ELISSEEFF\nDataset Description patterns variables classes References\nLineara,bArti\ufb01ciallinear 10-1200 100-240 reg-2 SWBe\nMulti-clustercArti\ufb01cialnon-linear 1000-1300 100-500 2 PS\nQSARdChemistry 30-300 500-700 reg Bt\nUCIeMLrepository 8-60 500-16000 2-30ReBnToPC\nLVQ-PAKfPhoneme data 1900 20 20 T\nRaetch bench.gUCI/Delve/Statlog 200-7000 8-20 2 Ra\nMicroarrayaCancer classif. 6-100 2000-4000 2 WRa\nMicroarrayaGene classi\ufb01cation 200 80 5 W\nAstonUnivhPipeline transport 1000 12 3 T\nNIPS 2000iUnlabeled data 200-400 5-800 reg Ri\n20 Newsgroupj,oNews postings 20000 300-15000 2-20 GBkD\nText\ufb01lteringkTREC/OSHUMED 200-2500 3000-30000 6-17 F\nIRdatasetslMED/CRAN/CISI 1000 5000 30-225 G\nReuters-21578m,onewswiredocs. 21578 300-15000 114 BkF\nOpenDir. Proj.nWebdirectory 5000 14500 50 D\nTable 1:Publicly available data sets used in the special issue. Approximate numbers or ranges\nof patterns, variables, and classes effectively used are provided. T he \u201cclasses\u201d column indicates\n\u201creg\u201d for regression problems, or the number of queries for Informatio n Retrieval (IR) problems.\nFor arti\ufb01cial data sets, the fraction of variables that are relevant range s from 2 to 10. The initial of\nthe \ufb01rst author are provided as reference: Bk=Bekkerman, Bn=Ben gio, Bt=Bennett, C=Caruana,\nD=Dhillon, F=Forman, G=Globerson, P=Perkins, Re=Reunanen, Ra=R akotomamonjy, Ri=Rivals,\nS=Stoppiglia, T=Torkkola, W=Weston. Please also check the JMLR web site for later additions\nandpreprocesseddata.\na.http://www.kyb.tuebingen.mpg.de/bs/people/weston/l0 ( /lscript0not10)\nb.http://www.clopinet.com/isabelle/Projects/NIPS2001/Arti\ufb01cial.zip\nc.http://nis-www.lanl.gov/ \u223csimes/data/jmlr02/\nd.http://www.rpi.edu/ \u223cbij2/featsele.html\ne.http://www.ics.uci.edu/ \u223cmlearn/MLRepository.html\nf.http://www.cis.hut.\ufb01/research/software.shtml\ng.http://ida.\ufb01rst.gmd.de/ \u223craetsch/data/benchmarks.htm\nh.http://www.nerg.aston.ac.uk/GTM/3PhaseData.html\ni.http://q.cis.uoguelph.ca/skremer/NIPS2000/\nj.http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.html\nk.http://trec.nist.gov/data.html(FilteringTrackCollection)\nl.http://www.cs.utk.edu/ \u223clsi/\nm.http://www.daviddlewis.com/resources/testcollections/reuters21578/\nn.http://dmoz.org/andhttp://www.cs.utexas.edu/users/manyam/dmoz.txt\no.http://www.cs.technion.ac.il/ \u223cronb/thesis.html\n1178", "start_char_idx": 0, "end_char_idx": 2295, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c86c275c-6617-4d6b-8698-c6218241d5dc": {"__data__": {"id_": "c86c275c-6617-4d6b-8698-c6218241d5dc", "embedding": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b92ce087-ec9b-4087-a88b-a778ba836acb", "node_type": null, "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9"}}, "hash": "5f94b6dddd071504ecc78fe0dc670bf309cf1cf27b638b9112a72b72da57f0c9", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\n8 Conclusion\nThe recent developments in variable and feature selection have address ed the problem from the\npragmatic point of view of improving the performance of predictors. They h ave met the challenge\nof operating on input spaces of several thousand variables. Sophistic ated wrapper or embedded\nmethods improve predictor performance compared to simpler variable ranking methods like corre-\nlation methods, but the improvements are not always signi\ufb01cant: domains with lar ge numbers of\ninput variables suffer from the curse of dimensionality and multivariate metho ds may over\ufb01t the\ndata. For some domains, applying \ufb01rst a method of automatic feature construc tion yields improved\nperformance and a more compact set of features. The methods propose d in this special issue have\nbeentestedonawidevarietyofdatasets(seeTable1),whichlimitsthepossibility ofmakingcom-\nparisonsacrosspapers. Furtherworkincludestheorganizationofab enchmark. Theapproachesare\nvery diverse and motivated by various theoretical arguments, but a unif ying theoretical framework\nislacking. Becauseoftheseshortcomings,itisimportantwhenstartingwithan ewproblemtohave\na few baseline performance values. To that end, we recommend using a line ar predictor of your\nchoice (e.g. a linear SVM) and select variables in two alternate ways: (1) w ith a variable ranking\nmethod using a correlation coef\ufb01cient or mutual information; (2) with a nested subset selection\nmethod performing forward or backward selection or with multiplicative update s. Further down\nthe road, connections need to be made between the problems of variable and feature selection and\nthoseofexperimentaldesignandactivelearning,inanefforttomoveawa yfromobservationaldata\ntowardexperimentaldata,and toaddressproblemsof causalityinference .\nReferences\nE. Amaldi and V. Kann. On the approximation of minimizing non zero variables or unsatis\ufb01ed\nrelations inlinear systems. TheoreticalComputer Science , 209:237\u2013260, 1998.\nR. Bekkerman, R. El-Yaniv, N. Tishby, and Y. Winter. Distributional wor d clusters vs. words for\ntextcategorization. JMLR,3:1183\u20131208 (thisissue),2003.\nA. Ben-Hur and I. Guyon. Detecting stable clusters using principal compo nent analysis. In M.J.\nBrownsteinandA.Kohodursky,editors, MethodsInMolecularBiology ,pages159\u2013182.Humana\nPress,2003.\nY.BengioandN.Chapados. Extensionstometric-basedmodelselection. JMLR,3:1209\u20131227(this\nissue),2003.\nJ. Bi, K. Bennett, M. Embrechts, C. Breneman, and M. Song. Dimensionality r eduction via sparse\nsupportvector machines. JMLR,3:1229\u20131243 (thisissue),2003.\nA.BlumandP.Langley. Selectionofrelevantfeaturesandexamplesinmac hinelearning. Arti\ufb01cial\nIntelligence ,97(1-2):245\u2013271,December 1997.\nB. Boser, I. Guyon, and V. Vapnik. A training algorithm for optimal margin c lassi\ufb01ers. In Fifth\nAnnualWorkshoponComputational LearningTheory ,pages 144\u2013152, Pittsburgh,1992.ACM.\nL. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classi\ufb01cation and Regression Trees .\nWadsworthandBrooks,1984.\n1179", "start_char_idx": 0, "end_char_idx": 3035, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "71f4dc22-f1bb-4723-9362-bd012126c23c": {"__data__": {"id_": "71f4dc22-f1bb-4723-9362-bd012126c23c", "embedding": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ee389505-386e-4390-9969-42d853b0d752", "node_type": null, "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7"}}, "hash": "919743e9eed94d9bef6b94d2539b5dc308bb8e8646c68f2d3a666c7cd5491cc7", "text": "GUYON AND ELISSEEFF\nR. Caruana and V. de Sa. Bene\ufb01tting from the variables that variable sele ction discards. JMLR, 3:\n1245\u20131264 (thisissue),2003.\nI. Dhillon, S. Mallela, and R. Kumar. A divisive information-theoretic featur e clustering algorithm\nfortextclassi\ufb01cation. JMLR,3:1265\u20131287(this issue),2003.\nT. G. Dietterich. Approximate statistical test for comparing supervised class i\ufb01cation learning algo-\nrithms.NeuralComputation , 10(7):1895\u20131924,1998.\nR.O.Duda,P.E.Hart,andD.G.Stork. PatternClassi\ufb01cation . JohnWiley&amp;Sons,USA,2nd\nedition,2001.\nT. R. Golub et al. Molecular classi\ufb01cation of cancer: Class discovery an d class prediction by gene\nexpressionmonitoring. Science, 286:531\u2013537, 1999.\nG.Forman. Anextensiveempiricalstudyoffeatureselectionmetricsfor tex tclassi\ufb01cation. JMLR,\n3:1289\u20131306 (thisissue),2003.\nT. Furey, N. Cristianini, Duffy, Bednarski N., Schummer D., M., and D. Ha ussler. Support vector\nmachine classi\ufb01cation and validation of cancer tissue samples using microarra y expression data.\nBioinformatics ,16:906\u2013914, 2000.\nA.GlobersonandN.Tishby. Suf\ufb01cientdimensionalityreduction. JMLR,3:1307\u20131331(thisissue),\n2003.\nY.GrandvaletandS.Canu. Adaptive scalingfor featureselectioninSV Ms. InNIPS15,2002.\nI. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene selection for can cer classi\ufb01cation using\nsupportvector machines. MachineLearning ,46(1-3):389\u2013422,2002.\nT. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning . Springer series in\nstatistics.Springer,NewYork,2001.\nT. Jebara and T. Jaakkola. Feature selection and dualities in maximum entrop y discrimination. In\n16thAnnualConference onUncertainty inArti\ufb01cialIntelligence ,2000.\nK. Kira and L. Rendell. A practical approach to feature selection. In D. S leeman and P. Edwards,\neditors,International Conference on Machine Learning , pages 368\u2013377, Aberdeen, July 1992.\nMorganKaufmann.\nR. Kohavi and G. John. Wrappers for feature selection. Arti\ufb01cial Intelligence , 97(1-2):273\u2013324,\nDecember 1997.\nD. Koller and M. Sahami. Toward optimal feature selection. In 13th International Conference on\nMachineLearning ,pages 284\u2013292,July1996.\nY. LeCun, J. Denker, S. Solla, R. E. Howard, and L. D. Jackel. Optimal brain damage. In D. S.\nTouretzky, editor, Advances in Neural Information Processing Systems II , San Mateo, CA, 1990.\nMorganKaufmann.\n1180", "start_char_idx": 0, "end_char_idx": 2346, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f585cddf-1482-487f-8059-e55daf73b770": {"__data__": {"id_": "f585cddf-1482-487f-8059-e55daf73b770", "embedding": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caa39d26-8851-4d00-8804-9fc5618760bd", "node_type": null, "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd"}}, "hash": "e96b681c47561501b2e984a87e604f9228ace62a9ef7b055ba93fbc7281958bd", "text": "ANINTRODUCTION TO VARIABLE AND FEATURESELECTION\nG. Monari and G. Dreyfus. Withdrawing an example from the training set: a n analytic estimation\nof itseffectonanonlinear parameterized model. Neurocomputing Letters ,35:195\u2013201,2000.\nC. Nadeau and Y. Bengio. Inference for the generalization error. Machine Learning (to appear) ,\n2001.\nA. Y. Ng. On feature selection: learning with exponentially many irrelevant f eatures as train-\ning examples. In 15th International Conference on Machine Learning , pages 404\u2013412. Morgan\nKaufmann, SanFrancisco,CA, 1998.\nA. Y. Ng and M. Jordan. Convergence rates of the voting Gibbs classi\ufb01e r, with application to\nBayesian featureselection. In 18thInternationalConference on MachineLearning ,2001.\nJ.Pearl.Causality . Cambridge UniversityPress,2000.\nF. Pereira, N. Tishby, and L. Lee. Distributional clustering of English wo rds. InProc. Meeting of\ntheAssociationfor Computational Linguistics ,pages 183\u2013190,1993.\nS. Perkins, K. Lacker, and J. Theiler. Grafting: Fast incremental fea ture selection by gradient\ndescentinfunctionspace. JMLR,3:1333\u20131356 (thisissue),2003.\nA.Rakotomamonjy. VariableselectionusingSVM-basedcriteria. JMLR,3:1357\u20131370(thisissue),\n2003.\nJ. Reunanen. Over\ufb01tting in making comparisons between variable selection me thods.JMLR, 3:\n1371\u20131382 (thisissue),2003.\nI. Rivals and L. Personnaz. MLPs (mono-layer polynomials and multi-layer perceptrons) for non-\nlinear modeling. JMLR,3:1383\u20131398 (thisissue),2003.\nB. Schoelkopf andA.Smola. Learningwith Kernels . MITPress,Cambridge MA,2002.\nD. Schuurmans. A new metric-based approach to model selection. In 9th Innovative Applications\nofArti\ufb01cialIntelligence Conference , pages 552\u2013558,1997.\nH. Stoppiglia, G. Dreyfus, R. Dubois, and Y. Oussar. Ranking a rando m feature for variable and\nfeatureselection. JMLR,3:1399\u20131414(this issue),2003.\nR. Tibshirani. Regression selection and shrinkage via the lasso. Technic al report, Stanford Univer-\nsity,PaloAlto,CA, June1994.\nN. Tishby, F. C. Pereira, and W. Bialek. The information bottleneck method. InProc. of the 37th\nAnnualAllertonConference onCommunication, ControlandComputing , pages 368\u2013377, 1999.\nK. Torkkola. Feature extraction by non-parametric mutual information maximiza tion.JMLR, 3:\n1415\u20131438 (thisissue),2003.\nV.G.Tusher,R.Tibshirani,andG.Chu. Signi\ufb01canceanalysisofmicroa rraysappliedtotheionizing\nradiationresponse. PNAS,98:5116\u20135121,April2001.\nV. Vapnik. Estimation of dependencies based on empirical data . Springer series in statistics.\nSpringer,1982.\n1181", "start_char_idx": 0, "end_char_idx": 2519, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d2c56a30-c946-48df-b908-c921747c9548": {"__data__": {"id_": "d2c56a30-c946-48df-b908-c921747c9548", "embedding": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a09d8f31-e978-4e2b-b1c9-fa78949ffc4e", "node_type": null, "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e"}}, "hash": "82d16216ba6d123146aa7e58c07f1bb54da6d4bba079358301190c96e91ca87e", "text": "GUYON AND ELISSEEFF\nV.Vapnik. StatisticalLearningTheory . JohnWiley&amp; Sons,N.Y.,1998.\nA. Vehtari and J. Lampinen. Bayesian input variable selection using poste rior probabilities and\nexpected utilities. ReportB31, 2002.\nJ.Weston,A.Elisseff,B.Schoelkopf,andM.Tipping. Useofthezero normwithlinearmodelsand\nkernelmethods. JMLR,3:1439\u20131461(thisissue),2003.\nJ. Weston, S. Mukherjee, O. Chapelle, M. Pontil, T. Poggio, and V. Vapn ik. Feature selection for\nSVMs. In NIPS13,2000.\nE.P. Xing and R.M. Karp. Cliff: Clustering of high-dimensional microarray d ata via iterative fea-\nture \ufb01ltering using normalized cuts. In 9th International Conference on Intelligence Systems for\nMolecular Biology ,2001.\n1182", "start_char_idx": 0, "end_char_idx": 701, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"81745cd0-1bf7-4fbd-88ac-74668c9bdd52": {"node_ids": ["c23fcaa9-1e1d-4364-ae60-b074cd1472e8", "ca29c6d2-8165-4fb8-b5c7-19a0be4485c5"], "metadata": {"page_label": "1", "file_name": "Clustering.pdf"}}, "2456850a-65d9-4aae-8eac-2037e676405f": {"node_ids": ["598882f9-dd8d-40c2-a43f-19a3a95789ec", "7d97b121-9283-4ff0-84a0-1eff38df493f"], "metadata": {"page_label": "2", "file_name": "Clustering.pdf"}}, "eb2959f7-1309-454e-a4c7-f48d9e283445": {"node_ids": ["193efdee-affc-47dd-b957-1495148e0469", "09fc3f4f-ac00-464e-bf5e-3b0068422cac", "c9983646-8fb8-497c-b167-66e800fdcc82"], "metadata": {"page_label": "3", "file_name": "Clustering.pdf"}}, "9b87ab90-37e6-4a3b-98dc-578421c56631": {"node_ids": ["a2a4a5ea-d13c-4753-ad8c-0b61cb72d0da"], "metadata": {"page_label": "4", "file_name": "Clustering.pdf"}}, "3a14d9d8-8dae-46ed-aa9f-203ef9a483a1": {"node_ids": ["9f7e9f0f-60c4-4f15-9742-d5b1a9582f46"], "metadata": {"page_label": "5", "file_name": "Clustering.pdf"}}, "6d2c02cc-542c-490a-8658-7f9e0b2cc3c4": {"node_ids": ["c8a2a2b4-e978-40c0-a1e6-2fdbc010f120", "a64fab5e-0406-46f8-9055-d14711882c66"], "metadata": {"page_label": "6", "file_name": "Clustering.pdf"}}, "19f775fe-746a-420a-a946-5981879a2641": {"node_ids": ["114811d9-85e3-4cf6-9789-9d2dcc7106e4", "d58e52a2-9c28-4857-895c-8364eef31bc0"], "metadata": {"page_label": "7", "file_name": "Clustering.pdf"}}, "c255f5fc-f07d-4daa-8f84-0d36de6c2941": {"node_ids": ["2fadee76-3fc6-489e-affe-96aa5ea758d7", "76da649e-54b2-4621-b782-5a3bb9f35beb"], "metadata": {"page_label": "8", "file_name": "Clustering.pdf"}}, "a5617f37-08ac-4921-bb85-c5619d31fe0a": {"node_ids": ["32fc8f4e-0ebd-447d-ab6d-d63bfc7f831e", "4332bf06-23a4-4f9e-90cc-f37645ddd5fb"], "metadata": {"page_label": "9", "file_name": "Clustering.pdf"}}, "2e951868-2342-4a1d-9c18-bad4bc6a98cd": {"node_ids": ["de67346e-2716-4865-a7c3-dca015ef3509", "f45ea891-8def-4990-b182-1f239497975d"], "metadata": {"page_label": "10", "file_name": "Clustering.pdf"}}, "b5bd3b13-52c3-47c1-8340-782660816dd3": {"node_ids": ["dcaca609-a339-413f-9417-4112d5e26099", "1370603c-d302-496a-ab5d-4d5d7bd0e02d"], "metadata": {"page_label": "11", "file_name": "Clustering.pdf"}}, "ec3a32df-42a7-4f8d-8320-9e16855fdedc": {"node_ids": ["1391ba94-bcad-4c9a-ad5d-bdbaa25d36b0", "9a12ce67-810f-4816-b5a6-24891d994b91"], "metadata": {"page_label": "12", "file_name": "Clustering.pdf"}}, "9df74eb1-5688-4979-a8aa-865fca7c833e": {"node_ids": ["5dbffff8-d037-4461-a8b5-a910192784bd", "d70b24b2-9928-432a-8f0d-9a1a91c7092d"], "metadata": {"page_label": "13", "file_name": "Clustering.pdf"}}, "92189415-813c-448f-9325-5d744b42daa2": {"node_ids": ["4e6e6472-5c48-4a8e-bfcd-3390f6e0e046", "040cae1b-d191-486d-9455-5d4bf878b3ea"], "metadata": {"page_label": "14", "file_name": "Clustering.pdf"}}, "8da82a75-5028-45a7-b216-e3136b94e7c7": {"node_ids": ["13798897-265c-4df0-b0cb-58b61d12451d", "b7454e40-c40e-4760-8320-87393fba0652"], "metadata": {"page_label": "15", "file_name": "Clustering.pdf"}}, "3ab8bb7c-d46e-41f4-b931-7c6eba3318d2": {"node_ids": ["254d23e2-4b68-41fe-a42e-8447042e7214", "b39d6656-38ae-4098-8d6c-1de0263626cf"], "metadata": {"page_label": "16", "file_name": "Clustering.pdf"}}, "c7f9d697-11df-4960-bb3d-59ed77b3e0dd": {"node_ids": ["7487076c-1967-4678-a14f-c86cf13dcead", "51777b76-56dd-4fd4-8d28-4dc30864db57"], "metadata": {"page_label": "17", "file_name": "Clustering.pdf"}}, "83dd834e-1c6c-4347-a22f-565adc1fae11": {"node_ids": ["7e464763-6c28-40ef-9d6c-ab5e84d8b075", "85c176b1-8119-4831-b3fe-6a680fe02035"], "metadata": {"page_label": "18", "file_name": "Clustering.pdf"}}, "50f89fe7-04a7-406e-b5db-f905d82e29d4": {"node_ids": ["fdac7e1e-859a-4913-8521-caa2a30b2d6b", "b71157bb-690f-44d9-a23e-dbd99c686820"], "metadata": {"page_label": "19", "file_name": "Clustering.pdf"}}, "9355e891-1acc-419d-8d45-c051be2b9689": {"node_ids": ["e2e37e5f-1ba6-455c-9a86-7c060a21c5da", "1ff8f34c-9eec-4f03-a610-841dc58c355e", "0b699348-c595-48ba-8e34-08a194068eb5"], "metadata": {"page_label": "20", "file_name": "Clustering.pdf"}}, "20ddc9a4-6f49-4d04-9bdf-c9c1e260ac52": {"node_ids": ["f4745372-c56a-435b-88ac-3b4d570e0370", "6e7fcb45-c5c5-4cb9-85a6-b5d91947530e"], "metadata": {"page_label": "21", "file_name": "Clustering.pdf"}}, "d61f6244-0721-4e88-bb6f-1621e60bd67d": {"node_ids": ["483d22db-5260-43f2-af63-1e2bfee184d1", "3116bcda-b1ca-41c2-a33a-4b34f9154094"], "metadata": {"page_label": "22", "file_name": "Clustering.pdf"}}, "c4260db6-594a-44d3-85dc-1f383c2273c2": {"node_ids": ["db902509-1cbb-4601-b036-b3b94682f814"], "metadata": {"page_label": "23", "file_name": "Clustering.pdf"}}, "57d562c2-8e0e-4b66-b8b4-cc63e15ac5fe": {"node_ids": ["dff121da-ad44-45e4-937c-2bbcb34a28a1", "d2e6aea4-c820-4237-8dbf-0f4d6addc68d"], "metadata": {"page_label": "24", "file_name": "Clustering.pdf"}}, "e6d26f89-b133-4052-b227-ae3409989906": {"node_ids": ["5bc3ddbc-348b-4558-8f6e-e0a6e567aa4a", "f083b3e1-dbd6-488d-86c9-e36f2a442076", "de81bd72-e92b-4660-9211-c8ee9f220638"], "metadata": {"page_label": "25", "file_name": "Clustering.pdf"}}, "21fd1fe9-c79c-4c93-a146-2c1dc4fb0f04": {"node_ids": ["60d5ebe6-9667-46c1-889e-c8d87f6cee52"], "metadata": {"page_label": "26", "file_name": "Clustering.pdf"}}, "0e4c2413-6aad-42c9-9b2f-cb493f42a576": {"node_ids": ["1cf809ca-64fa-40f0-9765-2d7bed70a722", "b546b495-5330-414f-a767-832cf1360cd9", "d935f193-6229-4fda-903a-0edf140774bb"], "metadata": {"page_label": "27", "file_name": "Clustering.pdf"}}, "edf30fb3-d6a7-4c79-b5ca-7c90196ee5d7": {"node_ids": ["2e081ef4-24e3-4c4d-94b6-28decf74f920", "38da4b93-3a72-4c12-affe-d9c5bfc8f8c9"], "metadata": {"page_label": "28", "file_name": "Clustering.pdf"}}, "08268d8c-aed2-42b4-9253-9fb1269c87cf": {"node_ids": ["84ca4505-a90d-4dd9-ae66-6b8908ca4715", "b6441978-722d-4618-a32f-3d44db20d1a6", "1c0957ca-5dec-4d24-8864-2416c3c32dfa", "139af044-4eb0-46df-9cf7-57eac878b175", "638d296f-3f65-4cf3-861c-0e4f1ee1cf2a"], "metadata": {"page_label": "29", "file_name": "Clustering.pdf"}}, "ac2d3b48-246a-4cbf-8e10-4ef11440b6d5": {"node_ids": ["08395673-3918-4a6a-be76-55677d1b7b72", "46de05ab-c6fe-46ab-bbe5-0b28c0333771", "62b80d0a-b9f9-40a5-8cba-6b6f0af10540", "5f82c25e-e80d-4f46-92a8-17282ca85b55", "c066405b-531f-4635-9414-e6276de78b29"], "metadata": {"page_label": "30", "file_name": "Clustering.pdf"}}, "d4cde2b0-39de-4bf5-8ae7-90e4641d506e": {"node_ids": ["f46c1b14-611f-4233-8b26-de370fecc8ff", "ad6b7b0e-5058-4e1a-8844-4783867f0856", "16be1f30-ceac-4b5e-9d9d-16e98e075545", "7c87c14d-9f34-40d7-8f9c-27aa05bc1f58", "7425ce7f-c899-4926-b6d6-adfb50948bba"], "metadata": {"page_label": "31", "file_name": "Clustering.pdf"}}, "77a4db53-f42f-4356-838a-929b3102cd92": {"node_ids": ["692ad769-f565-4860-a8fc-f16d2970f1c9", "a0c81609-7666-4f30-b0c9-bed065dfe2e7", "c0bc0df7-c298-46ea-a3c7-fe4ab3f94793", "d65554cf-768d-469d-9658-4308f214680b", "9e63e483-f011-4e84-84b0-51256bf47d20"], "metadata": {"page_label": "32", "file_name": "Clustering.pdf"}}, "86e38ea7-a328-4b53-b1a6-21a614131f54": {"node_ids": ["c312ba61-6eb2-4b47-a32e-baed6f6c350e", "481a801a-d8ed-4dfc-a30a-4afb6dbf2eac", "e21b1521-8bd2-456d-b23f-d6f5c6c72bd6", "a234f387-581a-47f1-83af-8a98dfd38cec", "2b510519-2c5e-488b-aaa4-fbf7a9789991"], "metadata": {"page_label": "33", "file_name": "Clustering.pdf"}}, "d15ab249-9163-4186-87e4-823febc0e4db": {"node_ids": ["c6ba7ed5-2ab8-4068-9576-3c6baa96aed9", "ef521592-a2e7-4340-ab49-87747949fafe"], "metadata": {"page_label": "34", "file_name": "Clustering.pdf"}}, "d904018d-af49-48f7-854a-71954d7293ed": {"node_ids": ["81421553-3b86-42b5-bf2e-1b7138f2b853"], "metadata": {"page_label": "1", "file_name": "Feature Selection.pdf"}}, "a92d053d-c7b3-46e4-8786-3c779f2b1ff2": {"node_ids": ["b5560d09-046f-4c51-8380-90520ee9abd1", "115e2887-4896-4804-b552-b4e4f031bc94"], "metadata": {"page_label": "2", "file_name": "Feature Selection.pdf"}}, "1828614e-2082-4396-b396-0068e7b6ff76": {"node_ids": ["4061679f-82cd-42d7-a83e-c2188dd0546b"], "metadata": {"page_label": "3", "file_name": "Feature Selection.pdf"}}, "5bf9a414-1d20-4b5a-b249-686a7a543d47": {"node_ids": ["4ce061cb-d032-4bcf-91d2-48f358f2db7c"], "metadata": {"page_label": "4", "file_name": "Feature Selection.pdf"}}, "5daa4e26-d5fe-4950-83a8-3e22305a1a94": {"node_ids": ["deb06ccc-8dbf-4b10-8d2a-50a3dfa2bf79", "dbe138dc-27d1-40cd-82ae-a26f6325c2ba"], "metadata": {"page_label": "5", "file_name": "Feature Selection.pdf"}}, "d0a47d64-b47a-44fa-b78b-a4de739208ea": {"node_ids": ["0c2aed0c-19a2-49ac-b20f-edab59243b01"], "metadata": {"page_label": "6", "file_name": "Feature Selection.pdf"}}, "5cb5b8f4-921e-43ac-814b-d705dd939634": {"node_ids": ["cc74ac2b-f0c4-4dbd-979e-e1d4bde5829f"], "metadata": {"page_label": "7", "file_name": "Feature Selection.pdf"}}, "a21d0bb2-5c95-4512-9771-2015b84ef71b": {"node_ids": ["2297bd97-eaa8-472f-b652-ca88608aaf1f"], "metadata": {"page_label": "8", "file_name": "Feature Selection.pdf"}}, "fae20a98-def8-487f-a902-69e703dd131b": {"node_ids": ["26f1f084-44a3-4b39-a03f-0dfebd9afb57"], "metadata": {"page_label": "9", "file_name": "Feature Selection.pdf"}}, "3836b47f-7bbc-4927-ada9-7dfca6564469": {"node_ids": ["3aa3e455-37c8-436b-9a39-a86c924314f9"], "metadata": {"page_label": "10", "file_name": "Feature Selection.pdf"}}, "0301e2f3-4c02-48e1-ba86-844d0d5c458b": {"node_ids": ["72810aca-171e-45e0-86d5-d5d6a5cc8224", "1889aed4-8ab1-413d-ba27-60b9f3577d0c"], "metadata": {"page_label": "11", "file_name": "Feature Selection.pdf"}}, "a86d6031-7944-48c7-8929-e81402eda8cb": {"node_ids": ["de12318d-d007-4b53-9030-1ac11fb79da4", "9d54f5cb-4543-41b5-99b5-2891300b7452"], "metadata": {"page_label": "12", "file_name": "Feature Selection.pdf"}}, "8e9104b1-6e3e-4aaf-84f6-aae1c8ac014a": {"node_ids": ["966b471a-3089-490c-8a0f-abb31d721c0e", "704e88cd-f1bc-4a76-ad6e-d5d1f352ba36"], "metadata": {"page_label": "13", "file_name": "Feature Selection.pdf"}}, "410c2ff4-68ff-43c0-827a-41580d9e4802": {"node_ids": ["82c0ed52-9311-4ebc-9757-ee0a5366c107", "2c22439d-abc5-4f02-86d1-674b55702e41"], "metadata": {"page_label": "14", "file_name": "Feature Selection.pdf"}}, "6a4c592e-0151-4012-8f25-fa4da077b701": {"node_ids": ["24271fce-0aef-4b36-bd9d-5e329ae162bf", "c9e82794-4c0d-4fae-8a62-6b75440d6850"], "metadata": {"page_label": "15", "file_name": "Feature Selection.pdf"}}, "3086518d-aacf-4f68-90bc-9c1990559fbc": {"node_ids": ["7298e8d9-a332-440b-aa04-b94bd357478d"], "metadata": {"page_label": "16", "file_name": "Feature Selection.pdf"}}, "a11a05f9-b233-4c1e-8b44-b6f8de747991": {"node_ids": ["6d4e2671-af99-4cea-89b1-4d3cb83c1b71", "a7809498-b4a3-48cb-a20c-63c3a0cdb239"], "metadata": {"page_label": "17", "file_name": "Feature Selection.pdf"}}, "347d6ad0-37d8-4fa7-bdd8-66b6a6196086": {"node_ids": ["2b32b82d-0bca-485f-a959-b39af5c9ae69", "8e8a509b-fe89-4043-b5b3-baeffd1cfac9"], "metadata": {"page_label": "18", "file_name": "Feature Selection.pdf"}}, "13bd34e2-5d37-4ce5-a3e8-8f4bbefd149c": {"node_ids": ["c322b139-26f8-42ff-a193-2c4f2e7cd4aa", "bc3543fc-9560-447f-a0c7-2e1fa2a23e46"], "metadata": {"page_label": "19", "file_name": "Feature Selection.pdf"}}, "34b7f9c2-6a26-403d-8da7-b1639a6b08d0": {"node_ids": ["bf2da5cf-8744-469a-9063-f614a0612590"], "metadata": {"page_label": "20", "file_name": "Feature Selection.pdf"}}, "e9762258-28bb-423f-ad27-7ef0ef76cd7e": {"node_ids": ["d07687e2-37d1-48fe-a725-67e4b0859386"], "metadata": {"page_label": "21", "file_name": "Feature Selection.pdf"}}, "c9aad5a8-0e25-4daf-bdaa-dee8e0d30b75": {"node_ids": ["fd8d547a-e8c9-4e39-a044-3a9b70d3d588"], "metadata": {"page_label": "22", "file_name": "Feature Selection.pdf"}}, "b92ce087-ec9b-4087-a88b-a778ba836acb": {"node_ids": ["c86c275c-6617-4d6b-8698-c6218241d5dc"], "metadata": {"page_label": "23", "file_name": "Feature Selection.pdf"}}, "ee389505-386e-4390-9969-42d853b0d752": {"node_ids": ["71f4dc22-f1bb-4723-9362-bd012126c23c"], "metadata": {"page_label": "24", "file_name": "Feature Selection.pdf"}}, "caa39d26-8851-4d00-8804-9fc5618760bd": {"node_ids": ["f585cddf-1482-487f-8059-e55daf73b770"], "metadata": {"page_label": "25", "file_name": "Feature Selection.pdf"}}, "a09d8f31-e978-4e2b-b1c9-fa78949ffc4e": {"node_ids": ["d2c56a30-c946-48df-b908-c921747c9548"], "metadata": {"page_label": "26", "file_name": "Feature Selection.pdf"}}}}